[
 {
  "id": "02521fd9721c264ee05315dec9b31d_0",
  "x": "To study impact of stacking multiple feature learning modules trained using different self-supervised loss functions, we test the discrete and continuous BERT pre-training approaches on spectral features and on learned acoustic representations, showing synergitic behaviour between acoustically motivated and masked language model loss functions. In low-resource conditions using only 10 hours of labeled data, we achieve Word Error Rates (WER) of 10.2% and 23.5% on the standard test \"clean\" and \"other\" benchmarks of the Librispeech dataset, which is almost on bar with previously published work that uses 10 times more labeled data. Moreover, compared to previous work that uses two models in tandem <cite>(Baevski et al., 2019b)</cite> , by using one model for both BERT pre-trainining and fine-tuning, our model provides an average relative WER reduction of 9%. 1 ---------------------------------- **INTRODUCTION** Representation learning has been an active research area for more than 30 years (Hinton et al., 1986) , with the goal of learning high level representations which separates different explanatory factors of the phenomena represented by the input data (LeCun et al., 2015; Bengio et al., 2013) . Disentangled representations provide models with exponentially higher ability to generalize, using little amount of labels, to new conditions by combining multiple sources of variations. 1 We will open source the code for our models.",
  "y": "differences"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_1",
  "x": "Disentangled representations provide models with exponentially higher ability to generalize, using little amount of labels, to new conditions by combining multiple sources of variations. 1 We will open source the code for our models. Building Automatic Speech Recognition (ASR) systems, for example, requires a large volume of training data to represent different factors contributing to the creation of speech signals, e.g. background noise, recording channel, speaker identity, accent, emotional state, topic under discussion, and the language used in communication. The practical need for building ASR systems for new conditions with limited resources spurred a lot of work focused on unsupervised speech recognition and representation learning (Park and Glass, 2008; Glass; et. al., a,f; van den Oord et al., 2018; , in addition to semiand weakly-supervised learning techniques aiming at reducing the supervised data needed in realworld scenarios (Vesely et al.; Li et al., b; Krishnan Parthasarathi and Strom; Chrupa\u0142a et al.; Kamper et al., 2017) . Recently impressive results have been reported for representation learning, that generalizes to different downstream tasks, through self-supervised learning for text and speech (Devlin et al., 2018; Baevski et al., 2019a; van den Oord et al., 2018;<cite> Baevski et al., 2019b)</cite> . Self-supervised representation learning is done through tasks to predict masked parts of the input, reconstruct inputs through low bit-rate channels, or contrast similar data points against different ones. Different from <cite>(Baevski et al., 2019b)</cite> where the a BERT-like model is trained with the masked language model loss, frozen, and then used as a feature extractor in tandem with a final fully supervised convolutional ASR model (Collobert et al., 2016) , in this work, our \"Discrete BERT\" approach achieves an average relative Word Error Rate (WER) reduction of 9% by pre-training and fine-tuning the same BERT model using a Connectionist Temporal Classification (Graves et al.) loss. In addition, we present a new approach for pre-training bi-directional transformer models on continuous speech data using the InfoNCE loss (van den Oord et al., 2018) -dubbed \"continuous BERT\".",
  "y": "background"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_2",
  "x": "Building Automatic Speech Recognition (ASR) systems, for example, requires a large volume of training data to represent different factors contributing to the creation of speech signals, e.g. background noise, recording channel, speaker identity, accent, emotional state, topic under discussion, and the language used in communication. The practical need for building ASR systems for new conditions with limited resources spurred a lot of work focused on unsupervised speech recognition and representation learning (Park and Glass, 2008; Glass; et. al., a,f; van den Oord et al., 2018; , in addition to semiand weakly-supervised learning techniques aiming at reducing the supervised data needed in realworld scenarios (Vesely et al.; Li et al., b; Krishnan Parthasarathi and Strom; Chrupa\u0142a et al.; Kamper et al., 2017) . Recently impressive results have been reported for representation learning, that generalizes to different downstream tasks, through self-supervised learning for text and speech (Devlin et al., 2018; Baevski et al., 2019a; van den Oord et al., 2018;<cite> Baevski et al., 2019b)</cite> . Self-supervised representation learning is done through tasks to predict masked parts of the input, reconstruct inputs through low bit-rate channels, or contrast similar data points against different ones. Different from <cite>(Baevski et al., 2019b)</cite> where the a BERT-like model is trained with the masked language model loss, frozen, and then used as a feature extractor in tandem with a final fully supervised convolutional ASR model (Collobert et al., 2016) , in this work, our \"Discrete BERT\" approach achieves an average relative Word Error Rate (WER) reduction of 9% by pre-training and fine-tuning the same BERT model using a Connectionist Temporal Classification (Graves et al.) loss. In addition, we present a new approach for pre-training bi-directional transformer models on continuous speech data using the InfoNCE loss (van den Oord et al., 2018) -dubbed \"continuous BERT\". To understand the nature of their learned representations, we train models using the continuous and the discrete BERT approaches on spectral features, e.g. Mel-frequency cepstral coefficients (MFCC), as well as on pre-trained Wav2vec features . These comparisons provide insights on how complementary the acoustically motivated contrastive loss function is to the other masked language model one.",
  "y": "differences"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_3",
  "x": "The learned high level features produced by the context network c i are shown to be better acoustic representations for speech recognition compared to standard spectral features. ---------------------------------- **VQ-WAV2VEC** vq-wav2vec <cite>(Baevski et al., 2019b)</cite> learns vector quantized (VQ) representations of audio data using a future time-step prediction task. Similar to wav2vec, there is a convolutional encoder and decoder networks f : X \u2192 Z and g :\u1e90 \u2192 C for feature extraction and aggregation. However, in between them there is a quantization module q : Z \u2192\u1e90 to build discrete representations which are input to the aggregator. First, 30ms segments of raw speech are mapped to a dense feature representation z at a stride of 10ms using the encoder f . Next, the quantizer (q) turns these dense representations into discrete indices which are mapped to a reconstruction\u1e91 of the original representation z. The\u1e91 is fed into the aggregator g and the model is optimized via the same context prediction task as wav2vec (cf. \u00a72.2).",
  "y": "background"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_4",
  "x": "---------------------------------- **APPROACH** ---------------------------------- **DISCRETE BERT** Our work builds on the recently proposed work in <cite>(Baevski et al., 2019b)</cite> where audio is quantized using a contrastive loss, then features learned on top by a BERT model (Devlin et al., 2018) . For the vq-wav2vec quantization, we use the gumbelsoftmax vq-wav2vec model with the same setup as described in <cite>(Baevski et al., 2019b)</cite> . This model quantizes the Librispeech dataset into 13.5k unique codes. To understand the impact of acoustic representations baked into the wav2vec features, as alternatives, we explore quantizing the standard melfrequency cepstral coefficients (MFCC) and logmel filterbanks coefficients (FBANK), choosing a subset small enough to fit into GPU memory and running k-means with 13.5k centroids (to match the vq-wav2vec setup) to convergence. We then assign the index of the closest centroid to represent each time-step.",
  "y": "extends"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_5",
  "x": "Our work builds on the recently proposed work in <cite>(Baevski et al., 2019b)</cite> where audio is quantized using a contrastive loss, then features learned on top by a BERT model (Devlin et al., 2018) . For the vq-wav2vec quantization, we use the gumbelsoftmax vq-wav2vec model with the same setup as described in <cite>(Baevski et al., 2019b)</cite> . This model quantizes the Librispeech dataset into 13.5k unique codes. To understand the impact of acoustic representations baked into the wav2vec features, as alternatives, we explore quantizing the standard melfrequency cepstral coefficients (MFCC) and logmel filterbanks coefficients (FBANK), choosing a subset small enough to fit into GPU memory and running k-means with 13.5k centroids (to match the vq-wav2vec setup) to convergence. We then assign the index of the closest centroid to represent each time-step. We train a standard BERT model (Devlin et al., 2018; with only the masked language modeling task on each set of inputs in the same way as described in <cite>(Baevski et al., 2019b)</cite> , namely by choosing tokens for masking with probability of 0.05, expanding each chosen token to a span of 10 masked tokens (spans may overlap) and then computing a cross-entropy loss which attempts to maximize the likelihood of predicting the true token for each one that was masked ( Figure  1a ). ---------------------------------- **CONTINUOUS BERT** A masked language modeling task cannot be performed with continuous inputs and outputs, as there are no targets to predict in place of the masked tokens.",
  "y": "similarities uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_6",
  "x": "To understand the impact of acoustic representations baked into the wav2vec features, as alternatives, we explore quantizing the standard melfrequency cepstral coefficients (MFCC) and logmel filterbanks coefficients (FBANK), choosing a subset small enough to fit into GPU memory and running k-means with 13.5k centroids (to match the vq-wav2vec setup) to convergence. We then assign the index of the closest centroid to represent each time-step. We train a standard BERT model (Devlin et al., 2018; with only the masked language modeling task on each set of inputs in the same way as described in <cite>(Baevski et al., 2019b)</cite> , namely by choosing tokens for masking with probability of 0.05, expanding each chosen token to a span of 10 masked tokens (spans may overlap) and then computing a cross-entropy loss which attempts to maximize the likelihood of predicting the true token for each one that was masked ( Figure  1a ). ---------------------------------- **CONTINUOUS BERT** A masked language modeling task cannot be performed with continuous inputs and outputs, as there are no targets to predict in place of the masked tokens. Instead of reconstructing the input as in (van den Oord et al., 2017), we classify the masked positive example among a set of negatives. The inputs to the model are dense wav2vec features , MFCC or FBANK features representing 10ms of audio data.",
  "y": "similarities uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_7",
  "x": "All of our experiments are implemented by extending the fairseq toolkit. ---------------------------------- **DATA** All of our experiments are performed by pretraining on 960 hours of Librispeech (Panayotov et al., 2015) training set, fine-tuning on labeled 10 hours and 1 hour sets sampled equally from the two conditions of the training set, and evaluating on the standard dev and test splits. ---------------------------------- **MODELS** ---------------------------------- **QUANTIZED INPUTS TRAINING** We first train the vq-wav2vec quantization model following the gumbel-softmax recipe described in <cite>(Baevski et al., 2019b)</cite> .",
  "y": "uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_8",
  "x": "The learning rate is warmed up over the first 10,000 updates to a peak value of 1 \u00d7 10 \u22125 , and then linearly decayed over a total of 250k updates. We train on 128 GPUs with a batch size of 3072 tokens per GPU giving a total batch size of 393k tokens (Ott et al., 2018) . Each token represents 10ms of audio data. To mask the input sequence, we follow <cite>(Baevski et al., 2019b)</cite> and randomly sample p = 0.05 of all tokens to be a starting index, without replacement, and mask M = 10 consecutive tokens from every sampled index; spans may overlap. ---------------------------------- **CONTINUOUS INPUTS TRAINING** For training on dense features, we use a model similar to a standard BERT model with the same parameterization as the one used for quantized input training, but we use the wav2vec, MFCC or FBANK inputs directly. We add 128 relative positional embeddings at every multi-head attention block as formulated in (Dai et al., 2019) instead of fixed positional embeddings to ease handling longer examples. We train this model on only 8 GPUs, with a batch size of 9600 inputs per GPU resulting in a total batch size of 76,800.",
  "y": "uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_9",
  "x": "The quantized BERT models have a limit of 2048 source tokens due to their use of fixed positional embeddings. During training we discard longer examples and during evaluation we discard randomly chosen tokens from each example until they are at most 2048 tokens long. We expect that increasing the size of the fixed positional embeddings, or switching to relative positional embeddings will improve performance on longer examples, but in this work we wanted to stay consistent with the setup in<cite> Baevski et al. (2019b)</cite> . The tandem model which uses the features extracted from the pre-trained BERT models is a character-based Wav2Letter setup of (Zeghidour et al., 2018) which uses seven consecutive blocks of convolutions (kernel size 5 with 1.000 \u00d7 10 3 channels), followed by a PReLU nonlinearity and a dropout rate of 1 \u00d7 10 \u22121 . The final representation is projected to a 28-dimensional probability over the vocabulary and decoded using the standard 4gram language model following the same protocol as for the fine-tuned models Table 1 presents WERs of different input features and pre-training methods on the standard Librispeech clean and other subsets using 10 hours and 1 hour of labeled data for fine-tuning. Compared to the two-model tandem system proposed in <cite>(Baevski et al., 2019b)</cite> , which uses a the discrete BERT features to train another ASR system from scratch, our discrete BERT model provides an average of 13% and 6% of WER reduction on clean and other subsets respectively, by pre-training and fine-tuning the same BERT model on the 10h labeled set. The wav2vec inputs represent one level of unsupervised feature discovery, which provides a better space for quantization compared to raw spectral features. The discrete BERT training augments the wav2vec features with a higher level of representation that captures the sequential structure of the full utterance through the masked language modeling loss. On the other hand, the continuous BERT training, given its contrastive InforNCE loss, can be viewed as another level of acoustic representations that captures longer range regularities.",
  "y": "uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_10",
  "x": "We use the weights found by these sweeps to evaluate and report results for all other splits. The sweeps are run with beam size of 250, while the final decoding is done with beam size of 1500. The quantized BERT models have a limit of 2048 source tokens due to their use of fixed positional embeddings. During training we discard longer examples and during evaluation we discard randomly chosen tokens from each example until they are at most 2048 tokens long. We expect that increasing the size of the fixed positional embeddings, or switching to relative positional embeddings will improve performance on longer examples, but in this work we wanted to stay consistent with the setup in<cite> Baevski et al. (2019b)</cite> . The tandem model which uses the features extracted from the pre-trained BERT models is a character-based Wav2Letter setup of (Zeghidour et al., 2018) which uses seven consecutive blocks of convolutions (kernel size 5 with 1.000 \u00d7 10 3 channels), followed by a PReLU nonlinearity and a dropout rate of 1 \u00d7 10 \u22121 . The final representation is projected to a 28-dimensional probability over the vocabulary and decoded using the standard 4gram language model following the same protocol as for the fine-tuned models Table 1 presents WERs of different input features and pre-training methods on the standard Librispeech clean and other subsets using 10 hours and 1 hour of labeled data for fine-tuning. Compared to the two-model tandem system proposed in <cite>(Baevski et al., 2019b)</cite> , which uses a the discrete BERT features to train another ASR system from scratch, our discrete BERT model provides an average of 13% and 6% of WER reduction on clean and other subsets respectively, by pre-training and fine-tuning the same BERT model on the 10h labeled set. The wav2vec inputs represent one level of unsupervised feature discovery, which provides a better space for quantization compared to raw spectral features.",
  "y": "differences"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_11",
  "x": "---------------------------------- **ABLATIONS** To understand the value of self-supervision in our setup, Table 3 shows WERs for both continuous and discrete input features fine-tuned from random weights, without BERT pre-training, using (Table  4 ). Adding a second layer of representation more than halved the WER, with more gains observed in the \"clean\" subset as also observed in 4.4. ---------------------------------- **DISCUSSION AND RELATED WORK** The the success of BERT (Devlin et al., 2018) and Word2Vec (Mikolov et al., 2013) for NLP tasks motivated more research on self-supervised approaches for acoustic word embedding and unsupervised acoustic feature representation (Bengio and Heigold; Levin et al.; Chung et al., b; He et al.; van den Oord et al., 2018;<cite> Baevski et al., 2019b)</cite> , either by predicting masked discrete or continuous input, or by contrastive prediction of neighboring or similarly sounding segments using distant supervision or proximity in the audio signal as an indication of similarity. In (Kamper et al.) a dynamic time warping alignment is used to discover similar segment pairs. Our work is inspired by the research efforts in reducing the dependence on labeled data for building ASR systems through unsupervised unit discovery and acoustic representation leaning (Park and Glass, 2008; Glass; et.",
  "y": "background"
 },
 {
  "id": "0593fb7ee345cf632e6a61f1f21e6c_0",
  "x": "This architecture helped the growth of machine text understanding which gave new boundaries to machine translation, text classification and contextual understanding. Another major breakthrough in the domain was the introduction of Long-Short Term Memory (LSTM) architecture [7] which improvised over the RNN by introducing a context cell which stores the prior relevant information. The vanilla VQA model [1] used a combination of VGGNet [3] and LSTM [7] . This model has been revised over the years, employing newer architectures and mathematical formulations. Along with this, many authors have worked on producing datasets for eliminating bias, strengthening the performance of the model by robust question-answer pairs which try to cover the various types of questions, testing the visual and language understanding of the system. In this survey, first we cover major datasets published for validating the Visual Question Answering task, such as VQA dataset [1] , DAQUAR [8] , Visual7W [9] and most recent datasets up to 2019 include Tally-QA [10] and KVQA [11] . Next, we discuss the stateof-the-art architectures designed for the task of Visual Question Answering such as Vanilla VQA [1] , Stacked Attention Networks [12] and Pythia v1.0 [13] . Next we present some of our computed results over the three architectures: vanilla VQA model [1] , Stacked Attention Network (SAN) [12] and Teney et al. model <cite>[14]</cite> . Finally, we discuss the observations and future directions.",
  "y": "uses"
 },
 {
  "id": "0593fb7ee345cf632e6a61f1f21e6c_1",
  "x": "It applies the attention based on the both text components, and finally classifies the features to answer the question. This model is better suited for the VQA in videos which has more use cases than images. [20] VQA [1] , TDIUC [29] , COCO-QA [21] Faster-RCNN [22] , Differential Modules [30] , GRU [31] 68.59 (VQA-v2), 86.73 (TDIUC), 69.36 (COCO-QA) AAAI 2019 Pythia v1.0 [28] : Pythia v1.0 is the award winning architecture for VQA Challenge 2018 1 . The architecture is similar to Teney et al. <cite>[14]</cite> with reduced computations with elementwise multiplication, use of GloVe vectors [23] , and ensemble of 30 models. Differential Networks [20] : This model uses the differences between forward propagation steps to reduce the noise and to learn the interdependency between features. Image features are extracted using Faster-RCNN [22] . The differential modules [30] are used to refine the features in both text and images. GRU [31] is used for question feature extraction. Finally, it is combined with an attention module to classify the answers.",
  "y": "similarities"
 },
 {
  "id": "0593fb7ee345cf632e6a61f1f21e6c_2",
  "x": "As part of this survey, we also implemented different methods over different datasets and performed the experiments. We considered the following three models for our experiments, 1) the baseline Vanilla VQA model [1] which uses the VGG16 CNN architecture [3] and LSTMs [7] , 2) the Stacked Attention Networks [12] architecture, and 3) the 2017 VQA challenge winner Teney et al. model <cite>[14]</cite> . We considered the widely adapted datasets such as standard VQA dataset [1] and Visual7W dataset [9] for the experiments. We used the Adam Optimizer for all models with Cross-Entropy loss function. Each model is trained for 100 epochs for each dataset. The experimental results are presented in Table III in terms of the accuracy for three models over two datasets. In the experiments, we found that the Teney et al. <cite>[14]</cite> is the best performing model on both VQA and Visual7W Dataset. The accuracies obtained over the Teney et al. model are 67.23% and 65.82% over VQA and Visual7W datasets for the openended question-answering task, respectively. The above results re-affirmed that the Teney et al. model is the best performing model till 2018 which has been pushed by Pythia v1.0 [13] , recently, where they have utilized the same model with more layers to boost the performance.",
  "y": "uses"
 },
 {
  "id": "0593fb7ee345cf632e6a61f1f21e6c_3",
  "x": "The Differentail Network is the very recent method proposed for VQA task and shows very promising performance over different datasets. As part of this survey, we also implemented different methods over different datasets and performed the experiments. We considered the following three models for our experiments, 1) the baseline Vanilla VQA model [1] which uses the VGG16 CNN architecture [3] and LSTMs [7] , 2) the Stacked Attention Networks [12] architecture, and 3) the 2017 VQA challenge winner Teney et al. model <cite>[14]</cite> . We considered the widely adapted datasets such as standard VQA dataset [1] and Visual7W dataset [9] for the experiments. We used the Adam Optimizer for all models with Cross-Entropy loss function. Each model is trained for 100 epochs for each dataset. The experimental results are presented in Table III in terms of the accuracy for three models over two datasets. In the experiments, we found that the Teney et al. <cite>[14]</cite> is the best performing model on both VQA and Visual7W Dataset. The accuracies obtained over the Teney et al. model are 67.23% and 65.82% over VQA and Visual7W datasets for the openended question-answering task, respectively.",
  "y": "uses"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_0",
  "x": "It is hence clear that one cannot learn all these diverse relations from the very small amounts of available training data. Instead, we would have to learn a more general representation of discourse expectations. Many recent discourse relation classification approaches have focused on cross-lingual data augmentation , training models to better represent the relational arguments by using various neural network models, including feed-forward network (Rutherford et al., 2017) , convolutional neural networks (Zhang et al., 2015) , recurrent neural network (Ji et al., 2016;<cite> Bai and Zhao, 2018)</cite> , character-based (Qin et al., 2016) or formulating relation classification as an adversarial task (Qin et al., 2017) . These models typically use pre-trained semantic embeddings generated from language modeling tasks, like Word2Vec (Mikolov et al., 2013) , GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) . However, previously proposed neural models still crucially lack a representation of the typical relations between sentences: to solve the task properly, a model should ideally be able to form discourse expectations, i.e., to represent the typical causes, consequences, next events or contrasts to a given event described in one relational argument, and then assess the content of the second relational argument with respect to these expectations (see Example 1). Previous models would have to learn these relations only from the annotated training data, which is much too sparse for learning all possible relations between all events, states or claims. The recently proposed BERT model (Devlin et al., 2019) takes a promising step towards addressing this problem: the BERT representations are trained using a language modelling and, crucially, a \"next sentence prediction\" task, where the model is presented with the actual next sentence vs. a different sentence and needs to select the original next sentence. We believe it is a good fit for discourse relation recognition, since the task allows the model to represent what a typical next sentence would look like. In this paper, we show that a BERT-based model outperforms the current state of the art by 8% points in 11-way implicit discourse relation classification on PDTB.",
  "y": "background"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_1",
  "x": "Recent models exploit such similarity relations between the two arguments, as well as simpler surface features that occur in one relational argument and correlate with specific coherence relations (e.g., the presence of negation, temporal expressions etc. may give hints as to what coherence relation may be present, see Park and Cardie (2012) ; Asr and Demberg (2015)). However, relations between arguments are often a lot more diverse than simple contrasts that can be captured through antonyms, and may rely on world knowledge (Kishimoto et al., 2018) . It is hence clear that one cannot learn all these diverse relations from the very small amounts of available training data. Instead, we would have to learn a more general representation of discourse expectations. Many recent discourse relation classification approaches have focused on cross-lingual data augmentation , training models to better represent the relational arguments by using various neural network models, including feed-forward network (Rutherford et al., 2017) , convolutional neural networks (Zhang et al., 2015) , recurrent neural network (Ji et al., 2016;<cite> Bai and Zhao, 2018)</cite> , character-based (Qin et al., 2016) or formulating relation classification as an adversarial task (Qin et al., 2017) . These models typically use pre-trained semantic embeddings generated from language modeling tasks, like Word2Vec (Mikolov et al., 2013) , GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) . However, previously proposed neural models still crucially lack a representation of the typical relations between sentences: to solve the task properly, a model should ideally be able to form discourse expectations, i.e., to represent the typical causes, consequences, next events or contrasts to a given event described in one relational argument, and then assess the content of the second relational argument with respect to these expectations (see Example 1). Previous models would have to learn these relations only from the annotated training data, which is much too sparse for learning all possible relations between all events, states or claims. The recently proposed BERT model (Devlin et al., 2019) takes a promising step towards addressing this problem: the BERT representations are trained using a language modelling and, crucially, a \"next sentence prediction\" task, where the model is presented with the actual next sentence vs. a different sentence and needs to select the original next sentence.",
  "y": "motivation"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_2",
  "x": "Following the experimental settings and evaluation metrics in<cite> Bai and Zhao (2018)</cite> , we use two most-used splitting methods of PDTB data, denoted as PDTB-Lin (Lin et al., 2009) , which uses sections 2-21, 22, 23 as training, validation and test sets, and PDTB-Ji (Ji and Eisenstein, 2015) , which uses 2-20, 0-1, 21-22 as training, validation and test sets and report the overall accuracy score. In addition, we also performed 10-fold cross validation among sections 0-22, as promoted in . We also follow the standard in the literature to formulate the task as an 11-way classification task. Results are presented in Table 1 . We evaluated three versions of the BERT-based model. All of our BERT models use the pre-trained representations and are fine-tuned on the PDTB training data. The version marked as \"BERT\" does not do any additional pre-training. BERT+WSJ in addition performs further pre-training on the 1 https://github.com/google-research/ bert#pre-trained-models parts of the Wall Street Journal corpus that do not have discourse relation annotation. The model version \"BERT+WJS w/o NSP\" also performs pre-training on the WSJ corpus, but only uses the Masked Language Modelling task, not the Next Sentence Prediction task in the pre-training.",
  "y": "uses"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_3",
  "x": "We compared the results with four state-of-theart systems: Cai and Zhao (2017) proposed a model that takes a step towards calculating discourse expectations by using attention over an encoding of the first argument, to generate the representation of the second argument, and then learning a classifier based on the concatenation of the encodings of the two discourse relation arguments. Kishimoto et al. (2018) fed external world knowledge (ConceptNet relations and coreferences) explicitly into MAGE-GRU (Dhingra et al., 2017) and achieved improvements compared to only using the relational arguments. However, we here show that it works even better when we learn this knowledge implicit through next sentence prediction task. used a seq2seq model that learns better argument representations due to being trained to explicitate the implicit connective. In addition, their classifier also uses a memory network that is intended to help remember similar argument pairs encountered during training. The current best performance was achieved by<cite> Bai and Zhao (2018)</cite> , who combined representations from different grained em-beddings including contextualized word vectors from ELMo (Peters et al., 2018) , which has been proved very helpful. In addition, we compared our results with a simple bidirectional LSTM network and pre-trained word embeddings from Word2Vec. We can see that on all settings, the model using BERT representations outperformed all existing systems with a substantial margin. It obtained improvements of 7.3% points on PDTB-Lin, 5.5% points on PDTB-Ji, compared with the ELMobased method proposed in <cite>(Bai and Zhao, 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_4",
  "x": "We compared the results with four state-of-theart systems: Cai and Zhao (2017) proposed a model that takes a step towards calculating discourse expectations by using attention over an encoding of the first argument, to generate the representation of the second argument, and then learning a classifier based on the concatenation of the encodings of the two discourse relation arguments. Kishimoto et al. (2018) fed external world knowledge (ConceptNet relations and coreferences) explicitly into MAGE-GRU (Dhingra et al., 2017) and achieved improvements compared to only using the relational arguments. However, we here show that it works even better when we learn this knowledge implicit through next sentence prediction task. used a seq2seq model that learns better argument representations due to being trained to explicitate the implicit connective. In addition, their classifier also uses a memory network that is intended to help remember similar argument pairs encountered during training. The current best performance was achieved by<cite> Bai and Zhao (2018)</cite> , who combined representations from different grained em-beddings including contextualized word vectors from ELMo (Peters et al., 2018) , which has been proved very helpful. In addition, we compared our results with a simple bidirectional LSTM network and pre-trained word embeddings from Word2Vec. We can see that on all settings, the model using BERT representations outperformed all existing systems with a substantial margin. It obtained improvements of 7.3% points on PDTB-Lin, 5.5% points on PDTB-Ji, compared with the ELMobased method proposed in <cite>(Bai and Zhao, 2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_5",
  "x": "The BioDRB contains a lot of professional words / phrases that are extremely hard to model. In order to test the ability of the BERT model on cross-domain data, we performed finetuning on PDTB while testing on BioDRB. We also tested the state of the art model of implicit discourse relation classification proposed by<cite> Bai and Zhao (2018)</cite> on BioDRB. From Table 2 , we can see that the BERT base model achieved almost 12% points improvement over the Bi-LSTM baseline and 15% points over<cite> Bai and Zhao (2018)</cite> . When fine-tuned on in-domain data in the crossvalidation setting, the improvement increases to around 17% points. ---------------------------------- **METHOD** Cross-Domain In-Domain Bi-LSTM + w2v 300 32.97 46.49<cite> Bai and Zhao (2018)</cite> 29.52 55.90 BioBERT Table 2 : Accuracy (%) on BioDRB level 2 relations with different settings.",
  "y": "uses"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_6",
  "x": "Compared with PDTB, some new discourse relations and changes have been introduced in the annotation of Bio-DRB. In order to make the results comparable, we preprocessed the BioDRB annotations to map the relations to the PDTB ones, following the instructions in Prasad et al. (2011) . The biomedical domain is very different from the WSJ or the data on which the BERT model was trained. The BioDRB contains a lot of professional words / phrases that are extremely hard to model. In order to test the ability of the BERT model on cross-domain data, we performed finetuning on PDTB while testing on BioDRB. We also tested the state of the art model of implicit discourse relation classification proposed by<cite> Bai and Zhao (2018)</cite> on BioDRB. From Table 2 , we can see that the BERT base model achieved almost 12% points improvement over the Bi-LSTM baseline and 15% points over<cite> Bai and Zhao (2018)</cite> . When fine-tuned on in-domain data in the crossvalidation setting, the improvement increases to around 17% points. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_8",
  "x": "We also tested the state of the art model of implicit discourse relation classification proposed by<cite> Bai and Zhao (2018)</cite> on BioDRB. From Table 2 , we can see that the BERT base model achieved almost 12% points improvement over the Bi-LSTM baseline and 15% points over<cite> Bai and Zhao (2018)</cite> . When fine-tuned on in-domain data in the crossvalidation setting, the improvement increases to around 17% points. ---------------------------------- **METHOD** Cross-Domain In-Domain Bi-LSTM + w2v 300 32.97 46.49<cite> Bai and Zhao (2018)</cite> 29.52 55.90 BioBERT Table 2 : Accuracy (%) on BioDRB level 2 relations with different settings. Cross-Domain means trained on PDTB and tested on BioDRB. For the In-Domain setting, we used 5-fold cross-validation and report average accuracy.",
  "y": "differences"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_0",
  "x": "In special, they have been more prevalent than syntactical networks because they represent a simplified representation of the complex syntactical analysis [7, 8] , as most of the syntactical links occur between neighboring words. Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition [9] , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies <cite>[8,</cite> 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ). However, when deep analyzes are performed, network-based strategies usually do not perform better than other techniques making extensive use of semantic resources and tools. In order to improve the performance of network-based applications, I suggest a twofold research line: (i) the introduction of measurements consistent with the nature of the problem; and (ii) the combination of topological strategies with other traditional natural language processing methods.",
  "y": "background"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_1",
  "x": "As for cooccurrence networks of characters, it was possible to verify that they follow the scale-free and small-world features [4] . Co-occurrence networks of words (or adjacency networks) have accounted for most of the models tackling textual applications. In special, they have been more prevalent than syntactical networks because they represent a simplified representation of the complex syntactical analysis [7, 8] , as most of the syntactical links occur between neighboring words. Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition [9] , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies <cite>[8,</cite> 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ).",
  "y": "motivation future_work"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_2",
  "x": "Co-occurrence networks of words (or adjacency networks) have accounted for most of the models tackling textual applications. In special, they have been more prevalent than syntactical networks because they represent a simplified representation of the complex syntactical analysis [7, 8] , as most of the syntactical links occur between neighboring words. Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition [9] , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies <cite>[8,</cite> 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ). However, when deep analyzes are performed, network-based strategies usually do not perform better than other techniques making extensive use of semantic resources and tools.",
  "y": "future_work motivation"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_3",
  "x": "The topological analysis of complex textual networks has been widely studied in the recent years. As for cooccurrence networks of characters, it was possible to verify that they follow the scale-free and small-world features [4] . Co-occurrence networks of words (or adjacency networks) have accounted for most of the models tackling textual applications. In special, they have been more prevalent than syntactical networks because they represent a simplified representation of the complex syntactical analysis [7, 8] , as most of the syntactical links occur between neighboring words. Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition [9] , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies <cite>[8,</cite> 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features.",
  "y": "motivation"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_4",
  "x": "However, when deep analyzes are performed, network-based strategies usually do not perform better than other techniques making extensive use of semantic resources and tools. In order to improve the performance of network-based applications, I suggest a twofold research line: (i) the introduction of measurements consistent with the nature of the problem; and (ii) the combination of topological strategies with other traditional natural language processing methods. More specifically, in (i), I propose 1 E-mail:diego.raphael@gmail.com, diego@icmc.usp.br December 4, 2014 the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors <cite>[8]</cite> . Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [17] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in [9] , [19] and [20]. In sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels.",
  "y": "uses"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_0",
  "x": "Previous discriminative parsing models usually factor a parse tree into a set of parts. Each part is scored separately to ensure tractability. In dependency parsing (DP), the number of dependencies in a part is called the order of a DP model (Koo and Collins, 2010) . Accordingly, existing graph-based DP models can be categorized into tree groups, namely, the first-order (Eisner, 1996; McDonald et al., 2005a; McDonald et al., 2005b) , second-order (McDonald and Pereira, 2006; Carreras, 2007) and third-order (Koo and Collins, 2010 ) models. Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a ; * The research reported in this paper was partially supported by the Research Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). Petrov and Klein, 2008b; Finkel et al., 2008) are the first-order ones, because there is only one grammar rule in a part. The discriminative re-scoring models (Collins, 2000; Collins and Duffy, 2002; Charniak and Johnson, 2005; <cite>Huang, 2008)</cite> can be viewed as previous attempts to higher-order constituent parsing, using some parts containing more than one grammar rule as non-local features. In this paper, we present a higher-order constituent parsing model 1 based on these previous works.",
  "y": "background"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_1",
  "x": "---------------------------------- **DECODING** The factorization of the parsing model allows us to develop an exact decoding algorithm for it. Following<cite> Huang (2008)</cite> , this algorithm traverses a parse forest in a bottom-up manner. However, it determines and keeps the best derivation for every grammar rule instance instead of for each node. Because all structures above the current rule instance is not determined yet, the computation of its nonlocal structural features, e.g., parent and sibling features, has to be delayed until it joins an upper level structure. For example, when computing the score of a derivation under the center rule NP \u2192 NP VP in Figure 1 , the algorithm will extract child features from its children NP \u2192 DT QP and VP \u2192 VBN PP. The parent and sibling features of the two child rules can also be extracted from the current derivation and used to calculate the score of this derivation. But parent and sibling features for the center rule will not be computed until the decoding process reaches the rule above, i.e., PP \u2192 IN NP.",
  "y": "uses background"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_2",
  "x": "Adding structural features, each involving a least a neighboring rule instance, makes it a higher-order parsing model. ---------------------------------- **DECODING** The factorization of the parsing model allows us to develop an exact decoding algorithm for it. Following<cite> Huang (2008)</cite> , this algorithm traverses a parse forest in a bottom-up manner. However, it determines and keeps the best derivation for every grammar rule instance instead of for each node. Because all structures above the current rule instance is not determined yet, the computation of its nonlocal structural features, e.g., parent and sibling features, has to be delayed until it joins an upper level structure. For example, when computing the score of a derivation under the center rule NP \u2192 NP VP in Figure 1 , the algorithm will extract child features from its children NP \u2192 DT QP and VP \u2192 VBN PP. The parent and sibling features of the two child rules can also be extracted from the current derivation and used to calculate the score of this derivation.",
  "y": "uses"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_3",
  "x": "**DECODING** The factorization of the parsing model allows us to develop an exact decoding algorithm for it. Following<cite> Huang (2008)</cite> , this algorithm traverses a parse forest in a bottom-up manner. However, it determines and keeps the best derivation for every grammar rule instance instead of for each node. Because all structures above the current rule instance is not determined yet, the computation of its nonlocal structural features, e.g., parent and sibling features, has to be delayed until it joins an upper level structure. For example, when computing the score of a derivation under the center rule NP \u2192 NP VP in Figure 1 , the algorithm will extract child features from its children NP \u2192 DT QP and VP \u2192 VBN PP. The parent and sibling features of the two child rules can also be extracted from the current derivation and used to calculate the score of this derivation. But parent and sibling features for the center rule will not be computed until the decoding process reaches the rule above, i.e., PP \u2192 IN NP. This algorithm is more complex than the approximate decoding algorithm of<cite> Huang (2008)</cite> .",
  "y": "differences"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_5",
  "x": "For parser combination, we follow the setting of Fossum and Knight (2009) , using Section 24 instead of Section 22 of WSJ treebank as development set. In this work, the lexical model of Chen and Kit (2011) is combined with our syntactic model under the framework of product-of-experts (Hinton, 2002) . A factor \u03bb is introduced to balance the two models. It is tuned on a development set using the gold sec- (2003) 90.70 Carreras et al. (2008) 91.1 Re-scoring Collins (2000) 89.70 Charniak and Johnson (2005) 91.02 The parser of Charniak and Johnson 91.40 43.54<cite> Huang (2008)</cite> 91.69 43.5 Combination Fossum and Knight (2009) 92. 4 Zhang et al. (2009) 92.3 Petrov (2010) 91.85 41.9 Self-training Zhang et al. (2009) (Kiefer, 1953) . The parameters \u03b8 of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and<cite> Huang (2008)</cite> . The performance of our first-and higher-order parsing models on all sentences of the two test sets is presented in Table 3 , where \u03bb indicates a tuned balance factor. This parser is also combined with the parser of Charniak and Johnson (2005) 2 and the Stanford. parser 3 The best combination results in Table 3 are achieved with k=70 for English and k=100 for Chinese for selecting the k-best parses.",
  "y": "uses"
 },
 {
  "id": "086619bef9b4e7851bf42bf36eb14a_0",
  "x": "Another way to investigate text structure is by the analysis of sentence lengths. Typically, each sentence carries a full message and transmits an idea in contrast with an isolated word. Therefore, mapping a text into a time series of sentence lengths is a natural way to investigate text structures. Recent methods applied to study texts at sentence level include probability distributions [24, 25, 26, 27] and correlations [26, 27, 17, <cite>28]</cite> . In recent studies, sentence length analysis have been related to style and authorship [29, 26, 27] . In general, sentence lengths have been quantified by the number of words [24, 29, 25, <cite>28]</cite> or characters [30, 31, 26, 27] . Also, note that the recurrence time of full stops also quantifies the sentence length [3] . However, there are other possible variations, such as word length or word frequency mappings, where all words are brought to lower case and punctuation marks are removed. Other possible variations are the removal of stop words and the lemmatization [21] .",
  "y": "background"
 },
 {
  "id": "086619bef9b4e7851bf42bf36eb14a_1",
  "x": "Therefore, mapping a text into a time series of sentence lengths is a natural way to investigate text structures. Recent methods applied to study texts at sentence level include probability distributions [24, 25, 26, 27] and correlations [26, 27, 17, <cite>28]</cite> . In recent studies, sentence length analysis have been related to style and authorship [29, 26, 27] . In general, sentence lengths have been quantified by the number of words [24, 29, 25, <cite>28]</cite> or characters [30, 31, 26, 27] . Also, note that the recurrence time of full stops also quantifies the sentence length [3] . However, there are other possible variations, such as word length or word frequency mappings, where all words are brought to lower case and punctuation marks are removed. Other possible variations are the removal of stop words and the lemmatization [21] . In a similar way, the number of characters and variations related to lemmatization and stop words removal could also be considered at sentence level. However, to choose between words or characters to measure a sentence may lead to doubts.",
  "y": "background"
 },
 {
  "id": "086619bef9b4e7851bf42bf36eb14a_2",
  "x": "However, an issue about this relationship was addressed in [37] : \"Somewhat more problematic is the relation of sentence length to the word length.\" This comment is consistent with the one in<cite> [28]</cite> asserting that the Menzerath-Altmann law does not hold if the sentence length is measured in terms of characters instead of the number of words. In a similar way, the data of Fig. 2a indicates, in the context of this law, that the relationship between the number of words and characters is also problematic. ---------------------------------- **SIMILARITIES BETWEEN DISTRIBUTIONS** Next, we investigated the distribution of sentence lengths within each book, for each measure of sentence length considered here. This test exploits the distance \u03ba between two empirical cumulated distributions C 1 and C 2 : where sup is the supremum function; the smaller the \u03ba the greater the similarity between the distributions. Typically, the number of characters is greater than the number of words (Figs. 1b and 1c) . Thus, to make a fair comparison, the all time series were normalized by their mean values prior to the KS test.",
  "y": "background"
 },
 {
  "id": "086619bef9b4e7851bf42bf36eb14a_3",
  "x": "Using the The Brothers Karamazov as example again, Fig. 4a and 4b illustrate, respectively, F (m) and the differences between the Hurst exponents for all six series. We can infer that h differs very little from one series to another and that their values are close to 0.8, with h * \u223c 0.5, implying in long-range correlations. All the series from the other books reflects this behavior (h \u223c 0.75). This result is consistent with the multifractal analysis performed in<cite> [28]</cite> . In addition, the values for the difference between Hurst exponents (\u2206h) of a given book are shown in Fig. 4c , where we can observe that the variation of the Hurst exponents is small from one series to another. Also, the standard deviation for h within each book was close to 0.001. Lastly, no correlations were found between the number of sentences in a text and the Hurst exponent. ---------------------------------- **CONCLUSIONS**",
  "y": "similarities"
 },
 {
  "id": "0888b30ae5dcc880761a92ffbdcd1b_0",
  "x": "**** Zipf's law for word frequencies states that the probability of the i-th most frequent word obeys with \u03b1 \u2248 1 [1, 2] . Here we explore the possibility that Zipf's law is a consequence of compression, the minimization of the mean length of the words of a vocabulary [3] . This principle has already been used to explain the origins of other linguistic laws: Zipf's law of abbreviation, namely, the frequency of more frequent words to be shorter [3,<cite> 4]</cite> , and Menzerath's law, the tendency of a larger linguistic construct to be made of smaller components [5] . Our argument combines two constraints for compression: (1) non-singular coding, i.e. any two different words should not be represented by the same string of letters or phonemes, and (2) unique decipherability, i.e. given a continuous sequence of letters or phonemes, there should be only one way of segmenting it into words [6] . The former is needed to reduce the cost of retrieving the original meaning. The latter is required to reduce the cost of determining word boundaries. Thus both constraints on compression and compression itself, are realistic cognitive pressures that are vital to fight against the now-or-never bottleneck of linguistic processing [7] . Suppose that words are coded using an alphabet of N letters (or phonemes) and that p i and l i are the probability and the length of the i-th most probable word.",
  "y": "background"
 },
 {
  "id": "0888b30ae5dcc880761a92ffbdcd1b_2",
  "x": "First, a relationship between the length of a word and its probability<cite> [4]</cite> l = a log p + b, where a and b are constants (a < 0) defined on the parameters of the model as and Eq. 7 can be interpreted, approximately, as a linear generalization of the relationship between l and p of optimal uniquely decipherable codes in Eq. 3). Second, a relationship between the length of a word and its rank that matches exactly that of optimal non-singular codes in Eq. 5 (while Eq. 5 is exact, Mandelbrot derived an approximate equation). Combining these two implications of random typing, Mandelbrot derived Zipf's law for word frequency with \u03b1 > 1. The exact (Eq. 5) and approximate (Eq. 7) connections between random typing and optimal coding challenge the view of random typing as totally detached from cost-cutting considerations [10, 11] . Our derivation of Zipf's law presents various advantages over random typing.",
  "y": "background"
 },
 {
  "id": "0888b30ae5dcc880761a92ffbdcd1b_3",
  "x": "Combining these two implications of random typing, Mandelbrot derived Zipf's law for word frequency with \u03b1 > 1. The exact (Eq. 5) and approximate (Eq. 7) connections between random typing and optimal coding challenge the view of random typing as totally detached from cost-cutting considerations [10, 11] . Our derivation of Zipf's law presents various advantages over random typing. First, it departs from realistic cognitive pressures [7] . Second, random typing is based exclusively on random choices but its parameters cannot be set at random: indeed, a precise tuning of the parameters is needed to mimic Zipf's law with \u03b1 = 1 [8] . In contrast, our argument only requires N to be large enough. Third, its assumptions are far reaching: compression allows one to shed light on the origins of three linguistic laws at the same time: Zipf's law for word frequencies, Zipf's law of abbreviation and Menzerath's law with the unifying principle of compression [3, <cite>4,</cite> 5] . There are many ways of explaining the origins of power-law-like distributions such as Zipf's law for word frequencies [12] but compression appears to be as the only one that can lead to a compact theory of statistical laws of language. Although uniquely decipherable codes are a subset of non-singular codes, it is tempting to think that both optimal non-singular coding and optimal uniquely decipherable coding cannot be satisfied to a large extend simultaneously.",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_0",
  "x": "The traditional clustering measure of F-Score (Zhao et al., 2005 ) is used to assess the performance of WSI systems. The second evaluation scheme, supervised evaluation, uses the training part of the dataset in order to map the automatically induced clusters to GS senses. In the next step, the testing corpus is used to measure the performance of systems in a Word Sense Disambiguation (WSD) setting. A significant limitation of F-Score is that it does not evaluate the make up of clusters beyond the majority class <cite>(Rosenberg and Hirschberg, 2007)</cite> . Moreover, F-Score might also fail to evaluate clusters which are not matched to any GS class due to their small size. These two limitations define the matching problem of F-Score <cite>(Rosenberg and Hirschberg, 2007)</cite> which can lead to: (1) identical scores between different clustering solutions, and (2) inaccurate assessment of the clustering quality. The supervised evaluation scheme employs a method in order to map the automatically induced clusters to GS senses. As a result, this process might change the distribution of clusters by mapping more than one clusters to the same GS sense. The outcome of this process might be more helpful for systems that produce a large number of clusters.",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_1",
  "x": "Given the significance of WSI, the objective assessment and comparison of WSI methods is crucial. The first effort to evaluate WSI methods under a common framework (evaluation schemes & dataset) was undertaken in the SemEval-2007 WSI task (SWSI) (Agirre and Soroa, 2007) , where two separate evaluation schemes were employed. The first one, unsupervised evaluation, treats the WSI results as clusters of target word contexts and Gold Standard (GS) senses as classes. The traditional clustering measure of F-Score (Zhao et al., 2005 ) is used to assess the performance of WSI systems. The second evaluation scheme, supervised evaluation, uses the training part of the dataset in order to map the automatically induced clusters to GS senses. In the next step, the testing corpus is used to measure the performance of systems in a Word Sense Disambiguation (WSD) setting. A significant limitation of F-Score is that it does not evaluate the make up of clusters beyond the majority class <cite>(Rosenberg and Hirschberg, 2007)</cite> . Moreover, F-Score might also fail to evaluate clusters which are not matched to any GS class due to their small size. These two limitations define the matching problem of F-Score <cite>(Rosenberg and Hirschberg, 2007)</cite> which can lead to: (1) identical scores between different clustering solutions, and (2) inaccurate assessment of the clustering quality.",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_2",
  "x": "A significant limitation of F-Score is that it does not evaluate the make up of clusters beyond the majority class <cite>(Rosenberg and Hirschberg, 2007)</cite> . Moreover, F-Score might also fail to evaluate clusters which are not matched to any GS class due to their small size. These two limitations define the matching problem of F-Score <cite>(Rosenberg and Hirschberg, 2007)</cite> which can lead to: (1) identical scores between different clustering solutions, and (2) inaccurate assessment of the clustering quality. The supervised evaluation scheme employs a method in order to map the automatically induced clusters to GS senses. As a result, this process might change the distribution of clusters by mapping more than one clusters to the same GS sense. The outcome of this process might be more helpful for systems that produce a large number of clusters. In this paper, we focus on analysing the SemEval-2007 WSI evaluation schemes showing their deficiencies. Subsequently, we present the use of V-measure <cite>(Rosenberg and Hirschberg, 2007)</cite> as an evaluation measure that can overcome the current limitations of F-Score. Finally, we also suggest a small modification on the supervised evaluation scheme, which will possibly allow for a more reliable estimation of WSD performance.",
  "y": "differences extends"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_3",
  "x": "If the clustering is identical to the original classes in the datasets, F-Score will be equal to one. In the example of Table 1 , F-Score is equal to 0.714. As it can be observed, F-Score assesses the quality of a clustering solution by considering two different angles, i.e. homogeneity and completeness <cite>(Rosenberg and Hirschberg, 2007)</cite> . Homogeneity refers to the degree that each cluster consists of data points, which primarily belong to a single GS class. On the other hand, completeness refers to the degree that each GS class consists of data points, which have primarily been assigned to a single cluster. A perfect homogeneity would result in a precision equal to 1, while a perfect completeness would result in a recall equal to 1. Purity and entropy (Zhao et al., 2005) are also used in SWSI as complementary measures. However, both of them evaluate only the homogeneity of a clustering solution disregarding completeness. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_4",
  "x": "On the contrary, F-Score has a significant advantage over purity and entropy, since it measures both homogeneity (precision) and completeness (recall) of a clustering solution. However, F-Score suffers from the matching problem, which manifests itself either by not evaluating the entire membership of a cluster, or by not evaluating every cluster <cite>(Rosenberg and Hirschberg, 2007)</cite> . The former situation is present, due to the fact that F-Score does not consider the make-up of the clusters beyond the majority class <cite>(Rosenberg and Hirschberg, 2007)</cite> . For example, in Table 3 the F-Score of the clustering so- lution is 0.714 and equal to the F-Score of the clustering solution shown in Table 1 , although these are two significantly different clustering solutions. In fact, the clustering shown in Table 3 should have a better homogeneity than the clustering shown in Table 1 , since intuitively speaking each cluster contains fewer classes. Moreover, the second clustering should also have a better completeness, since each GS class contains fewer clusters. An additional instance of the matching problem manifests itself, when F-Score fails to evaluate the quality of smaller clusters. For example, if we add in Table 3 one more cluster (cl 4 ), which only tags 50 additional instances of gs 1 , then we will be able to observe that this cluster will not be matched to any of the GS senses, since cl 1 is matched to gs 1 . Although F-Score will decrease since the recall of gs 1 will decrease, the evaluation setting ignores the perfect homogeneity of this small cluster.",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_5",
  "x": "This is due to the fact that entropy and purity only measure the homogeneity of a clustering solution. For that reason, the 1c1inst baseline achieves a perfect entropy and purity, although its clustering solution is far from ideal. On the contrary, F-Score has a significant advantage over purity and entropy, since it measures both homogeneity (precision) and completeness (recall) of a clustering solution. However, F-Score suffers from the matching problem, which manifests itself either by not evaluating the entire membership of a cluster, or by not evaluating every cluster <cite>(Rosenberg and Hirschberg, 2007)</cite> . The former situation is present, due to the fact that F-Score does not consider the make-up of the clusters beyond the majority class <cite>(Rosenberg and Hirschberg, 2007)</cite> . For example, in Table 3 the F-Score of the clustering so- lution is 0.714 and equal to the F-Score of the clustering solution shown in Table 1 , although these are two significantly different clustering solutions. In fact, the clustering shown in Table 3 should have a better homogeneity than the clustering shown in Table 1 , since intuitively speaking each cluster contains fewer classes. Moreover, the second clustering should also have a better completeness, since each GS class contains fewer clusters. An additional instance of the matching problem manifests itself, when F-Score fails to evaluate the quality of smaller clusters.",
  "y": "differences"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_6",
  "x": "For example, high supervised recall also means high purity and low entropy as in I2R, but not vice versa as in UOY. UOY produces a large number of clean clusters, in effect suffering from an unreliable mapping of clusters to senses due to the lack of adequate training data. Moreover, an additional supervised evaluation of WSI methods using a different dataset split resulted in a different ranking, in which all of the systems outperformed the MFS baseline (Agirre and Soroa, 2007) . This result indicates that the supervised evaluation might not provide a reliable estimation of WSD performance, particularly in the case where the mapping relies on a single dataset split. 3 SemEval-2010 WSI evaluation setting 3.1 Unsupervised evaluation using V-measure Let us assume that the dataset of a target word tw comprises of N instances (data points). These data points are divided into two partitions, i.e. a set of automatically generated clusters C = {c j |j = 1 . . . n} and a set of gold standard classes GS = {gs i |gs = 1 . . . m}. Moreover, let a ij be the number of data points, which are members of class gs i and elements of cluster c j . V-measure assesses the quality of a clustering solution by explicitly measuring its homogeneity and its completeness <cite>(Rosenberg and Hirschberg, 2007)</cite> .",
  "y": "similarities"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_7",
  "x": "As a result, instead of taking the raw conditional entropy, V-measure normalises it by the maximum reduction in entropy the clustering information could provide, i.e. H(GS). ---------------------------------- **FORMULAS 2 AND 3 DEFINE H(GS) AND H(GS|C).** When there is only a single class (H(GS) = 0), any clustering would produce a perfectly homogeneous solution. In the worst case, the class distribution within each cluster is equal to the overall class distribution (H(GS|C) = H(GS)), i.e. clustering provides no new information. Overall, in accordance with the convention of 1 being desirable and 0 undesirable, the homogeneity (h) of a clustering solution is 1 if there is only a single class, and 1\u2212 H(GS|C) H(GS) in any other case <cite>(Rosenberg and Hirschberg, 2007)</cite> . Symmetrically to homogeneity, completeness refers to the degree that each GS class consists of data points, which have primarily been assigned to a single cluster. To evaluate completeness, V-measure examines the distribution of cluster assignments within each class. The conditional entropy of the cluster given the class distribution, H(C|GS), quantifies the remaining entropy (uncertainty) of the cluster given that the class distribution is known.",
  "y": "similarities"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_8",
  "x": "In the worst case, completeness will be equal to 0, particularly when H(C|GS) is maximal and equal to H(C). This happens when each GS class is included in all clusters with a distribution equal to the distribution of sizes <cite>(Rosenberg and Hirschberg, 2007)</cite> . Formulas 4 and 5 define H(C) and H(C|GS). Finally h and c can be combined and produce V-measure, which is the harmonic mean of homogeneity and completeness. Returning to our clustering example in Table 1 , its V-measure is equal to 0.275. In section 2.3, we also presented an additional clustering (Table 3) , which had the same F-Score as the clustering in Table 1, despite the fact that it intuitively had a better completeness and homogeneity. The V-measure of the second clustering solution is equal to 0.45, and higher than the V-measure of the first clustering. This result shows that V-measure is able to discriminate between these two clusterings by considering the make-up of the clusters beyond the majority class. Furthermore, it is straightforward from the description in this section, that V-measure evaluates each cluster in terms of homogeneity and completeness, unlike F-Score which relies on a post-hoc matching.",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_9",
  "x": "In nouns only I2R is able to outperform this baseline, while in verbs the 1c1inst baseline achieves the highest result. By the definition of homogeneity (section 3.1), this baseline is perfectly homogeneous, since each cluster contains one instance of a single sense. However, its completeness is not 0, as one might intuitively expect. This is due to the fact that V-measure considers as the worst solution in terms of completeness the one, in which each class is represented by every cluster, and specifically with a distribution equal to the distribution of cluster sizes <cite>(Rosenberg and Hirschberg, 2007)</cite> . This worst solution is not equivalent to the 1c1inst, hence completeness of 1c1inst is greater than 0. Additionally, completeness of this baseline benefits from the fact that around 18% of GS senses have only one instance in the test set. Note however, that on average this baseline achieves a lower completeness than most of the systems. Another observation from Table 4 is that upv si and UOY have a better ranking than in Table 2 . Note that these systems have generated a higher number of clusters than the GS number of senses.",
  "y": "similarities"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_0",
  "x": "A large and growing body of research is directed towards having robots learn new cognitive skills, or improving their capabilities, by interacting autonomously with their surrounding environment. In particular, robots operating in an unstructured scenario may understand available opportunities conditioned on their body, perception and sensorimotor experiences: the intersection of these elements gives rise to object affordances (action possibilities), as they are called in psychology [6] . The usefulness of affordances in cognitive robotics is in the fact that they capture essential properties of environment objects in terms of the actions that a robot is able to perform with them [7, 8] . Some authors have suggested an alternative computational model called Object-Action Complexes (OACs) [9] , which links low-level sensorimotor knowledge with high-level symbolic reasoning hierarchically in autonomous robots. In addition, several works have demonstrated how combining robot affordance learning with language grounding can provide cognitive robots with new and useful skills, such as learning the association of spoken words with sensorimotor experience<cite> [10,</cite> 11] or sensorimotor representations [12] , learning tool use capabilities [13, 14] , and carrying out complex manipulation tasks expressed in natural language instructions which require planning and reasoning [15] . In <cite>[10]</cite> , a joint model is proposed to learn robot affordances (i. e., relationships between actions, objects and resulting effects) together with word meanings. The data contains robot manipulation experiments, each of them associated with a number of alternative verbal descriptions uttered by two speakers for a total of 1270 recordings. That framework assumes that the robot action is known a priori during the training phase (e. g., the information \"grasping\" during a grasping experiment is given), and the resulting model can be used at testing to make inferences about the environment, including estimating the most likely action, based on evidence from other pieces of information. Several neuroscience and psychology studies build upon the theory of mirror neurons which we brought up in the Introduction.",
  "y": "background"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_1",
  "x": "The usefulness of affordances in cognitive robotics is in the fact that they capture essential properties of environment objects in terms of the actions that a robot is able to perform with them [7, 8] . Some authors have suggested an alternative computational model called Object-Action Complexes (OACs) [9] , which links low-level sensorimotor knowledge with high-level symbolic reasoning hierarchically in autonomous robots. In addition, several works have demonstrated how combining robot affordance learning with language grounding can provide cognitive robots with new and useful skills, such as learning the association of spoken words with sensorimotor experience<cite> [10,</cite> 11] or sensorimotor representations [12] , learning tool use capabilities [13, 14] , and carrying out complex manipulation tasks expressed in natural language instructions which require planning and reasoning [15] . In <cite>[10]</cite> , a joint model is proposed to learn robot affordances (i. e., relationships between actions, objects and resulting effects) together with word meanings. The data contains robot manipulation experiments, each of them associated with a number of alternative verbal descriptions uttered by two speakers for a total of 1270 recordings. That framework assumes that the robot action is known a priori during the training phase (e. g., the information \"grasping\" during a grasping experiment is given), and the resulting model can be used at testing to make inferences about the environment, including estimating the most likely action, based on evidence from other pieces of information. Several neuroscience and psychology studies build upon the theory of mirror neurons which we brought up in the Introduction. These studies indicate that perceptual input can be linked with the human action system for predicting future outcomes of actions, i. e., the effect of actions, particularly when the person possesses concrete personal experience of the actions being observed in others [16, 17] . This has also been exploited under the deep learning paradigm [18] , by using a Multiple Timescales Recurrent Neural Network (MTRNN) to have an artificial simulated agent infer human intention from joint information about object affordances and human actions.",
  "y": "background"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_2",
  "x": "One difference between this line of research and ours is that we use real, noisy data acquired from robots and sensors to test our models, rather than virtual simulations. ---------------------------------- **PROPOSED APPROACH** In this paper, we combine (1) the robot affordance model of <cite>[10]</cite> , which associates verbal descriptions to the physical interactions of an agent with the environment, with (2) the gesture recognition system of [4] , which infers the type of action from human user movements. We consider three manipulative gestures corresponding to physical actions performed by agent(s) onto objects on a table (see Fig. 1 ): grasp, tap, and touch. We reason on the effects of these actions onto the objects of the world, and on the co-occurring verbal description of the experiments. In the complete framework, we will use Bayesian Networks (BNs), which are a probabilistic model that represents random variables and conditional dependencies on a graph, such as in Fig. 2 . One of the advantages of using BNs is that their expressive power allows the marginalization over any set of variables given any other set of variables. Our main contribution is that of extending <cite>[10]</cite> by relaxing the assumption that the action is known during the learning phase.",
  "y": "differences extends"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_3",
  "x": "In the complete framework, we will use Bayesian Networks (BNs), which are a probabilistic model that represents random variables and conditional dependencies on a graph, such as in Fig. 2 . One of the advantages of using BNs is that their expressive power allows the marginalization over any set of variables given any other set of variables. Our main contribution is that of extending <cite>[10]</cite> by relaxing the assumption that the action is known during the learning phase. This assumption is acceptable when the robot learns through self-exploration and interaction with the environment, but must be relaxed if the robot needs to generalize the acquired knowledge through the observation of another (human) agent. We estimate the action performed by a human user during a human-robot collaborative task, by employing statistical inference methods and Hidden Markov Models (HMMs). This provides two advantages. First, we can infer the executed action during training. Secondly, at testing time we can merge the action information obtained from gesture recognition with the information about affordances. ----------------------------------",
  "y": "differences extends"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_4",
  "x": "Following the method adopted in <cite>[10]</cite> , we use a Bayesian probabilistic framework to allow a robot to ground the basic world behavior and verbal descriptions associated to it. The world behavior is defined by random variables describing: the actions A, defined over the set A = {ai}, object properties F , over F = {fi}, and effects E, over E = {ei}. We denote X = {A, F, E} the state of the world as experienced by the robot. The verbal descriptions are denoted by the set of words W = {wi}. Consequently, the relationships between words and concepts are expressed by the joint probability distribution p(X, W ) of actions, object features, effects, and words in the spoken utterance. The symbolic variables and their discrete values are listed in Table 1 . In addition to the symbolic variables, the model also includes word variables, describing Figure 3 : Structure of the HMMs used for human gesture recognition, adapted from [4] . In this work, we consider three independent, multiple-state HMMs, each of them trained to recognize one of the considered manipulation gestures. the probability of each word co-occurring in the verbal description associated to a robot experiment in the environment. This joint probability distribution, that is illustrated by the part of Fig. 2 enclosed in the dashed box, is estimated by the robot in an ego-centric way through interaction with the environment, as in <cite>[10]</cite> . As a consequence, during learning, the robot knows what action it is performing with certainty, and the variable A assumes a deterministic value.",
  "y": "similarities uses"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_5",
  "x": "This joint probability distribution, that is illustrated by the part of Fig. 2 enclosed in the dashed box, is estimated by the robot in an ego-centric way through interaction with the environment, as in <cite>[10]</cite> . As a consequence, during learning, the robot knows what action it is performing with certainty, and the variable A assumes a deterministic value. This assumption is relaxed in the present study, by extending the model to the observation of external (human) agents as explained below. ---------------------------------- **HIDDEN MARKOV MODELS FOR GESTURE RECOGNITION** As for the gesture recognition HMMs, we use the models that we previously trained in [4] for spotting the manipulationrelated gestures under consideration. Our input features are the 3D coordinates of the tracked human hand: the coordinates are obtained with a commodity depth sensor, then transformed to be centered on the person torso (to be invariant to the distance of the user from the sensor) and normalized to account for variability in amplitude (to be invariant to wide/emphatic vs narrow/subtle executions of the same gesture class). The gesture recognition models are represented in Fig. 3 , and correspond to the Gesture HMMs block in Fig. 2 . The HMM for one gesture is defined by a set of (hidden) discrete states S = {s1, . . . , sQ} which model the temporal phases comprising the dynamic execution of the gesture, and by a set of parameters \u03bb = {A, B, \u03a0}, where A = {aij} is the transition probability matrix, aij is the transition probability from state si at time t to state sj at time t + 1, B = {fi} is the set of Q observation probability functions (one per state i) with continuous mixtures of Gaussian values, and \u03a0 is the initial probability distribution for the states.",
  "y": "background"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_6",
  "x": "In Sec. 3.3, we discuss different ways in which the output information of the gesture recognizer can be combined with the Bayesian Network of words and affordances. ---------------------------------- **COMBINING THE BN WITH GESTURE HMMS** In this study we wish to generalize the model of <cite>[10]</cite> by observing external (human) agents, as shown in Fig. 1 . For this reason, the full model is now extended with a perception module capable of inferring the action of the agent from visual inputs. This corresponds to the Gesture HMMs block in Fig. 2 . The Affordance-Words Bayesian Network (BN) model and the Gestures HMMs may be combined in different ways [19] : 1. the Gesture HMMs may provide a hard decision on the action performed by the human (i. e., considering only the top result) to the BN, 2. the Gesture HMMs may provide a posterior distribution (i. e., soft decision) to the BN, 3. if the task is to infer the action, the posterior from the Gesture HMMs and the one from the BN may be combined as follows, assuming that they provide independent information:",
  "y": "extends"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_7",
  "x": "**COMBINING THE BN WITH GESTURE HMMS** In this study we wish to generalize the model of <cite>[10]</cite> by observing external (human) agents, as shown in Fig. 1 . For this reason, the full model is now extended with a perception module capable of inferring the action of the agent from visual inputs. This corresponds to the Gesture HMMs block in Fig. 2 . The Affordance-Words Bayesian Network (BN) model and the Gestures HMMs may be combined in different ways [19] : 1. the Gesture HMMs may provide a hard decision on the action performed by the human (i. e., considering only the top result) to the BN, 2. the Gesture HMMs may provide a posterior distribution (i. e., soft decision) to the BN, 3. if the task is to infer the action, the posterior from the Gesture HMMs and the one from the BN may be combined as follows, assuming that they provide independent information: In the experimental section, we will show that what the robot has learned subjectively or alone (by self-exploration, knowing the action identity as a prior <cite>[10]</cite> ), can subsequently be used when observing a new agent (human), provided that the actions can be estimated with Gesture HMMs as in [4] . ----------------------------------",
  "y": "background"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_0",
  "x": "By sampling the code vector from specific regions of this latent space during decoding and imposing prior attention generated from it in the seq2seq model, output can be steered towards having certain attributes. This is demonstrated for the task of sentence simplification, where the latent code vector allows control over output length and lexical simplification, and enables fine-tuning to optimize for different evaluation metrics. ---------------------------------- **INTRODUCTION** Apart from its application to machine translation, the encoder-decoder or sequence-to-sequence (seq2seq) paradigm has been successfully applied to monolingual text-to-text tasks including simplification <cite>(Nisioi et al., 2017)</cite> , paraphrasing (Mallinson et al., 2017) , style transfer (Jhamtani et al., 2017) , sarcasm interpretation (Peled and Reichart, 2017) , automated lyric annotation (Sterckx et al., 2017) and dialogue systems (Serban et al., 2016) . A sequence of input tokens is encoded to a series of hidden states using an encoder network and decoded to a target domain by a decoder network. During decoding, an attention mechanism is used to indicate which are the relevant input tokens at each step. This attention component is computed as an intermediate part of the model, and is trained jointly with the rest of the model. Alongside being crucial for effective translation, attention -while not necessarily correlated with human attention -brings interpretability to seq2seq models by visualizing how individual input elements contribute to the model's decisions.",
  "y": "background"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_1",
  "x": "The goal of sentence simplification is to reduce the linguistic complexity of text, while still retaining its original information and meaning. It has been suggested that sentence simplification can be defined by three major types of operations: splitting, deletion, and paraphrasing (Shardlow, 2014) . We hypothesize that these operations occur at varying frequencies in the training data. We adopt our model in an attempt to capture these operations into attention matrices and the latent vector space, and thus control the form and degree of simplification through sampling from that space. We train on the Wikilarge collection used by Zhu (2010) . Wikilarge is a collection of 296,402 automatically aligned complex and simple sentences from the ordinary and simple English Wikipedia corpora, used extensively in previous work (Wubben et al., 2012; Woodsend and Lapata, 2011; Zhang and Lapata, 2017;<cite> Nisioi et al., 2017)</cite> . The training data includes 2,000 development and 359 test instances created by Xu et al. (2016) . These are complex sentences paired with simplifications provided by Amazon Mechanical Turk workers and provide a more reliable evaluation of the task. ----------------------------------",
  "y": "background"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_2",
  "x": "The training data includes 2,000 development and 359 test instances created by Xu et al. (2016) . These are complex sentences paired with simplifications provided by Amazon Mechanical Turk workers and provide a more reliable evaluation of the task. ---------------------------------- **HYPERPARAMETERS AND OPTIMIZATION** We extend the OpenNMT (Klein et al., 2017) framework with functions for attention generation and release our code as a submodule. We use a similar archi- Table 2 : Quantitative evaluation of existing baselines from previous work and seq2seq with prior attention from the CVAE when choosing an optimal z sample for BLEU scores. tecture as Zhu et al. (2010) and<cite> Nisioi et al. (2017)</cite> : 2 layers of stacked unidirectional LSTMs with bi-linear global attention as proposed by Luong et al. (2015) , with hidden states of 512 dimensions. The vocabulary is reduced to the 50,000 most frequent tokens and embedded in a shared 500-dimensional space. We train using SGD with batches of 64 samples for 13 epochs after which the autoencoder is trained by translating sequences from training data.",
  "y": "uses"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_3",
  "x": "Inclusion of an attention mechanism was instrumental to match existing baselines. Our standard seq2seq model with attention, without prior attention, obtains a score of 89.92 BLEU points, which is close to scores obtained by similar models used in existing 1 Fleish-Kincaid Grade Level index. work on neural text simplification (Zhang and Lapata, 2017;<cite> Nisioi et al., 2017)</cite> . In Table 2 , we compare our seq2seq model with attention and without prior attention. A value for BLEU of 90.14 is found for z =[\u22122,0] which was tuned on a development set. For the same z value, a SARI value of 38.30 was reached. For comparison, we include the SMT-based model by (Wubben et al., 2012) , the NTS model by <cite>(Nisioi et al., 2017)</cite> and the EncDecA by (Zhang and Lapata, 2017) . For decreasing values of the first hidden dimension z 1 , we observe that attention becomes situated at the diagonal, thus keeping closer to the structure of the source sentence and having one-to-one word alignments. For increasing values of z 1 , attention becomes more vertical and focused on single encoder states.",
  "y": "similarities"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_4",
  "x": "A value for BLEU of 90.14 is found for z =[\u22122,0] which was tuned on a development set. For the same z value, a SARI value of 38.30 was reached. For comparison, we include the SMT-based model by (Wubben et al., 2012) , the NTS model by <cite>(Nisioi et al., 2017)</cite> and the EncDecA by (Zhang and Lapata, 2017) . For decreasing values of the first hidden dimension z 1 , we observe that attention becomes situated at the diagonal, thus keeping closer to the structure of the source sentence and having one-to-one word alignments. For increasing values of z 1 , attention becomes more vertical and focused on single encoder states. This type of attention gives more control to the language model, as exemplified by output samples shown in Table 1 . Output from this region is far longer and less related to the source sentence. Influence of the second latent variable z 2 is less apparent from the attention matrices. However, sampling across this dimension shows large effects on evaluation metrics.",
  "y": "uses"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_0",
  "x": "**ABSTRACT** In this work, we investigate the task of textual response generation in a multimodal task-oriented dialogue system. Our work is based on the recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> in the fashion domain. We introduce a multimodal extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model and show that this extension outperforms strong baselines in terms of text-based similarity metrics. We also showcase the shortcomings of current vision and language models by performing an error analysis on our system's output. ---------------------------------- **INTRODUCTION** This work aims to learn strategies for textual response generation in a multimodal conversation directly from data. Conversational AI has great potential for online retail: It greatly enhances user experience and in turn directly affects user retention (Chai et al., 2000) , especially if the interaction is multi-modal in nature.",
  "y": "extends"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_1",
  "x": "We introduce a multimodal extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model and show that this extension outperforms strong baselines in terms of text-based similarity metrics. We also showcase the shortcomings of current vision and language models by performing an error analysis on our system's output. ---------------------------------- **INTRODUCTION** This work aims to learn strategies for textual response generation in a multimodal conversation directly from data. Conversational AI has great potential for online retail: It greatly enhances user experience and in turn directly affects user retention (Chai et al., 2000) , especially if the interaction is multi-modal in nature. So far, most conversational agents are uni-modal -ranging from opendomain conversation (Ram et al., 2018; Papaioannou et al., 2017; Fang et al., 2017) to task oriented dialogue systems Lemon, 2010, 2011; Young et al., 2013; Singh et al., 2000; Wen et al., 2016) . While recent progress in deep learning has unified research at the intersection of vision and language, the availability of open-source multimodal dialogue datasets still remains a bottleneck. This research makes use of a recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> , which contains multiple dialogue sessions in the fashion domain.",
  "y": "uses"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_2",
  "x": "The <cite>MMD</cite> dataset provides an interesting new challenge, combining recent efforts on task-oriented dialogue systems, as well as visually grounded dialogue. In contrast to simple QA tasks in visually grounded dialogue, e.g. (Antol et al., 2015) , <cite>it contains</cite> conversations with a clear end-goal. However, in contrast to previous slot-filling dialogue systems, e.g. (Rieser and Lemon, 2011; Young et al., 2013) , <cite>it heavily relies on</cite> the extra visual modality to drive the conversation forward (see Figure 1) . In the following, we propose a fully data-driven response generation model for this task. Our work is able to ground the system's textual response with language and images by learning the semantic correspondence between them while modelling long-term dialogue context. Lu et al., 2016) . In contrast to standard sequenceto-sequence models (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) , HREDs model the dialogue context by introducing a context Recurrent Neural Network (RNN) over the encoder RNN, thus forming a hierarchical encoder. We build on top of the HRED architecture to include multimodality over multiple images. A simple HRED consists of three RNN modules: encoder, context and decoder.",
  "y": "background"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_3",
  "x": "This work aims to learn strategies for textual response generation in a multimodal conversation directly from data. Conversational AI has great potential for online retail: It greatly enhances user experience and in turn directly affects user retention (Chai et al., 2000) , especially if the interaction is multi-modal in nature. So far, most conversational agents are uni-modal -ranging from opendomain conversation (Ram et al., 2018; Papaioannou et al., 2017; Fang et al., 2017) to task oriented dialogue systems Lemon, 2010, 2011; Young et al., 2013; Singh et al., 2000; Wen et al., 2016) . While recent progress in deep learning has unified research at the intersection of vision and language, the availability of open-source multimodal dialogue datasets still remains a bottleneck. This research makes use of a recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> , which contains multiple dialogue sessions in the fashion domain. The <cite>MMD</cite> dataset provides an interesting new challenge, combining recent efforts on task-oriented dialogue systems, as well as visually grounded dialogue. In contrast to simple QA tasks in visually grounded dialogue, e.g. (Antol et al., 2015) , <cite>it contains</cite> conversations with a clear end-goal. However, in contrast to previous slot-filling dialogue systems, e.g. (Rieser and Lemon, 2011; Young et al., 2013) , <cite>it heavily relies on</cite> the extra visual modality to drive the conversation forward (see Figure 1) . In the following, we propose a fully data-driven response generation model for this task.",
  "y": "background"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_4",
  "x": "However, in contrast to previous slot-filling dialogue systems, e.g. (Rieser and Lemon, 2011; Young et al., 2013) , <cite>it heavily relies on</cite> the extra visual modality to drive the conversation forward (see Figure 1) . In the following, we propose a fully data-driven response generation model for this task. Our work is able to ground the system's textual response with language and images by learning the semantic correspondence between them while modelling long-term dialogue context. Lu et al., 2016) . In contrast to standard sequenceto-sequence models (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) , HREDs model the dialogue context by introducing a context Recurrent Neural Network (RNN) over the encoder RNN, thus forming a hierarchical encoder. We build on top of the HRED architecture to include multimodality over multiple images. A simple HRED consists of three RNN modules: encoder, context and decoder. In multimodal HRED, we combine the output representations from the utterance encoder with concatenated multiple image representations and pass them as input to the context encoder (see Figure 2) . A dialogue is modelled as a sequence of utterances (turns), which in turn are modelled as sequences of words and images.",
  "y": "background"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_5",
  "x": "During generation, the decoder conditions on the previous output token. <cite>Saha et al. (2017)</cite> propose a similar baseline model for the <cite>MMD</cite> dataset, extending HREDs to include the visual modality. However, for simplicity's sake, they 'unroll' multiple images in a single utterance to include only one image per utterance. While computationally leaner, this approach ultimately loses the objective of capturing multimodality over the context of multiple images and text. In contrast, we combine all the image representations in the utterance using a linear layer. We argue that modelling all images is necessary to answer questions that address previous agent responses. For example in Figure 3 , when the user asks \"what about the 4th image?\", it is impossible to give a correct response without reasoning over all images in the previous response. In the following, we empirically show that our extension leads to better results in terms of text-based similarity measures, as well as quality of generated dialogues. Example contexts for a given system utterance; note the difference in our approach from <cite>Saha et al. (2017)</cite> when extracting the training data from the original chat logs.",
  "y": "similarities"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_6",
  "x": "However, for simplicity's sake, they 'unroll' multiple images in a single utterance to include only one image per utterance. While computationally leaner, this approach ultimately loses the objective of capturing multimodality over the context of multiple images and text. In contrast, we combine all the image representations in the utterance using a linear layer. We argue that modelling all images is necessary to answer questions that address previous agent responses. For example in Figure 3 , when the user asks \"what about the 4th image?\", it is impossible to give a correct response without reasoning over all images in the previous response. In the following, we empirically show that our extension leads to better results in terms of text-based similarity measures, as well as quality of generated dialogues. Example contexts for a given system utterance; note the difference in our approach from <cite>Saha et al. (2017)</cite> when extracting the training data from the original chat logs. For simplicity, in this illustration we consider a context size of 2 previous utterances. '|' differentiates turns for a given context.",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_7",
  "x": "Each session contains an average of 40 dialogue turns (average of 8 words per textual response, 4 images per image response). <cite>The data</cite> contains complex user queries, which pose new challenges for multimodal, task-based dialogue, such as quantitative inference (sorting, counting and filtering): \"Show me more images of the 3rd product in some different directions\", inference using domain knowledge and long term context: \"Will the 5th result go well with a large sized messenger bag?\", inference over aggregate of images: \"List more in the upper material of the 5th image and style as the 3rd and the 5th\", co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of <cite>the dataset</cite> for the model. This is done since <cite>the authors</cite> originally consider each image as a different context, while we consider all the images in a single turn as one concatenated context (cf. Figure 3 ). ---------------------------------- **IMPLEMENTATION** We use the PyTorch 1 framework (Paszke et al., 2017) for our implementation. 2 We used 512 as the word embedding size as well as hidden dimension for all the RNNs using GRUs (Cho et al., 2014) with tied embeddings for the (bidirectional) encoder and decoder. The decoder uses Luong-style attention mechanism (Luong et al., 2015) with input feeding.",
  "y": "extends"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_8",
  "x": "**EXPERIMENTS AND RESULTS** ---------------------------------- **DATASET** The <cite>MMD</cite> dataset <cite>(Saha et al., 2017)</cite> consists of 100/11/11k train/validation/test chat sessions comprising 3.5M context-response pairs for the model. Each session contains an average of 40 dialogue turns (average of 8 words per textual response, 4 images per image response). <cite>The data</cite> contains complex user queries, which pose new challenges for multimodal, task-based dialogue, such as quantitative inference (sorting, counting and filtering): \"Show me more images of the 3rd product in some different directions\", inference using domain knowledge and long term context: \"Will the 5th result go well with a large sized messenger bag?\", inference over aggregate of images: \"List more in the upper material of the 5th image and style as the 3rd and the 5th\", co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of <cite>the dataset</cite> for the model. This is done since <cite>the authors</cite> originally consider each image as a different context, while we consider all the images in a single turn as one concatenated context (cf. Figure 3 ). ----------------------------------",
  "y": "differences motivation"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_9",
  "x": "We report sentence-level BLEU-4 (Papineni et al., 2002) , METEOR (Lavie and Agarwal, 2007) and ROUGE-L (Lin and Och, 2004) using the evaluation scripts provided by (Sharma et al., 2017) . We compare our results against <cite>Saha et al. (2017)</cite> by using <cite>their code</cite> and data-generation scripts. 4 Note that the results reported in <cite>their paper</cite> are on a different version of the corpus, hence not directly comparable. Table 1 provides results for different configurations of our model (\"T\" stands for text-only in the encoder, \"M\" for multimodal, and \"attn\" for using attention in the decoder). We experimented with different context sizes and found that output quality improved with increased context size (models with 5-turn context perform better than those with a 2-turn context), confirming the observation by Serban et al. (2016 Serban et al. ( , 2017 . 5 Using attention clearly helps: even T-HRED-attn outperforms M-HRED (without attention) for the same context size. We also tested whether multimodal input has an impact on the generated outputs. However, there was only a slight increase in BLEU score (M-HRED-attn vs T-HRED-attn). To summarize, our best performing model (M-HRED-attn) outperforms the model of <cite>Saha et al.</cite> by 7 BLEU points.",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_10",
  "x": "To summarize, our best performing model (M-HRED-attn) outperforms the model of <cite>Saha et al.</cite> by 7 BLEU points. 6 This can be primarily attributed to the way we created the input for our model from raw chat logs, as well as incorporating more information during decoding via attention. Figure 4 provides example output utterances using M-HRED-attn with a context size of 5. Our model is able to accurately map the response to previous textual context turns as shown in (a) and (c). In (c), it is able to capture that the user is asking about the style in the 1st and 2nd image. (d) shows an example where our model is able to relate that the corresponding product is 'jeans' from visual features, while it is not able to model finegrained details like in (b) that the style is 'casual fit' but resorts to 'woven'. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this research, we address the novel task of response generation in search-based multimodal dialogue by learning from the recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> .",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_11",
  "x": "In (c), it is able to capture that the user is asking about the style in the 1st and 2nd image. (d) shows an example where our model is able to relate that the corresponding product is 'jeans' from visual features, while it is not able to model finegrained details like in (b) that the style is 'casual fit' but resorts to 'woven'. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this research, we address the novel task of response generation in search-based multimodal dialogue by learning from the recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> . We introduce a novel extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model (Serban et al., 2016) and show that our implementation significantly outperforms the model of <cite>Saha et al. (2017)</cite> by modelling the full multimodal context. Contrary to <cite>their results</cite>, our generation outputs improved by adding attention and increasing context size. However, we also show that multimodal HRED does not improve significantly over text-only HRED, similar to observations by Agrawal et al. (2016) and Qian et al. (2018) . Our model learns to handle textual correspondence between the questions and answers, while mostly ignoring the visual context.",
  "y": "motivation"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_12",
  "x": "Our model is able to accurately map the response to previous textual context turns as shown in (a) and (c). In (c), it is able to capture that the user is asking about the style in the 1st and 2nd image. (d) shows an example where our model is able to relate that the corresponding product is 'jeans' from visual features, while it is not able to model finegrained details like in (b) that the style is 'casual fit' but resorts to 'woven'. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this research, we address the novel task of response generation in search-based multimodal dialogue by learning from the recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> . We introduce a novel extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model (Serban et al., 2016) and show that our implementation significantly outperforms the model of <cite>Saha et al. (2017)</cite> by modelling the full multimodal context. Contrary to <cite>their results</cite>, our generation outputs improved by adding attention and increasing context size. However, we also show that multimodal HRED does not improve significantly over text-only HRED, similar to observations by Agrawal et al. (2016) and Qian et al. (2018) .",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_13",
  "x": "(d) shows an example where our model is able to relate that the corresponding product is 'jeans' from visual features, while it is not able to model finegrained details like in (b) that the style is 'casual fit' but resorts to 'woven'. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this research, we address the novel task of response generation in search-based multimodal dialogue by learning from the recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> . We introduce a novel extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model (Serban et al., 2016) and show that our implementation significantly outperforms the model of <cite>Saha et al. (2017)</cite> by modelling the full multimodal context. Contrary to <cite>their results</cite>, our generation outputs improved by adding attention and increasing context size. However, we also show that multimodal HRED does not improve significantly over text-only HRED, similar to observations by Agrawal et al. (2016) and Qian et al. (2018) . Our model learns to handle textual correspondence between the questions and answers, while mostly ignoring the visual context. This indicates that we need better visual models to en-code the image representations when he have multiple similar-looking images, e.g., black hats in Figure 3 .",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_14",
  "x": "---------------------------------- **ANALYSIS AND RESULTS** We report sentence-level BLEU-4 (Papineni et al., 2002) , METEOR (Lavie and Agarwal, 2007) and ROUGE-L (Lin and Och, 2004) using the evaluation scripts provided by (Sharma et al., 2017) . We compare our results against <cite>Saha et al. (2017)</cite> by using <cite>their code</cite> and data-generation scripts. 4 Note that the results reported in <cite>their paper</cite> are on a different version of the corpus, hence not directly comparable. Table 1 provides results for different configurations of our model (\"T\" stands for text-only in the encoder, \"M\" for multimodal, and \"attn\" for using attention in the decoder). We experimented with different context sizes and found that output quality improved with increased context size (models with 5-turn context perform better than those with a 2-turn context), confirming the observation by Serban et al. (2016 Serban et al. ( , 2017 . 5 Using attention clearly helps: even T-HRED-attn outperforms M-HRED (without attention) for the same context size. We also tested whether multimodal input has an impact on the generated outputs.",
  "y": "uses"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_15",
  "x": "We concatenate the representation vector of all images in one turn of a dialogue to form the image context. If there is no image in the utterance, we consider a 0 4096 vector to form the image context. In this work, we focus only on the textual response of the agent. ---------------------------------- **EXPERIMENTS AND RESULTS** ---------------------------------- **DATASET** The <cite>MMD</cite> dataset <cite>(Saha et al., 2017)</cite> consists of 100/11/11k train/validation/test chat sessions comprising 3.5M context-response pairs for the model. Each session contains an average of 40 dialogue turns (average of 8 words per textual response, 4 images per image response).",
  "y": "background"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_16",
  "x": "---------------------------------- **DATASET** The <cite>MMD</cite> dataset <cite>(Saha et al., 2017)</cite> consists of 100/11/11k train/validation/test chat sessions comprising 3.5M context-response pairs for the model. Each session contains an average of 40 dialogue turns (average of 8 words per textual response, 4 images per image response). <cite>The data</cite> contains complex user queries, which pose new challenges for multimodal, task-based dialogue, such as quantitative inference (sorting, counting and filtering): \"Show me more images of the 3rd product in some different directions\", inference using domain knowledge and long term context: \"Will the 5th result go well with a large sized messenger bag?\", inference over aggregate of images: \"List more in the upper material of the 5th image and style as the 3rd and the 5th\", co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of <cite>the dataset</cite> for the model. This is done since <cite>the authors</cite> originally consider each image as a different context, while we consider all the images in a single turn as one concatenated context (cf. Figure 3 ). ---------------------------------- **IMPLEMENTATION**",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_0",
  "x": "is study is signi cant as it presents one of the initial stance detection data sets proposed so far and the rst one for Turkish language, to the best of our knowledge. e data set and the evaluation results of the corresponding SVM-based approaches will form plausible baselines for the comparison of future studies on stance detection. ---------------------------------- **INTRODUCTION** Stance detection (also called stance identi cation or stance classication) is one of the considerably recent research topics in natural language processing (NLP). It is usually de ned as a classi cation problem where for a text and target pair, the stance of the author of the text for that target is expected as a classi cation output from the set: {Favor, Against, Neither} <cite>[12]</cite> . Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_1",
  "x": "Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIDEWAYS'17, Prague, Czech Republic Stance detection is usually considered as a subtask of sentiment analysis (opinion mining) [13] topic in NLP. Both are mostly performed on social media texts, particularly on tweets, hence both are important components of social media analysis. Nevertheless, in sentiment analysis, the sentiment of the author of a piece of text usually as Positive, Negative, and Neutral is explored while in stance detection, the stance of the author of the text for a particular target (an entity, event, etc.) either explicitly or implicitly referred to in the text is considered. Like sentiment analysis, stance detection systems can be valuable components of information retrieval and other text analysis systems <cite>[12]</cite> . Previous work on stance detection include [16] where a stance classi er based on sentiment and arguing features is proposed in addition to an arguing lexicon automatically compiled. e ultimate approach performs be er than distribution-based and unigram-based baseline systems [16] .",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_2",
  "x": "Stance detection on a corpus of student essays is considered in [5] . A er using linguistically-motivated feature sets together with multivalued NB and SVM as the learning models, the authors conclude that they outperform two baseline approaches [5] . In [4] , the author claims that Wikipedia can be used to determine stances about controversial topics based on their previous work regarding controversy extraction on the Web. Among more recent related work, in [1] stance detection for unseen targets is studied and bidirectional conditional encoding is employed. e authors state that their approach achieves stateof-the art performance rates [1] on SemEval 2016 Twi er Stance Detection corpus <cite>[12]</cite> . In [3] , a stance-community detection approach called SCIFNET is proposed. SCIFNET creates networks of people who are stance targets, automatically from the related document collections [3] using stance expansion and re nement techniques to arrive at stance-coherent networks. A tweet data set annotated with stance information regarding six prede ned targets is proposed in [11] where this data set is annotated through crowdsourcing. e authors indicate that the data set is also annotated with sentiment information in addition to stance, so it can help SIDEWAYS'17, July 2017, Prague, Czech Republic D. K\u00fc\u00e7\u00fck reveal associations between stance and sentiment [11] .",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_3",
  "x": "Among more recent related work, in [1] stance detection for unseen targets is studied and bidirectional conditional encoding is employed. e authors state that their approach achieves stateof-the art performance rates [1] on SemEval 2016 Twi er Stance Detection corpus <cite>[12]</cite> . In [3] , a stance-community detection approach called SCIFNET is proposed. SCIFNET creates networks of people who are stance targets, automatically from the related document collections [3] using stance expansion and re nement techniques to arrive at stance-coherent networks. A tweet data set annotated with stance information regarding six prede ned targets is proposed in [11] where this data set is annotated through crowdsourcing. e authors indicate that the data set is also annotated with sentiment information in addition to stance, so it can help SIDEWAYS'17, July 2017, Prague, Czech Republic D. K\u00fc\u00e7\u00fck reveal associations between stance and sentiment [11] . Lastly, in <cite>[12]</cite> , SemEval 2016's aforementioned shared task on Twi er Stance Detection is described. Also provided are the results of the evaluations of 19 systems participating in two subtasks (one with training data set provided and the other without an annotated data set) of the shared task <cite>[12]</cite> . In this paper, we present a tweet data set in Turkish annotated with stance information, where the corresponding annotations are made publicly available.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_4",
  "x": "In [3] , a stance-community detection approach called SCIFNET is proposed. SCIFNET creates networks of people who are stance targets, automatically from the related document collections [3] using stance expansion and re nement techniques to arrive at stance-coherent networks. A tweet data set annotated with stance information regarding six prede ned targets is proposed in [11] where this data set is annotated through crowdsourcing. e authors indicate that the data set is also annotated with sentiment information in addition to stance, so it can help SIDEWAYS'17, July 2017, Prague, Czech Republic D. K\u00fc\u00e7\u00fck reveal associations between stance and sentiment [11] . Lastly, in <cite>[12]</cite> , SemEval 2016's aforementioned shared task on Twi er Stance Detection is described. Also provided are the results of the evaluations of 19 systems participating in two subtasks (one with training data set provided and the other without an annotated data set) of the shared task <cite>[12]</cite> . In this paper, we present a tweet data set in Turkish annotated with stance information, where the corresponding annotations are made publicly available. e domain of the tweets comprises two popular football clubs which constitute the targets of the tweets included. We also provide the evaluation results of SVM classi ers (for each target) on this data set using unigram, bigram, and hashtag features.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_5",
  "x": "**STANCE DETECTION EXPERIMENTS USING SVM CLASSIFIERS** It is emphasized in the related literature that unigram-based methods are reliable for the stance detection task [16] and similarly unigram-based models have been used as baseline models in studies such as <cite>[12]</cite> . In order to be used as a baseline and reference system for further studies on stance detection in Turkish tweets, we have trained two SVM classi ers (one for each target) using unigrams as features. Before the extraction of unigrams, we have employed automated preprocessing to lter out the stopwords in our annotated data set of 700 tweets. e stopword list used is the list presented in [8] which, in turn, is the slightly extended version of the stopword list provided in [2] . We have used the SVM implementation available in the Weka data mining application [6] where this particular implementation employs the SMO algorithm [14] to train a classi er with a linear kernel. e 10-fold cross-validation results of the two classi ers are provided in Table 1 using the metrics of precision, recall, and F-Measure. e evaluation results are quite favorable for both targets and particularly higher for Target-1, considering the fact that they are the initial experiments on the data set. e performance of the classi ers is be er for the Favor class for both targets when compared with the performance results for the Against class.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_6",
  "x": "Before the extraction of unigrams, we have employed automated preprocessing to lter out the stopwords in our annotated data set of 700 tweets. e stopword list used is the list presented in [8] which, in turn, is the slightly extended version of the stopword list provided in [2] . We have used the SVM implementation available in the Weka data mining application [6] where this particular implementation employs the SMO algorithm [14] to train a classi er with a linear kernel. e 10-fold cross-validation results of the two classi ers are provided in Table 1 using the metrics of precision, recall, and F-Measure. e evaluation results are quite favorable for both targets and particularly higher for Target-1, considering the fact that they are the initial experiments on the data set. e performance of the classi ers is be er for the Favor class for both targets when compared with the performance results for the Against class. is outcome may be due to the common use of some terms when expressing positive stance towards sports clubs in Turkish tweets. e same percentage of common terms may not have been observed in tweets during the expression of negative stances towards the targets. Yet, completely the opposite pa ern is observed in stance detection results of baseline systems given in <cite>[12]</cite> , i.e., be er FMeasure rates have been obtained for the Against class when compared with the Favor class <cite>[12]</cite> .",
  "y": "differences"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_7",
  "x": "Some of the baseline systems reported in <cite>[12]</cite> are SVM-based systems using unigrams and ngrams as features similar to our study, but their data sets include all three stance classes of Favor, Against, and Neither, while our data set comprises only tweets classi ed as belonging to Favor or Against classes. Another di erence is that the data sets in <cite>[12]</cite> have been divided into training and test sets, while in our study we provide 10-fold cross-validation results on the whole data set. On the other hand, we should also note that SVM-based sentiment analysis systems (such as those given in [15] ) have been reported to achieve be er F-Measure rates for the Positive sentiment class when compared with the results obtained for the Negative class. erefore, our evaluation results for each stance class seem to be in line with such sentiment analysis systems. Yet, further experiments on the extended versions of our data set should be conducted and the results should again be compared with the stance detection results given in the literature. We have also evaluated SVM classi ers which use only bigrams as features, as ngram-based classi ers have been reported to perform be er for the stance detection problem <cite>[12]</cite> . However, we have observed that using bigrams as the sole features of the SVM classi ers leads to quite poor results. is observation may be due to the relatively limited size of the tweet data set employed. Still, we can conclude that unigram-based features lead to superior results compared to the results obtained using bigrams as features, based on our experiments on our data set.",
  "y": "differences"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_8",
  "x": "We have used the SVM implementation available in the Weka data mining application [6] where this particular implementation employs the SMO algorithm [14] to train a classi er with a linear kernel. e 10-fold cross-validation results of the two classi ers are provided in Table 1 using the metrics of precision, recall, and F-Measure. e evaluation results are quite favorable for both targets and particularly higher for Target-1, considering the fact that they are the initial experiments on the data set. e performance of the classi ers is be er for the Favor class for both targets when compared with the performance results for the Against class. is outcome may be due to the common use of some terms when expressing positive stance towards sports clubs in Turkish tweets. e same percentage of common terms may not have been observed in tweets during the expression of negative stances towards the targets. Yet, completely the opposite pa ern is observed in stance detection results of baseline systems given in <cite>[12]</cite> , i.e., be er FMeasure rates have been obtained for the Against class when compared with the Favor class <cite>[12]</cite> . Some of the baseline systems reported in <cite>[12]</cite> are SVM-based systems using unigrams and ngrams as features similar to our study, but their data sets include all three stance classes of Favor, Against, and Neither, while our data set comprises only tweets classi ed as belonging to Favor or Against classes. Another di erence is that the data sets in <cite>[12]</cite> have been divided into training and test sets, while in our study we provide 10-fold cross-validation results on the whole data set.",
  "y": "differences"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_9",
  "x": "We have also evaluated SVM classi ers which use only bigrams as features, as ngram-based classi ers have been reported to perform be er for the stance detection problem <cite>[12]</cite> . However, we have observed that using bigrams as the sole features of the SVM classi ers leads to quite poor results. is observation may be due to the relatively limited size of the tweet data set employed. Still, we can conclude that unigram-based features lead to superior results compared to the results obtained using bigrams as features, based on our experiments on our data set. Yet, ngram-based features may be employed on the extended versions of the data set to verify this conclusion within the course of future work. With an intention to exploit the contribution of hashtag use to stance detection, we have also used the existence of hashtags in tweets as an additional feature to unigrams. e corresponding evaluation results of the SVM classi ers using unigrams together the existence of hashtags as features are provided in Table 2 . When the results given in Table 2 are compared with the results in Table 1 , a slight decrease in F-Measure (0.5%) for Target-1 is observed, while the overall F-Measure value for Target-2 has increased by 1.8%. Although we could not derive sound conclusions mainly due to the relatively small size of our data set, the increase in the performance of the SVM classi er Target-2 is an encouraging evidence for the exploitation of hashtags in a stance detection system.",
  "y": "uses"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_10",
  "x": "Some of the baseline systems reported in <cite>[12]</cite> are SVM-based systems using unigrams and ngrams as features similar to our study, but their data sets include all three stance classes of Favor, Against, and Neither, while our data set comprises only tweets classi ed as belonging to Favor or Against classes. Another di erence is that the data sets in <cite>[12]</cite> have been divided into training and test sets, while in our study we provide 10-fold cross-validation results on the whole data set. On the other hand, we should also note that SVM-based sentiment analysis systems (such as those given in [15] ) have been reported to achieve be er F-Measure rates for the Positive sentiment class when compared with the results obtained for the Negative class. erefore, our evaluation results for each stance class seem to be in line with such sentiment analysis systems. Yet, further experiments on the extended versions of our data set should be conducted and the results should again be compared with the stance detection results given in the literature. We have also evaluated SVM classi ers which use only bigrams as features, as ngram-based classi ers have been reported to perform be er for the stance detection problem <cite>[12]</cite> . However, we have observed that using bigrams as the sole features of the SVM classi ers leads to quite poor results. is observation may be due to the relatively limited size of the tweet data set employed. Still, we can conclude that unigram-based features lead to superior results compared to the results obtained using bigrams as features, based on our experiments on our data set.",
  "y": "differences"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_11",
  "x": "e procedure will improve the quality the data set as well as the quality of prospective systems to be trained and tested on it. \u2022 Other features like emoticons (as commonly used for sentiment analysis), features based on hashtags, and ngram features can also be used by the classi ers and these classiers can be tested on larger data sets. Other classi cation approaches could also be implemented and tested against our baseline classi ers. Particularly, related methods presented in recent studies such as <cite>[12]</cite> can be tested on our data set. \u2022 Lastly, the SVM classi ers utilized in this study and their prospective versions utilizing other features can be tested on stance data sets in other languages (such as English) for comparison purposes. ---------------------------------- **CONCLUSION** Stance detection is a considerably new research area in natural language processing and is considered within the scope of the wellstudied topic of sentiment analysis. It is the detection of stance within text towards a target which may be explicitly speci ed in the text or not.",
  "y": "uses future_work"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_12",
  "x": "\u2022 Other features like emoticons (as commonly used for sentiment analysis), features based on hashtags, and ngram features can also be used by the classi ers and these classiers can be tested on larger data sets. Other classi cation approaches could also be implemented and tested against our baseline classi ers. Particularly, related methods presented in recent studies such as <cite>[12]</cite> can be tested on our data set. \u2022 Lastly, the SVM classi ers utilized in this study and their prospective versions utilizing other features can be tested on stance data sets in other languages (such as English) for comparison purposes. ---------------------------------- **CONCLUSION** Stance detection is a considerably new research area in natural language processing and is considered within the scope of the wellstudied topic of sentiment analysis. It is the detection of stance within text towards a target which may be explicitly speci ed in the text or not. In this study, we present a stance-annotated tweet data set in Turkish where the targets of the annotated stances are two popular sports clubs in Turkey.",
  "y": "future_work"
 },
 {
  "id": "0bfea881773f504206bef9c1394f20_0",
  "x": "We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC <cite>[4]</cite>), and of n-gram perplexity. Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) [5] . In our semantic tests, on average 85% of the quality can be obtained by training on a random \u223c 4% subset of the original corpus (e.g. as in Fig. 1 , 5 random million lines yield 64.14% instead of 75.14%). Our claims are that i) such evaluation posteriors are Normally distributed (Tab. I), and that ii) the variance is inversely proportional to the subset size (Tab. II). It is therefore possible to select the best random subset for a given size, if an information criterion is known. Such metric is currently under investigation. Within the robotics domain, in order to reduce computational complexity of the training phase, cardinality reduction of human-written instructions is particularly important for non-recursive online training algorithms, such as current symbol-based probabilistic reasoning systems [1] , [3] , [6] . ---------------------------------- ****",
  "y": "uses"
 },
 {
  "id": "0bfea881773f504206bef9c1394f20_1",
  "x": "---------------------------------- **POSTERIOR EVALUATION DISTRIBUTION OF SUBSETS** We computed multiple random subsets of sentences from the UMBC WEBBASE CORPUS (\u223c 17.13GB) via a custom implementation using the SPARK distributed framework. We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC <cite>[4]</cite> ), and of n-gram perplexity. Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) [5] . In our semantic tests, on average 85% of the quality can be obtained by training on a random \u223c 4% subset of the original corpus (e.g. as in Fig. 1 , 5 random million lines yield 64.14% instead of 75.14%). Our claims are that i) such evaluation posteriors are Normally distributed (Tab. I), and that ii) the variance is inversely proportional to the subset size (Tab. II). It is therefore possible to select the best random subset for a given size, if an information criterion is known. Such metric is currently under investigation.",
  "y": "uses"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_0",
  "x": "**ABSTRACT** Variational Autoencoder (VAE) is a powerful method for learning representations of highdimensional data. However, VAEs can suffer from an issue known as latent variable collapse (or KL loss vanishing), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks. Such an issue is particularly prevalent when employing VAE-RNN architectures for text modelling <cite>(Bowman et al., 2016)</cite> . In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse. Compared to existing VAE-RNN architectures, we show that our model can achieve much more stable training process and can generate text with significantly better quality. ---------------------------------- **INTRODUCTION** Variational Autoencoder (VAE) (Kingma and Welling, 2013 ) is a powerful method for learning representations of high-dimensional data.",
  "y": "motivation"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_1",
  "x": "In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse. Compared to existing VAE-RNN architectures, we show that our model can achieve much more stable training process and can generate text with significantly better quality. ---------------------------------- **INTRODUCTION** Variational Autoencoder (VAE) (Kingma and Welling, 2013 ) is a powerful method for learning representations of high-dimensional data. However, recent attempts of applying VAEs to text modelling are still far less successful compared to its application to image and speech (Bachman, 2016; Fraccaro et al., 2016; Semeniuta et al., 2017) . When applying VAEs for text modelling, recurrent neural networks (RNNs) 1 are commonly used as the architecture for both encoder and decoder <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018; Dieng et al., 2019) . While such a VAE-RNN based architecture allows encoding and generating sentences (in the decoding phase) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse (or KL loss vanishing) , where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks. Various efforts have been made to alleviate the latent variable collapse issue.",
  "y": "background"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_2",
  "x": "---------------------------------- **INTRODUCTION** Variational Autoencoder (VAE) (Kingma and Welling, 2013 ) is a powerful method for learning representations of high-dimensional data. However, recent attempts of applying VAEs to text modelling are still far less successful compared to its application to image and speech (Bachman, 2016; Fraccaro et al., 2016; Semeniuta et al., 2017) . When applying VAEs for text modelling, recurrent neural networks (RNNs) 1 are commonly used as the architecture for both encoder and decoder <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018; Dieng et al., 2019) . While such a VAE-RNN based architecture allows encoding and generating sentences (in the decoding phase) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse (or KL loss vanishing) , where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks. Various efforts have been made to alleviate the latent variable collapse issue. <cite>Bowman et al. (2016)</cite> uses KL annealing, where a variable weight is added to the KL term in the cost function at training time. Yang et al. (2017) discovered that there is a trade-off between the contextual capacity of the decoder and effective use of encoding information, and developed a dilated CNN as decoder which can vary the amount of conditioning context.",
  "y": "background"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_3",
  "x": "While such a VAE-RNN based architecture allows encoding and generating sentences (in the decoding phase) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse (or KL loss vanishing) , where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks. Various efforts have been made to alleviate the latent variable collapse issue. <cite>Bowman et al. (2016)</cite> uses KL annealing, where a variable weight is added to the KL term in the cost function at training time. Yang et al. (2017) discovered that there is a trade-off between the contextual capacity of the decoder and effective use of encoding information, and developed a dilated CNN as decoder which can vary the amount of conditioning context. They also introduced a loss clipping strategy in order to make the model more robust. Xu and Durrett (2018) addressed the problem by replacing the standard normal distribution for the prior with the von Mises-Fisher (vMF) distribution. With vMF, the KL loss only depends on the concentration parameter which is fixed during training and testing, and hence results in a constant KL loss. In a more recent work, Dieng et al. (2019) avoided latent variable collapse by including skip connections in the generative model, where the skip connections enforce strong links between the latent variables and the likelihood function. Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss <cite>(Bowman et al., 2016</cite>; , or resort to designing more sophisticated model structures (Yang et al., 2017; Xu and Durrett, 2018; Dieng et al., 2019) .",
  "y": "motivation"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_4",
  "x": "With vMF, the KL loss only depends on the concentration parameter which is fixed during training and testing, and hence results in a constant KL loss. In a more recent work, Dieng et al. (2019) avoided latent variable collapse by including skip connections in the generative model, where the skip connections enforce strong links between the latent variables and the likelihood function. Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss <cite>(Bowman et al., 2016</cite>; , or resort to designing more sophisticated model structures (Yang et al., 2017; Xu and Durrett, 2018; Dieng et al., 2019) . In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse. In contrast to existing VAE-RNN models for text modelling which merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation for all hidden states of the RNN encoder. Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures. We evaluate our model against several strong baselines which apply VAE for text modelling <cite>(Bowman et al., 2016</cite>; Yang et al., 2017; Xu and Durrett, 2018) . We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation dataset (Novikova et al., 2017) . Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation.",
  "y": "differences"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_5",
  "x": "---------------------------------- **VARIATIONAL AUTOENDODER WITH HOLISTIC REGULARISATION** In this section, we discuss the technical details of the proposed holistic regularisation VAE (HR-VAE) model, a general architecture which can effectively mitigate the KL vanishing phenomenon. Our model design is motivated by one noticeable defect shared by the VAE-RNN based models in previous works <cite>(Bowman et al., 2016</cite>; Yang et al., 2017; Xu and Durrett, 2018; Dieng et al., 2019) . That is, all these models, as shown in Figure 1a , only impose a standard normal distribution prior on the last hidden state of the RNN encoder, which potentially leads to learning a suboptimal representation of the latent variable and results in model vulnerable to KL loss vanishing. Our hypothesis is that to learn a good representation of data and a good generative model, it is crucial to impose the standard normal prior on all the hidden states of the RNN-based encoder (see Figure 1b ), which allows a better regularisation of the model learning process. We implement the HR-VAE model using a twolayer LSTM for both the encoder and decoder. However, one should note that our architecture can be readily applied to other types of RNN such as GRU. For each time stamp t (see Figure 1b) , we concatenate the hidden state h t and the cell state c t of the encoder.",
  "y": "motivation"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_6",
  "x": "However, one should note that our architecture can be readily applied to other types of RNN such as GRU. For each time stamp t (see Figure 1b) , we concatenate the hidden state h t and the cell state c t of the encoder. The concatenation (i.e., [h t ; c t ]) is then fed into two linear transformation layers for estimating \u00b5 t and \u03c3 2 t , which are parameters of a normal distribution corresponding to the concatenation of h t and c t . Let Q \u03c6t (z t |x) = N (z t |\u00b5 t , \u03c3 2 t ), we wish Q \u03c6t (z t |x) to be close to a prior P (z t ), which is a standard Gaussian. Finally, the KL divergence between these two multivariate Gaussian distributions (i.e., Q \u03c6t and P (z t )) will contribute to the overall KL loss of the ELBO. By taking the average of the KL loss at each time stamp t, the resulting ELBO takes the following form KL(Q \u03c6t (z t |x) P (z t )). ( 3) As can be seen in Eq. 3, our solution to the KL collapse issue does not require any engineering for balancing the weight between the reconstruction term and KL loss as commonly the case in existing works <cite>(Bowman et al., 2016</cite>; . The weight between these two terms of our model is simply 1 : 1.",
  "y": "differences"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_7",
  "x": "KL(Q \u03c6t (z t |x) P (z t )). ( 3) As can be seen in Eq. 3, our solution to the KL collapse issue does not require any engineering for balancing the weight between the reconstruction term and KL loss as commonly the case in existing works <cite>(Bowman et al., 2016</cite>; . The weight between these two terms of our model is simply 1 : 1. ---------------------------------- **EXPERIMENTAL SETUP** ---------------------------------- **DATASETS** We evaluate our model on two public datasets, namely, Penn Treebank (PTB) (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation corpus (Novikova et al., 2017) , which have been used in a number of previous works for text generation <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018; Wiseman et al., 2018; Su et al., 2018) . PTB consists of more than 40,000 sentences from Wall Street Journal articles whereas the E2E dataset contains over 50,000 sen-tences of restaurant reviews.",
  "y": "uses background"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_8",
  "x": "---------------------------------- **DATASETS** We evaluate our model on two public datasets, namely, Penn Treebank (PTB) (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation corpus (Novikova et al., 2017) , which have been used in a number of previous works for text generation <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018; Wiseman et al., 2018; Su et al., 2018) . PTB consists of more than 40,000 sentences from Wall Street Journal articles whereas the E2E dataset contains over 50,000 sen-tences of restaurant reviews. The statistics of these two datasets are summarised in Table 1 . ---------------------------------- **IMPLEMENTATION DETAILS** For the PTB dataset, we used the train-test split following <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018) . For the E2E dataset, we used the train-test split from the original dataset (Novikova et al., 2017) and indexed the words with a frequency higher than 3.",
  "y": "uses"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_9",
  "x": "KL annealing is used to tackled the latent variable collapse issue <cite>(Bowman et al., 2016)</cite> ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder (Yang et al., 2017) ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (Xu and Durrett, 2018) . the decoder needs to predict the entire sequence with only the help of the given latent variable z. In this way, a high-quality representation abstracting the information of the input sentence is much needed for the decoder, and hence enforcing z to learn the required information. Overall performance. Table 2 shows the language modelling results of our approach and the baselines. We report negative log likelihood (NLL), KL loss, and perplexity (PPL) on the test set. As expected, all the models have a higher KL loss in the inputless setting than the standard setting, as z is required to encode more information about the input data for reconstruction. In terms of overall performance, our model outperforms all the baselines in both datasets (i.e., PTB and E2E). For instance, when comparing with the strongest baseline vMF-VAE in the standard setting, our model reduces NLL from 96 to 79 and PPL from 98 to 43 in PTB, respectively.",
  "y": "uses"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_10",
  "x": "These plots were obtained based on the E2E training set using the inputless setting. We can see that the KL loss of VAE-LSTMbase, which uses Sigmoid annealing <cite>(Bowman et al., 2016)</cite> , collapses to zero, leading to a poor generative performance as indicated by the high reconstruction loss. The KL loss for both VAE-CNN and vMF-VAE are nonzero, where the former mitigates the KL collapse issue with a KL loss clipping strategy and the latter by replacing the standard normal distribution for the prior with the vMF distribution (i.e., with the vMF distribution, the KL loss only depends on a fixed concentration parameter, and hence results in a constant KL loss). Although both VAE-CNN and vMF-VAE outperform VAE-LSTM-base by a large margin in terms of reconstruction loss as shown in Figure 2 , one should also notice that these two models actually overfit the training data, as their performance on the test set is much worse (cf. Table 2 ). In contrast to the baselines which mitigate the KL collapse issue by carefully engineering the weight between the reconstruction loss and KL loss or choosing a different choice of prior, we provide a simple and elegant solution through holistic KL regularisation, which can effectively mitigate the KL collapse issue and achieve a better reconstruction error in both training and testing. Sentence reconstruction. Lastly, we show some sentence examples reconstructed by vMF-VAE (i.e., the best baseline) and our model in the inputless setting using sentences from the E2E test set as input. As shown in Table 3 , the sentences generated by vMF-VAE contain repeated words in quite a few cases, such as 'city city area' and 'blue spice spice'.",
  "y": "uses"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_0",
  "x": "****CONNECTING SUPERVISED AND UNSUPERVISED SENTENCE EMBEDDINGS**** **ABSTRACT** Representing sentences as numerical vectors while capturing their semantic context is an important and useful intermediate step in natural language processing. Representations that are both general and discriminative can serve as a tool for tackling various NLP tasks. While common sentence representation methods are unsupervised in nature, recently, an approach for learning universal sentence representation in a supervised setting was presented in <cite>(Conneau et al., 2017)</cite> . We argue that although promising results were obtained, an improvement can be reached by adding various unsupervised constraints that are motivated by auto-encoders and by language models. We show that by adding such constraints, superior sentence embeddings can be achieved. We compare our method with the original implementation and show improvements in several tasks. ----------------------------------",
  "y": "background"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_1",
  "x": "While sentence embedding are not always used in similarity probing, we find this formulation useful as the similarity assumption is implicitly made when training classifiers on top of the embeddings in downstream tasks. Sentences embedding methods were mostly trained in an unsupervised setting. In (Le and Mikolov, 2014 ) the ParagraphVector model was proposed which is trained to predict words in the document. SkipThought (Kiros et al., 2015) vectors rely on the continuity of text to train an encoder-decoder model that tries to reconstruct the surrounding sentences of a given passage. In Sequential Denoising Autoencoders (SDAE) (Hill et al., 2016) high-dimensional input data is corrupted according to some noise function, and the model is trained to recover the original data from the corrupted version. FastSent (Hill et al., 2016) learns to predicts a Bag-Of-Word (BOW) representation of adjacent sentences given a BOW representation of some sentence. In (Klein et al., 2015) a Hybrid Gaussian Laplacian density function is fitted to the sentence to derive Fisher Vectors. While previous methods train sentence embeddings in an unsupervised manner, a recent work <cite>(Conneau et al., 2017)</cite> argued that better representations can be achieved via supervised training on a general sentence inference dataset (Bowman et al., 2015) . To this end, the authors use the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) to train different Table 1 : Sentence embedding results.",
  "y": "background"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_2",
  "x": "In (Le and Mikolov, 2014 ) the ParagraphVector model was proposed which is trained to predict words in the document. SkipThought (Kiros et al., 2015) vectors rely on the continuity of text to train an encoder-decoder model that tries to reconstruct the surrounding sentences of a given passage. In Sequential Denoising Autoencoders (SDAE) (Hill et al., 2016) high-dimensional input data is corrupted according to some noise function, and the model is trained to recover the original data from the corrupted version. FastSent (Hill et al., 2016) learns to predicts a Bag-Of-Word (BOW) representation of adjacent sentences given a BOW representation of some sentence. In (Klein et al., 2015) a Hybrid Gaussian Laplacian density function is fitted to the sentence to derive Fisher Vectors. While previous methods train sentence embeddings in an unsupervised manner, a recent work <cite>(Conneau et al., 2017)</cite> argued that better representations can be achieved via supervised training on a general sentence inference dataset (Bowman et al., 2015) . To this end, the authors use the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) to train different Table 1 : Sentence embedding results. BiLSTM refers to the original BiLSTM followed by MaxPooling implementation of <cite>(Conneau et al., 2017)</cite> which is the baseline for our work. AE Reg and LM Reg refers to the Auto-Encoder and Language-Model regularization terms described in 2.1 and Combined refers to optimizing with both terms.",
  "y": "uses"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_3",
  "x": "A different, unsupervised, task in NLP is estimating the probability of word sequences. A family of algorithms for this task titled word language models seek to model the problem as estimating the probability of a word, given the previous words in the text. In (Bengio et al., 2003) neural networks were employed and (Mikolov et al., 2010) was among the first methods to use recurrent neural networks (RNN) for modeling the problem, where the probability of the a word is estimated based on the previous words fed to the RNN. A variant of RNN -Long Short Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997 ) -were used in (Sundermeyer et al., 2012) . Following that, (Zaremba et al., 2014) proposed a dropout augmented LSTM. We note that there exists a connection between those two problems and try to model it more explicitly. Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -Peters et al. (2018), CoVe -McCann et al. (2017 ) Peters et al. (2017 , Salant and Berant (2017) ) -in this work we jointly train the unsupervised and supervised tasks. To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by <cite>(Conneau et al., 2017)</cite> . We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of <cite>(Conneau et al., 2017)</cite> .",
  "y": "extends"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_4",
  "x": "A variant of RNN -Long Short Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997 ) -were used in (Sundermeyer et al., 2012) . Following that, (Zaremba et al., 2014) proposed a dropout augmented LSTM. We note that there exists a connection between those two problems and try to model it more explicitly. Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -Peters et al. (2018), CoVe -McCann et al. (2017 ) Peters et al. (2017 , Salant and Berant (2017) ) -in this work we jointly train the unsupervised and supervised tasks. To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by <cite>(Conneau et al., 2017)</cite> . We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of <cite>(Conneau et al., 2017)</cite> . ---------------------------------- **METHOD** Our approach builds upon the previous work of <cite>(Conneau et al., 2017)</cite> .",
  "y": "differences"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_5",
  "x": "---------------------------------- **METHOD** Our approach builds upon the previous work of <cite>(Conneau et al., 2017)</cite> . Specifically, we use their BiLSTM model with max pooling. More concretely, given a sequence of T words, {w t } t=1,...,T with given word embedding (Mikolov et al., 2013; Pennington et al., 2014) {v t } t=1,...,T ,a bidirectional LSTM computes a set of T vectors {h t } t=1,...,T where each h t is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions. We denote { \u2212 \u2192 h t } and { \u2190 \u2212 h t } as the hidden states of the left and right LSTM's respectively, where t = 1, . . . , T . The final sentence representation is obtained by taking the maximal value of each dimension of the {h t } hidden units (i.e.: max pooling). The original model of <cite>(Conneau et al., 2017)</cite> was trained on the SNLI dataset in a supervised fashion -given pairs of sentences s 1 and s 2 , denote their representation bys 1 and s 2 . During training, the concatenation ofs 1 ,s 2 , |s 1 \u2212s 2 | ands 1 * s 2 is fed to a three layer fully connected network followed by a softmax classifier.",
  "y": "extends"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_6",
  "x": "The original model of <cite>(Conneau et al., 2017)</cite> was trained on the SNLI dataset in a supervised fashion -given pairs of sentences s 1 and s 2 , denote their representation bys 1 and s 2 . During training, the concatenation ofs 1 ,s 2 , |s 1 \u2212s 2 | ands 1 * s 2 is fed to a three layer fully connected network followed by a softmax classifier. ---------------------------------- **REGULARIZATION TERMS** We note that by training on SNLI, the model might overfit and would not be general enough to provide universal sentence embedding. We devise several regularization criteria that incentivize the hidden states to maintain more information about the input sequence. Specifically, denote the dimension of the word embedding by d and the dimension of the hidden state by l. We add a linear transformation layer L l\u00d7d : H \u2192 W on top of the BiLSTM to transform the hidden states back to the dimension of word embeddings and denote its output by {w t } t=1,...,T . Recall that in the training process, we minimize the log-likelihood loss of the fully connected network predictions which we denote by y i where y gt is the prediction score given to the correct ground truth class. Now, the total loss criteria with our regularization term can be written as",
  "y": "uses"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_7",
  "x": "We call the second regularization term in (3) a bi-directional auto-encoder regularization and in (4) a bi-directional language model regularization term. Again, \u03bb 1 and \u03bb 2 are hyper-parameters controlling the amount of regularization and were set to 0.5 in our experiments. ---------------------------------- **EXPERIMENTS** Following <cite>(Conneau et al., 2017)</cite> we have tested our approach on a wide array of classification tasks, including sentiment analysis (MR -Pang and Lee (2005) , SST -Socher et al. (2013) ), question-type (TREC -Li and Roth (2002) ), product reviews (CR - Hu and Liu (2004) ), subjectivity/objectivity (SUBJ - Pang and Lee (2005) ) and opinion polarity (MPQA -Wiebe et al. (2005) ). We also tested our approach on semantic textual similarity (STS 14 - Agirre et al. (2014) ), paraphrase detection (MRPC - Dolan et al. (2004) ), entailment and semantic relatedness tasks (SICK-R and SICK-E - Marelli et al. (2014) ), though those tasks are more close in nature to the task of the SNLI dataset which the model was trained on. In our experiments we have set \u03bb from eq. (1) and eq. (2) to be 1 and \u03bb 1 , \u03bb 2 from eq. (3) and eq. (4) to be 0.5. All other hyper-parameters and implementation details were left unchanged to provide a fair comparison to the baseline method of <cite>(Conneau et al., 2017)</cite> . Our results are summarized in table 1.",
  "y": "uses"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_10",
  "x": "Our results are summarized in table 1. We compared out method against the baseline BiL-STM implementation of <cite>(Conneau et al., 2017)</cite> and included FastSent (Hill et al., 2016) and SkipThought vectors (Kiros et al., 2015) as a reference. As evident from table 1 in almost all the tasks evaluated, adding the proposed regularization terms improves performance. This serve to show that in a supervised learning setting, additional information on the input sequence can be leveraged and injected to the model by adding simple unsupervised loss criteria. ---------------------------------- **CONCLUSIONS** In our work, we have sought to connect unsupervised and supervised learning in the context of sentence embeddings. Leveraging supervision given by some general task aided in obtaining state-of-the-art sentence representations <cite>(Conneau et al., 2017)</cite> . However, every supervised learning tasks is prone to overfit.",
  "y": "motivation"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_0",
  "x": "Extracting relations from a text corpus (Mintz et al., 2009; Surdeanu et al., 2012; or inferring facts from the relationships among known entities (Lao and Cohen, 2010) are thus important approaches for populating existing knowledge bases. Originally proposed as an alternative statistical relational learning method, the knowledge base embedding approach has gained a significant amount of attention, due to its simple prediction time computation and strong empirical performance (Nickel et al., 2011; Chang et al., 2014) . In this framework, entities and relations in a knowledge base are represented in a continuous space, such as vectors and matrices. Whether two entities have a previously unknown relationship can be predicted by simple functions of their corresponding vectors or matrices. Early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities (e.g., father(barack, sasha)) (Nickel et al., 2011; Socher et al., 2013; Bordes et al., 2013; Chang et al., 2014; Yang et al., 2015) . In contrast, using multi-step relation paths (e.g., husband(barack, michelle) \u2227 mother(michelle, sasha) to train KB embeddings has been proposed very recently <cite>(Guu et al., 2015</cite>; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015) . While using relation paths improves model performance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need to make approximations by sampling or pruning.",
  "y": "background"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_1",
  "x": "Originally proposed as an alternative statistical relational learning method, the knowledge base embedding approach has gained a significant amount of attention, due to its simple prediction time computation and strong empirical performance (Nickel et al., 2011; Chang et al., 2014) . In this framework, entities and relations in a knowledge base are represented in a continuous space, such as vectors and matrices. Whether two entities have a previously unknown relationship can be predicted by simple functions of their corresponding vectors or matrices. Early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities (e.g., father(barack, sasha)) (Nickel et al., 2011; Socher et al., 2013; Bordes et al., 2013; Chang et al., 2014; Yang et al., 2015) . In contrast, using multi-step relation paths (e.g., husband(barack, michelle) \u2227 mother(michelle, sasha) to train KB embeddings has been proposed very recently <cite>(Guu et al., 2015</cite>; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015) . While using relation paths improves model performance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; .",
  "y": "motivation background"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_2",
  "x": "The two approaches we consider here are: using relation paths to generate new auxiliary triples for training<cite> (Guu et al., 2015)</cite> and using relation paths as features for scoring (Lin et al., 2015) . Both approaches take into account embeddings of relation paths between entities, and both of them used vector space compositions to combine the embeddings of individual relation links r i into an embedding of the path \u03c0. The intermediate nodes e i are neglected. The natural composition function of a BILINEAR model is matrix multiplication<cite> (Guu et al., 2015)</cite> . For this model, the embedding of a length-n path \u03a6 \u03c0 \u2208 R d\u00d7d is defined as the matrix product of the sequence of relation matrices for the relations in \u03c0. For the BILINEAR-DIAG model, all the matrices are diagonal and the computation reduces to coordinate-wise product of vectors in R d . In Guu et al. (2015) , information from relation paths was used to generate additional auxiliary terms in training, which serve to provide a compositional regularizer for the learned node and relation embeddings. A more limited version of the same method was simultaneously proposed in Garcia-Duran et al. (2015) . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_3",
  "x": "The intermediate nodes e i are neglected. The natural composition function of a BILINEAR model is matrix multiplication<cite> (Guu et al., 2015)</cite> . For this model, the embedding of a length-n path \u03a6 \u03c0 \u2208 R d\u00d7d is defined as the matrix product of the sequence of relation matrices for the relations in \u03c0. For the BILINEAR-DIAG model, all the matrices are diagonal and the computation reduces to coordinate-wise product of vectors in R d . In Guu et al. (2015) , information from relation paths was used to generate additional auxiliary terms in training, which serve to provide a compositional regularizer for the learned node and relation embeddings. A more limited version of the same method was simultaneously proposed in Garcia-Duran et al. (2015) . ---------------------------------- **RELATION PATHS AS A COMPOSITIONAL REGULARIZER** The method works as follows: starting from each node in the knowledge base, it samples m random walks of length 2 to a maximum length L, resulting in a list of samples {[s i , \u03c0 i , t i ]}.",
  "y": "background"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_4",
  "x": "---------------------------------- **PRUNED-PATHS THIS METHOD COMPUTES AND** stores the values of the random walk probabilities for all pairs of nodes and relation paths, for which these probabilities are non-zero. This can be done in time O T where Triples is the same quantity used in the analysis of<cite> Guu et al. (2015)</cite> . The memory requirements of this method are the same as these of<cite> (Guu et al., 2015)</cite> , up to a constant to store random-walk probabilities for paths. The time requirements are different, however. At training time, we compute scores and update gradients for triples corresponding to direct 5 The computation uses the fact that the number of path type sequences of length l is N l r . We use a, the average branching factor of nodes given relation types, to derive the estimated number of triples of a given relation type for a path of length l. knowledge base edges, whose number is E kb .",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_5",
  "x": "The memory requirements of this method are the same as these of<cite> (Guu et al., 2015)</cite> , up to a constant to store random-walk probabilities for paths. The time requirements are different, however. At training time, we compute scores and update gradients for triples corresponding to direct 5 The computation uses the fact that the number of path type sequences of length l is N l r . We use a, the average branching factor of nodes given relation types, to derive the estimated number of triples of a given relation type for a path of length l. knowledge base edges, whose number is E kb . For each considered triple, however, we need to compute the sum of representations of path features that are active for the triple. We estimate the average number of active paths per node pair as T Ne 2 . Therefore the overall time for this method per training iteration is O 2d(\u03b7 + 1)E kb We should note that whether this method or the one of<cite> Guu et al. (2015)</cite> will be faster in training depends on whether the average number of paths per node pair multiplied by E kb is bigger or smaller than the total number of triples T .",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_6",
  "x": "---------------------------------- **PRUNED-PATHS THIS METHOD COMPUTES AND** stores the values of the random walk probabilities for all pairs of nodes and relation paths, for which these probabilities are non-zero. This can be done in time O T where Triples is the same quantity used in the analysis of<cite> Guu et al. (2015)</cite> . The memory requirements of this method are the same as these of<cite> (Guu et al., 2015)</cite> , up to a constant to store random-walk probabilities for paths. The time requirements are different, however. At training time, we compute scores and update gradients for triples corresponding to direct 5 The computation uses the fact that the number of path type sequences of length l is N l r . We use a, the average branching factor of nodes given relation types, to derive the estimated number of triples of a given relation type for a path of length l. knowledge base edges, whose number is E kb .",
  "y": "uses differences"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_7",
  "x": "We should note that whether this method or the one of<cite> Guu et al. (2015)</cite> will be faster in training depends on whether the average number of paths per node pair multiplied by E kb is bigger or smaller than the total number of triples T . Unlike the method of<cite> Guu et al. (2015)</cite> , the evaluation-time memory requirements of this approach are the same as its training memory requirements, or they could be reduced slightly to match the evaluation-time memory requirements of ALL-PATHS, if these are lower as determined by the specific problem instance. ---------------------------------- **ALL-PATHS THIS METHOD DOES NOT EXPLICITLY CONSTRUCT OR STORE FULLY CONSTRUCTED PATHS (S, \u03a0, T).** Instead, memory and time is determined by the dynamic program in Algorithm 1, as well as the forward-backward algorithm for computation of gradients. The memory required to store path representation sums F l (s, t) is O dLN e 2 in the worst case. Denote E = E kb + E txt . The time to compute these sums is O dE(1 + l=2...L (l \u2212 1)N e ) . After this computation, the time to compute the scores of training positive and negative triples is O d2(\u03b7 + 1)E kb L .",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_8",
  "x": "Therefore the overall time for this method per training iteration is O 2d(\u03b7 + 1)E kb We should note that whether this method or the one of<cite> Guu et al. (2015)</cite> will be faster in training depends on whether the average number of paths per node pair multiplied by E kb is bigger or smaller than the total number of triples T . Unlike the method of<cite> Guu et al. (2015)</cite> , the evaluation-time memory requirements of this approach are the same as its training memory requirements, or they could be reduced slightly to match the evaluation-time memory requirements of ALL-PATHS, if these are lower as determined by the specific problem instance. ---------------------------------- **ALL-PATHS THIS METHOD DOES NOT EXPLICITLY CONSTRUCT OR STORE FULLY CONSTRUCTED PATHS (S, \u03a0, T).** Instead, memory and time is determined by the dynamic program in Algorithm 1, as well as the forward-backward algorithm for computation of gradients. The memory required to store path representation sums F l (s, t) is O dLN e 2 in the worst case. Denote E = E kb + E txt . The time to compute these sums is O dE(1 + l=2...L (l \u2212 1)N e ) .",
  "y": "differences"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_9",
  "x": "The memory required to store path representation sums F l (s, t) is O dLN e 2 in the worst case. Denote E = E kb + E txt . The time to compute these sums is O dE(1 + l=2...L (l \u2212 1)N e ) . After this computation, the time to compute the scores of training positive and negative triples is O d2(\u03b7 + 1)E kb L . The time to increment gradients using each triple considered in training is O dEL 2 . The evaluation time memory is reduced relative to training time memory by a factor of L and the evaluation time per triple can also be reduced by a factor of L using precomputation. Based on this analysis, we computed training time and memory estimates for our NCI+Txt knowledge base. Given the values of the quantities from our knowledge graph and d = 50, \u03b7 = 50, and maximum path length of 5, the estimated memory for<cite> (Guu et al., 2015)</cite> and PRUNED-PATHS is 4.0 \u00d7 10 18 and for ALL-PATHS the memory is 1.9\u00d710 9 . The time estimates are 2.4\u00d710 21 , 2.6 \u00d7 10 25 , and 7.3 \u00d7 10 15 for<cite> (Guu et al., 2015)</cite> , PRUNED-PATHS, and ALL-PATHS, respectively.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_10",
  "x": "The memory required to store path representation sums F l (s, t) is O dLN e 2 in the worst case. Denote E = E kb + E txt . The time to compute these sums is O dE(1 + l=2...L (l \u2212 1)N e ) . After this computation, the time to compute the scores of training positive and negative triples is O d2(\u03b7 + 1)E kb L . The time to increment gradients using each triple considered in training is O dEL 2 . The evaluation time memory is reduced relative to training time memory by a factor of L and the evaluation time per triple can also be reduced by a factor of L using precomputation. Based on this analysis, we computed training time and memory estimates for our NCI+Txt knowledge base. Given the values of the quantities from our knowledge graph and d = 50, \u03b7 = 50, and maximum path length of 5, the estimated memory for<cite> (Guu et al., 2015)</cite> and PRUNED-PATHS is 4.0 \u00d7 10 18 and for ALL-PATHS the memory is 1.9\u00d710 9 . The time estimates are 2.4\u00d710 21 , 2.6 \u00d7 10 25 , and 7.3 \u00d7 10 15 for<cite> (Guu et al., 2015)</cite> , PRUNED-PATHS, and ALL-PATHS, respectively.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_11",
  "x": "Based on this analysis, we computed training time and memory estimates for our NCI+Txt knowledge base. Given the values of the quantities from our knowledge graph and d = 50, \u03b7 = 50, and maximum path length of 5, the estimated memory for<cite> (Guu et al., 2015)</cite> and PRUNED-PATHS is 4.0 \u00d7 10 18 and for ALL-PATHS the memory is 1.9\u00d710 9 . The time estimates are 2.4\u00d710 21 , 2.6 \u00d7 10 25 , and 7.3 \u00d7 10 15 for<cite> (Guu et al., 2015)</cite> , PRUNED-PATHS, and ALL-PATHS, respectively. Table 1 : KB completion results on NCI-PID test: comparison of our compositional learning approach (ALL-PATHS+NODES) with baseline systems. d is the embedding dimension; sampled paths occurring less than c times were pruned in PRUNED-PATHS. ---------------------------------- **EXPERIMENTS** Our experiments are designed to study three research questions: (i) What is the impact of using path representations as a source of compositional regularization as in<cite> (Guu et al., 2015)</cite> versus using them as features for scoring as in PRUNED-PATHS and ALL-PATHS? (ii) What is the impact of using textual mentions for KB completion in different models? (iii) Does modeling intermediate path nodes improve the accuracy of KB completion?",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_12",
  "x": "The original networks are in the form of hypergraphs, where nodes could be complex gene products (e.g., \"protein complex\" with multiple proteins bound together) and regulations could have multiple inputs and outputs. Following the convention of most network modeling approaches, we simplified the hypergraphs into binary regulations between genes (e.g., GRB2 positive reg MAPK3), which yields a graph with 2774 genes and 14323 triples. The triples are then split into train, dev, and test sets, of size 10224, 1315, 2784, respectively. We identified genes belonging to the same family via the common letter prefix in their names, which adds 1936 triples to training. As a second dataset, we used a WordNet KB with the same train, dev, and test splits as<cite> Guu et al. (2015)</cite> . There are 38,696 entities and 11 types of knowledge base relations. The KB includes 112,581 triples for training, 2,606 triples for validation, and 10,544 triples for testing. WordNet does not contain textual relations and is used for a more direct comparison with recent works. Textual Relations We used PubMed abstracts for text for NCI-PID.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_13",
  "x": "Systems ALL-PATHS denotes our compositional learning approach that sums over all paths using dynamic programming; ALL-PATHS+NODES additionally models nodes in the paths. PRUNED-PATHS denotes the traditional approach that learns from sampled paths detailed in \u00a73.1.2; paths with occurrence less than a cutoff are pruned (c = 1 in Table 1 means that all sampled paths are used). The most relevant prior approach is<cite> Guu et al. (2015)</cite> . We ran experiments using both their publicly available code and our re-implementation. We also included the BILINEAR-DIAG baseline. Implementation Details We used batch training with RProp (Riedmiller and Braun, 1993) . The L 2 penalty \u03bb was set to 0.1 for all models, and the entity vectors x e were normalized to unit vectors. For each positive example we sample 500 negative examples. For our implementation of<cite> (Guu et al., 2015)</cite> , we run 5 random walks of each length starting from each node and we found that adding a weight \u03b2 to the multi-step path triples improves the results.",
  "y": "background"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_14",
  "x": "Additionally, we define trigger-mapped dependency paths, where only important \"trigger\" words are lexicalized and the rest of the words are replaced with a wild-card character X. A set of 333 words often associated with regulation events in Literome (e.g. induce, inhibit, reduce, suppress) were used as trigger words. To avoid introducing too much noise, we only included textual relations that occur at least 5 times between mentions of two genes that have a KB relation. This resulted in 3,827 distinct textual relations and 1,244,186 mentions. 6 The number of textual relations is much larger than that of KB relations, and it helped induce much larger connectivity among genes (390,338 pairs of genes are directly connected in text versus 12,100 pairs in KB). Systems ALL-PATHS denotes our compositional learning approach that sums over all paths using dynamic programming; ALL-PATHS+NODES additionally models nodes in the paths. PRUNED-PATHS denotes the traditional approach that learns from sampled paths detailed in \u00a73.1.2; paths with occurrence less than a cutoff are pruned (c = 1 in Table 1 means that all sampled paths are used). The most relevant prior approach is<cite> Guu et al. (2015)</cite> . We ran experiments using both their publicly available code and our re-implementation. We also included the BILINEAR-DIAG baseline.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_15",
  "x": "PRUNED-PATHS denotes the traditional approach that learns from sampled paths detailed in \u00a73.1.2; paths with occurrence less than a cutoff are pruned (c = 1 in Table 1 means that all sampled paths are used). The most relevant prior approach is<cite> Guu et al. (2015)</cite> . We ran experiments using both their publicly available code and our re-implementation. We also included the BILINEAR-DIAG baseline. Implementation Details We used batch training with RProp (Riedmiller and Braun, 1993) . The L 2 penalty \u03bb was set to 0.1 for all models, and the entity vectors x e were normalized to unit vectors. For each positive example we sample 500 negative examples. For our implementation of<cite> (Guu et al., 2015)</cite> , we run 5 random walks of each length starting from each node and we found that adding a weight \u03b2 to the multi-step path triples improves the results. After preliminary experimentation, we fixed \u03b2 to 0.1.",
  "y": "extends"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_16",
  "x": "Comparison among the baselines also offers valuable insights. The implementation of<cite> Guu et al. (2015)</cite> with default parameters performed significantly worse than our re-implementation. Also, our re-implementation achieves only a slight gain over the BILINEAR-DIAG baseline, whereas the original implementation obtains substantial improvement over its own version of BILINEAR-DIAG. These results underscore the importance of hyper-parameters and optimization, and invite future systematic research on the impact of such modeling choices. 11 Model MAP HITS@10 BILINEAR-DIAG<cite> (Guu et al., 2015)</cite> N/A 12.9 BILINEAR-DIAG 8.0 12.2 +Guu et al. (2015) N/A 14.4 PRUNED-PATHS l = 3 c=10 9.5 14.8 PRUNED-PATHS l = 3 c=1 9.5 14.9 PRUNED-PATHS l = 5 c=10 8.9 14.4 ALL-PATHS l = 3 9.4 14.7 ALL-PATHS+NODES l=3 9.4 15.2 ALL-PATHS l = 5 9.6 16.6 ALL-PATHS+NODES l=5 9.8 16.7 Table 2 : KB completion results on the WordNet test set: comparison of our compositional learning approach (ALL-PATHS) with baseline systems. The maximum length of paths is denoted by l. Sampled paths occurring less than c times were pruned in PRUNED-PATHS. (Guu et al., 2015) and our implementation. The MAP results were not reported in<cite> Guu et al. (2015)</cite> ; hence the NA value for MAP in row one.",
  "y": "differences"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_17",
  "x": "---------------------------------- **NCI-PID RESULTS** Comparison among the baselines also offers valuable insights. The implementation of<cite> Guu et al. (2015)</cite> with default parameters performed significantly worse than our re-implementation. Also, our re-implementation achieves only a slight gain over the BILINEAR-DIAG baseline, whereas the original implementation obtains substantial improvement over its own version of BILINEAR-DIAG. These results underscore the importance of hyper-parameters and optimization, and invite future systematic research on the impact of such modeling choices. 11 Model MAP HITS@10 BILINEAR-DIAG<cite> (Guu et al., 2015)</cite> N/A 12.9 BILINEAR-DIAG 8.0 12.2 +Guu et al. (2015) N/A 14.4 PRUNED-PATHS l = 3 c=10 9.5 14.8 PRUNED-PATHS l = 3 c=1 9.5 14.9 PRUNED-PATHS l = 5 c=10 8.9 14.4 ALL-PATHS l = 3 9.4 14.7 ALL-PATHS+NODES l=3 9.4 15.2 ALL-PATHS l = 5 9.6 16.6 ALL-PATHS+NODES l=5 9.8 16.7 Table 2 : KB completion results on the WordNet test set: comparison of our compositional learning approach (ALL-PATHS) with baseline systems. The maximum length of paths is denoted by l. Sampled paths occurring less than c times were pruned in PRUNED-PATHS.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_18",
  "x": "These results underscore the importance of hyper-parameters and optimization, and invite future systematic research on the impact of such modeling choices. 11 Model MAP HITS@10 BILINEAR-DIAG<cite> (Guu et al., 2015)</cite> N/A 12.9 BILINEAR-DIAG 8.0 12.2 +Guu et al. (2015) N/A 14.4 PRUNED-PATHS l = 3 c=10 9.5 14.8 PRUNED-PATHS l = 3 c=1 9.5 14.9 PRUNED-PATHS l = 5 c=10 8.9 14.4 ALL-PATHS l = 3 9.4 14.7 ALL-PATHS+NODES l=3 9.4 15.2 ALL-PATHS l = 5 9.6 16.6 ALL-PATHS+NODES l=5 9.8 16.7 Table 2 : KB completion results on the WordNet test set: comparison of our compositional learning approach (ALL-PATHS) with baseline systems. The maximum length of paths is denoted by l. Sampled paths occurring less than c times were pruned in PRUNED-PATHS. (Guu et al., 2015) and our implementation. The MAP results were not reported in<cite> Guu et al. (2015)</cite> ; hence the NA value for MAP in row one. 12 On this dataset, our implementation of the baseline model does not have substantially different results than<cite> Guu et al. (2015)</cite> and we use their reported results for the baseline and compositionally trained model. Compositional training improved performance in Hits@10 from 12.9 to 14.4 in<cite> Guu et al. (2015)</cite> , and we find that using PRUNED-PATHS as features gives similar, but a bit higher performance gains. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_19",
  "x": "These results underscore the importance of hyper-parameters and optimization, and invite future systematic research on the impact of such modeling choices. 11 Model MAP HITS@10 BILINEAR-DIAG<cite> (Guu et al., 2015)</cite> N/A 12.9 BILINEAR-DIAG 8.0 12.2 +Guu et al. (2015) N/A 14.4 PRUNED-PATHS l = 3 c=10 9.5 14.8 PRUNED-PATHS l = 3 c=1 9.5 14.9 PRUNED-PATHS l = 5 c=10 8.9 14.4 ALL-PATHS l = 3 9.4 14.7 ALL-PATHS+NODES l=3 9.4 15.2 ALL-PATHS l = 5 9.6 16.6 ALL-PATHS+NODES l=5 9.8 16.7 Table 2 : KB completion results on the WordNet test set: comparison of our compositional learning approach (ALL-PATHS) with baseline systems. The maximum length of paths is denoted by l. Sampled paths occurring less than c times were pruned in PRUNED-PATHS. (Guu et al., 2015) and our implementation. The MAP results were not reported in<cite> Guu et al. (2015)</cite> ; hence the NA value for MAP in row one. 12 On this dataset, our implementation of the baseline model does not have substantially different results than<cite> Guu et al. (2015)</cite> and we use their reported results for the baseline and compositionally trained model. Compositional training improved performance in Hits@10 from 12.9 to 14.4 in<cite> Guu et al. (2015)</cite> , and we find that using PRUNED-PATHS as features gives similar, but a bit higher performance gains. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_20",
  "x": "Model MAP HITS@10 BILINEAR-DIAG<cite> (Guu et al., 2015)</cite> N/A 12.9 BILINEAR-DIAG 8.0 12.2 +Guu et al. (2015) N/A 14.4 PRUNED-PATHS l = 3 c=10 9.5 14.8 PRUNED-PATHS l = 3 c=1 9.5 14.9 PRUNED-PATHS l = 5 c=10 8.9 14.4 ALL-PATHS l = 3 9.4 14.7 ALL-PATHS+NODES l=3 9.4 15.2 ALL-PATHS l = 5 9.6 16.6 ALL-PATHS+NODES l=5 9.8 16.7 Table 2 : KB completion results on the WordNet test set: comparison of our compositional learning approach (ALL-PATHS) with baseline systems. The maximum length of paths is denoted by l. Sampled paths occurring less than c times were pruned in PRUNED-PATHS. (Guu et al., 2015) and our implementation. The MAP results were not reported in<cite> Guu et al. (2015)</cite> ; hence the NA value for MAP in row one. 12 On this dataset, our implementation of the baseline model does not have substantially different results than<cite> Guu et al. (2015)</cite> and we use their reported results for the baseline and compositionally trained model. Compositional training improved performance in Hits@10 from 12.9 to 14.4 in<cite> Guu et al. (2015)</cite> , and we find that using PRUNED-PATHS as features gives similar, but a bit higher performance gains. ---------------------------------- **WORDNET RESULTS** The PRUNED-PATHS method is evaluated using count cutoffs of 1 and 10, and maximum path lengths of 3 and 5.",
  "y": "similarities"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_21",
  "x": "---------------------------------- **WORDNET RESULTS** The PRUNED-PATHS method is evaluated using count cutoffs of 1 and 10, and maximum path lengths of 3 and 5. As can be seen, lower count cutoff performed better for paths up to length 3, but we could not run the method with path lengths up to 5 and count cutoff of 1, due to excessive memory requirements (more than 248GB). When using count cutoff of 10, paths up to length 5 performed worse than paths up to length 3. This performance degradation could be avoided with 12 We ran the trained model distributed by<cite> Guu et al. (2015)</cite> and obtained a much lower Hits@10 value of 6.4 and MAP of of 3.5. Due to the discrepancy, we report the original results from the authors' paper which lack MAP values instead. a staged training regiment where models with shorter paths are first trained and used to initialize models using longer paths. The performance of the ALL-PATHS method can be seen for maximum paths up to lengths 3 and 5, and with or without using features on intermediate path nodes.",
  "y": "uses"
 },
 {
  "id": "117d30ddacd28478c6cce1e6d39c12_0",
  "x": "No matter which method is used, Treebank is a widely used resource in parsing task. Most approaches utilize complex features to re-estimate the tree structures of a given sentence [1, 2, 3] . Unfortunately, sizes of treebanks are generally small and insufficient, which results in a common problem of data sparseness. Learning knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, 5, 6] . How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, <cite>8,</cite> 9 ]. The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words.",
  "y": "motivation"
 },
 {
  "id": "117d30ddacd28478c6cce1e6d39c12_1",
  "x": "Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather ---------------------------------- **** knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, 5, 6] . How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, <cite>8</cite>, 9] . The word2vec [10] is among the most widely used word embedding models today.",
  "y": "motivation"
 },
 {
  "id": "117d30ddacd28478c6cce1e6d39c12_2",
  "x": "Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, <cite>8</cite>, 9] . The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather The 2016 Conference on Computational Linguistics and Speech Processing ROCLING 2016, pp. 100-102 \uf0d3 The Association for Computational Linguistics and Chinese Language Processing 100 than window contexts in word2vec. Bansal et al. [<cite>8</cite>] and Melamud et al. [11] show the benefits of such modified-context embeddings in dependency parsing task. The dependency-based word embedding can relieve the problem of data sparseness, since even without occurrence of dependency word pairs in a corpus, dependency scores can be still calculated by word embeddings [12] .",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_0",
  "x": "Yet machine translation may be used in settings in which robustness to such errors is critical: for example, social media text in which there is little emphasis on standard spelling (Michel and Neubig, 2018) , and interactive settings in which users must enter text on a mobile device. Systems that are trained on clean data generally perform poorly when faced with such errors at test time (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) . One potential solution is to introduce noise at training time, an approach that is similar in spirit to the use of adversarial examples in other areas of machine learning (Goodfellow et al., 2014) and natural language processing (Ebrahimi et al., 2018) . So far, using synthetic noise at training time has been found to only improve performance on test data with exactly the same kind of synthetic noise, while at the same time impairing performance on clean test data (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) . We desire methods that yield good performance on both clean text as well as naturally-occurring noise, but this is beyond the reach of current techniques. Drawing inspiration from dropout (Srivastava et al., 2014) and noise-based regularization methods, we explore the space of random noising methods at training time, and evaluate performance on both clean text and text corrupted by natural noise based on real spelling mistakes on Wikipedia (Max and Wisniewski, 2010 ). We find that by feeding our translation models a balanced diet of several types of synthetic noise at training time -random character deletions, insertions, substitutions, and swapsit is possible to obtain substantial improvements on such naturally noisy data, with minimal impact on the performance on clean data, and without accessing the test noise data or even its distribution. We demonstrate that our method substantially improves the robustness of a transformer-based machine translation model with CNN character encoders to spelling errors across multiple input languages (German, French, and Czech) . Of the different noise types we use at training, we find that random character deletions are particularly useful, followed by character insertions.",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_1",
  "x": "Machine translation systems are generally trained on clean data, without spelling errors. Yet machine translation may be used in settings in which robustness to such errors is critical: for example, social media text in which there is little emphasis on standard spelling (Michel and Neubig, 2018) , and interactive settings in which users must enter text on a mobile device. Systems that are trained on clean data generally perform poorly when faced with such errors at test time (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) . One potential solution is to introduce noise at training time, an approach that is similar in spirit to the use of adversarial examples in other areas of machine learning (Goodfellow et al., 2014) and natural language processing (Ebrahimi et al., 2018) . So far, using synthetic noise at training time has been found to only improve performance on test data with exactly the same kind of synthetic noise, while at the same time impairing performance on clean test data (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) . We desire methods that yield good performance on both clean text as well as naturally-occurring noise, but this is beyond the reach of current techniques. Drawing inspiration from dropout (Srivastava et al., 2014) and noise-based regularization methods, we explore the space of random noising methods at training time, and evaluate performance on both clean text and text corrupted by natural noise based on real spelling mistakes on Wikipedia (Max and Wisniewski, 2010 ). We find that by feeding our translation models a balanced diet of several types of synthetic noise at training time -random character deletions, insertions, substitutions, and swapsit is possible to obtain substantial improvements on such naturally noisy data, with minimal impact on the performance on clean data, and without accessing the test noise data or even its distribution. We demonstrate that our method substantially improves the robustness of a transformer-based machine translation model with CNN character encoders to spelling errors across multiple input languages (German, French, and Czech) .",
  "y": "background motivation"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_2",
  "x": "**NOISE MODELS** We focus on orthographical noise, which is noise at the character level, affecting the spelling of individual terms. Orthographical noise is obviously problematic for machine translation systems that operate on token-level embeddings, because noised terms are likely to be out-ofvocabulary, even when pre-segmented into subwords using techniques such as byte-pair encoding (Sennrich et al., 2015) . A more subtle issue is that orthographical noise can also pose problems for character-level encoding models. Typical character-level encoders are based on models such as convolutional neural networks (Kim et al., 2016) , which learn to match filters against specific character n-grams. When these n-grams are disrupted by orthographical noise, the resulting encoding may be radically different from the encoding of a \"clean\" version of the same text. <cite>Belinkov and Bisk (2018)</cite> report significant degradations in performance after applying noise to only a small fraction of input tokens. 1 Table 1 describes the four types of synthetic orthographic noise we used during training. Substitutions and swaps were experimented with extensively in previous work (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) , but deletion and insertion were not.",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_3",
  "x": "A more subtle issue is that orthographical noise can also pose problems for character-level encoding models. Typical character-level encoders are based on models such as convolutional neural networks (Kim et al., 2016) , which learn to match filters against specific character n-grams. When these n-grams are disrupted by orthographical noise, the resulting encoding may be radically different from the encoding of a \"clean\" version of the same text. <cite>Belinkov and Bisk (2018)</cite> report significant degradations in performance after applying noise to only a small fraction of input tokens. 1 Table 1 describes the four types of synthetic orthographic noise we used during training. Substitutions and swaps were experimented with extensively in previous work (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) , but deletion and insertion were not. Deletion and insertion pose a different challenge to character encoders, since they alter the distances between character sequences in the word, as well as its overall length. In Section 3.2, we show that they are indeed the primary contributors in improving our model's robustness to natural noise. During training, we used a balanced diet of all four noise types by sampling the noise, for each to-ken, from a multinomial distribution of 60% clean (no noise) and 10% probability for each type of noise.",
  "y": "motivation background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_4",
  "x": "We optimized the model with Adam and used the inverse square-root learning rate schedule typically used for transformers, but with a peek learning rate of 0.001. Each batch contained a maximum of 8,000 tokens. We used a dropout rate of 0.2. We used beam search for generating the translations (5 beams), and computed BLEU scores to measure performance on the test 3 https://github.com/pytorch/fairseq set. Table 2 shows the performance of the model on data with varying amounts of natural orthographical errors (see Section 2.2). As observed in prior art (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) , when there are significant amounts of natural noise, the model's performance drops significantly. However, training on our synthetic noise cocktail greatly improves performance, regaining between 20% (Czech) and 50% (German) of the BLEU score that was lost to natural noise. Moreover, the negative effects of training on synthetic noise seem to be limited to both negative and positive fluctuations that are smaller than 1 BLEU point. ----------------------------------",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_5",
  "x": "As observed in prior art (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) , when there are significant amounts of natural noise, the model's performance drops significantly. However, training on our synthetic noise cocktail greatly improves performance, regaining between 20% (Czech) and 50% (German) of the BLEU score that was lost to natural noise. Moreover, the negative effects of training on synthetic noise seem to be limited to both negative and positive fluctuations that are smaller than 1 BLEU point. ---------------------------------- **RESULTS** ---------------------------------- **ABLATION ANALYSIS** To determine the individual contribution of each type of synthetic noise, we conduct an ablation study. We first add only one type of synthetic noise at 10% (i.e. 90% of the training data is clean), and measure performance.",
  "y": "differences"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_6",
  "x": "Table 3 shows the model's performance on the German-to-English dataset when training with various mixtures of noise. We find that deletion is by far the most effective synthetic noise in preparing our model for natural orthographical errors, followed by insertion. The French and Czech datasets exhibit the same trend. We conjecture that the importance of deletion and insertion is that they distort the typical distances between characters, requiring the CNN character encoder to become more invariant to unexpected character movements. The fact that we use deletion and insertion also explains why our model was able to regain a significant portion of its original performance when confronted with natural noise at test time, while <cite>previous work</cite> that trained only on substitutions and swaps was not able to do so <cite>(Belinkov and Bisk, 2018)</cite> . ---------------------------------- **TRANSLATING SOCIAL MEDIA TEXT** We also apply our synthetic noise training procedure to translation of social media, using the recently-released MTNT dataset of Reddit posts (Michel and Neubig, 2018) , focusing on the English-French translation pair. Note that no noise was inserted into the test data in this case; the only source of noise is the non-standard spellings inherent to the dataset.",
  "y": "differences extends"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_7",
  "x": "The use of noise to improve robustness in machine learning has a long history (e.g., Holmstrom and Koistinen, 1992; Wager et al., 2013) , with early work by Bishop (1995) demonstrating a connection between additive noise and regularization. To achieve robustness to orthographical errors, we require noise that operates on the sequence of characters. Heigold et al. (2017) demonstrated that synthetic noising operations such as random swaps and replacements can significantly degrade performance when inserted at test time; they also show that some robustness can be obtained by inserting the same synthetic noise at training time. Similarly, the impact of speech-like noise is explored by Sperber et al. (2017) . Most relevant for us is the work of <cite>Belinkov and Bisk (2018)</cite> , who evaluated on natural noise obtained from Wikipedia edit histories (e.g., Max and Wisniewski, 2010) . <cite>They</cite> find that robustness to natural noise can be obtained by training on the same noise model, but that (a) training on synthetic noise does not yield robustness to natural noise at test time, and (b) training on natural noise significantly impairs performance on clean text. In contrast, we show that training on the right kind and the right amount of synthetic noise can yield substantial improvements on natural noise at test time, without significantly impairing performance on clean data. Our ablation results suggest that deletion and insertion noise -which were not included by <cite>Belinkov and Bisk</cite> -are essential to achieving robustness to natural noise. An alternative approach is to build robustness to character permutations directly into the design of the character-level encoder.",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_8",
  "x": "The use of noise to improve robustness in machine learning has a long history (e.g., Holmstrom and Koistinen, 1992; Wager et al., 2013) , with early work by Bishop (1995) demonstrating a connection between additive noise and regularization. To achieve robustness to orthographical errors, we require noise that operates on the sequence of characters. Heigold et al. (2017) demonstrated that synthetic noising operations such as random swaps and replacements can significantly degrade performance when inserted at test time; they also show that some robustness can be obtained by inserting the same synthetic noise at training time. Similarly, the impact of speech-like noise is explored by Sperber et al. (2017) . Most relevant for us is the work of <cite>Belinkov and Bisk (2018)</cite> , who evaluated on natural noise obtained from Wikipedia edit histories (e.g., Max and Wisniewski, 2010) . <cite>They</cite> find that robustness to natural noise can be obtained by training on the same noise model, but that (a) training on synthetic noise does not yield robustness to natural noise at test time, and (b) training on natural noise significantly impairs performance on clean text. In contrast, we show that training on the right kind and the right amount of synthetic noise can yield substantial improvements on natural noise at test time, without significantly impairing performance on clean data. Our ablation results suggest that deletion and insertion noise -which were not included by <cite>Belinkov and Bisk</cite> -are essential to achieving robustness to natural noise. An alternative approach is to build robustness to character permutations directly into the design of the character-level encoder.",
  "y": "differences extends"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_9",
  "x": "Heigold et al. (2017) demonstrated that synthetic noising operations such as random swaps and replacements can significantly degrade performance when inserted at test time; they also show that some robustness can be obtained by inserting the same synthetic noise at training time. Similarly, the impact of speech-like noise is explored by Sperber et al. (2017) . Most relevant for us is the work of <cite>Belinkov and Bisk (2018)</cite> , who evaluated on natural noise obtained from Wikipedia edit histories (e.g., Max and Wisniewski, 2010) . <cite>They</cite> find that robustness to natural noise can be obtained by training on the same noise model, but that (a) training on synthetic noise does not yield robustness to natural noise at test time, and (b) training on natural noise significantly impairs performance on clean text. In contrast, we show that training on the right kind and the right amount of synthetic noise can yield substantial improvements on natural noise at test time, without significantly impairing performance on clean data. Our ablation results suggest that deletion and insertion noise -which were not included by <cite>Belinkov and Bisk</cite> -are essential to achieving robustness to natural noise. An alternative approach is to build robustness to character permutations directly into the design of the character-level encoder. <cite>Belinkov and Bisk (2018)</cite> experiment with a bag of characters, while Sakaguchi et al. (2017) use character RNNs combined with special representations for the first and last characters of each token. <cite>These models</cite> are particularly suited for specific types of swapping and scrambling noises, but are not robust to natural noise.",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_10",
  "x": "Our ablation results suggest that deletion and insertion noise -which were not included by <cite>Belinkov and Bisk</cite> -are essential to achieving robustness to natural noise. An alternative approach is to build robustness to character permutations directly into the design of the character-level encoder. <cite>Belinkov and Bisk (2018)</cite> experiment with a bag of characters, while Sakaguchi et al. (2017) use character RNNs combined with special representations for the first and last characters of each token. <cite>These models</cite> are particularly suited for specific types of swapping and scrambling noises, but are not robust to natural noise. We also conducted preliminary experiments with similar noise-invariant models, but found that training a CNN with synthetic noise to work better. ---------------------------------- **CONCLUSION** In this work we take a step towards addressing the challenge of making machine translation robust to character-level noise. We show how training on synthetic character-level noise, similar in spirit to dropout, can significantly improve a translation model's robustness to natural spelling mistakes.",
  "y": "background motivation"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_0",
  "x": "If we compose the meaning representations for red and herring, we might expect to get a very different representation from the one which could be directly inferred from corpus observations of the phrase red herring. Thus any judgements of the similarity of two composed phrases may be confounded by the degree to which those phrases are compositional. In this paper, we use a compound noun compositionality dataset (<cite>Reddy et al., 2011</cite>) to investigate the extent to which the underlying definition of context has an effect on a model's ability to support composition. We compare the Anchored Packed Tree (APT) model (Weir et al., 2016) , where composition is an integral part of the distributional model, with the commonly employed approach of applying na\u00efve compositional operations to state-of-the-art distributional representations. Consider the occurrence of the word student in the sentence \"The recently graduated student folded the dry clothes.\" Different distributional representations leverage the context, e.g., the fact that the target word student has occurred in the context folded, in different ways. Table 1 illustrates the contextual features which might be generated for student given different definitions of context. The most commonly used definition of context, in both traditional count-based representations and in more recent distributed embeddings, is proximity, i.e., the contextual features of a word occurrence are all those words which occur within a certain context window around the occurrence. However, contextual features may also be defined in terms of dependency relations. For example, in a dependency parse of the sentence we would expect to see a direct-object relation from folded to student.",
  "y": "uses"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_1",
  "x": "The APT approach (Weir et al., 2016) overcomes this problem by defining contextual features in terms of complete dependency paths and then ensuring that the representations of target words are properly aligned before composition. For example, to carry out the composition of student with folded in the example sentence, it is necessary to align the representations. This can be done by offsetting all of the features of student by its dependency relation (NSUBJ) with folded. Intuitively we are viewing the representation of student from the perspective of actions (i.e., verbs) which are likely to be carried out by students. This view can be straightforwardly composed with the representation of folded because the representations are aligned i.e., they have features of the same type (e.g., DOBJ). ---------------------------------- **COMPOSITIONALITY OF COMPOUND NOUNS** Compositionality detection (<cite>Reddy et al., 2011</cite>) involves deciding whether a given multiword expression is compositional or not i.e., whether the meaning can be understood from the literal meaning of its parts. <cite>Reddy et al. (2011)</cite> introduced a dataset consisting of 90 compound nouns along with human judgments of their literality or compositionally at both the constituent and the phrase level.",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_2",
  "x": "Accordingly, as observed elsewhere (<cite>Reddy et al., 2011</cite>; Salehi et al., 2015; Yazdani et al., 2015) , compositional methods can be evaluated by correlating the similarity of composed and observed phrase representations with the human judgments of compositionality. A similar idea is also explored by Kiela and Clark (2013) who detect noncompositional phrases by comparing the neighbourhoods of phrases where individual words have been substituted for similar words. <cite>Reddy et al. (2011)</cite> carried out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100). Used 3-fold cross-validation, <cite>they</cite> found that using weighted addition outperformed multiplication as a compositionality function. With <cite>their</cite> optimal settings, <cite>they</cite> achieved a Spearman's rank correlation coefficient of 0.714 with the human judgments, which remains the state-of-the-art on this dataset 1 . For consistency with the experiments of <cite>Reddy et al. (2011)</cite> , the corpus used in this experiment is the same fullyannotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008) . This corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependency-parsed with the Malt Parser (Nivre, 2004) . It contains about 1.9 billion tokens. In order to create a corpus which contains compound nouns, we further preprocessed the corpus by identifying occurrences of the 90 target compound nouns and recombining them into a single lexical item.",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_3",
  "x": "A similar idea is also explored by Kiela and Clark (2013) who detect noncompositional phrases by comparing the neighbourhoods of phrases where individual words have been substituted for similar words. <cite>Reddy et al. (2011)</cite> carried out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100). Used 3-fold cross-validation, <cite>they</cite> found that using weighted addition outperformed multiplication as a compositionality function. With <cite>their</cite> optimal settings, <cite>they</cite> achieved a Spearman's rank correlation coefficient of 0.714 with the human judgments, which remains the state-of-the-art on this dataset 1 . For consistency with the experiments of <cite>Reddy et al. (2011)</cite> , the corpus used in this experiment is the same fullyannotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008) . This corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependency-parsed with the Malt Parser (Nivre, 2004) . It contains about 1.9 billion tokens. In order to create a corpus which contains compound nouns, we further preprocessed the corpus by identifying occurrences of the 90 target compound nouns and recombining them into a single lexical item. We then created a number of elementary representations for every token in the corpus.",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_4",
  "x": "Assuming the distributional hypothesis (Harris, 1954) , the observed co-occurrences of compositional target phrases are highly likely to have occurred with one or both of the constituents independently. On the other hand, the observed cooccurrences of non-compositional target phrases are much less likely to have occurred with either of the constituents independently. Thus, a good compositionality function, without any access to the observed co-occurrences of the target phrases, is highly likely to return vectors which are similar to observed phrasal vectors for compositional phrases but much less likely to return similar vectors for non-compositional phrases. Accordingly, as observed elsewhere (<cite>Reddy et al., 2011</cite>; Salehi et al., 2015; Yazdani et al., 2015) , compositional methods can be evaluated by correlating the similarity of composed and observed phrase representations with the human judgments of compositionality. A similar idea is also explored by Kiela and Clark (2013) who detect noncompositional phrases by comparing the neighbourhoods of phrases where individual words have been substituted for similar words. <cite>Reddy et al. (2011)</cite> carried out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100). Used 3-fold cross-validation, <cite>they</cite> found that using weighted addition outperformed multiplication as a compositionality function. With <cite>their</cite> optimal settings, <cite>they</cite> achieved a Spearman's rank correlation coefficient of 0.714 with the human judgments, which remains the state-of-the-art on this dataset 1 . For consistency with the experiments of <cite>Reddy et al. (2011)</cite> , the corpus used in this experiment is the same fullyannotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008) .",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_5",
  "x": "A similar idea is also explored by Kiela and Clark (2013) who detect noncompositional phrases by comparing the neighbourhoods of phrases where individual words have been substituted for similar words. <cite>Reddy et al. (2011)</cite> carried out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100). Used 3-fold cross-validation, <cite>they</cite> found that using weighted addition outperformed multiplication as a compositionality function. With <cite>their</cite> optimal settings, <cite>they</cite> achieved a Spearman's rank correlation coefficient of 0.714 with the human judgments, which remains the state-of-the-art on this dataset 1 . For consistency with the experiments of <cite>Reddy et al. (2011)</cite> , the corpus used in this experiment is the same fullyannotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008) . This corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependency-parsed with the Malt Parser (Nivre, 2004) . It contains about 1.9 billion tokens. In order to create a corpus which contains compound nouns, we further preprocessed the corpus by identifying occurrences of the 90 target compound nouns and recombining them into a single lexical item. We then created a number of elementary representations for every token in the corpus.",
  "y": "motivation"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_6",
  "x": "In relation to the construction of the elementary APTs, the most obvious parameter is the nature of the weight associated with each feature. We consider both the use of probabilities 2 and positive pointwise mutual information (PPMI) 1 Hermann et al. (2012) proposed using generative models for modeling the compositionality of noun-noun compounds. Using interpolation to mitigate the sparse data problem, their model beat the baseline of weighted addition on the <cite>Reddy et al. (2011)</cite> evaluation task when trained on the BNC. However, these results were still significantly lower than those reported by <cite>Reddy et al. (2011)</cite> using the larger ukWaC corpus. 2 referred to as normalised counts by Weir et al. (2016) values. Levy et al. (2015) showed that the use of context distribution smoothing (\u03b1 = 0.75) in the PMI calculation can lead to performance comparable with state-of-the-art word embeddings on word similarity tasks. We use this modified definition of PMI and experiment with \u03b1 = 0.75 and \u03b1 = 1. 3 Having constructed elementary APTs, the APT composition process involves aligning and composing these elementary APTs.",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_7",
  "x": "We investigate using INT , which takes the minimum of each of the constituent's feature values and UNI , which performs pointwise addition. Following <cite>Reddy et al. (2011)</cite> , when using the UNI operation, we experiment with weighting the contributions of each constituent to the composed APT representation using the parameter, h. For example, if A 2 is the APT associated with the head of the phrase and A \u03b4 1 is the properly aligned APT associated with the modifier where \u03b4 is the dependency path from the head to the modifier (e.g. NMOD or AMOD), the composition operations can be defined as: (1) We have also considered composition without alignment of the modifier's APT, i.e, using A 1 : In general, one would expect there to be little overlap between APTs which have not been properly aligned. However, in the case where \u03b4 is the NMOD relation, i.e., the internal relation in the vast majority of the compound phrases, both modifier and head are nouns and therefore there may well be considerable overlap between their unaligned dependency features. In order to examine the contribution of both the aligned and unaligned APTs in the composition process, we used a hybrid method where the composed representation is defined as: Table 2 : Average \u03c1 using neural word embeddings In the case where representations consist of APT weights which are probabilities, PPMI is estimated after composition. Therefore we refer to this as compose-first (CF) in contrast to composesecond (CS) where composition is carried out after PPMI calculations.",
  "y": "uses"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_8",
  "x": "---------------------------------- **RESULTS** We used repeated 3-fold cross-validation to enable us to estimate 4 the model parameters h and q. Results for all models are then reported in terms of average Spearman rank correlation scores (\u03c1) of phrase compositionality scores with human judgements on the corresponding testing samples. We used a sufficiently large number of repetitions that errors are all small (\u2264 0.0015) and thus any difference observed which is greater than 0.005 is statistically significant at the 95% level. Boldface is used to indicate the best performing configuration of parameters for a particular model. Table 2 summarises results for different parameter settings for the neural word embeddings. Looking at the results in Table 2 , we see that the cbow model significantly outperforms the skip-gram model. Using the cbow model with 100 dimensions and a subsampling threshold of t = 10 \u22123 gives a performance of 0.74 which is significantly higher than the previous state-ofthe-art reported in <cite>Reddy et al. (2011)</cite> . Since both of these models are based on untyped cooccurrences, this performance gain can be seen as the result of implicit parameter optimisation.",
  "y": "differences"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_9",
  "x": "APT representations. We see that the results using standard PPMI (\u03b1 = 1) significantly outperform the result reported in <cite>Reddy et al. (2011)</cite> , which demonstrates the superiority of a typed dependency space over an untyped dependency space. Smoothing the PPMI calculation with a value of \u03b1 = 0.75 generally has a further small positive effect. On average, the results when probabilities are composed and PPMI is calculated as part of the similarity calculation (CF) are slightly higher than the results when PPMI weights are composed (CS) . Regarding different composition operations, UNI generally outperforms INT . In general, the unaligned model outperforms the aligned model. However, a small but statistically significant performance gain is generally made using the hybrid model. Therefore aligned APT composition and unaligned APT composition are predicting different contexts for compound nouns which all contribute to a better estimate of the compositionality of the phrase. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_0",
  "x": "This work sets a new state-of-the-art in those tasks for Basque. All benchmarks and models used in this work are publicly available. ---------------------------------- **INTRODUCTION** Word embeddings and pre-trained language models allow to build rich representations of text and have enabled improvements across most NLP tasks. The most successful models include word embeddings like Fast-Text (Bojanowski et al., 2017) , and, more recently, pretrained language models which can be used to produce contextual embeddings or directly fine-tuned for each task. Good examples of the later are character-based models like Flair (Akbik et al., 2018) and masked language models like BERT<cite> (Devlin et al., 2019)</cite> . In many cases the teams that developed the algorithms also release their models, which facilitates both reproducibility and their application in downstream tasks. Given that the models are expensive to train, many companies and research groups tend to use those models, rather than building their own.",
  "y": "background"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_1",
  "x": "Good examples of the later are character-based models like Flair (Akbik et al., 2018) and masked language models like BERT<cite> (Devlin et al., 2019)</cite> . In many cases the teams that developed the algorithms also release their models, which facilitates both reproducibility and their application in downstream tasks. Given that the models are expensive to train, many companies and research groups tend to use those models, rather than building their own. This could be suboptimal as, for many languages, the models have been trained on easily obtained public corpora, which tends to be smaller and/or of lower quality that other existing corpora. In addition, pre-trained language models for non-English languages are not always available. In that case, only multilingual versions are available, where each language shares the quota of substrings and parameters with the rest of the languages, leading to a decrease in performance<cite> (Devlin et al., 2019)</cite> . The chances for smaller languages, as for instance Basque, seem even direr, as easily available public corpora is very limited, and the quota of substrings depends on corpus size. As an illustration of the issues mentioned above, the multilingual BERT which Basque shares with other 103 languages is based on Wikipedia corpora, where English amounts to 2.5 thousand million tokens whereas for Basque it contains around 35 million tokens. Our corpus uses, in addition to the Basque Wikipedia, corpora crawled from news outlets (191M tokens) .",
  "y": "background"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_2",
  "x": "The chances for smaller languages, as for instance Basque, seem even direr, as easily available public corpora is very limited, and the quota of substrings depends on corpus size. As an illustration of the issues mentioned above, the multilingual BERT which Basque shares with other 103 languages is based on Wikipedia corpora, where English amounts to 2.5 thousand million tokens whereas for Basque it contains around 35 million tokens. Our corpus uses, in addition to the Basque Wikipedia, corpora crawled from news outlets (191M tokens) . Another important issue is subword tokenization. Thus, for some common Basque words such as etxerantz (to the house) or medikuarenera (to the doctor), the subword tokenization generated by the monolingual BERT we trained will substantially differ from the output produced by the multilingual BERT: mBERT: Et #xer #ant #z ours: Etxera #ntz mBERT: Medi #kua #rene #ra ours: Mediku #aren #era More specifically, mBERT's subwords tend to be shorter and less interpretable, while our subwords are closer to linguistically interpretable strings, like mediku (doctor) aren ('s) and era (to the). Furthermore, most of the time the released models have been thoroughly tested only in English. Alternatively, multilingual versions have been tested in transfer learning scenarios for other languages, where they have not been compared to monolingual versions<cite> (Devlin et al., 2019)</cite> . The goal of this paper is to compare publicly available models for Basque with analogous models which have been trained with a larger, better quality corpus.",
  "y": "background"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_3",
  "x": "Fast-Text (Bojanowski et al., 2017) proposes an improvement over those models, consisting on embedding subword units, thereby attempting to introduce morphological information. Rich morphology languages such as Basque should especially profit from such word representations. FastText distributes embeddings for more than 150 languages trained on Common Crawl and Wikipedia. In this paper we build FastText embeddings using a carefully collected corpus in Basque and show that it performs better than the officially distributed embeddings in all NLP we tested, which stresses the importance of a following a carefully designed method when building and collecting the corpus. The aforementioned methods generate static word embeddings, that is, they provide a unique vector-based representation for a given word independently of the context in which the word occurs. Thus, if we consider the Basque word banku 3 , static word embedding approaches will calculate one vector irrespective of the fact that the same word banku may convey different senses when used in different contexts, namely, \"financial institution\",\"bench\", \"supply or stock\", among others. In order to address this problem, contextual word embeddings are proposed; the idea is to be able to generate different word representations according to the context in which the word appears. Examples of such contextual representations are ELMO (Peters et al., 2018) and Flair (Akbik et al., 2018) , which are built upon LSTM-based architectures and trained as language models. More recently,<cite> Devlin et al. (2019)</cite> introduced BERT, a model based on the transformer architecture trained as a masked language model, which has obtained very good results on a variety of NLP tasks.",
  "y": "motivation"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_4",
  "x": "Flair (embeddings and system) have been successfully applied to sequence labeling tasks obtaining state-of-the-art results for a number of English Named Entity Recognition (NER) and Part-of-Speech tagging benchmarks (Akbik et al., 2018) , outperforming other well-known approaches such as BERT and ELMO <cite>(Devlin et al., 2019</cite>; Peters et al., 2018) . In any case, Flair is of interest to us because they distribute their own Basque pre-trained embedding models obtained from a corpus of 36M tokens (combining OPUS and Wikipedia). Flair-BMC models: We train our own Flair embeddings using the BMC corpus with the following parameters: Hidden size 2048, sequence length of 250, and a mini-batch size of 100. The rest of the parameters are left in their default setting. Training was done for 5 epochs over the full training corpus. The training of each model took 48h on a Nvidia Titan V GPU. Flair Embeddings: Flair's embeddings model words as sequences of characters. Moreover, the vector-based representation of a word will depend on its surrounding context. More specifically, to generate word embeddings they feed sentences as sequences of characters into a character-level Long short-term memory (LSTM) model which at each point in the sequence is trained to predict the next character.",
  "y": "background"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_5",
  "x": "**BERT LANGUAGE MODELS** We have trained a BERT<cite> (Devlin et al., 2019)</cite> model for Basque Language using the BMC corpus motivated by the rather low representation this language has in the original multilingual BERT model. In this section we describe the methods used for creating the vocabulary, the model architecture, the pre-training objective and procedure. The main differences between our model and the original implementation are the corpus used for the pre-training, the algorithm for sub-word vocabulary creation and the usage of a different masking strategy that is not available for the BERT BASE model yet. Sub-word vocabulary We create a cased sub-word vocabulary containing 50,000 tokens using the unigram language model based sub-word segmentation algorithm proposed by Kudo (2018) . We do not use the same algorithm as BERT because the WordPiece (Wu et al., 2016) implementation they originally used is not publicly available. We have increased the vocabulary size from 30,000 sub-word units up to 50,000 expecting to be beneficial for the Basque language due to its agglutinative nature. Our vocabulary is learned from the whole training corpus but we do not cover all the characters in order to avoid very rare ones. We set the coverage percentage to 99.95.",
  "y": "uses"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_6",
  "x": "We have increased the vocabulary size from 30,000 sub-word units up to 50,000 expecting to be beneficial for the Basque language due to its agglutinative nature. Our vocabulary is learned from the whole training corpus but we do not cover all the characters in order to avoid very rare ones. We set the coverage percentage to 99.95. Model Architecture In the same way as the original BERT architecture proposed by<cite> Devlin et al. (2019)</cite> our model is composed by stacked layers of Transformer encoders (Vaswani et al., 2017) . Our approach follows the BERT BASE configuration containing 12 Transformer encoder layers, a hidden size of 768 and 12 self-attention heads for a total of 110M parameters. Pre-training objective Following BERT original implementation, we train our model on the Masked Language Model (MLM) and Next Sentence Prediction (NSP) tasks. Even if the necessity of the NSP task has been questioned by some recent works (Yang et al., 2019; Liu et al., 2019; Lample and Conneau, 2019) we have decided to keep it as in the original paper to allow for head-to-head comparison. For the MLM, given an input sequence composed of N tokens x 1 , x 2 , ..., x n we select a 15% of them as masking candidates. Then, 80% of these selected tokens are masked by replacing them with the [MASK] token, 10% are replaced with a random word of the vocabulary and the remaining 10% are left unchanged.",
  "y": "differences extends"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_7",
  "x": "At the end, the model is trained to optimize the sum of the means of the MLM and NSP likelihoods. As our vocabulary consists of sub-word units, we use whole-word masking (WWM), that applies the masking to whole words instead of sub-word units. This new masking strategy makes the MLM task more difficult for the system as it has to predict the whole word instead of predicting just part of it. An upgraded version of BERT LARGE 7 has proven that WWM has substantial benefits in comparison with previous masking that was done after the sub-word tokenization. Pre-training procedure Similar to<cite> (Devlin et al., 2019)</cite> we use Adam with learning rate of 1e \u2212 4, \u03b2 1 = 0.9, \u03b2 2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10, 0000 steps, and linear decay of the learning rate. The dropout probability is fixed to 0.1 on all the layers. As the attentions are quadratic to the sequence length, making longer sequences much more expensive, we pre-train the model with sequence length of 128 for 90% of the steps and sequence length of 512 for 10% of the steps. In total we train for 1, 000, 000 steps and a batch size of 256. The first 90% steps are trained using Cloud v2 TPUs and for the rest of the steps we use Cloud v3 TPUs 8 .",
  "y": "similarities"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_8",
  "x": "To train the Flair system we use the parameters specified in (Akbik et al., 2019) for Pooled Contextual Embeddings. Flair is tuned on the development data using the test only for the final evaluation. We do not use the development set for training. For comparison between BERT models we fine-tune on the training data provided for each of the four tasks with both the official multilingual BERT<cite> (Devlin et al., 2019)</cite> model and with our BERTeus model (trained as described in Section 3.3.). Every reported result for every system is the average of five randomly initialized runs. The POS and NER experiments using mBERT and BERTeus are performed using the transformers library (Wolf et al., 2019) where it is recommended to remove the seed for random initialization. ---------------------------------- **TOPIC CLASSIFICATION** For the task of topic classification a dataset containing 12k news headlines (brief article descriptions) was compiled from the Basque weekly newspaper Argia 9 .",
  "y": "similarities uses"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_0",
  "x": "Recently, the mechanism of self-attention [22, 24] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time. Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer [22] ), sentiment analysis [25] , and other tasks. These successes have motivated preliminary work in self-attention for ASR. Time-restricted self-attention was used as a drop-in replacement for individual layers in the state-of-theart lattice-free MMI model [26] , an HMM-NN system. Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) <cite>[27]</cite> , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems. In this work, we propose and evaluate fully self-attentional networks for CTC (SAN-CTC). We are motivated by practicality: selfattention could be used as a drop-in replacement in existing CTClike systems, where only attention has been evaluated in the past [30, 31] ; unlike encoder-decoder systems, SAN-CTC is able to predict tokens in parallel at inference time; an analysis of SAN-CTC is useful for future state-of-the-art ASR systems, which may equip self-attentive encoders with auxiliary CTC losses [17, 20] . Unlike past works, we do not require convolutional frontends [19] or interleaved recurrences <cite>[27]</cite> to train self-attention for ASR. In Section 2, we motivate the model and relevant design choices (position, downsampling) for ASR.",
  "y": "background"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_1",
  "x": "Recently, the mechanism of self-attention [22, 24] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time. Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer [22] ), sentiment analysis [25] , and other tasks. These successes have motivated preliminary work in self-attention for ASR. Time-restricted self-attention was used as a drop-in replacement for individual layers in the state-of-theart lattice-free MMI model [26] , an HMM-NN system. Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) <cite>[27]</cite> , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems. In this work, we propose and evaluate fully self-attentional networks for CTC (SAN-CTC). We are motivated by practicality: selfattention could be used as a drop-in replacement in existing CTClike systems, where only attention has been evaluated in the past [30, 31] ; unlike encoder-decoder systems, SAN-CTC is able to predict tokens in parallel at inference time; an analysis of SAN-CTC is useful for future state-of-the-art ASR systems, which may equip self-attentive encoders with auxiliary CTC losses [17, 20] . Unlike past works, we do not require convolutional frontends [19] or interleaved recurrences <cite>[27]</cite> to train self-attention for ASR. In Section 2, we motivate the model and relevant design choices (position, downsampling) for ASR.",
  "y": "differences"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_2",
  "x": "However, convolutional models must be significantly deeper to retrieve the same temporal receptive field [23] . Recently, the mechanism of self-attention [22, 24] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time. Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer [22] ), sentiment analysis [25] , and other tasks. These successes have motivated preliminary work in self-attention for ASR. Time-restricted self-attention was used as a drop-in replacement for individual layers in the state-of-theart lattice-free MMI model [26] , an HMM-NN system. Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) <cite>[27]</cite> , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems. In this work, we propose and evaluate fully self-attentional networks for CTC (SAN-CTC). We are motivated by practicality: selfattention could be used as a drop-in replacement in existing CTClike systems, where only attention has been evaluated in the past [30, 31] ; unlike encoder-decoder systems, SAN-CTC is able to predict tokens in parallel at inference time; an analysis of SAN-CTC is useful for future state-of-the-art ASR systems, which may equip self-attentive encoders with auxiliary CTC losses [17, 20] . Unlike past works, we do not require convolutional frontends [19] or interleaved recurrences <cite>[27]</cite> to train self-attention for ASR.",
  "y": "motivation"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_3",
  "x": "Convolutions over time and/or frequency were first used as initial layers to recurrent neural models, beginning with HMM-NNs [34] and later with CTC, where they are viewed as promoting invariance to temporal and spectral translation in ASR [8] , or image translation in handwriting recognition [35] ; they also serve as a form of dimensionality reduction (Section 2.4). However, these networks were still bottlenecked by the sequentiality of operations at the recurrent layers, leading [8] to propose row convolutions for unidirectional RNNs, which had finite lookaheads to enable online processing while having some future context. This led to convolution-only CTC models for long-range temporal dependencies [9] [10] [11] . However, these models have to be very deep (e.g., 17-19 convolutional layers on LibriSpeech [23] ) to cover the same context (Table 1) . While in theory, a relatively local context could suffices for ASR, this is complicated by alphabets L which violate the conditional independence assumption of CTC (e.g., English characters [36] ). Wide contexts also enable incorporation of noise/speaker contexts, as <cite>[27]</cite> suggest regarding the broad-context attention heads in the first layer of their self-attentional LAS model. ---------------------------------- **MOTIVATING THE SELF-ATTENTION LAYER** We now replace recurrent and convolutional layers for CTC with self-attention [24] .",
  "y": "motivation"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_4",
  "x": "---------------------------------- **MOTIVATING THE SELF-ATTENTION LAYER** We now replace recurrent and convolutional layers for CTC with self-attention [24] . Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder [22] , previous explorations of self-attention in ASR [19,<cite> 27]</cite> , and defined in Section 2.3. The other stages are downsampling, which reduces input length T via methods like those in Section 2.4; embedding, which learns a dh-dim. embedding that also describes token position (Section 2.5); and projection, where each final representation is mapped framewise to logits over the intermediate alphabet L . The first implements self-attention, where the success of attention in CTC and encoder-decoder models [14, 31] is parallelized by using each position's representation to attend to all others, giving a contextualized representation for that position. Hence, the full receptive field is immediately available at the cost of O(T 2 ) inner products (Table 1) , enabling richer representations in fewer layers. ---------------------------------- **MODEL**",
  "y": "similarities"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_5",
  "x": "---------------------------------- **SEQUENTIAL OPERATIONS** Maximum path length Table 1 : Operation complexity of each layer type, based on [22] . T is input length, d is no. of hidden units, and k is filter/context width. We also see inspiration from convolutional blocks: residual connections, layer normalization, and tied dense layers with ReLU for representation learning. In particular, multi-head attention is akin to having a number of infinitely-wide filters whose weights adapt to the content (allowing fewer \"filters\" to suffice). One can also assign interpretations; for example, <cite>[27]</cite> argue their LAS self-attention heads are differentiated phoneme detectors. Further inductive biases like filter widths and causality could be expressed through time-restricted self-attention [26] and directed self-attention [25] , respectively. ----------------------------------",
  "y": "background"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_6",
  "x": "This sublayer aggregates the multiple heads at time t into the attention layer's final output at t. All together, the layer is given by: ---------------------------------- **DOWNSAMPLING** In speech, the input length T of frames can be many times larger than the output length U , in contrast to the roughly word-to-word setting of machine translation. This is especially prohibitive for self-attention in terms of memory: recall that an attention matrix of dimension \u2208 R T \u00d7T is created, giving the T 2 factor in Table 1 . A convolutional frontend is a typical downsampling strategy [8, 19] ; however, we leave integrating other layer types into SAN-CTC as future work. Instead, we consider three fixed approaches, from least-to most-preserving of the input data: subsampling, which only takes every k-th frame; pooling, which aggregates every k consecutive frames via a statistic (average, maximum); reshaping, where one concatenates k consecutive frames into one <cite>[27]</cite> . Note that CTC will still require U \u2264 T /k, however.",
  "y": "uses"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_7",
  "x": "for position t. We consider three approaches: content-only [21] , which forgoes position encodings; additive [19] , which takes demb = dh and adds the encoding to the embedding; and concatenative, where one takes demb = 40 and concatenates it to the embedding. The latter was found necessary for self-attentional LAS <cite>[27]</cite> , as additive encodings did not give convergence. However, the monotonicity of CTC is a further positional inductive bias, which may enable the success of content-only and additive encodings. ---------------------------------- **EXPERIMENTS** We take (nlayers, dh, nheads, dff) = (10, 512, 8, 2048), giving \u223c30M parameters. This is on par with models on WSJ (10-30M) [4, 5, 9] and an order of magnitude below models on LibriSpeech (100-250M) [8, 23] . We use MXNet [37] for modeling and Kaldi/EESEN [7, 38] for data preparation and decoding. Our self-attention code is based on GluonNLP's implementation.",
  "y": "motivation"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_8",
  "x": "These models train in one day (Tesla V100), comparable to the Speech Transformer [19] ; however, SAN-CTC gives further benefits at inference time as token predictions are generated in parallel. We also evaluate design choices in Table 4 . Here, we consider the effects of downsampling and position encoding on accuracy for our fixed training regime. We see that unlike self-attentional LAS <cite>[27]</cite> , SAN-CTC works respectably even with no position en- coding; in fact, the contribution of position is relatively minor (compare with [21] , where location in an encoder-decoder system improved CER by 3% absolute). Lossy downsampling appears to preserve performance in CER while degrading WER (as information about frame transitions is lost). We believe these observations align with the monotonicity and independence assumptions of CTC. Inspired by <cite>[27]</cite> , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details. In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike <cite>[27]</cite> we still see mild differentiation of variances. Inspired by [26] , we further plot the attention weights relative to the current time position (here, per head).",
  "y": "differences"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_9",
  "x": "These models train in one day (Tesla V100), comparable to the Speech Transformer [19] ; however, SAN-CTC gives further benefits at inference time as token predictions are generated in parallel. We also evaluate design choices in Table 4 . Here, we consider the effects of downsampling and position encoding on accuracy for our fixed training regime. We see that unlike self-attentional LAS <cite>[27]</cite> , SAN-CTC works respectably even with no position en- coding; in fact, the contribution of position is relatively minor (compare with [21] , where location in an encoder-decoder system improved CER by 3% absolute). Lossy downsampling appears to preserve performance in CER while degrading WER (as information about frame transitions is lost). We believe these observations align with the monotonicity and independence assumptions of CTC. Inspired by <cite>[27]</cite> , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details. In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike <cite>[27]</cite> we still see mild differentiation of variances. Inspired by [26] , we further plot the attention weights relative to the current time position (here, per head).",
  "y": "similarities"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_10",
  "x": "Here, we consider the effects of downsampling and position encoding on accuracy for our fixed training regime. We see that unlike self-attentional LAS <cite>[27]</cite> , SAN-CTC works respectably even with no position en- coding; in fact, the contribution of position is relatively minor (compare with [21] , where location in an encoder-decoder system improved CER by 3% absolute). Lossy downsampling appears to preserve performance in CER while degrading WER (as information about frame transitions is lost). We believe these observations align with the monotonicity and independence assumptions of CTC. Inspired by <cite>[27]</cite> , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details. In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike <cite>[27]</cite> we still see mild differentiation of variances. Inspired by [26] , we further plot the attention weights relative to the current time position (here, per head). Character labels gave forward-and backward-attending heads (incidentally, averaging these would retrieve the bimodal distribution in [26] ) at all layers. This suggests a gradual expansion of context over depth, as is often engineered in convolutional CTC.",
  "y": "differences"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_11",
  "x": "Here, we consider the effects of downsampling and position encoding on accuracy for our fixed training regime. We see that unlike self-attentional LAS <cite>[27]</cite> , SAN-CTC works respectably even with no position en- coding; in fact, the contribution of position is relatively minor (compare with [21] , where location in an encoder-decoder system improved CER by 3% absolute). Lossy downsampling appears to preserve performance in CER while degrading WER (as information about frame transitions is lost). We believe these observations align with the monotonicity and independence assumptions of CTC. Inspired by <cite>[27]</cite> , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details. In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike <cite>[27]</cite> we still see mild differentiation of variances. Inspired by [26] , we further plot the attention weights relative to the current time position (here, per head). Character labels gave forward-and backward-attending heads (incidentally, averaging these would retrieve the bimodal distribution in [26] ) at all layers. This suggests a gradual expansion of context over depth, as is often engineered in convolutional CTC.",
  "y": "similarities differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_0",
  "x": "Although traditional AES methods typically rely on handcrafted features (Larkey, 1998; Foltz et al., 1999; Attali and Burstein, 2006; Dikli, 2006; Wang and Brown, 2008; Chen and He, 2013; Somasundaran et al., 2014; Yannakoudakis et al., 2014; Phandi et al., 2015) , recent results indicate that state-of-the-art deep learning methods reach better performance (Alikaniotis et al., 2016; <cite>Dong and Zhang, 2016</cite>; Taghipour and Ng, 2016; Song et al., 2017; Tay et al., 2018) , perhaps because these methods are able to capture subtle and complex information that is relevant to the task <cite>(Dong and Zhang, 2016)</cite> . In this paper, we propose to combine string kernels (low-level character n-gram features) and word embeddings (high-level semantic features) to obtain state-of-the-art AES results. Since recent methods based on string kernels have demonstrated remarkable performance in various text classification tasks ranging from authorship identification (Popescu and Grozea, 2012) and sentiment analysis (Gim\u00e9nez-P\u00e9rez et al., 2017; to native language identification (Popescu and Ionescu, 2013; Ionescu et al., 2014; Ionescu, 2015; and dialect identification , we believe that string kernels can reach equally good results in AES. To the best of our knowledge, string kernels have never been used for this task. As string kernels are a simple approach that relies solely on character n-grams as features, it is fairly obvious that such an approach will not to cover several aspects (e.g.: semantics, discourse) required for the AES task. To solve this problem, we propose to combine string kernels with a recent approach based on word embeddings, namely the bag-of-super-wordembeddings (BOSWE) . To our knowledge, this is the first successful attempt to combine string kernels and word embeddings. We evaluate our approach on the Automated Student Assessment Prize data set, in both in-domain and cross-domain settings. The empirical results indicate that our approach yields a better performance than state-of-the-art approaches (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) .",
  "y": "background"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_1",
  "x": "As string kernels are a simple approach that relies solely on character n-grams as features, it is fairly obvious that such an approach will not to cover several aspects (e.g.: semantics, discourse) required for the AES task. To solve this problem, we propose to combine string kernels with a recent approach based on word embeddings, namely the bag-of-super-wordembeddings (BOSWE) . To our knowledge, this is the first successful attempt to combine string kernels and word embeddings. We evaluate our approach on the Automated Student Assessment Prize data set, in both in-domain and cross-domain settings. The empirical results indicate that our approach yields a better performance than state-of-the-art approaches (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . ---------------------------------- **METHOD** String kernels. Kernel functions (Shawe-Taylor and Cristianini, 2004) capture the intuitive notion of similarity between objects in a specific domain.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_2",
  "x": "As a consequence of kernel summation, the search space of linear patterns grows, which should help the kernel classifier, in our case \u03bd-SVR, to find a better regression function. ---------------------------------- **EXPERIMENTS** Data set. To evaluate our approach, we use the Automated Student Assessment Prize (ASAP) 1 data set from Kaggle. The ASAP data set contains 8 prompts of different genres. The number of essays per prompt along with the score ranges are presented in Table 1 . Since the official test data of the ASAP competition is not released to the public, we, as well as others before us (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>;  1 https://www.kaggle.com/c/asap-aes/data Tay et al., 2018) , use only the training data in our experiments. Evaluation procedure.",
  "y": "uses similarities"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_3",
  "x": "As <cite>Dong and Zhang (2016)</cite>, we scaled the essay scores into the range 0-1. We closely followed the same settings for data preparation as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . For the in-domain experiments, we use 5-fold cross-validation. The 5-fold cross-validation procedure is repeated for 10 times and the results were averaged to reduce the accuracy variation introduced by randomly selecting the folds. We note that the standard deviation in all cases in below 0.2%. For the cross-domain experiments, we use the same source\u2192target domain pairs as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) , namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928. All essays in the source domain are used as training data. Target domain samples are randomly divided into 5 folds, where one fold is used as test data, and the other 4 folds are collected together to sub-sample target domain train data. The sub-sample sizes are n t = {10, 25, 50, 100}. The sub-sampling is repeated for 5 times as in (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) to reduce bias.",
  "y": "uses similarities"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_4",
  "x": "The ASAP data set contains 8 prompts of different genres. The number of essays per prompt along with the score ranges are presented in Table 1 . Since the official test data of the ASAP competition is not released to the public, we, as well as others before us (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>;  1 https://www.kaggle.com/c/asap-aes/data Tay et al., 2018) , use only the training data in our experiments. Evaluation procedure. As <cite>Dong and Zhang (2016)</cite>, we scaled the essay scores into the range 0-1. We closely followed the same settings for data preparation as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . For the in-domain experiments, we use 5-fold cross-validation. The 5-fold cross-validation procedure is repeated for 10 times and the results were averaged to reduce the accuracy variation introduced by randomly selecting the folds. We note that the standard deviation in all cases in below 0.2%.",
  "y": "similarities uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_5",
  "x": "The 5-fold cross-validation procedure is repeated for 10 times and the results were averaged to reduce the accuracy variation introduced by randomly selecting the folds. We note that the standard deviation in all cases in below 0.2%. For the cross-domain experiments, we use the same source\u2192target domain pairs as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) , namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928. All essays in the source domain are used as training data. Target domain samples are randomly divided into 5 folds, where one fold is used as test data, and the other 4 folds are collected together to sub-sample target domain train data. The sub-sample sizes are n t = {10, 25, 50, 100}. The sub-sampling is repeated for 5 times as in (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) to reduce bias. As our approach performs very well in the cross-domain setting, we also present experiments without subsampling data from the target domain, i.e. when the sub-sample size is n t = 0. As evaluation metric, we use the quadratic weighted kappa (QWK). Baselines.",
  "y": "uses similarities"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_6",
  "x": "Target domain samples are randomly divided into 5 folds, where one fold is used as test data, and the other 4 folds are collected together to sub-sample target domain train data. The sub-sample sizes are n t = {10, 25, 50, 100}. The sub-sampling is repeated for 5 times as in (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) to reduce bias. As our approach performs very well in the cross-domain setting, we also present experiments without subsampling data from the target domain, i.e. when the sub-sample size is n t = 0. As evaluation metric, we use the quadratic weighted kappa (QWK). Baselines. We compare our approach with stateof-the-art methods based on handcrafted features (Phandi et al., 2015) , as well as deep features (<cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . We note that results for the cross-domain setting are reported only in some of these recent works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Implementation choices. For the string kernels approach, we used the histogram intersection string kernel (HISK) based on the blended range of character n-grams from 1 to 15.",
  "y": "uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_7",
  "x": "The sub-sample sizes are n t = {10, 25, 50, 100}. The sub-sampling is repeated for 5 times as in (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) to reduce bias. As our approach performs very well in the cross-domain setting, we also present experiments without subsampling data from the target domain, i.e. when the sub-sample size is n t = 0. As evaluation metric, we use the quadratic weighted kappa (QWK). Baselines. We compare our approach with stateof-the-art methods based on handcrafted features (Phandi et al., 2015) , as well as deep features (<cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . We note that results for the cross-domain setting are reported only in some of these recent works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Implementation choices. For the string kernels approach, we used the histogram intersection string kernel (HISK) based on the blended range of character n-grams from 1 to 15. To compute the intersection string kernel, we used the open-source code provided by Ionescu et al. (2014) .",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_8",
  "x": "The sub-sample sizes are n t = {10, 25, 50, 100}. The sub-sampling is repeated for 5 times as in (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) to reduce bias. As our approach performs very well in the cross-domain setting, we also present experiments without subsampling data from the target domain, i.e. when the sub-sample size is n t = 0. As evaluation metric, we use the quadratic weighted kappa (QWK). Baselines. We compare our approach with stateof-the-art methods based on handcrafted features (Phandi et al., 2015) , as well as deep features (<cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . We note that results for the cross-domain setting are reported only in some of these recent works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Implementation choices. For the string kernels approach, we used the histogram intersection string kernel (HISK) based on the blended range of character n-grams from 1 to 15. To compute the intersection string kernel, we used the open-source code provided by Ionescu et al. (2014) .",
  "y": "background"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_9",
  "x": "We compare our approach with stateof-the-art methods based on handcrafted features (Phandi et al., 2015) , as well as deep features (<cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . We note that results for the cross-domain setting are reported only in some of these recent works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Implementation choices. For the string kernels approach, we used the histogram intersection string kernel (HISK) based on the blended range of character n-grams from 1 to 15. To compute the intersection string kernel, we used the open-source code provided by Ionescu et al. (2014) . For the BOSWE approach, we used the pre-trained word embeddings computed by the word2vec toolkit (Mikolov et al., 2013) on the Google News data set using the Skip-gram model, which produces 300-dimensional vectors for 3 million words and phrases. We used functions from the VLFeat li- Table 2 : In-domain automatic essay scoring results of our approach versus several state-of-the-art methods (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using 5-fold cross-validation. The best QWK score (among the machine learning systems) for each prompt is highlighted in bold.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_10",
  "x": "In our empirical study, we also include feature ablation results. We report the QWK measure on each prompt as well as the overall average. We first note that the histogram intersection string kernel alone reaches better overall performance (0.780) than all previous works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . Remarkably, the overall performance of the HISK is also higher than the inter-human agreement (0.754). Although the BOSWE model can be regarded as a shallow approach, its overall results are comparable to those of deep learning approaches (<cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . When we combine the two models (HISK and BOSWE), we obtain even better results. Indeed, the combination of string kernels and word embeddings attains the best performance on 7 out of 8 prompts. The average QWK score of HISK and BOSWE (0.785) is more than 2% better the average scores of the best-performing state-of-the-art approaches Tay et al., 2018) . Cross-domain results.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_11",
  "x": "We report the QWK measure on each prompt as well as the overall average. We first note that the histogram intersection string kernel alone reaches better overall performance (0.780) than all previous works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . Remarkably, the overall performance of the HISK is also higher than the inter-human agreement (0.754). Although the BOSWE model can be regarded as a shallow approach, its overall results are comparable to those of deep learning approaches (<cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . When we combine the two models (HISK and BOSWE), we obtain even better results. Indeed, the combination of string kernels and word embeddings attains the best performance on 7 out of 8 prompts. The average QWK score of HISK and BOSWE (0.785) is more than 2% better the average scores of the best-performing state-of-the-art approaches Tay et al., 2018) . Cross-domain results. The results for the crossdomain automatic essay scoring task are presented in Table 3 .",
  "y": "similarities"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_12",
  "x": "Cross-domain results. The results for the crossdomain automatic essay scoring task are presented in Table 3 . For each and every source\u2192target pair, we report better results than both state-of-theart methods (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . We observe that the difference between our best QWK scores and the other approaches are sometimes much higher in the cross-domain setting than in the in-domain setting. We particularly notice that the difference from (Phandi et al., 2015) when n t = 0 is always higher than 10%. Our highest improvement (more than 54%, from 0.187 to 0.728) over (Phandi et al., 2015) is recorded for the pair 5\u21926, when n t = 0. Our score in this case (0.728) is even higher than both scores of Phandi et al. (2015) and <cite>Dong and Zhang (2016)</cite> when they use n t = 50. Different from the in-domain setting, we note that the combination of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples (n t ) added into the training set is less or equal to 25. Discussion.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_13",
  "x": "Indeed, the combination of string kernels and word embeddings attains the best performance on 7 out of 8 prompts. The average QWK score of HISK and BOSWE (0.785) is more than 2% better the average scores of the best-performing state-of-the-art approaches Tay et al., 2018) . Cross-domain results. The results for the crossdomain automatic essay scoring task are presented in Table 3 . For each and every source\u2192target pair, we report better results than both state-of-theart methods (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . We observe that the difference between our best QWK scores and the other approaches are sometimes much higher in the cross-domain setting than in the in-domain setting. We particularly notice that the difference from (Phandi et al., 2015) when n t = 0 is always higher than 10%. Our highest improvement (more than 54%, from 0.187 to 0.728) over (Phandi et al., 2015) is recorded for the pair 5\u21926, when n t = 0. Our score in this case (0.728) is even higher than both scores of Phandi et al. (2015) and <cite>Dong and Zhang (2016)</cite> when they use n t = 50.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_14",
  "x": "Discussion. It is worth noting that in a set of preliminary experiments (not included in the paper), we actually considered another approach based on word embeddings. We tried to obtain a document embedding by averaging the word vectors for each document. We computed the average as well as the standard deviation for each component of the word vectors, resulting in a total of 600 features, since the word vectors are 300-dimensional. We applied this method in the in-domain setting and we obtained a surprisingly low overall QWK score, around 0.251. We concluded that this simple apSource\u2192Target Method n t = 0 n t = 10 n t = 25 n t = 50 n t = 100 1\u21922 (Phandi et al., 2015) Table 3 : Corss-domain automatic essay scoring results of our approach versus two state-of-the-art methods (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using the same evaluation procedure as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . The best QWK scores for each source\u2192target domain pair are highlighted in bold. proach is not useful, and decided to use BOSWE instead.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_15",
  "x": "Our score in this case (0.728) is even higher than both scores of Phandi et al. (2015) and <cite>Dong and Zhang (2016)</cite> when they use n t = 50. Different from the in-domain setting, we note that the combination of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples (n t ) added into the training set is less or equal to 25. Discussion. It is worth noting that in a set of preliminary experiments (not included in the paper), we actually considered another approach based on word embeddings. We tried to obtain a document embedding by averaging the word vectors for each document. We computed the average as well as the standard deviation for each component of the word vectors, resulting in a total of 600 features, since the word vectors are 300-dimensional. We applied this method in the in-domain setting and we obtained a surprisingly low overall QWK score, around 0.251. We concluded that this simple apSource\u2192Target Method n t = 0 n t = 10 n t = 25 n t = 50 n t = 100 1\u21922 (Phandi et al., 2015) Table 3 : Corss-domain automatic essay scoring results of our approach versus two state-of-the-art methods (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using the same evaluation procedure as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) .",
  "y": "similarities uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_16",
  "x": "proach is not useful, and decided to use BOSWE instead. It would have been interesting to present an error analysis based on the discriminant features weighted higher by the \u03bd-SVR method. Unfortunately, this is not possible because our approach works in the dual space and we cannot transform the dual weights into primal weights, as long as the histogram intersection kernel does not have an explicit embedding map associated to it. In future work, however, we aim to replace the histogram intersection kernel with the presence bits kernel, which will enable us to perform an error analysis based on the overused or underused patterns, as described by . ---------------------------------- **CONCLUSION** In this paper, we described an approach based on combining string kernels and word embeddings for automatic essay scoring. We compared our approach on the Automated Student Assessment Prize data set, in both in-domain and crossdomain settings, with several state-of-the-art approaches (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . Overall, the in-domain and the cross-domain comparative studies indicate that string kernels, both alone and in combination with word embeddings, attain the best performance on the automatic essay scoring task.",
  "y": "differences"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_0",
  "x": "The task of referring expression generation (REG) has been studied since the 1970s [40, 22, 30, 7] , with most work focused on studying particular aspects of the problem in some relatively constrained datasets. Recent approaches have pushed this work toword more realistic scenarios. Kazemzadeh et al [19] introduced the first large-scale dataset of referring expressions for objects in real-world natural images, collected in a two-player game. This dataset was originally collected on top of the 20,000 image ImageCleft dataset, but has recently been extended to images from the MSCOCO collection. We make use of the RefCOCO and RefCOCO+ datasets in our work along with another recently collected referring expression dataset, released by Google, denoted in our paper as RefCOCOg <cite>[26]</cite> . The most relevant work to ours is Mao et al <cite>[26]</cite> which introduced the first deep learning approach to REG. In this model, the authors use a Convolutional Neural Network (CNN) [36] model pre-trained on ImageNet [34] to extract visual features from a bounding box around the target object and from the entire image. They use these features plus 5 features encoding the target object location and size as input to a Long Short-term Memory (LSTM) [10] network that generates expressions. Additionally, they apply the same model to the inverse problem of referring expression comprehension where the input is a natural language expression and the goal is to localize the referred object in the image.",
  "y": "uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_1",
  "x": "The task of referring expression generation (REG) has been studied since the 1970s [40, 22, 30, 7] , with most work focused on studying particular aspects of the problem in some relatively constrained datasets. Recent approaches have pushed this work toword more realistic scenarios. Kazemzadeh et al [19] introduced the first large-scale dataset of referring expressions for objects in real-world natural images, collected in a two-player game. This dataset was originally collected on top of the 20,000 image ImageCleft dataset, but has recently been extended to images from the MSCOCO collection. We make use of the RefCOCO and RefCOCO+ datasets in our work along with another recently collected referring expression dataset, released by Google, denoted in our paper as RefCOCOg <cite>[26]</cite> . The most relevant work to ours is Mao et al <cite>[26]</cite> which introduced the first deep learning approach to REG. In this model, the authors use a Convolutional Neural Network (CNN) [36] model pre-trained on ImageNet [34] to extract visual features from a bounding box around the target object and from the entire image. They use these features plus 5 features encoding the target object location and size as input to a Long Short-term Memory (LSTM) [10] network that generates expressions. Additionally, they apply the same model to the inverse problem of referring expression comprehension where the input is a natural language expression and the goal is to localize the referred object in the image.",
  "y": "background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_2",
  "x": "Although our task is somewhat different, we borrow machinery from state of the art caption generation [3, 39, 27, 5, 18, 21, 41] using LSTM to generate captions based on CNN features computed on an input image. Three recent approaches for referring expression generation <cite>[26]</cite> and comprehension [14, 33] also take a deep learning approach. However, we add visual object comparisons and tie together language generation for multiple objects. Referring expression generation has been studied for many years [40, 22, 30] in linguistics and natural language processing. These works were limited by data collection and insufficient computer vision algorithms. Together Amazon Mechanical Turk and CNNs have somewhat mitigated these limitations, allowing us to revisit these ideas on large-scale datasets. We still use such work to motivate the architecture of our pipeline. For instance, Mitchell and Jordan et al [30, 16] show the importance of using attributes, Funakoshi et al [8] show the importance of relative relations between objects in the same perceptual group, and Kelleher et al [20] show the importance of spatial relationships. These provide motivation for our modeling choices: when considering a referring expression for an object, the model takes into account the relative spatial location of other objects of the same type and visual comparisons to objects in the same perceptual group.",
  "y": "differences"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_3",
  "x": "These works were limited by data collection and insufficient computer vision algorithms. Together Amazon Mechanical Turk and CNNs have somewhat mitigated these limitations, allowing us to revisit these ideas on large-scale datasets. We still use such work to motivate the architecture of our pipeline. For instance, Mitchell and Jordan et al [30, 16] show the importance of using attributes, Funakoshi et al [8] show the importance of relative relations between objects in the same perceptual group, and Kelleher et al [20] show the importance of spatial relationships. These provide motivation for our modeling choices: when considering a referring expression for an object, the model takes into account the relative spatial location of other objects of the same type and visual comparisons to objects in the same perceptual group. The REG datasets of the past were sometimes limited to using computer generated images [38] , or relatively small collections of natural objects [29, 28, 7] . Recently, a large-scale referring expression dataset was collected by Kazemzadeh et al [19] featuring natural objects in the real world. Since then, another three REG datasets based on the object labels in MSCOCO have been collected [19,<cite> 26]</cite> . The availability of large-scale referring expression datasets allows us to train deep learning models.",
  "y": "background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_4",
  "x": "We implement several model variations for referring expression generation and comprehension. The first set of models are recent state of the art deep learning approaches from Mao et al <cite>[26]</cite> . We use these as our baselines (Sec 3.1). Next, we investigate incorporating better visual context features into the models (Sec 3.2). Finally, we explore methods to jointly produce an entire set of referring expressions for all depicted objects of the same category (Sec 3.3). ---------------------------------- **BASELINES** For comparison, we implement both the baseline and strong model of Mao et al <cite>[26]</cite> . Both models utilize a pre-trained CNN network to model the target object and its context within the image, and then use a LSTM for generation.",
  "y": "uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_5",
  "x": "---------------------------------- **BASELINES** For comparison, we implement both the baseline and strong model of Mao et al <cite>[26]</cite> . Both models utilize a pre-trained CNN network to model the target object and its context within the image, and then use a LSTM for generation. In particular, object and context are modeled as features from a CNN trained to recognize 1,000 object categories [36] from ImageNet [34] . Specifically, the visual representation is composed of: -Target object representation, o i . The object is modeled as features extracted from the VGG-fc7 layer by forwarding its bounding box through the network. -Global context representation, g i .",
  "y": "uses motivation"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_6",
  "x": "-Location/size representation, l i , for the target object. Location and size are modeled as a 5-d vector encoding the x and y locations of the top left and bottom right corners of the target object bounding box, as well as the bounding box size with respect to the image, i.e., l i = [ Language generation is handled by a long short-term memory network (LSTM) [10] where inputs are the above visual features and the network is trained to generate natural language referring expressions. In Mao et al's baseline <cite>[26]</cite> , the model uses maximum likelihood training and outputs the most likely referring expression given the target object, context, and location/size features. In addition, they also propose a stronger model that uses maximum mutual information (MMI) training to consider whether a listener would interpret a referring expression unambiguously. They impose this by penalizing the model if a generated referring expression could also be generated by some other object within the image. We implement both their original model and MMI model in our experiments. We subsequently refer to these two models as Baseline and MMI, respectively. ----------------------------------",
  "y": "background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_7",
  "x": "wihi ]. In summary, our final visual representation for a target object is: where o i , g i , l i are the target object, global context, and location/size features from the baseline model, \u03b4v i and \u03b4l i encodes visual appearance difference and location difference. W m and b m project the concatenation of the five types of features to be the final representation. ---------------------------------- **JOINT LANGUAGE GENERATION** For the referring expression generation task, rather than generating sentences for each object in an image separately [15] <cite>[26]</cite>, we consider tying the generation process together into a single task to jointly generate expressions for all objects of the same object category depicted in an image. This makes sense intuitively -when a person attempts to generate a referring expression for an object in an image they inherently compose that expression while keeping in mind expressions for the other objects in the picture. This can be observed in the fact that the expressions people generate for objects in an image tend to share similar patterns of expression.",
  "y": "differences"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_8",
  "x": "As visual comparison, we aggregate the difference of hidden outputs to push away ambiguous information. . There, n is the the number of other objects of the same type. The hidden difference is jointly embedded with the target object's hidden output, and forwarded to the softmax layer for predicting the word. ---------------------------------- **DATA** We make use of 3 referring expression datasets in our work, all collected on top of the Microsoft COCO image collection [24] . One dataset, RefCOCOg <cite>[26]</cite> is collected in a non-interactive setting, while the other two datasets, RefCOCO and RefCOCO+, are collected interactively in a two-player game [19] . In the following, we describe each dataset and provide some analysis of their similarities and differences, and then discuss splits of the datasets used in our experiments . ----------------------------------",
  "y": "uses background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_9",
  "x": "This means that each object will only appear either in training or testing set, but that one object from an image may appear in the training set while another object from the same image may appear in the test set. We use this split for RefCOCOg since same division was used in the previous state-of-the-art approach <cite>[26]</cite> . The second type is people-vs-objects splits. One thing we observe from analyzing the datasets is that about half of the referred objects are people. Therefore, we create a split for RefCOCO and RefCOCO+ datasets that evaluates images containing multiple people (testA) vs images containing multiple instances of all other objects (testB). In this split all objects from an image will appear either in the training or testing sets, but not both. This split creates a more meaningfully separated division between training and testing, allowing us to evaluate the usefulness of context more fairly. ---------------------------------- **EXPERIMENTS**",
  "y": "similarities uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_10",
  "x": "In this split all objects from an image will appear either in the training or testing sets, but not both. This split creates a more meaningfully separated division between training and testing, allowing us to evaluate the usefulness of context more fairly. ---------------------------------- **EXPERIMENTS** We first perform some experiments to analyze the use of context in referring expressions (Sec 5.1). Given these findings, we then perform experiments evaluating the usefulness of our proposed visual and language innovations on the comprehension (Sec 5.2) and generation tasks (Sec 5.3). In experiments for the referring expression comprehension task, we use the same evaluation as Mao et al <cite>[26]</cite> , namely we first predict the region referred by the given expression, then we compute the intersection over union (IOU) ratio between the true and predicted bounding box. If the IOU is larger than 0.5 we count it as a true positive. Otherwise, we count it as a false positive.",
  "y": "similarities uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_12",
  "x": "Context Representation As previously discussed, we suggest that the approaches proposed in recent referring expression works<cite> [26,</cite> 14] make use of relatively weak contextual information, by only considering a single global image context for all objects. To verify this intuition, we implemented both the baseline and strong MMI models from Mao et al <cite>[26]</cite> , and compare the results for referring expression comprehension task with and without global context on RefCOCO and Refcoco+ in Table 1 . Surprisingly we find that the global context does not improve the performance of the model. In fact, adding context even decreases performance slightly. This may be due to the fact that the global context for each object in an image would be the same, introducing some ambiguity into the referring expression comprehension task. Given these findings, we implemented a simple modification to the global context, computing the same visual representation, but on a somewhat scaled window centered around the target object. We found this to improve performance, suggesting room for improving the visual context feature. This motivate our development of a better context feature. Visual Comparison For our visual comparison model, there could be several choices regarding which objects from the image should be compared to the target object.",
  "y": "background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_13",
  "x": "We also perform an ablation study, evaluating the combinations. ---------------------------------- **ANALYSIS EXPERIMENTS** Context Representation As previously discussed, we suggest that the approaches proposed in recent referring expression works<cite> [26,</cite> 14] make use of relatively weak contextual information, by only considering a single global image context for all objects. To verify this intuition, we implemented both the baseline and strong MMI models from Mao et al <cite>[26]</cite> , and compare the results for referring expression comprehension task with and without global context on RefCOCO and Refcoco+ in Table 1 . Surprisingly we find that the global context does not improve the performance of the model. In fact, adding context even decreases performance slightly. This may be due to the fact that the global context for each object in an image would be the same, introducing some ambiguity into the referring expression comprehension task. Given these findings, we implemented a simple modification to the global context, computing the same visual representation, but on a somewhat scaled window centered around the target object.",
  "y": "uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_14",
  "x": "This is more like mimicing how human refer object -we tend to point out the difference between the target object with the other same-category objects within the same image. Fig. 3 . Comprehension accuracies on RefCOCO and RefCOCO+ datasets. We compare the performance of \"visdif\" model without visual comparison, and visual comparison between different-category objects, between all objects, and between same-type objects. ---------------------------------- **REFERRING EXPRESSION COMPREHENSION** We evaluate performance on the referring expression comprehension task on RefCOCO, RefCOCO+ and RefCOCOg datasets. For RefCOCO and RefCOCO+, we evaluate on the two subsets of people (testA) and all other objects (testB). For RefCOCOg, we evaluate on the per-object split as previous work <cite>[26]</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_15",
  "x": "Table 2 shows the comprehension accuracies. We observe that our implementation of Mao et al <cite>[26]</cite> achieves comparable performance to the numbers reported in their paper. We also find that adding visual comparison features to the Baseline model improves performance across all datasets and splits. Similar improvements are also observed on top of the MMI model. In order to make a fully automatic referring system, we also train a Fast-RCNN [9] detector and build our system on top of the detections. We train Fast-RCNN on the validation portion only as the RefCOCO and RefCOCO+ are collected using MSCOCO training data. For RefCOCOg, we use the detection results provided by <cite>[26]</cite> , which were trained uisng Multibox [4] . Results on shown in the bottom half of Table 2 . Although all comprehension accuracies drop due to imperfect detections, the improvements of our models over Baseline and MMI are still observed.",
  "y": "similarities uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_16",
  "x": "Since the authors haven't released their testing set, we show the performance on their validation set only, using the optimized hyper-parameters on RefCOCO. Table 2 shows the comprehension accuracies. We observe that our implementation of Mao et al <cite>[26]</cite> achieves comparable performance to the numbers reported in their paper. We also find that adding visual comparison features to the Baseline model improves performance across all datasets and splits. Similar improvements are also observed on top of the MMI model. In order to make a fully automatic referring system, we also train a Fast-RCNN [9] detector and build our system on top of the detections. We train Fast-RCNN on the validation portion only as the RefCOCO and RefCOCO+ are collected using MSCOCO training data. For RefCOCOg, we use the detection results provided by <cite>[26]</cite> , which were trained uisng Multibox [4] . Results on shown in the bottom half of Table 2 .",
  "y": "uses"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_0",
  "x": "Rashkin et al. (2017) break down fake news into three categories, hoax, propaganda and satire. A hoax article typically tries to convince the reader about a cookedup story while propaganda ones usually mislead the reader into believing a false political or social agenda. Burfoot and Baldwin (2009) defines a satirical article as the one which deliberately exposes real-world individuals, organisations and events to ridicule. Previous works<cite> (Rubin et al., 2016</cite>; Rashkin et al., 2017) rely on various linguistic and handcrafted semantic features for differentiating between news articles. However, none of them try to model the interaction of sentences within the document. We observed a pattern in the way sentences cluster in different kind of news articles. Specifically, satirical articles had a more coherent story and thus all the sentences in the document seemed similar to each other. On the other hand, the trusted news articles were also coherent but the similarity between sentences from different parts of the document was not that strong, as depicted in Figure 1 . We believe that the reason for such kind of behaviour is the presence of factual jumps across sections in a trusted document.",
  "y": "background"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_1",
  "x": "On the other hand, the trusted news articles were also coherent but the similarity between sentences from different parts of the document was not that strong, as depicted in Figure 1 . We believe that the reason for such kind of behaviour is the presence of factual jumps across sections in a trusted document. In this work, we propose a graph neural network-based model to classify news articles while capturing the interaction of sentences across the document. We present a series of experiments on News Corpus with Varying Reliability dataset (Rashkin et al., 2017) and Satirical Legitimate News dataset<cite> (Rubin et al., 2016)</cite> . Our results demonstrate that the proposed model achieves state-of-the-art performance on these datasets and provides interesting insights. Experiments performed in out-of-domain settings establish the generalizability of our proposed method. ---------------------------------- **RELATED WORK** Satire, according to Simpson (2003) , is complicated because it occupies more than one place in the framework for humor, proposed by Ziv (1988) : it clearly has an aggressive and social function, and often expresses an intellectual aspect as well.",
  "y": "uses"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_2",
  "x": "Our results demonstrate that the proposed model achieves state-of-the-art performance on these datasets and provides interesting insights. Experiments performed in out-of-domain settings establish the generalizability of our proposed method. ---------------------------------- **RELATED WORK** Satire, according to Simpson (2003) , is complicated because it occupies more than one place in the framework for humor, proposed by Ziv (1988) : it clearly has an aggressive and social function, and often expresses an intellectual aspect as well. <cite>Rubin et al. (2016)</cite> defines news satire as a genre of satire that mimics the format and style of journalistic reporting. Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources<cite> (Rubin et al., 2016)</cite> . McHardy et al. (2019) hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire. In this work, we show that our proposed model generalizes to articles from unseen publication sources.",
  "y": "background"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_3",
  "x": "Satire, according to Simpson (2003) , is complicated because it occupies more than one place in the framework for humor, proposed by Ziv (1988) : it clearly has an aggressive and social function, and often expresses an intellectual aspect as well. <cite>Rubin et al. (2016)</cite> defines news satire as a genre of satire that mimics the format and style of journalistic reporting. Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources<cite> (Rubin et al., 2016)</cite> . McHardy et al. (2019) hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire. In this work, we show that our proposed model generalizes to articles from unseen publication sources. <cite>Rubin et al. (2016)</cite> 's work by offering a quantitative study of linguistic differences found in articles of different types of fake news such as hoax, propaganda and satire. They also proposed predictive models for graded deception across multiple domains. Rashkin et al. (2017) found that neural methods didn't perform well for this task and proposed to use a Max-Entropy classifier. We show that our proposed neural network based on graph convolutional layers can outperform this model.",
  "y": "background"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_4",
  "x": "<cite>Rubin et al. (2016)</cite> defines news satire as a genre of satire that mimics the format and style of journalistic reporting. Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources<cite> (Rubin et al., 2016)</cite> . McHardy et al. (2019) hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire. In this work, we show that our proposed model generalizes to articles from unseen publication sources. <cite>Rubin et al. (2016)</cite> 's work by offering a quantitative study of linguistic differences found in articles of different types of fake news such as hoax, propaganda and satire. They also proposed predictive models for graded deception across multiple domains. Rashkin et al. (2017) found that neural methods didn't perform well for this task and proposed to use a Max-Entropy classifier. We show that our proposed neural network based on graph convolutional layers can outperform this model. Recent works by Yang et al. (2017) ; De Sarkar et al. (2018) show that sophisticated neural models can be used for satirical news detection.",
  "y": "differences"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_5",
  "x": "We use SLN: Satirical and Legitimate News Database<cite> (Rubin et al., 2016)</cite> , RPN: Random Political News Dataset (Horne and Adali, 2017) and LUN: Labeled Unreliable News Dataset Rashkin et al. (2017) for our experiments. Table 1 shows the statistics. Since all of the previous methods on the aforementioned datasets are non-neural, we implement the following neural baselines, \u2022 CNN: In this model, we apply a 1-d CNN (Convolutional Neural Network) layer (Kim, 2014) with filter size 3 over the word embeddings of the sentences within a document. This is followed by a max-pooling layer to get a single document vector which is passed to a fully connected projection layer to get the logits over output classes. \u2022 LSTM: In this model, we encode the document using a LSTM (Long Short-Term Memory) layer (Hochreiter and Schmidhuber, 1997) . We use the hidden state at the last time step as the document vector which is passed to a fully connected projection layer to get the logits over output classes. \u2022 BERT: In this model, we extract the sentence vector (representation corresponding to [CLS] token) using BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) for each sentence in the document. We then apply a LSTM layer on the sentence embeddings, followed by a projection layer to make the prediction for each document.",
  "y": "uses"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_6",
  "x": "We conduct experiments across various settings and datasets. We report macro-averaged scores in Table 3 : 4-way classification results for different models. We only report F1-score following the SoTA paper. similarity model does not seem to have much impact on the GCN model, and considering the computing cost, we don't experiment with it for the 4-way classification scenario. Given that we use SLN as an out of domain test set (just one overlapping source, no overlap in articles), whereas the SoTA paper<cite> (Rubin et al., 2016)</cite> reports a 10fold cross validation number on SLN. We believe that our results are quite strong, the GAT + 2 Attn Heads model achieves an accuracy of 87% on the entire RPN dataset when used as an out-of-domain test set. The SoTA paper (Horne and Adali, 2017) on RPN reports a 5-fold cross validation accuracy of 91%. These results indicate the generalizability of our proposed model across datasets. We also present results of four way classification in Table  3 .",
  "y": "differences"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_0",
  "x": "In 2009, we put forward another fundamental word order principle, i.e. predictability maximization, and presented a theoretical framework culminating in a conflict between dependency length minimization and predictability maximization [19] . For sociological reasons, these arguments started appearing in print many years later [20, 5, <cite>21]</cite> . For the case of a single head and its dependents, the minimization of dependency lengths yields that the head should be placed at the center of the sequence whereas the principle of predictability maximization (or uncertainty minimization) yields that the head should be placed at one of the ends of the sequence (last if the head is the target of the prediction; first otherwise)<cite> [21,</cite> 20] . Liu et al review two major sources of evidence of dependency length minimization: the analysis of dependency treebanks and psychological experiments. A critical difference between them is that the former is based on the calculation of the total cost of the sentence (as a sum or mean of all the dependency lengths of the sentence) while the latter is based on a partial calculation and thus it can be misleading. Suppose that one wishes to compare the cost of two orderings of the same sentence. The observation that the processing cost of a sentence decreases when the length of a dependency 2 increases, does not allow one to conclude that dependency length minimization cannot explain the results because shortening an edge implies moving at least one of the words defining it, and every move could imply the reduction of other edges eventually reducing the total sum of dependency lengths or altering the so-called complexity profile (e.g., [22] ), rendering fair comparison impractical. The problem of partial calculation of length costs has already been discussed in the context of research on the cost of crossing dependencies [23] and worsens when the sentences being compared differ not only in order but also in content. Another challenge is the precision of dependency length that is typically measured in words.",
  "y": "background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_1",
  "x": "First, distance minimization allows one to unify pressure to reduce dependency lengths (still distances) with constraints on word order variation and change arising from a principle of swap distance minimization [11] . \"Distance minimization\" has therefore a higher predictive power and greater utility in a general theory of communication. Second, distance provides a \"formal background\" or a \"specific background\" (following Bunge's terminology [10]) from physics or mathematics such as the theory of geographical or spatial networks (where the syntactic dependency structures of sentences are particular cases in one dimension) [12, 13] or the theory for the distance between like elements in sequences (where the couple of words involved in a syntactic dependency are particular cases of like elements) [14] . Therefore we agree with [1] on the convenience of the term distance. A less flashy contribution of [6] has been promoting the need of controlling for sentence length (as a predictor of dependency length in their mixed-effects regression model) in research on dependency length minimization, an important methodological issue [15] that was addressed early [2] but neglected in subsequent research (e.g., [16, 17, 18] ). Liu et al focus their review on the fundamental principle of dependency length minimization but understanding how it interacts with other principles is vital. In 2009, we put forward another fundamental word order principle, i.e. predictability maximization, and presented a theoretical framework culminating in a conflict between dependency length minimization and predictability maximization [19] . For sociological reasons, these arguments started appearing in print many years later [20, 5, <cite>21]</cite> . For the case of a single head and its dependents, the minimization of dependency lengths yields that the head should be placed at the center of the sequence whereas the principle of predictability maximization (or uncertainty minimization) yields that the head should be placed at one of the ends of the sequence (last if the head is the target of the prediction; first otherwise)<cite> [21,</cite> 20] . Liu et al review two major sources of evidence of dependency length minimization: the analysis of dependency treebanks and psychological experiments.",
  "y": "background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_2",
  "x": "A critical difference between them is that the former is based on the calculation of the total cost of the sentence (as a sum or mean of all the dependency lengths of the sentence) while the latter is based on a partial calculation and thus it can be misleading. Suppose that one wishes to compare the cost of two orderings of the same sentence. The observation that the processing cost of a sentence decreases when the length of a dependency 2 increases, does not allow one to conclude that dependency length minimization cannot explain the results because shortening an edge implies moving at least one of the words defining it, and every move could imply the reduction of other edges eventually reducing the total sum of dependency lengths or altering the so-called complexity profile (e.g., [22] ), rendering fair comparison impractical. The problem of partial calculation of length costs has already been discussed in the context of research on the cost of crossing dependencies [23] and worsens when the sentences being compared differ not only in order but also in content. Another challenge is the precision of dependency length that is typically measured in words. Lengths in phonemes or syllables shed light on why SVO languages show SOV order when the object is a short word such as a clitic [24] . Without addressing these issues, the anti-locality effects or long-distance dependencies reviewed by Liu et al can neither be attributed to predictability maximization nor be interpreted as a violation of dependency length minimization safely; an effective evaluation of the theoretical framework above can be impossible (as that framework makes theoretical predictions based on the calculation of full length costs). The real challenge for psycholinguistic research is not the extent to which the theoretical framework above is supported by current results in the lab but rather to increase the precision of dependency length measurements and investigate the experimental conditions in which the following theoretical predictions are observed [20, <cite>21]</cite> : one principle beating the other, coexistence, collaboration between principles or the very same trade-off causing the delusion that word order constraints have relaxed dramatically or even disappeared. This is the way of physics.",
  "y": "uses background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_3",
  "x": "In 2009, we put forward another fundamental word order principle, i.e. predictability maximization, and presented a theoretical framework culminating in a conflict between dependency length minimization and predictability maximization [19] . For sociological reasons, these arguments started appearing in print many years later [20, 5, <cite>21]</cite> . For the case of a single head and its dependents, the minimization of dependency lengths yields that the head should be placed at the center of the sequence whereas the principle of predictability maximization (or uncertainty minimization) yields that the head should be placed at one of the ends of the sequence (last if the head is the target of the prediction; first otherwise)<cite> [21,</cite> 20] . Liu et al review two major sources of evidence of dependency length minimization: the analysis of dependency treebanks and psychological experiments. A critical difference between them is that the former is based on the calculation of the total cost of the sentence (as a sum or mean of all the dependency lengths of the sentence) while the latter is based on a partial calculation and thus it can be misleading. Suppose that one wishes to compare the cost of two orderings of the same sentence. The observation that the processing cost of a sentence decreases when the length of a dependency increases, does not allow one to conclude that dependency length minimization cannot explain the results because shortening an edge implies moving at least one of the words defining it, and every move could imply the reduction of other edges eventually reducing the total sum of dependency lengths or altering the so-called complexity profile (e.g., [22] ), rendering fair comparison impractical. The problem of partial calculation of length costs has already been discussed in the context of research on the cost of crossing dependencies [23] and worsens when the sentences being compared differ not only in order but also in content. Another challenge is the precision of dependency length that is typically measured in words.",
  "y": "background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_4",
  "x": "For sociological reasons, these arguments started appearing in print many years later [20, 5, <cite>21]</cite> . For the case of a single head and its dependents, the minimization of dependency lengths yields that the head should be placed at the center of the sequence whereas the principle of predictability maximization (or uncertainty minimization) yields that the head should be placed at one of the ends of the sequence (last if the head is the target of the prediction; first otherwise)<cite> [21,</cite> 20] . Liu et al review two major sources of evidence of dependency length minimization: the analysis of dependency treebanks and psychological experiments. A critical difference between them is that the former is based on the calculation of the total cost of the sentence (as a sum or mean of all the dependency lengths of the sentence) while the latter is based on a partial calculation and thus it can be misleading. Suppose that one wishes to compare the cost of two orderings of the same sentence. The observation that the processing cost of a sentence decreases when the length of a dependency increases, does not allow one to conclude that dependency length minimization cannot explain the results because shortening an edge implies moving at least one of the words defining it, and every move could imply the reduction of other edges eventually reducing the total sum of dependency lengths or altering the so-called complexity profile (e.g., [22] ), rendering fair comparison impractical. The problem of partial calculation of length costs has already been discussed in the context of research on the cost of crossing dependencies [23] and worsens when the sentences being compared differ not only in order but also in content. Another challenge is the precision of dependency length that is typically measured in words. Lengths in phonemes or syllables shed light on why SVO languages show SOV order when the object is a short word such as a clitic [24] .",
  "y": "background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_5",
  "x": "The observation that the processing cost of a sentence decreases when the length of a dependency increases, does not allow one to conclude that dependency length minimization cannot explain the results because shortening an edge implies moving at least one of the words defining it, and every move could imply the reduction of other edges eventually reducing the total sum of dependency lengths or altering the so-called complexity profile (e.g., [22] ), rendering fair comparison impractical. The problem of partial calculation of length costs has already been discussed in the context of research on the cost of crossing dependencies [23] and worsens when the sentences being compared differ not only in order but also in content. Another challenge is the precision of dependency length that is typically measured in words. Lengths in phonemes or syllables shed light on why SVO languages show SOV order when the object is a short word such as a clitic [24] . Without addressing these issues, the anti-locality effects or long-distance dependencies reviewed by Liu et al can neither be attributed to predictability maximization nor be interpreted as a violation of dependency length minimization safely; an effective evaluation of the theoretical framework above can be impossible (as that framework makes theoretical predictions based on the calculation of full length costs). The real challenge for psycholinguistic research is not the extent to which the theoretical framework above is supported by current results in the lab but rather to increase the precision of dependency length measurements and investigate the experimental conditions in which the following theoretical predictions are observed [20, <cite>21]</cite> : one principle beating the other, coexistence, collaboration between principles or the very same trade-off causing the delusion that word order constraints have relaxed dramatically or even disappeared. This is the way of physics. Our concern for units of measurement is not a simple matter of precision but one of great theoretical importance: if the length of a dependency is measured in units of word length (e.g., syllables or phonemes) then it follows that the length of a dependency will be strongly determined by the length of the words defining the dependency and that of the intermediate words. Therefore, pressure to reduce dependency lengths implies pressure for compression [25, 26] , linking a principle of word order with a principle that operates (nonexclusively) on individual words.",
  "y": "uses background"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_0",
  "x": "**INTRODUCTION** There is a deep tension in statistical modeling of grammatical structure between providing good expressivity -to allow accurate modeling of the data with sparse grammars -and low complexitymaking induction of the grammars and parsing of novel sentences computationally practical. Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the problem of segmenting training data trees into elementary parse tree fragments to form the grammar (Cohn et al., 2009;<cite> Cohn and Blunsom, 2010</cite>; Post and Gildea, 2009) . DP inference tackles this problem by exploring the space of all possible segmentations of the data, in search for fragments that are on the one hand large enough so that they incorporate the useful dependencies, and on the other small enough so that they recur and have a chance to be useful in analyzing unseen data. The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) \"split them up\", preventing their reuse, leading to less sparse grammars than might be ideal. For instance, imagine modeling the following set of structures: \u2022 A natural recurring structure here would be the structure \"[ N P the [ N N president]]\", yet it occurs not at all in the data. TSGs are a special case of the more flexible grammar formalism of tree adjoining grammar (TAG) (Joshi et al., 1975) . TAG augments TSG with an adjunction operator and a set of auxiliary trees in addition to the substitution operator and initial trees of TSG, allowing for \"splicing in\" of syntactic fragments within trees.",
  "y": "background"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_1",
  "x": "However we show that the grammars we induce are compact yet rich, in that they succinctly represent complex linguistic structures. ---------------------------------- **PROBABILISTIC MODEL** In the basic nonparametric TSG model, there is an independent DP for every grammar category (such as c = N P ), each of which uses a base distribution P 0 that generates an initial tree by making stepwise decisions. The canonical P 0 uses a probabilistic CFGP that is fixed a priori to sample CFG rules top-down and Bernoulli variables for determining where substitutions should occur (Cohn et al., 2009;<cite> Cohn and Blunsom, 2010)</cite> . We extend this model by adding specialized DPs for left and right auxiliary trees. 3 Therefore, we have an exchangeable process for generating right auxiliary trees as for initial trees in TSG.",
  "y": "background"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_2",
  "x": "From a practical point of view, we show that an induced TIG provides modeling performance superior to TSG and comparable with TIG 0 . However we show that the grammars we induce are compact yet rich, in that they succinctly represent complex linguistic structures. ---------------------------------- **PROBABILISTIC MODEL** In the basic nonparametric TSG model, there is an independent DP for every grammar category (such as c = N P ), each of which uses a base distribution P 0 that generates an initial tree by making stepwise decisions. The canonical P 0 uses a probabilistic CFGP that is fixed a priori to sample CFG rules top-down and Bernoulli variables for determining where substitutions should occur (Cohn et al., 2009;<cite> Cohn and Blunsom, 2010)</cite> . We extend this model by adding specialized DPs for left and right auxiliary trees. 3 Therefore, we have an exchangeable process for generating right auxiliary trees",
  "y": "extends"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_3",
  "x": "In any derivation, for every initial tree node labelled c (except for frontier nodes) we determine whether or not there are insertions at this node by sampling a Bernoulli(\u00b5 left c ) distributed left insertion variable and a Bernoulli(\u00b5 right c ) distributed right insertion variable. For left auxiliary trees, we treat the nodes that are not along the spine of the auxiliary tree the same way we treat initial tree nodes, however for nodes that are along the spine (including root nodes, excluding foot nodes) we consider only left insertions by sampling the left insertion variable (symmetrically for right insertions). ---------------------------------- **INFERENCE** Given this model, our inference task is to explore optimal derivations underlying the data. Since TIG derivations are highly structured objects, a basic sampling strategy based on local node-level moves such as Gibbs sampling (Geman and Geman, 1984) would not hold much promise. Following previous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion<cite> (Cohn and Blunsom, 2010</cite>; Shindo et al., 2011) . This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (Johnson et al., 2007) . Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a CFG rule CFG probability proposal distribution.",
  "y": "uses"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_4",
  "x": "---------------------------------- **INFERENCE** Given this model, our inference task is to explore optimal derivations underlying the data. Since TIG derivations are highly structured objects, a basic sampling strategy based on local node-level moves such as Gibbs sampling (Geman and Geman, 1984) would not hold much promise. Following previous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion<cite> (Cohn and Blunsom, 2010</cite>; Shindo et al., 2011) . This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (Johnson et al., 2007) . Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a CFG rule CFG probability proposal distribution. Fortunately, Schabes and Waters (1995) provide an (exact) transformation from a fully general TIG into a TSG that generates the same string languages. It is then straightforward to represent this TSG as a CFG using the Goodman transform (Goodman, 2002;<cite> Cohn and Blunsom, 2010)</cite> .",
  "y": "uses"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_5",
  "x": "As has become standard, we carried out a small treebank experiment where we train on Section 2, and a large one where we train on the full training set. All hyperparameters are resampled under appropriate vague gamma and beta priors. All reported numbers are averages over three runs. Parsing results are based on the maximum probability parse which was obtained by sampling derivations under the transform CFG. We compare our system (referred to as TIG) to our implementation of the TSG system of<cite> (Cohn and Blunsom, 2010</cite> ) (referred to as TSG) and the constrained TIG variant of (Shindo et al., 2011 ) (referred to as TIG 0 ). The upshot of our experiments is that, while on the large training set all models have similar performance (85.6, 85.3, 85 .4 for TSG, TIG 0 and TIG respectively), on the small dataset insertion helps nonparametric model to find more compact and generalizable representations for the data, which affects parsing performance (Figure 4 ). Although TIG 0 has performance close to TIG, note that TIG achieves this performance using a more succinct representation and extracting a rich set of auxiliary trees. As a result, TIG finds many chances to apply insertions to test sentences, whereas TIG 0 depends mostly on TSG rules. If we look at the most likely derivations for the test data, TIG 0 assigns 663 insertions (351 left insertions) in the parsing of entire Section 23, meanwhile TIG assigns 3924 (2100 left insertions).",
  "y": "uses"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_6",
  "x": "**EVALUATION RESULTS** We use the standard Penn treebank methodology of training on sections 2-21 and testing on section 23. All our data is head-binarized and words occurring only once are mapped into unknown categories of the Berkeley parser. As has become standard, we carried out a small treebank experiment where we train on Section 2, and a large one where we train on the full training set. All hyperparameters are resampled under appropriate vague gamma and beta priors. All reported numbers are averages over three runs. Parsing results are based on the maximum probability parse which was obtained by sampling derivations under the transform CFG. We compare our system (referred to as TIG) to our implementation of the TSG system of<cite> (Cohn and Blunsom, 2010</cite> ) (referred to as TSG) and the constrained TIG variant of (Shindo et al., 2011 ) (referred to as TIG 0 ). The upshot of our experiments is that, while on the large training set all models have similar performance (85.6, 85.3, 85 .4 for TSG, TIG 0 and TIG respectively), on the small dataset insertion helps nonparametric model to find more compact and generalizable representations for the data, which affects parsing performance (Figure 4 ).",
  "y": "uses similarities"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_0",
  "x": "Unlike previous models, we do not collapse the query into a single vector, instead we deploy an iterative alternating attention mechanism that allows a fine-grained exploration of both the query and the document. Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children's Book Test (CBT) dataset. ---------------------------------- **INTRODUCTION** Recently, the idea of training machine comprehension models that can read, understand, and answer questions about a text has come closer to reality principally through two factors. The first is the advent of deep learning techniques (Goodfellow et al., 2016) , which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data. The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries<cite> (Hill et al., 2015</cite>; Hermann et al., 2015) , which permit fast integration loops between model conception and experimental evaluation. Cloze-style queries (Taylor, 1953) are created by deleting a particular word in a natural-language statement. The task is to guess which word was deleted.",
  "y": "background"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_1",
  "x": "Cloze-style queries (Taylor, 1953) are created by deleting a particular word in a natural-language statement. The task is to guess which word was deleted. In a pragmatic approach, recent work<cite> (Hill et al., 2015)</cite> formed such questions by extracting a sentence from a larger document. In contrast to considering a stand-alone statement, the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word. Such contextual dependencies may also be injected by removing a word from a short human-crafted summary of a larger body of text. The abstractive nature of the summary is likely to demand a higher level of comprehension of the original text (Hermann et al., 2015) . In both cases, the machine comprehension system is presented with an ablated query and the document to which the original query refers. The missing word is assumed to appear in the document. Encouraged by the recent success of deep learning attention architectures (Bahdanau et al., 2015; Sukhbaatar et al., 2015) , we propose a novel neural attention-based inference model designed to perform machine reading comprehension tasks.",
  "y": "background"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_2",
  "x": "After a fixed number of iterations, the model uses a summary of its inference process to predict the answer. This paper makes the following contributions. We present a novel iterative, alternating attention mechanism that, unlike existing models<cite> (Hill et al., 2015</cite>; Kadlec et al., 2016) , does not compress the query to a single representation, but instead alternates its attention between the query and the document to obtain a fine-grained query representation within a fixed computation time. Our architecture tightly integrates previous ideas related to bidirectional readers (Kadlec et al., 2016) and iterative attention processes<cite> (Hill et al., 2015</cite>; Sukhbaatar et al., 2015) . It obtains state-of-theart results on two machine comprehension datasets and shows promise for application to a broad range of natural language processing tasks. ---------------------------------- **TASK DESCRIPTION** One of the advantages of using Cloze-style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention. The CBT<cite> (Hill et al., 2015)</cite> and CNN (Hermann et al., 2015) corpora are two such datasets.",
  "y": "differences"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_4",
  "x": "This paper makes the following contributions. We present a novel iterative, alternating attention mechanism that, unlike existing models<cite> (Hill et al., 2015</cite>; Kadlec et al., 2016) , does not compress the query to a single representation, but instead alternates its attention between the query and the document to obtain a fine-grained query representation within a fixed computation time. Our architecture tightly integrates previous ideas related to bidirectional readers (Kadlec et al., 2016) and iterative attention processes<cite> (Hill et al., 2015</cite>; Sukhbaatar et al., 2015) . It obtains state-of-theart results on two machine comprehension datasets and shows promise for application to a broad range of natural language processing tasks. ---------------------------------- **TASK DESCRIPTION** One of the advantages of using Cloze-style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention. The CBT<cite> (Hill et al., 2015)</cite> and CNN (Hermann et al., 2015) corpora are two such datasets. The CBT 1 corpus was generated from well-known children's books available through Project Gutenberg.",
  "y": "background"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_5",
  "x": "**TASK DESCRIPTION** One of the advantages of using Cloze-style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention. The CBT<cite> (Hill et al., 2015)</cite> and CNN (Hermann et al., 2015) corpora are two such datasets. The CBT 1 corpus was generated from well-known children's books available through Project Gutenberg. Documents consist of 20-sentence excerpts from these books. The related query is formed from an excerpt's 21st sentence by replacing a single word with an anonymous placeholder token. The dataset is divided into four subsets depending on the type of the word replaced. The subsets are named entity, common noun, verb, and preposition. We will focus our evaluation solely on the first two subsets, i.e. CBT-NE (named entity) and CBT-CN (common nouns), since the latter two are relatively simple as demonstrated by<cite> (Hill et al., 2015)</cite> .",
  "y": "motivation"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_6",
  "x": "The recurrent network iteratively performs an alternating search step to gather information that may be useful to predict the answer. In particular, at each time step: (1) it performs an attentive read on the query encodings, resulting in a query glimpse, q t , and (2) given the current query glimpse, it extracts a conditional document glimpse, d t , representing the parts of the document that are relevant to the current query glimpse. In turn, both attentive reads are conditioned on the previous hidden state of the inference GRU s t\u22121 , summarizing the information that has been gathered from the query and the document up to time t. The inference GRU uses both glimpses to update its recurrent state and thus decides which information needs to be gathered to complete the inference process. Query Attentive Read Given the query encodings {q i }, we formulate a query glimpse q t at timestep t by: where q i, t are the query attention weights and A q \u2208 R 2h\u00d7s , where s is the dimensionality of the inference GRU state, and a q \u2208 R 2h . The attention we use here is similar to the formulation used in<cite> (Hill et al., 2015</cite>; Sukhbaatar et al., 2015) , but with two differences. First, we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step. This simple bilinear attention has been successfully used in (Luong et al., 2015) . Second, we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t\u22121 .",
  "y": "similarities differences"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_8",
  "x": "**RESULTS** We report the results of our model on the CBT-CN, CBT-NE and CNN datasets, previously described in Section 2. Table 2 reports our results on the CBT-CN and CBT-NE dataset. The Humans, LSTMs and Memory Networks (MemNNs) results are taken from<cite> (Hill et al., 2015)</cite> and the Attention-Sum Reader (AS Reader) is a state-of-the-art result recently obtained by (Kadlec et al., 2016) . ---------------------------------- **CBT** Main result Our model (line 7) sets a new stateof-the-art on the common noun category by gaining 3.6 and 5.6 points in validation and test over the best baseline AS Reader (line 5). This performance gap is only partially reflected on the CBT-NE dataset. We observe that the 1.4 accuracy points on the validation set do not reflect better performance on the test set, which sits on par with the best baseline.",
  "y": "uses"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_10",
  "x": "However, we hypothesize that more (fewer) timesteps would benefit harder (easier) examples. A straight-forward extension of the model would be to dynamically select the number of inference steps conditioned on each example. ---------------------------------- **RELATED WORKS** Neural attention models have been applied recently to a sm\u00f6rg\u00e5sbord of machine learning and natural language processing problems. These include, but are not limited to, handwriting recognition (Graves, 2013) , digit classification (Mnih et al., 2014) , machine translation (Bahdanau et al., 2015) , question answering (Sukhbaatar et al., 2015; Hermann et al., 2015) and caption generation (Xu et al., 2015) . In general, attention models keep a memory of states that can be accessed at will by learned attention policies. In our case, the memory is represented by the set of document and query contextual encodings. Our model is closely related to (Sukhbaatar et al., 2015; Kumar et al., 2015; Hermann et al., 2015; Kadlec et al., 2016;<cite> Hill et al., 2015)</cite> , which were also applied to question answering.",
  "y": "similarities"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_11",
  "x": "To our knowledge, embedding the query into a single vector representation is a choice that is shared by most machine reading comprehension models. In our model, the repeated, tight integration between query attention and document attention allows the model to explore dynamically which parts of the query are most important to predict the answer, and then to focus on the parts of the document that are most salient to the currently-attended query components. A similar attempt in attending different components of the query may be found in (Hermann et al., 2015) . In that model, the document is processed once for each query word. This can be computationally intractable for large documents, since it involves unrolling a bidirectional recurrent neural network over the entire document multiple times. In contrast, our model only estimates query and document encodings once and can learn how to attend different parts of those encodings in a fixed number of steps. The inference network is responsible for making sense of the current attention step with respect to what has been gathered before. In addition to achieving state-ofthe-art performance, this technique may also prove to be more scalable than alternative query attention models. Finally, our iterative inference process shares similarities to the iterative hops in Memory Networks (Sukhbaatar et al., 2015;<cite> Hill et al., 2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "1f463f2f87bc2d572299d96481084f_0",
  "x": "**INTRODUCTION** Automatic metrics, such as BLEU (Papineni et al., 2002) , are widely used in machine translation (MT) as a substitute for human evaluation. Such metrics commonly take the form of an automatic comparison of MT output text with one or more human reference translations. Small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance. For several metrics, such as BLEU, standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods. Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004;<cite> Koehn, 2004)</cite> , to assess for a pair of systems how likely a difference in BLEU scores occurred by chance. Empirical tests detailed in<cite> Koehn (2004)</cite> show that even for test sets as small as 300 translations, BLEU confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large. Approximate randomization was subsequently proposed as an alternate to bootstrap resampling (Riezler and Maxwell, 2005) . Theoretically speaking, approximate randomization has an advantage over bootstrap resampling, in that it does not make the assumption that samples are representative of the populations from which they are drawn.",
  "y": "background"
 },
 {
  "id": "1f463f2f87bc2d572299d96481084f_1",
  "x": "For several metrics, such as BLEU, standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods. Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004;<cite> Koehn, 2004)</cite> , to assess for a pair of systems how likely a difference in BLEU scores occurred by chance. Empirical tests detailed in<cite> Koehn (2004)</cite> show that even for test sets as small as 300 translations, BLEU confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large. Approximate randomization was subsequently proposed as an alternate to bootstrap resampling (Riezler and Maxwell, 2005) . Theoretically speaking, approximate randomization has an advantage over bootstrap resampling, in that it does not make the assumption that samples are representative of the populations from which they are drawn. Both methods require some adaptation in order to be used for the purpose of MT evaluation, such as combination with an automatic metric, and therefore it cannot be taken for granted that approximate randomization will be more accurate in practice. Within MT, approximate randomization for the purpose of statistical testing is also less common. Riezler and Maxwell (2005) provide a comparison of approximate randomization with bootstrap resampling (distinct from paired bootstrap resampling), and conclude that since approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011) .",
  "y": "background"
 },
 {
  "id": "1f463f2f87bc2d572299d96481084f_2",
  "x": "**REVISITING STATISTICAL SIGNIFICANCE TESTS FOR MT EVALUATION** First, we revisit the formulations of bootstrap resampling and approximate randomization algorithms as presented in Riezler and Maxwell (2005) . At first glance, both methods appear to be two-tailed tests, with the null hypothesis that the two systems perform equally well. A better comparison of p-values would first require doubling the values of the one-sided bootstrap, leaving those of the two-sided approximate randomization algorithm as-is. The results of the two tests on this basis are extremely close, and in fact, in two out of the five comparisons, those of the bootstrap would have marginally higher pvalues than those of approximate randomization. As such, it is conceivable to conclude that the experiments actually show no substantial difference in Type I error between the two tests, which is consistent with results published in other fields of research (Smucker et al., 2007) . We also note that the pseudo-code contains an unconventional computation of mean pseudo-statistics, \u03c4 B , for shiftto-zero. Rather than speculate over whether these issues with the original paper were simply presentational glitches or the actual basis of the experiments reported on in the paper, we present a normalized version of the two-sided bootstrap algorithm in Figure 1 , and report on the results of our own experiments in Section 4. We compare this method with approximate randomization and also paired bootstrap resampling<cite> (Koehn, 2004)</cite> , which is widely used in MT evaluation.",
  "y": "motivation differences"
 },
 {
  "id": "1f463f2f87bc2d572299d96481084f_3",
  "x": "Since all randomized tests only function in combination with an automatic MT evaluation metric, we present results of each randomized test across four different MT metrics. ---------------------------------- **RANDOMIZED SIGNIFICANCE TESTS** ---------------------------------- **BOOTSTRAP RESAMPLING** Bootstrap resampling provides a way of estimating the population distribution by sampling with replacement from a representative sample (Efron and Tibshirani, 1993) . The test statistic is taken as the difference in scores of the two systems, S X \u2212 S Y , which has an expected value of 0 under the null hypothesis that the two systems perform equally well. A bootstrap pseudo-sample consists of the translations by the two systems (X b , Y b ) of a bootstrapped test set<cite> (Koehn, 2004)</cite> , constructed by sampling with replacement from the original test set translations. The bootstrap distribution S boot of the test statistic is estimated by calculating the value of the pseudo-statistic The null hypothesis distribution S H 0 can be estimated from S boot by applying the shift method (Noreen, 1989 ), which assumes that S H 0 has the same shape but a different mean than S boot .",
  "y": "uses background"
 },
 {
  "id": "1f72d18331beaef7adf4a78d1619c6_0",
  "x": "In this paper, we introduce QVEC-CCA, which simultaneously addresses both problems, while preserving major strengths of QVEC. 1 ---------------------------------- **QVEC AND QVEC-CCA** We introduce QVEC-CCA-an intrinsic evaluation measure of the quality of word embeddings. Our method is a modification of QVEC-an evaluation based on alignment of embeddings to a matrix of features extracted from a linguistic resource <cite>(Tsvetkov et al., 2015)</cite> . We review QVEC, and then describe QVEC-CCA. ---------------------------------- **QVEC.**",
  "y": "extends"
 },
 {
  "id": "1f72d18331beaef7adf4a78d1619c6_1",
  "x": "---------------------------------- **SEMANTIC VECTORS.** To evaluate the semantic content of word vectors,<cite> Tsvetkov et al. (2015)</cite> exploit supersense annotations in a WordNetannotated corpus-SemCor (Miller et al., 1993 Table 2 : Linguistic dimension word vector matrix with syntactic vectors, constructed using PTB. \u2022 We first train 21 word vector models: variants of CBOW and Skip-Gram models (Mikolov et al., 2013) ; their modifications CWindow, Structured Skip-Gram, and CBOW with Attention (Ling et al., 2015b; Ling et al., 2015a) ; GloVe vectors (Pennington et al., 2014) ; Latent Semantic Analysis (LSA) based vectors (Church and Hanks, 1990) ; and retrofitted GloVe and LSA vectors . \u2022 We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC-CCA, and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353) , MEN dataset (Bruni et al., 2012) , and SimLex-999 dataset (Hill et al., 2014, SimLex) . 3 \u2022 In addition, the same vectors are evaluated using extrinsic text classification tasks. Our semantic benchmarks are four binary categorization tasks from the 20 Newsgroups (20NG); sentiment analysis task (Socher et al., 2013, Senti) ; and the metaphor detection (Tsvetkov et al., 2014, Metaphor) . \u2022 Finally, we compute the Pearson's correlation coefficient r to quantify the linear relationship between the intrinsic and extrinsic scorings. The higher the correlation, the better suited the intrinsic evaluation to be used as a proxy to the extrinsic task.",
  "y": "background"
 },
 {
  "id": "1f72d18331beaef7adf4a78d1619c6_2",
  "x": "Our semantic benchmarks are four binary categorization tasks from the 20 Newsgroups (20NG); sentiment analysis task (Socher et al., 2013, Senti) ; and the metaphor detection (Tsvetkov et al., 2014, Metaphor) . \u2022 Finally, we compute the Pearson's correlation coefficient r to quantify the linear relationship between the intrinsic and extrinsic scorings. The higher the correlation, the better suited the intrinsic evaluation to be used as a proxy to the extrinsic task. We extend the setup of<cite> Tsvetkov et al. (2015)</cite> with two syntactic benchmarks, and evaluate QVEC-CCA with the syntactic matrix. The first task is POS tagging; we use the LSTM-CRF model (Lample et al., 2016) , and the second is dependency parsing (Parse), using the stack-LSTM model of . ---------------------------------- **RESULTS.** To test the efficiency of QVEC-CCA in capturing the semantic content of word vectors, we evaluate how well the scores correspond to the 3 We employ an implementation of a suite of word similarity tasks at wordvectors.org (Faruqui and Dyer, 2014) . scores of word vector models on semantic benchmarks.",
  "y": "extends"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_0",
  "x": "Previous work has demonstrated that intervention by social media has modest but significant success in decreasing obesity (Ashrafian et al., 2014) . Furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible<cite> (Fried et al., 2014</cite>; Culotta, 2014) . However, in all cases, classification is made on aggregated data from cities, counties, or states, so these models are not immediately applicable to the task of classifying individuals. Our work takes the first steps towards transferring a classification model that identifies communities that are more overweight than average to classifying overweight (and thus at-risk for T2DM) individuals. The contributions of our work are: 1. We introduce a random-forest (RF) model that classifies US states as more or less overweight than average using only 7 decision trees with a maximum depth of 3. Despite the model's simplicity, it outperforms<cite> Fried et al. (2014)</cite> 's best model by 2% accuracy. 2. Using this model, we introduce a novel semi-automated process that converts the decision nodes in the RF model into natural language questions.",
  "y": "background"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_1",
  "x": "Previous work has demonstrated that intervention by social media has modest but significant success in decreasing obesity (Ashrafian et al., 2014) . Furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible<cite> (Fried et al., 2014</cite>; Culotta, 2014) . However, in all cases, classification is made on aggregated data from cities, counties, or states, so these models are not immediately applicable to the task of classifying individuals. Our work takes the first steps towards transferring a classification model that identifies communities that are more overweight than average to classifying overweight (and thus at-risk for T2DM) individuals. The contributions of our work are: 1. We introduce a random-forest (RF) model that classifies US states as more or less overweight than average using only 7 decision trees with a maximum depth of 3. Despite the model's simplicity, it outperforms<cite> Fried et al. (2014)</cite> 's best model by 2% accuracy. 2. Using this model, we introduce a novel semi-automated process that converts the decision nodes in the RF model into natural language questions.",
  "y": "extends"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_2",
  "x": "However, in all cases, classification is made on aggregated data from cities, counties, or states, so these models are not immediately applicable to the task of classifying individuals. Our work takes the first steps towards transferring a classification model that identifies communities that are more overweight than average to classifying overweight (and thus at-risk for T2DM) individuals. The contributions of our work are: 1. We introduce a random-forest (RF) model that classifies US states as more or less overweight than average using only 7 decision trees with a maximum depth of 3. Despite the model's simplicity, it outperforms<cite> Fried et al. (2014)</cite> 's best model by 2% accuracy. 2. Using this model, we introduce a novel semi-automated process that converts the decision nodes in the RF model into natural language questions. We then use these questions to implement a quiz that mimics a 20-questions-like game. The quiz aims to detect if the person taking it is overweight or not based on indirect questions related to food or use of food-related words.",
  "y": "differences"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_3",
  "x": "Mysl\u00edn et al. (2013) focus on understanding the perception of emerging tobacco products by analyzing tweets. Social media, especially Twitter, has been recently utilized as a popular source of data for public health monitoring, such as tracking diseases (Ginsberg et al., 2009; YomTov et al., 2014; Nascimento et al., 2014; Greene et al., 2011; Chew and Eysenbach, 2010) , mining drug-related adverse events (Bian et al., 2012) , predicting postpartum psychological changes in new mothers (De Choudhury et al., 2013) , and detecting life satisfaction (Schwartz et al., 2013) and obesity (Chunara et al., 2013; Cohen-Cole and Fletcher, 2008; Fernandez-Luque et al., 2011) . We focus our attention on the language of food on social media to identify overweight communities and individuals. In the last couple of years, several variants of this problem have been considered<cite> (Fried et al., 2014</cite>; Abbar et al., 2015; Culotta, 2014; Ardehaly and Culotta, 2015) . food-related tweets and use it to predict several population characteristics, namely diabetes rate, overweight rate and political tendency. Generally, they use state-level populations, e.g., one of their classification tasks is to label whether a state is more overweight than the national median. Overweight rate is the percentage of adults whose Body Mass Index (BMI) is larger than a normal range defined by NIH. The classification task is to label whether a state is more overweight than the national median. Individuals' tweets are localized at state level as a single instance to train several classifier models, and the performance of models is evaluated using leave-one-out cross-validation.",
  "y": "background"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_4",
  "x": "The classification task is to label whether a state is more overweight than the national median. Individuals' tweets are localized at state level as a single instance to train several classifier models, and the performance of models is evaluated using leave-one-out cross-validation. Importantly,<cite> Fried et al. (2014)</cite> train and test their models on communities rather than individuals, which limits the applicability of their approach to individualized public health. Abbar et al. (2015) also used aggregated information for predicting obesity and diabetes statistics. They considered energy intake based on caloric values in food mentioned on social media, demographic variables, and social networks. This paper begins to address individual predictions, based on the simplifying assumption that all individuals can be labeled based on the known label of their home county, e.g., all individuals in an overweight county are overweight, which is less than ideal. In contrast, our work collects actual individual information through the survey derived from community information. Even though performing classification at state or county granularity tends to be robust and accurate<cite> (Fried et al., 2014)</cite> , characteristics that are specific to individuals are more meaningful and practical. A wave of computational work on the automatic identification of latent attributes of individuals has recently emerged.",
  "y": "motivation"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_5",
  "x": "This paper begins to address individual predictions, based on the simplifying assumption that all individuals can be labeled based on the known label of their home county, e.g., all individuals in an overweight county are overweight, which is less than ideal. In contrast, our work collects actual individual information through the survey derived from community information. Even though performing classification at state or county granularity tends to be robust and accurate<cite> (Fried et al., 2014)</cite> , characteristics that are specific to individuals are more meaningful and practical. A wave of computational work on the automatic identification of latent attributes of individuals has recently emerged. Ardehaly and Culotta (2015) utilize label regularization, a lightly supervised learning method, to infer latent attributes of individuals, such as age and ethnicity. Other efforts have focused on inferring the gender of people on Twitter (Bamman et al., 2014; Burger et al., 2011) or their location on the basis of the text in their tweets (Cheng et al., 2010; Eisenstein et al., 2010) . These are exciting approaches, but it is unlikely they will perform as well as a fully supervised model, which is the ultimate goal of our work. ---------------------------------- **METHOD**",
  "y": "differences"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_6",
  "x": "The first step is to develop an interpretable predictive model that identifies communities that are more overweight than average, in a way that can be converted into fun, engaging natural language questions. To this end, we started with the same settings as<cite> Fried et al. (2014)</cite> : we used the 887,310 tweets they collected which were localizable to a specific state and contained at least one relevant hashtag, such as #breakfast or #dinner. Each state was assigned a binary label (more or less overweight than the median) by comparing the percentage of overweight adults against the median state. For each state, we extracted features based on unigram (i.e., single) words and hashtags from all the above tweets localized to the corresponding state. To mitigate sparsity, we also included topics generated using Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and all tweets collected by Fried et al. For example, one of the generated topics contains words that approximate the standard American diet (e.g., chicken, potatoes, cheese, baked, beans, fried, mac), which has already been shown to correlate with higher overweight and T2DM rates<cite> (Fried et al., 2014</cite> Figure 2 : A decision tree from the random forest classifier trained using state-level Twitter data. motivation for this decision was interpretability: as shown below, decision trees can be easily converted into a series of if . . . then . . . else . . . statements, which form the building blocks of the quiz. To minimize the number of questions, we trained a random forest with 7 trees with maximum depth of 3, and we ignored tokens that appear fewer than 3 times in the training data. These parameter values were selected to make the quiz of reasonable length.",
  "y": "uses"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_7",
  "x": "Each state was assigned a binary label (more or less overweight than the median) by comparing the percentage of overweight adults against the median state. For each state, we extracted features based on unigram (i.e., single) words and hashtags from all the above tweets localized to the corresponding state. To mitigate sparsity, we also included topics generated using Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and all tweets collected by Fried et al. For example, one of the generated topics contains words that approximate the standard American diet (e.g., chicken, potatoes, cheese, baked, beans, fried, mac), which has already been shown to correlate with higher overweight and T2DM rates<cite> (Fried et al., 2014</cite> Figure 2 : A decision tree from the random forest classifier trained using state-level Twitter data. motivation for this decision was interpretability: as shown below, decision trees can be easily converted into a series of if . . . then . . . else . . . statements, which form the building blocks of the quiz. To minimize the number of questions, we trained a random forest with 7 trees with maximum depth of 3, and we ignored tokens that appear fewer than 3 times in the training data. These parameter values were selected to make the quiz of reasonable length. We aimed at 20 questions, as in the popular \"20 questions\" game, in which one player must guess what object the other is thinking of by asking 20 or fewer yes-or-no questions. Further tuning confirmed that a small number of shallow trees are most effective in accurately partitioning the state-level data.",
  "y": "uses"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_8",
  "x": "---------------------------------- **QUIZ** We next manually converted all decision statements in the random forest classifier into natural language questions. The main assumption behind this process is that language use parallels actual behavior, e.g., a person who talks about fruit on social media will also eat fruit in real life. This allowed us to produce more intuitive questions, such as How often do you eat fruit? for the top node in Figure 2, Figure 2 . ---------------------------------- **MODEL ACCURACY** Majority baseline 50.89 SVM<cite> (Fried et al., 2014)</cite> 80.39 RF (food + hashtags) 82.35 Discretized RF (food + hashtags) 78.43 Table 2 : Random forest (RF) classifier performance on state-level data relative to majority baseline and<cite> Fried et al. (2014)</cite> 's best classifier. We include two versions of our classifier: the first keeps numeric features (e.g., word counts) as is, whereas the second discretizes numeric features to three bins.",
  "y": "uses"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_9",
  "x": "In all, we generated 33 questions that cover all decision nodes in the random forest classifier. However, when taking the quiz, each individual participant answered between 12 and 24 questions, depending on their answers and the corresponding traversal of the decision trees. This quiz serves to gather training data, which will be used in future work to train a supervised model for the identification of individuals at risk. To our knowledge, this approach is a novel strategy for quiz generation, and it serves as an important stepping-stone toward our goal of building individualized public health tools driven by social media. With respect to data retention, we collect (with the permission of the participants) the following additional data to be used for future research: height, weight, sex, location, age, and social media handles for Twitter, Instagram, and Facebook. We only downloaded public posts using these handles. This data (specifically height and weight) is also immediately used to compute the participant's BMI, to verify whether the classifier was correct. identical experimental settings as<cite> (Fried et al., 2014)</cite> , i.e., leave-one-out-cross-validation on the 50 states plus the District of Columbia. The table shows that our best model performs 2% better than the best model of<cite> (Fried et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_10",
  "x": "However, when taking the quiz, each individual participant answered between 12 and 24 questions, depending on their answers and the corresponding traversal of the decision trees. This quiz serves to gather training data, which will be used in future work to train a supervised model for the identification of individuals at risk. To our knowledge, this approach is a novel strategy for quiz generation, and it serves as an important stepping-stone toward our goal of building individualized public health tools driven by social media. With respect to data retention, we collect (with the permission of the participants) the following additional data to be used for future research: height, weight, sex, location, age, and social media handles for Twitter, Instagram, and Facebook. We only downloaded public posts using these handles. This data (specifically height and weight) is also immediately used to compute the participant's BMI, to verify whether the classifier was correct. identical experimental settings as<cite> (Fried et al., 2014)</cite> , i.e., leave-one-out-cross-validation on the 50 states plus the District of Columbia. The table shows that our best model performs 2% better than the best model of<cite> (Fried et al., 2014)</cite> . Our second classifier, which used discretized numeric features and was the source of the quiz, performed 2% worse, but it still had acceptable accuracy, nearing 80%.",
  "y": "differences"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_0",
  "x": "**INTRODUCTION** The automatic identification of offensive content online is an important task which has gained more attention in recent years. Social media platforms such as Facebook and Twitter have been investing heavily in ways to cope with the widespread forms of such content. The task is usually modelled as supervised classification problem in which systems are trained using a dataset containing posts which are annotated with respect to the presence of some form(s) of abusive or offensive content. Examples of offensive content investigated in previous studies include hate speech Zampieri, 2017, 2018) , cyberbulling (Dinakar et al., 2011) , and aggression (Kumar et al., 2018) . Given the multitude of terms and definitions used in the literature, recent studies have investigated common aspects of the abusive language detection sub-tasks (Waseem et al., 2017; Wiegand et al., 2018) . However, none of these initial studies focused on both the type and the target of the offensive language. Therefore, in conjunction with this task, we present the Offensive Language Identification Dataset (OLID)<cite> (Zampieri et al., 2019)</cite> . OLID is an annotated dataset with a three-level annotation model.",
  "y": "similarities"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_1",
  "x": "Examples of offensive content investigated in previous studies include hate speech Zampieri, 2017, 2018) , cyberbulling (Dinakar et al., 2011) , and aggression (Kumar et al., 2018) . Given the multitude of terms and definitions used in the literature, recent studies have investigated common aspects of the abusive language detection sub-tasks (Waseem et al., 2017; Wiegand et al., 2018) . However, none of these initial studies focused on both the type and the target of the offensive language. Therefore, in conjunction with this task, we present the Offensive Language Identification Dataset (OLID)<cite> (Zampieri et al., 2019)</cite> . OLID is an annotated dataset with a three-level annotation model. We show that breaking down offensive content into sub-categories by taking the type and target of offenses into account results in a flexible annotation model that can relate to the phenomena captured by previously annotated datasets such as the one by . Hate speech, for example, is commonly understood as an insult targeted at a group whereas cyberbulling is typically targeted at an individual). In OffensEval 1 we use OLID<cite> (Zampieri et al., 2019)</cite> and propose one sub-task for each layer of annotation as presented in Section 3. The remainder of this paper is organized as follows: Section 3 presents the shared task description and the sub-tasks included in OffensEval and Section 4 includes a brief description of OLID based on <cite>Zampieri et al. (2019)</cite> .",
  "y": "differences extends"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_2",
  "x": "The remainder of this paper is organized as follows: Section 3 presents the shared task description and the sub-tasks included in OffensEval and Section 4 includes a brief description of OLID based on <cite>Zampieri et al. (2019)</cite> . Section 5 presents an analysis of the results of the shared task, and, finally, Section 6 concludes this paper presenting avenues for future work. ---------------------------------- **RELATED WORK** Different abusive and offense language identification sub-tasks have been explored in the past few years including aggression identification, bullying detection, hate speech, toxic comments, and offensive language. Aggression identification: The TRAC shared task on Aggression Identification (Kumar et al., 2018) provided participants with a dataset containing 15,000 annotated Facebook posts and com-ments in English and Hindi for training and validation. For testing, two different sets, one from Facebook and one from Twitter were provided. Systems were trained to discriminate between three classes: non-aggressive, covertly aggressive, and overtly aggressive. The best performing systems in this competition used deep learning approaches based on convolutional neural networks (CNN), recurrent neural networks, and LSTMs (Aroyehun and Gelbukh, 2018; Majumder et al., 2018) .",
  "y": "uses similarities"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_3",
  "x": "While each of these sub-tasks tackle a particular type of abuse or offense, they share similar properties and the hierarchical annotation model pro-posed proposed in OLID<cite> (Zampieri et al., 2019)</cite> and used in OffensEval aims to capture this. Considering that, for example, an insult targeted at an individual is commonly known as cyberbulling and that insults targeted at a group are known as hate speech, we pose that OLID's hierarchical annotation model makes it a useful resource for various offensive language identification sub-tasks. ---------------------------------- **TASK DESCRIPTION AND EVALUATION** The training and testing material used for OffensEval is the aforementioned Offensive Language Identification Dataset (OLID) dataset, built for this task. OLID was annotated using a hierarchical three-level annotation model introduced in <cite>Zampieri et al. (2019)</cite> . We use the annotation of each of the three layers in OLID to each sub-task in OffensEval as follows: ---------------------------------- **SUB-TASK A: OFFENSIVE LANGUAGE IDENTIFICATION**",
  "y": "similarities uses"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_4",
  "x": "While each of these sub-tasks tackle a particular type of abuse or offense, they share similar properties and the hierarchical annotation model pro-posed proposed in OLID<cite> (Zampieri et al., 2019)</cite> and used in OffensEval aims to capture this. Considering that, for example, an insult targeted at an individual is commonly known as cyberbulling and that insults targeted at a group are known as hate speech, we pose that OLID's hierarchical annotation model makes it a useful resource for various offensive language identification sub-tasks. ---------------------------------- **TASK DESCRIPTION AND EVALUATION** The training and testing material used for OffensEval is the aforementioned Offensive Language Identification Dataset (OLID) dataset, built for this task. OLID was annotated using a hierarchical three-level annotation model introduced in <cite>Zampieri et al. (2019)</cite> . We use the annotation of each of the three layers in OLID to each sub-task in OffensEval as follows: ---------------------------------- **SUB-TASK A: OFFENSIVE LANGUAGE IDENTIFICATION**",
  "y": "uses similarities"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_5",
  "x": "The values are then averaged, giving equal weight to all classes, regardless of the number of samples. ---------------------------------- **DATA** In this Section we summarize OLID, the dataset used for this task. A detailed description of the data collection process and annotation is presented in <cite>Zampieri et al. (2019)</cite> . OLID is a large collection of English tweets annotated using a hierarchical three-layer annotation model. It contains 14,100 annotated tweets divided in a training partition containing 13,240 tweets and a test partition containing 860 tweets. Additionally, a small trial set containing 320 tweets was made available before the start of the competition. The distribution of the labels in OLID is shown in Table 1 .",
  "y": "similarities uses"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_6",
  "x": "**CONCLUSION** In this paper, we presented the results of SemEval-2016 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval). In OffensEval we used OLID<cite> (Zampieri et al., 2019)</cite> , a dataset containing English tweets annotated with a hierarchical three-layer annotation model which considers 1) whether a message is offensive or not (sub-task A); 2) what is the type of the offensive 7 In the camera-ready version of this report we will be including a Table with references to all system descriptions papers. message (sub-task B); and 3) what is the target of the offensive (sub-task C). OLID is publicly available to the research community. 8 In total, nearly 800 teams signed up to participate in OffensEval and 115 of them submitted results across the three sub-tasks. In Section 5 we discussed the approaches used by the 115 teams in the shared task. We observed that both deep learning and traditional ML classifiers and classifier ensembles have been widely use and that most high-performing systems used state-of-theart deep learning models, in particular BERT (Devlin et al., 2018) . Our public dataset can continue to be used to explore future advances in detecting offensive content and provide a a benchmark for evaluating different models.",
  "y": "similarities uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_0",
  "x": "The distortion model may be lexicalised but does not typically incorporate information about the syntactic structure of the sentence. As such, although PSMT has been very successful, it suffers from the lack of a principled mechanism for handling long-distance reordering phenomena due to word order differences between languages. One method for addressing this difficulty is the reordering-as-preprocessing approach, exemplified by <cite>Collins et al. (2005)</cite> and Xia and McCord (2004) , where PSMT is coupled with a preprocessing step that reorders input sentences to more closely parallel the target language word order. Although this leads to improved performance overall, <cite>Collins et al. (2005)</cite> show that the reordering-as-preprocessing system does not consistently provide better translations than the PSMT baseline on a sentence-by-sentence basis. One possible reason could be errors in the parse or the consequent reordering. Chiang et al. (2009) used features indicating problematic use of syntax to improve performance within hierarchical and syntax-based translation. In this work, we want to see whether syntax-related features can help choose between original and reordered sentence translations in PSMT. We use as our starting point the PSMT system Moses . In order to use features within the system's log-linear model to assess the reliability of syntax, it is necessary to input both variants simultaneously.",
  "y": "background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_1",
  "x": "One method for addressing this difficulty is the reordering-as-preprocessing approach, exemplified by <cite>Collins et al. (2005)</cite> and Xia and McCord (2004) , where PSMT is coupled with a preprocessing step that reorders input sentences to more closely parallel the target language word order. Although this leads to improved performance overall, <cite>Collins et al. (2005)</cite> show that the reordering-as-preprocessing system does not consistently provide better translations than the PSMT baseline on a sentence-by-sentence basis. One possible reason could be errors in the parse or the consequent reordering. Chiang et al. (2009) used features indicating problematic use of syntax to improve performance within hierarchical and syntax-based translation. In this work, we want to see whether syntax-related features can help choose between original and reordered sentence translations in PSMT. We use as our starting point the PSMT system Moses . In order to use features within the system's log-linear model to assess the reliability of syntax, it is necessary to input both variants simultaneously. To do this, we adapt in a novel way the lattice input of Moses; we refer to this new system as dual-path PSMT ( \u00a73). We then augment the model with a number of confidence features to enable it to evaluate which of the two paths is more likely to yield the best translation ( \u00a73.2).",
  "y": "motivation background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_2",
  "x": "We use as our starting point the PSMT system Moses . In order to use features within the system's log-linear model to assess the reliability of syntax, it is necessary to input both variants simultaneously. To do this, we adapt in a novel way the lattice input of Moses; we refer to this new system as dual-path PSMT ( \u00a73). We then augment the model with a number of confidence features to enable it to evaluate which of the two paths is more likely to yield the best translation ( \u00a73.2). We reimplement the <cite>Collins et al. (2005)</cite> reordering preprocessing step and conduct some preliminary experiments in German-toEnglish translation ( \u00a74). Our results ( \u00a75) do not replicate the finding of <cite>Collins et al. (2005)</cite> that the preprocessing step produces better translation results overall. However, results for our dual-path PSMT system do show an improvement, with our plain system achieving a BLEU score (Papineni et al., 2002) of 21.39, an increase of 0.62 over the baseline. We therefore conclude that a syntactically-informed reordering preprocessing step is inconsistently of use in PSMT, and that enabling the system to choose when to use the reordering leads to improved translation performance. 2 Background and Related Work 2.1 Phrase-based statistical MT Phrase-based statistical MT (PSMT) systems such as Marcu and Wong (2002) and have set the standard for statistical MT for many years.",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_3",
  "x": "One possible reason could be errors in the parse or the consequent reordering. Chiang et al. (2009) used features indicating problematic use of syntax to improve performance within hierarchical and syntax-based translation. In this work, we want to see whether syntax-related features can help choose between original and reordered sentence translations in PSMT. We use as our starting point the PSMT system Moses . In order to use features within the system's log-linear model to assess the reliability of syntax, it is necessary to input both variants simultaneously. To do this, we adapt in a novel way the lattice input of Moses; we refer to this new system as dual-path PSMT ( \u00a73). We then augment the model with a number of confidence features to enable it to evaluate which of the two paths is more likely to yield the best translation ( \u00a73.2). We reimplement the <cite>Collins et al. (2005)</cite> reordering preprocessing step and conduct some preliminary experiments in German-toEnglish translation ( \u00a74). Our results ( \u00a75) do not replicate the finding of <cite>Collins et al. (2005)</cite> that the preprocessing step produces better translation results overall.",
  "y": "differences"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_4",
  "x": "Common options are distance-based models (where movement is penalised proportionally to the distance moved) and lexicalised models (where probability of movement is conditioned upon the phrase being moved). Without syntactic information here, PSMT systems lack a principled way to manage long-distance movements, leading to difficulty in language pairs where this is needed, such as English and Japanese or German. ---------------------------------- **REORDERING-AS-PREPROCESSING** The reordering-as-preprocessing approach addresses the PSMT reordering difficulty by removing word order differences prior to translation. This is done with a preprocessing step where the input sentence is parsed and a reordered alternative created on the basis of the resulting parse tree. Our work builds on the reordering-aspreprocessing approach of <cite>Collins et al. (2005)</cite> . Working with German-to-English translation, <cite>Collins et al. (2005)</cite> parse input sentences with a constituent-structure parser and apply six hand-crafted rules to reorder the German text toward English word order. These rules target the placement of non-finite and finite verbs, subjects, particles and negation.",
  "y": "extends"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_5",
  "x": "---------------------------------- **REORDERING-AS-PREPROCESSING** The reordering-as-preprocessing approach addresses the PSMT reordering difficulty by removing word order differences prior to translation. This is done with a preprocessing step where the input sentence is parsed and a reordered alternative created on the basis of the resulting parse tree. Our work builds on the reordering-aspreprocessing approach of <cite>Collins et al. (2005)</cite> . Working with German-to-English translation, <cite>Collins et al. (2005)</cite> parse input sentences with a constituent-structure parser and apply six hand-crafted rules to reorder the German text toward English word order. These rules target the placement of non-finite and finite verbs, subjects, particles and negation. The authors demonstrate a statistically significant improvement in BLEU score over the baseline PSMT system. Many other reordering-as-preprocessing systems exist.",
  "y": "background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_6",
  "x": "Automatically-acquired rules may be noisier and less intuitive than hand-crafted rules but the approach has the advantage of being more easily extended to new language pairs. Other examples of systems include Wang et al. (2007) (manual rules, Chinese-to-English), Habash (2007) (automatic rules, Arabic-to-English) and Popovi\u0107 and Ney (2006) (manual rules, Spanish/English-toSpanish/English/German). Despite the success of the reordering-aspreprocessing approach overall, <cite>Collins et al. (2005)</cite> found that in a human evaluation on 100 sentences, there were still several cases in which the baseline system translation was preferred over that produced with the reordering. The authors note this finding but do not analyse it further. ---------------------------------- **FEATURES FOR IMPROVED TRANSLATION** Zwarts and Dras (2008) explore the <cite>Collins et al. (2005)</cite> finding by examining whether machine learning techniques can be used to predict, on a sentence-by-sentence basis, whether the translation of the reordered sentence is to be preferred over the alternative. For features, they use sentence length, parse probability from the Collins parser and unlinked fragment count from the Link Grammar parser on the English side of the translation. The authors find that, when used on the source side (in English-to-Dutch translation), these features provide no significant improvement in BLEU score, while as target-side features (in Dutch-to-English translation) they improve the BLEU score by 1.7 points over and above the 1.3 point improvement from reordering.",
  "y": "background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_7",
  "x": "Many other reordering-as-preprocessing systems exist. Xia and McCord (2004) present a system for French-English translation that, instead of using hand-crafted reordering rules, automatically learns reordering patterns from the corpus. Automatically-acquired rules may be noisier and less intuitive than hand-crafted rules but the approach has the advantage of being more easily extended to new language pairs. Other examples of systems include Wang et al. (2007) (manual rules, Chinese-to-English), Habash (2007) (automatic rules, Arabic-to-English) and Popovi\u0107 and Ney (2006) (manual rules, Spanish/English-toSpanish/English/German). Despite the success of the reordering-aspreprocessing approach overall, <cite>Collins et al. (2005)</cite> found that in a human evaluation on 100 sentences, there were still several cases in which the baseline system translation was preferred over that produced with the reordering. The authors note this finding but do not analyse it further. ---------------------------------- **FEATURES FOR IMPROVED TRANSLATION** Zwarts and Dras (2008) explore the <cite>Collins et al. (2005)</cite> finding by examining whether machine learning techniques can be used to predict, on a sentence-by-sentence basis, whether the translation of the reordered sentence is to be preferred over the alternative.",
  "y": "background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_8",
  "x": "Zwarts and Dras (2008) explore the <cite>Collins et al. (2005)</cite> finding by examining whether machine learning techniques can be used to predict, on a sentence-by-sentence basis, whether the translation of the reordered sentence is to be preferred over the alternative. For features, they use sentence length, parse probability from the Collins parser and unlinked fragment count from the Link Grammar parser on the English side of the translation. The authors find that, when used on the source side (in English-to-Dutch translation), these features provide no significant improvement in BLEU score, while as target-side features (in Dutch-to-English translation) they improve the BLEU score by 1.7 points over and above the 1.3 point improvement from reordering. Our work has some similarities to that of Zwarts and Dras (2008) but uses the log-linear model of the translation system itself to include features, rather than a separate classifier that does not permit interaction between the confidence features and features used during translation. This idea of using linguistic features to improve statistical MT has appeared in a number of recent papers. Chiang et al. (2009) demonstrate an improvement in hierarchical PSMT and syntaxbased (string-to-tree) statistical MT through the addition of features pinpointing possible errors in the translation, for example the number of occurrences of a particular grammar production rule, or non-terminal in a rule. Xiong et al. (2010) derive features from the Link Grammar parser, in combination with word posterior probabilities, to detect MT errors (in order to subsequently improve translation quality). Unlike Chiang et al. (2009) , we work with PSMT and use features that consider the parse tree as a whole or aspects of the reordering process itself. Unlike Xiong et al. (2010) , we use these features directly in translation.",
  "y": "differences background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_9",
  "x": "Similar to our work and the work in the previous section, Ge (2010) includes features to assess reordering options, based on the structure of the resulting tree, for example which nonterminal appears as the first child and the size of jump required to reach the nonterminal used as the next child. In addition to the difference with the distortion model mentioned above, our work differs in that Ge (2010) focuses on finding the best reordering using syntactic features plus a few surface and POS-tag features as a way of \"guarding against parsing errors\", whereas we also look at using features to represent confidence in a parse. 3 Dual-Path PSMT In this paper, we develop a dual-path PSMT system. \u00a73.1 introduces the lattice input format, by which we provide the system with two variants of the input sentence: the original and the reordered alternative produced by the preprocessing step. \u00a73.2 outlines the confidence features that we include in the translation model to help the system choose between the two alternatives. Our system is built upon the PSMT system Moses . For reordering, we use the Berkeley parser (Petrov et al., 2006 ) and the rules given by <cite>Collins et al. (2005)</cite> , but any reordering preprocessing step could equally be used. Further details of our systems are given in \u00a74. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_10",
  "x": "This procedure is limited in the number of features that it can efficiently process. Further extending the feature set would require a different optimisation procedure, such as MIRA, as discussed by Arun and Koehn (2007) . ---------------------------------- **EXPERIMENTS** ---------------------------------- **MODELS AND EVALUATION** Our baseline PSMT system is Moses , repository revision 3590. 4 We run all of our experiments using the Moses Experiment Management System; configuration files and scripts to reproduce our experiments are available online. 5 For the reordering preprocessing step we reimplement the <cite>Collins et al. (2005)</cite> rules and use this to recreate the <cite>Collins et al. (2005)</cite> reordering-aspreprocessing system as our second baseline.",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_11",
  "x": "**MODELS AND EVALUATION** Our baseline PSMT system is Moses , repository revision 3590. 4 We run all of our experiments using the Moses Experiment Management System; configuration files and scripts to reproduce our experiments are available online. 5 For the reordering preprocessing step we reimplement the <cite>Collins et al. (2005)</cite> rules and use this to recreate the <cite>Collins et al. (2005)</cite> reordering-aspreprocessing system as our second baseline. We use the Berkeley parser (Petrov et al., 2006) , repository revision 14, 6 to provide the parse trees for the reordering process. Since the German parsing model provided on the parser website does not include the function labels needed by the <cite>Collins et al. (2005)</cite> rules, we trained a new parsing model on the Tiger corpus (version 1). The reordering script and parsing model, along with details of how the parsing model was trained, are available online with the configuration files above. We compare four systems on German-toEnglish translation: the Moses baseline (MOSES), the <cite>Collins et al. (2005)</cite> baseline (REORDER), the lattice system with just the reordering indicator feature (LATTICE), and the lattice system with all 3 It is possible that in practice the imbalance in number of non-zero features between the two paths could cause the system some difficulty in assigning the weights for each feature. In future it would be interesting to investigate this possibility by introducing extra features to balance the two paths.",
  "y": "differences"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_12",
  "x": "We compare four systems on German-toEnglish translation: the Moses baseline (MOSES), the <cite>Collins et al. (2005)</cite> baseline (REORDER), the lattice system with just the reordering indicator feature (LATTICE), and the lattice system with all 3 It is possible that in practice the imbalance in number of non-zero features between the two paths could cause the system some difficulty in assigning the weights for each feature. In future it would be interesting to investigate this possibility by introducing extra features to balance the two paths. confidence features (+FEATURES). We do not explore different subsets of the features here. For evaluation we use the standard BLEU metric (Papineni et al., 2002) , which measures n-gram overlap between the candidate translation and the given reference translation. ---------------------------------- **APPROXIMATE ORACLE** To get an idea of a rough upper bound, we implement the approximate oracle outlined in Zwarts and Dras (2008) . For every sentence, the approximate oracle compares the outputs of MOSES and REORDER with the reference translation and chooses the output that it expects will contribute to a higher BLEU score overall.",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_13",
  "x": "The number of sentence pairs in each corpus are given in Table 2 . We trained 5-gram language models with SRILM (Stolcke, 2002) using the three language model files listed in Table 3 : BLEU scores for every system the last containing the remainder. One language model was produced for each file or subfile, giving a total of ten models. The final language model was produced by interpolation between these ten, with weights assigned based on the tuning corpus. Table 3 gives the BLEU score for each of our four systems and the approximate oracle. We note that these numbers are lower than those reported by <cite>Collins et al. (2005)</cite> . However, this is most likely due to differences in the training and testing data; our results are roughly in line with the numbers reported in the Euromatrix project for this test set. 8 Interestingly, our reimplementation of the <cite>Collins et al. (2005)</cite> baseline does not outperform the plain PSMT baseline. Possible explanations include variability due to differences in training data, noisier parser output in our system, or differing interpretation of the description of the reordering rules.",
  "y": "differences"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_14",
  "x": "The final language model was produced by interpolation between these ten, with weights assigned based on the tuning corpus. Table 3 gives the BLEU score for each of our four systems and the approximate oracle. We note that these numbers are lower than those reported by <cite>Collins et al. (2005)</cite> . However, this is most likely due to differences in the training and testing data; our results are roughly in line with the numbers reported in the Euromatrix project for this test set. 8 Interestingly, our reimplementation of the <cite>Collins et al. (2005)</cite> baseline does not outperform the plain PSMT baseline. Possible explanations include variability due to differences in training data, noisier parser output in our system, or differing interpretation of the description of the reordering rules. It may also be that the inconsistency of improvement noted by <cite>Collins et al. (2005)</cite> is the cause; sometimes the reordering produces better results and sometimes the baseline, with the effect just by chance favouring the baseline here. To explore this, we look at the approximate oracle. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_15",
  "x": "8 Interestingly, our reimplementation of the <cite>Collins et al. (2005)</cite> baseline does not outperform the plain PSMT baseline. Possible explanations include variability due to differences in training data, noisier parser output in our system, or differing interpretation of the description of the reordering rules. It may also be that the inconsistency of improvement noted by <cite>Collins et al. (2005)</cite> is the cause; sometimes the reordering produces better results and sometimes the baseline, with the effect just by chance favouring the baseline here. To explore this, we look at the approximate oracle. ---------------------------------- **RESULTS AND DISCUSSION** In our experiment, the oracle preferred the baseline output in 848 cases and the reordered in 1,070 cases. 215 sentences were identical between the two systems, while in 392 cases the sentences differed but had equal numbers of n-gram overlaps. The BLEU score for the oracle is higher than that of both baselines; from this and the distribution of the oracle's choices, we conclude that the difference between our findings and those of <cite>Collins et al. (2005)</cite> is at least partly due to the inconsistency that they identified.",
  "y": "uses background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_16",
  "x": "**RESULTS AND DISCUSSION** In our experiment, the oracle preferred the baseline output in 848 cases and the reordered in 1,070 cases. 215 sentences were identical between the two systems, while in 392 cases the sentences differed but had equal numbers of n-gram overlaps. The BLEU score for the oracle is higher than that of both baselines; from this and the distribution of the oracle's choices, we conclude that the difference between our findings and those of <cite>Collins et al. (2005)</cite> is at least partly due to the inconsistency that they identified. It is especially interesting to note that the reordered system's translations are preferred by the oracle more often even though its overall performance is lower. Turning now to the results of our systems, we see that simply giving the system the option of both reordered and non-reordered versions of the sentence (LATTICE) produces an improved translation performance overall. While the addition of our confidence features (+FEATURES) leaves the performance roughly unchanged, the gap between LATTICE and the approximate oracle implies that this is due to the choice of features, and that a feature set may yet be found that will improve performance over the plain LATTICE system. In light of the Zwarts and Dras (2008) finding that source-side features in the classifier do not help translation performance, our negative result with +FEATURES may appear unsurprising, as all of our features may be classified as source-side. However, we note that there remains a considerable feature space to explore.",
  "y": "differences"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_17",
  "x": "For example, the original sentence could be compared with reorderings created by different reordering-as-preprocessing approaches. In this instance, it would be advisable to use a different vocabulary (by using a different token prefix) for each path, as each reordering is likely to require a different lexicalised distortion model. In the case where these reordered alternatives are all possible combinations of parts of one reordering process, our system approaches the work described in \u00a72.4, and in fact those systems will probably be more suitable as the preprocessing takes over the role of the PSMT distortion model. Alternatively, the multiple options could be created by the same preprocessor but based on different parses, say the n best parses returned by one parser, or the output of n different parsers with comparable outputs. This extension would be quite different from the lattice-based systems in \u00a72.4, which are all based on a single parse. For future systems, we would like to replace the <cite>Collins et al. (2005)</cite> reordering rules with a set of automatically-extracted reordering rules (as in Xia and McCord (2004) ) so that we may more easily explore the usefulness of our system and confidence features in new language pairs with a variety of reordering requirements. The next major phase of this work is to extend and explore the feature space. This entails examining subsets of confidence features to establish which are the most useful indicators of reliable reordering, and possibly replacing the MERT tuning process with another algorithm, such as MIRA, to handle a greater quantity of features. In addition, we wish to explore more fully our negative result with the reimplementation of the <cite>Collins et al. (2005)</cite> system, to investigate the effect of balancing features in the lattice, and to examine the variability of the BLEU scores for each system.",
  "y": "future_work"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_18",
  "x": "In this instance, it would be advisable to use a different vocabulary (by using a different token prefix) for each path, as each reordering is likely to require a different lexicalised distortion model. In the case where these reordered alternatives are all possible combinations of parts of one reordering process, our system approaches the work described in \u00a72.4, and in fact those systems will probably be more suitable as the preprocessing takes over the role of the PSMT distortion model. Alternatively, the multiple options could be created by the same preprocessor but based on different parses, say the n best parses returned by one parser, or the output of n different parsers with comparable outputs. This extension would be quite different from the lattice-based systems in \u00a72.4, which are all based on a single parse. For future systems, we would like to replace the <cite>Collins et al. (2005)</cite> reordering rules with a set of automatically-extracted reordering rules (as in Xia and McCord (2004) ) so that we may more easily explore the usefulness of our system and confidence features in new language pairs with a variety of reordering requirements. The next major phase of this work is to extend and explore the feature space. This entails examining subsets of confidence features to establish which are the most useful indicators of reliable reordering, and possibly replacing the MERT tuning process with another algorithm, such as MIRA, to handle a greater quantity of features. In addition, we wish to explore more fully our negative result with the reimplementation of the <cite>Collins et al. (2005)</cite> system, to investigate the effect of balancing features in the lattice, and to examine the variability of the BLEU scores for each system. ----------------------------------",
  "y": "future_work"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_19",
  "x": "In addition, we wish to explore more fully our negative result with the reimplementation of the <cite>Collins et al. (2005)</cite> system, to investigate the effect of balancing features in the lattice, and to examine the variability of the BLEU scores for each system. ---------------------------------- **CONCLUSION** We adapt the lattice input to the PSMT system Moses to create a system that can simultaneously translate a sentence and its reordered variant produced by a syntactically-informed preprocessing step. We find that providing the system with this choice results in improved translation performance, achieving a BLEU score of 21.39, 0.62 higher than the baseline. We then augment the translation model of our system with a number of features to express our confidence in the reordering. While these features do not yield further improvement, a rough upper bound provided by our approximate oracle suggests that other features may still be found to guide the system in choosing whether or not to use the syntactically-informed reordering. While our reordering step is a reimplementation of the <cite>Collins et al. (2005)</cite> system, contrary to their findings we do not see an improvement using the reordering step alone. This provides evidence against the idea that reordering improves translation performance absolutely.",
  "y": "uses differences"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_0",
  "x": "Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram LMs. The most widely used technique is the use of beam search with n-gram LMs proposed by<cite> Nuhn et al. (2013)</cite> . We propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural LM. We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural LM. We compare against the state of the art n-gram based methods on many different decipherment tasks. On challenging ciphers such as the Beale cipher we provide significantly better error rates with much smaller beam sizes. ---------------------------------- **INTRODUCTION** Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic cipher key.",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_1",
  "x": "We compare against the state of the art n-gram based methods on many different decipherment tasks. On challenging ciphers such as the Beale cipher we provide significantly better error rates with much smaller beam sizes. ---------------------------------- **INTRODUCTION** Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic cipher key. Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (Ravi and Knight, 2011;<cite> Nuhn et al., 2013)</cite> . Some methods use the ExpectationMaximization (EM) algorithm (Knight et al., 2006) while most state-of-the-art approaches for decipherment of 1:1 and homophonic substitution ciphers use beam search and rely on the clever use of n-gram LMs (Nuhn et al., 2014; Hauer et al., 2014) . Neural LMs globally score the entire candidate plaintext sequence (Mikolov et al., 2010) . However, using a neural LM for decipherment is not trivial because scoring the entire candidate partially deciphered plaintext is computationally challenging.",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_2",
  "x": "---------------------------------- **DECIPHERMENT MODEL** We use the notation from<cite> Nuhn et al. (2013)</cite> . Ciphertext f N 1 = f 1 ..f i ..f N and plaintext e N 1 = e 1 ..e i ..e N consist of vocabularies f i \u2208 V f and e i \u2208 V e respectively. The beginning tokens in the ciphertext (f 0 ) and plaintext (e 0 ) are set to \"$\" denoting the beginning of a sentence. The substitutions are represented by a function \u03c6 : V f \u2192 V e such that 1:1 substitutions are bijective while homophonic substitutions are general. A cipher function \u03c6 which does not have every \u03c6(f ) fixed is called a partial cipher function (Corlett and Penn, 2010) . The number of f s that are fixed in \u03c6 is given by its cardinality. \u03c6 is called an extension of \u03c6, if f is fixed in \u03c6 such that \u03b4(\u03c6 (f ), \u03c6(f )) yields true \u2200f \u2208 V f which are already fixed in \u03c6 where \u03b4 is Kronecker delta.",
  "y": "uses"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_3",
  "x": "Decipherment is then the task of finding the \u03c6 for which the probability of the deciphered text is maximized. where p(.) is the language model (LM). Finding this argmax is solved using a beam search algorithm<cite> (Nuhn et al., 2013)</cite> which incrementally finds the most likely substitutions using the language model scores as the ranking. ---------------------------------- **NEURAL LANGUAGE MODEL** The advantage of a neural LM is that it can be used to score the entire candidate plaintext for a hypothesized partial decipherment. In this work, we use a state of the art byte (character) level neural LM using a multiplicative LSTM (Radford et al., 2017) . Consider a sequence S = w 1 , w 2 , w 3 , ..., w N . The LM score of S is SCORE(S):",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_4",
  "x": "Algorithm 1 is the beam search algorithm<cite> (Nuhn et al., 2013</cite> (Nuhn et al., , 2014 Hs. ADD((\u2205,0)) 5: while for all \u03c6 \u2208 Hs do 8: for all e \u2208 Ve do 9: if EXT LIMITS(\u03c6') then 11: Ht. ADD(\u03c6',SCORE(\u03c6')) 12: HISTOGRAM PRUNE(Ht) 13: CARDINALITY = CARDINALITY + 1 14:",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_5",
  "x": "States were initialized to zero at the beginning of each data shard and persisted across updates to simulate full-backprop and allow for the forward propagation of information outside of a given subsequence. In all the experiments we use a character NLM trained on English Gigaword corpus augmented with a short corpus of plaintext letters of about 2000 words authored by the Zodiac killer 2 . ---------------------------------- **1:1 SUBSTITUTION CIPHERS** In this experiment we use a synthetic 1:1 letter substitution cipher dataset following Ravi and Knight (2008) ,<cite> Nuhn et al. (2013)</cite> and Hauer et al. (2014) . The text is from English Wikipedia articles about history 3 , preprocessed by stripping the text of all images, tables, then lower-casing all characters, and removing all non-alphabetic and non-space characters. We create 50 cryptograms for each length 16, 32, 64, 128 and 256 using a random Caesar-cipher 1:1 substitution. ---------------------------------- **I H A V E D E P O S I T E D I N T H E C O U N T Y O F B E D F O R D A B O U T F O U R M I L E S F R O M B U F O R D S I N A N E X C A V A T I O N O R V A U L T S I X F E E T B E L O W T H E S U R F A C E O F T H E G R O U N D T H E F O L L O W I N G A R T I C L E S B E L O N G I N G J O I N T L Y T O T H E P A**",
  "y": "similarities uses"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_6",
  "x": "The text is from English Wikipedia articles about history 3 , preprocessed by stripping the text of all images, tables, then lower-casing all characters, and removing all non-alphabetic and non-space characters. We create 50 cryptograms for each length 16, 32, 64, 128 and 256 using a random Caesar-cipher 1:1 substitution. ---------------------------------- **I H A V E D E P O S I T E D I N T H E C O U N T Y O F B E D F O R D A B O U T F O U R M I L E S F R O M B U F O R D S I N A N E X C A V A T I O N O R V A U L T S I X F E E T B E L O W T H E S U R F A C E O F T H E G R O U N D T H E F O L L O W I N G A R T I C L E S B E L O N G I N G J O I N T L Y T O T H E P A** ---------------------------------- **AN EASY CIPHER: ZODIAC-408** Zodiac-408, a homophonic cipher, is commonly used to evaluate decipherment algorithms. Our neural LM model with global rest cost estimation and frequency matching heuristic with a beam size of 1M has SER of 1.2% compared to the beam search algorithm<cite> (Nuhn et al., 2013)</cite> with beam size of 10M with a 6-gram LM which gives an SER of 2%. The improved beam search (Nuhn et al., 2014) with an 8-gram LM, however, gets 52 out of 54 mappings correct on the Zodiac-408 cipher.",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_7",
  "x": "Corlett and Penn (2010) presented an efficient A* search algorithm to solve letter substitution ciphers. Nuhn et al. (2013) produce better results in faster time compared to ILP and EM-based decipherment methods by employing a higher order language model and an iterative beam search algorithm. Nuhn et al. (2014) present various improvements to the beam search algorithm in<cite> Nuhn et al. (2013)</cite> including improved rest cost estimation and an optimized strategy for ordering decipherment of the cipher symbols. Hauer et al. (2014) propose a novel approach for solving mono-alphabetic substitution ciphers which combines character-level and word-level language model. They formulate decipherment as a tree search problem, and use Monte Carlo Tree Search (MCTS) as an alternative to beam search. Their approach is the best for short ciphers. Greydanus (2017) frames the decryption process as a sequence-to-sequence translation task and uses a deep LSTM-based model to learn the decryption algorithms for three polyalphabetic ciphers including the Enigma cipher. However, this approach needs supervision compared to our approach which uses a pre-trained neural LM. Gomez et al. (2018) (CipherGAN) use a generative adversarial network to learn the mapping between the learned letter embedding distributions in the ciphertext and plaintext.",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_8",
  "x": "However, this approach needs supervision compared to our approach which uses a pre-trained neural LM. Gomez et al. (2018) (CipherGAN) use a generative adversarial network to learn the mapping between the learned letter embedding distributions in the ciphertext and plaintext. They apply this approach to shift ciphers (including Vigen\u00e8re ciphers). Their approach cannot be extended to homophonic ciphers and full message neural LMs as in our work. ---------------------------------- **CONCLUSION** This paper presents, to our knowledge, the first application of large pre-trained neural LMs to the decipherment problem. We modify the beam search algorithm for decipherment from<cite> Nuhn et al. (2013</cite>; and extend it to use global scoring of the plaintext message using neural LMs. To enable full plaintext scoring we use the neural LM to sample plaintext characters which reduces the beam size required.",
  "y": "differences extends"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_0",
  "x": "Learning Rate Schedulers update the learning rate over the course of training. We provide several popular schedulers, e.g., the inverse square-root scheduler from <cite>Vaswani et al. (2017)</cite> and cyclical schedulers based on warm restarts (Loshchilov and Hutter, 2016) . Reproducibility and forward compatibility. FAIRSEQ includes features designed to improve reproducibility and forward compatibility. For example, checkpoints contain the full state of the model, optimizer and dataloader, so that results are reproducible if training is interrupted and resumed. FAIRSEQ also provides forward compatibility, i.e., models trained using old versions of the toolkit will continue to run on the latest version through automatic checkpoint upgrading. ---------------------------------- **IMPLEMENTATION** FAIRSEQ is implemented in PyTorch and it provides efficient batching, mixed precision training, multi-GPU as well as multi-machine training.",
  "y": "similarities uses"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_1",
  "x": "FAIRSEQ provides fast inference for non-recurrent models (Gehring et al., 2017;<cite> Vaswani et al., 2017</cite>; Fan et al., 2018b; Wu et al., 2019) through incremental decoding, where the model states of previously generated tokens are cached in each active beam and re-used. This can speed up a na\u00efve implementation without caching by up to an order of magnitude, since only new states are computed for each token. For some models, this requires a component-specific caching implementation, e.g., multi-head attention in the Transformer architecture. During inference we build batches with a variable number of examples up to a user-specified number of tokens, similar to training. FAIRSEQ also supports inference in FP16 which increases decoding speed by 54% compared to FP32 with no loss in accuracy (Table 1) . ---------------------------------- **APPLICATIONS** FAIRSEQ has been used in many applications, such as machine translation (Gehring et al., 2017; Edunov et al., 2018b,a; Chen et al., 2018; Song et al., 2018; Wu et al., 2019) , language modeling , abstractive document summarization (Fan et al., 2018a; Narayan et al., 2018) , story generation (Fan et al., 2018b , error correction (Chollampatt and Ng, 2018) , multilingual sentence embeddings (Artetxe and Schwenk, 2018) , and dialogue (Miller et al., 2017; Dinan et al., 2019) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_2",
  "x": "FAIRSEQ has been used in many applications, such as machine translation (Gehring et al., 2017; Edunov et al., 2018b,a; Chen et al., 2018; Song et al., 2018; Wu et al., 2019) , language modeling , abstractive document summarization (Fan et al., 2018a; Narayan et al., 2018) , story generation (Fan et al., 2018b , error correction (Chollampatt and Ng, 2018) , multilingual sentence embeddings (Artetxe and Schwenk, 2018) , and dialogue (Miller et al., 2017; Dinan et al., 2019) . ---------------------------------- **MACHINE TRANSLATION** We provide reference implementations of several popular sequence-to-sequence models which can be used for machine translation, including LSTM (Luong et al., 2015) , convolutional models (Gehring et al., 2017; Wu et al., 2019) and Transformer<cite> (Vaswani et al., 2017)</cite> . We evaluate a \"big\" Transformer encoderdecoder model on two language pairs, WMT English to German (En-De) and WMT English to French (En-Fr). For En-De we replicate the setup of <cite>Vaswani et al. (2017)</cite> which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14. The 32K vocabulary is based on a joint source and target byte pair encoding (BPE; Sennrich et al. 2016 ). For En-Fr, we train on WMT'14 and borrow the setup of Gehring et al. (2017) with 36M training sentence pairs. We use newstest12+13 for validation and newstest14 for test.",
  "y": "background"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_3",
  "x": "**APPLICATIONS** FAIRSEQ has been used in many applications, such as machine translation (Gehring et al., 2017; Edunov et al., 2018b,a; Chen et al., 2018; Song et al., 2018; Wu et al., 2019) , language modeling , abstractive document summarization (Fan et al., 2018a; Narayan et al., 2018) , story generation (Fan et al., 2018b , error correction (Chollampatt and Ng, 2018) , multilingual sentence embeddings (Artetxe and Schwenk, 2018) , and dialogue (Miller et al., 2017; Dinan et al., 2019) . ---------------------------------- **MACHINE TRANSLATION** We provide reference implementations of several popular sequence-to-sequence models which can be used for machine translation, including LSTM (Luong et al., 2015) , convolutional models (Gehring et al., 2017; Wu et al., 2019) and Transformer<cite> (Vaswani et al., 2017)</cite> . We evaluate a \"big\" Transformer encoderdecoder model on two language pairs, WMT English to German (En-De) and WMT English to French (En-Fr). For En-De we replicate the setup of <cite>Vaswani et al. (2017)</cite> which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14. The 32K vocabulary is based on a joint source and target byte pair encoding (BPE; Sennrich et al. 2016 ). For En-Fr, we train on WMT'14 and borrow the setup of Gehring et al. (2017) with 36M training sentence pairs.",
  "y": "similarities uses"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_4",
  "x": "We evaluate a \"big\" Transformer encoderdecoder model on two language pairs, WMT English to German (En-De) and WMT English to French (En-Fr). For En-De we replicate the setup of <cite>Vaswani et al. (2017)</cite> which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14. The 32K vocabulary is based on a joint source and target byte pair encoding (BPE; Sennrich et al. 2016 ). For En-Fr, we train on WMT'14 and borrow the setup of Gehring et al. (2017) with 36M training sentence pairs. We use newstest12+13 for validation and newstest14 for test. The 40K vocabulary is based on a joint source and target BPE. We measure case-sensitive tokenized BLEU with multi-bleu (Hoang et al., 2006) and detokenized BLEU with SacreBLEU 1 (Post, 2018) . All results use beam search with a beam width of 4 and length penalty of 0.6, following<cite> Vaswani et al. 2017</cite> . FAIRSEQ results are summarized in Table 2 .",
  "y": "similarities uses"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_5",
  "x": "FAIRSEQ results are summarized in Table 2 . We reported improved BLEU scores over <cite>Vaswani et al. (2017)</cite> by training with a bigger batch size and an increased learning rate . ---------------------------------- **LANGUAGE MODELING** FAIRSEQ supports language modeling with gated convolutional models and Transformer models<cite> (Vaswani et al., 2017)</cite> . Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. Gehring et al. (2017) 25.2 40.5 b. <cite>Vaswani et al. (2017)</cite> 28.4 41.0 c. Ahmed et al. (2017) 28.9 41.4 d. Shaw et al. (2018) 29 et al., 2016), adaptive softmax (Grave et al., 2017) , and adaptive inputs . We also provide tutorials and pre-trained models that replicate the results of and ---------------------------------- **ABSTRACTIVE DOCUMENT SUMMARIZATION**",
  "y": "differences"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_6",
  "x": "**LANGUAGE MODELING** FAIRSEQ supports language modeling with gated convolutional models and Transformer models<cite> (Vaswani et al., 2017)</cite> . Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. Gehring et al. (2017) 25.2 40.5 b. <cite>Vaswani et al. (2017)</cite> 28.4 41.0 c. Ahmed et al. (2017) 28.9 41.4 d. Shaw et al. (2018) 29 et al., 2016), adaptive softmax (Grave et al., 2017) , and adaptive inputs . We also provide tutorials and pre-trained models that replicate the results of and ---------------------------------- **ABSTRACTIVE DOCUMENT SUMMARIZATION** Next, we experiment with abstractive document summarization where we use a base Transformer to encode the input document and then generate a summary with a decoder network. We use the CNN-Dailymail dataset (Hermann et al., 2015; Nallapati et al., 2016) of news articles paired with multi-sentence summaries. We evaluate on Perplexity 31.9 J\u00f3zefowicz et al. (2016) 30.0 28.0 FAIRSEQ Adaptive inputs 23.0 the full-text version with no entity anonymization (See et al., 2017) ; we truncate articles to 400 tokens (See et al., 2017) .",
  "y": "background"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_7",
  "x": "We measure case-sensitive tokenized BLEU with multi-bleu (Hoang et al., 2006) and detokenized BLEU with SacreBLEU 1 (Post, 2018) . All results use beam search with a beam width of 4 and length penalty of 0.6, following<cite> Vaswani et al. 2017</cite> . FAIRSEQ results are summarized in Table 2 . We reported improved BLEU scores over <cite>Vaswani et al. (2017)</cite> by training with a bigger batch size and an increased learning rate . ---------------------------------- **LANGUAGE MODELING** FAIRSEQ supports language modeling with gated convolutional models and Transformer models<cite> (Vaswani et al., 2017)</cite> . Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. Gehring et al. (2017) 25.2 40.5 b. <cite>Vaswani et al. (2017)</cite> 28.4 41.0 c. Ahmed et al. (2017) 28.9 41.4 d. Shaw et al. (2018) 29 et al., 2016), adaptive softmax (Grave et al., 2017) , and adaptive inputs . We also provide tutorials and pre-trained models that replicate the results of and",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_0",
  "x": "Our results show that indeed the performance gains surpass a pure regularization effect and that knowledge about language and morphology can be transferred. ---------------------------------- **INTRODUCTION** Neural sequence-to-sequence models define the state of the art for paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> Kann and Sch\u00fctze, 2016) , the task of generating inflected forms of a lemma's paradigm, e.g., filling the empty fields in Table 1 using one of the non-empty fields. However, those models are in general very datahungry, and do not reach good performances in low-resource settings. Therefore,<cite> Kann et al. (2017)</cite> propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language). This is achieved by a form of multi-task learning -they train an encoder-decoder model simultaneously on training examples for both languages. While closer related languages seem to help more than distant ones, the mechanisms how this transfer works still remain largely obscure. Several possibilities exist: (i) learning of target tag specific word transformations from the high-resource language (trans); (ii) training of the character language model of the decoder (LM); (iii) learning a bias to copy a large part of the input (copy), since members of the same paradigm mostly share the same stem; (iv) a general regularization effect obtained by multitask training (reg).",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_1",
  "x": "It has recently been applied for crosslingual transfer learning for paradigm completion-the task of producing inflected forms of lemmata-with sequenceto-sequence networks. However, it is still vague how the model transfers knowledge across languages, as well as if and which information is shared. To investigate this, we propose a set of data-dependent experiments using an existing encoder-decoder recurrent neural network for the task. Our results show that indeed the performance gains surpass a pure regularization effect and that knowledge about language and morphology can be transferred. ---------------------------------- **INTRODUCTION** Neural sequence-to-sequence models define the state of the art for paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> Kann and Sch\u00fctze, 2016) , the task of generating inflected forms of a lemma's paradigm, e.g., filling the empty fields in Table 1 using one of the non-empty fields. However, those models are in general very datahungry, and do not reach good performances in low-resource settings. Therefore,<cite> Kann et al. (2017)</cite> propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language).",
  "y": "background motivation"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_2",
  "x": "---------------------------------- **INTRODUCTION** Neural sequence-to-sequence models define the state of the art for paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> Kann and Sch\u00fctze, 2016) , the task of generating inflected forms of a lemma's paradigm, e.g., filling the empty fields in Table 1 using one of the non-empty fields. However, those models are in general very datahungry, and do not reach good performances in low-resource settings. Therefore,<cite> Kann et al. (2017)</cite> propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language). This is achieved by a form of multi-task learning -they train an encoder-decoder model simultaneously on training examples for both languages. While closer related languages seem to help more than distant ones, the mechanisms how this transfer works still remain largely obscure. Several possibilities exist: (i) learning of target tag specific word transformations from the high-resource language (trans); (ii) training of the character language model of the decoder (LM); (iii) learning a bias to copy a large part of the input (copy), since members of the same paradigm mostly share the same stem; (iv) a general regularization effect obtained by multitask training (reg). In this work, we intend to shed light on the way cross-lingual transfer learning for paradigm completion with an encoder-decoder model works, and will especially focus on the role of the character and tag embeddings.",
  "y": "motivation"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_3",
  "x": "Therefore,<cite> Kann et al. (2017)</cite> propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language). However,<cite> Kann et al. (2017)</cite> show that transferring morphological knowledge from Spanish to Portuguese, two languages with similar morphology and 89% lexical similarity, works well and, more surprisingly, even supposedly very different languages like Arabic and Spanish can benefit from each other.",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_4",
  "x": "**INTRODUCTION** Neural sequence-to-sequence models define the state of the art for paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> Kann and Sch\u00fctze, 2016) , the task of generating inflected forms of a lemma's paradigm, e.g., filling the empty fields in Table 1 using one of the non-empty fields. However, those models are in general very datahungry, and do not reach good performances in low-resource settings. Therefore,<cite> Kann et al. (2017)</cite> propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language). This is achieved by a form of multi-task learning -they train an encoder-decoder model simultaneously on training examples for both languages. While closer related languages seem to help more than distant ones, the mechanisms how this transfer works still remain largely obscure. Several possibilities exist: (i) learning of target tag specific word transformations from the high-resource language (trans); (ii) training of the character language model of the decoder (LM); (iii) learning a bias to copy a large part of the input (copy), since members of the same paradigm mostly share the same stem; (iv) a general regularization effect obtained by multitask training (reg). In this work, we intend to shed light on the way cross-lingual transfer learning for paradigm completion with an encoder-decoder model works, and will especially focus on the role of the character and tag embeddings. In particular we aim at answering the following questions: (i) What does the neural model learn from the tags of a highresource language for the tags of a low-resource language? (ii) Is sharing an alphabet important for the transfer? (iii) How much of the transfer learning can be reduced to a regularization effect achieved by multi-task learning?",
  "y": "motivation"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_5",
  "x": "Even between two morphologically rich languages transfer is difficult if they are unrelated, since inflections often mark dissimilar subcategories and word forms do not share similarities. However,<cite> Kann et al. (2017)</cite> show that transferring morphological knowledge from Spanish to Portuguese, two languages with similar morphology and 89% lexical similarity, works well and, more surprisingly, even supposedly very different languages like Arabic and Spanish can benefit from each other. They make this possible by training an encoder-decoder model and appending a special tag (i.e., embedding) for each language to the input of the system, similar to (Johnson et al., 2016) . It is currently unclear, though, what the nature of this transfer is, motivating our work which explores this in more detail. Model description. The model<cite> Kann et al. (2017)</cite> use and we explore in more detail here is an encoder-decoder recurrent neural network (RNN) with attention (Bahdanau et al., 2015) . It is trained on maximizing the following log-likelihood: We denote the source training examples as D s and the target training examples as D s . w s represents Figure 1 : Overview of an encoder-decoder RNN, mapping the Spanish lemma so\u00f1ar to the target form sue\u00f1a.",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_6",
  "x": "The model<cite> Kann et al. (2017)</cite> use and we explore in more detail here is an encoder-decoder recurrent neural network (RNN) with attention (Bahdanau et al., 2015) . It is trained on maximizing the following log-likelihood: We denote the source training examples as D s and the target training examples as D s . w s represents Figure 1 : Overview of an encoder-decoder RNN, mapping the Spanish lemma so\u00f1ar to the target form sue\u00f1a. The thickness of the arrows towards the circled plus symbol corresponds to each attention weight. All tags in the input are omitted. a lemma in a high-resource source language s and w t represents a lemma in a low-resource target language t . k represents a given slot in the paradigm and f k [w ] is the inflected form of w corresponding to the morphological tag t k . The parameters \u03b8 of the model are tied for both the high-resource language and the low-resource language to enable transfer learning.",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_7",
  "x": "---------------------------------- **DATA** We use the Romance and Arabic language data from<cite> Kann et al. (2017)</cite> . In particular, each training file contains 12, 000 high-resource examples mixed with 50 or 200 fixed Spanish instances. We ---------------------------------- **EXPERIMENTS** Letter cipher (l-ciph). Let C = C low \u222a C high be the union of the sets of all characters in the alphabets of the low-resource language and the high-resource language, respectively.",
  "y": "uses"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_8",
  "x": "t-ciph) correspond to a setting with no additional information vs. a setting with potentially wrong information. Generally higher accuracies for separate embedding spaces indicate that the model can learn incorrect information via transfer. Thus, the choice of the source language seems to be very important. The differences in performance between original and l-emb represent the influence of shared vs. separate embedding spaces, i.e., vocabularies in the case of the letters. Sharing a vocabulary seems to influence the final accuracy a lot, and more positively for 50 lowresource examples. We can explain this with the model learning to copy -it has no intrinsic way of knowing which input character equals which output character in the vocabulary unless it has seen it at least once. However, for 200 Spanish examples, we can expect all characters to appear in the Spanish training data, such that the character language model and tag-output correspondence get more important. This explains the unexpected result that l-emb performs best for Arabic (200) and Portuguese (200): both source languages potentially confuse the language model; in Portuguese we contribute this to a big overlap of lemmata in the two languages with Portuguese often inflecting in a different way<cite> (Kann et al., 2017)</cite> . Further, the differences in performance between original and t-emb show that the model indeed learns information from the tags, supposedly which output sequence is more likely to appear with which tag.",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_9",
  "x": "Luong et al. (2015) investigated multi-task setups for sequence-to-sequence learning, combining multiple encoders and decoders. In contrast, in our experiments, we use only one encoder and one decoder. There exists much work on multi-task learning with encoderdecoder RNNs for machine translation (Johnson et al., 2016; Dong et al., 2015; Firat et al., 2016; Ha et al., 2016) . Alonso and Plank (2016) explored multi-task learning empirically, analyzing when it improves performance. Here, we focus on how transfer via multi-task learning works. Paradigm completion. SIGMORPHON hosted two shared tasks on paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> , in order to encourage the development of systems for the task. One approach is to treat it as a string transduction problem by applying an alignment model with a semi-Markov model (Durrett and DeNero, 2013; Nicolai et al., 2015) . Recently, neural sequenceto-sequence models are also widely used (Faruqui et al., 2016; Kann and Sch\u00fctze, 2016; Aharoni and Goldberg, 2017; Zhou and Neubig, 2017) .",
  "y": "background"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_0",
  "x": "In other words, as most of the pre-trained LMs are designed to be of help to the tasks which can be categorized as classification including extractive summarization, they are not guaranteed to be advantageous to abstractive summarization models that should be capable of generating language (Wang and Cho, 2019; Zhang et al., 2019b) . On the other hand, recent studies for abstractive summarization<cite> (Chen and Bansal, 2018</cite>; Hsu et al., 2018; Gehrmann et al., 2018) have attempted to exploit extractive models. Among these, a notable one is<cite> Chen and Bansal (2018)</cite> , in which a sophisticated model called Reinforce-Selected Sentence Rewriting is proposed. The model consists of both an extractor and abstractor, where the extractor picks out salient sentences first from a source article, and then the abstractor rewrites and compresses the extracted sentences into a complete summary. It is further fine-tuned by training the extractor with the rewards derived from sentencelevel ROUGE scores of the summary generated from the abstractor. In this paper, we improve the model of<cite> Chen and Bansal (2018)</cite> , addressing two primary issues. Firstly, we argue there is a bottleneck in the existing extractor on the basis of the observation that its performance as an independent summarization model (i.e., without the abstractor) is no better than solid baselines such as selecting the first 3 sentences. To resolve the problem, we present a novel neural extractor exploiting the pre-trained LMs (BERT in this work) which are expected to perform better according to the recent studies (Liu, arXiv:1909 .08752v3 [cs.CL] 26 Sep 2019 2019 Zhang et al., 2019c) . Since the extractor is a sort of sentence classifier, we expect that it can make good use of the ability of pre-trained LMs which is proven to be effective in classification.",
  "y": "background"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_1",
  "x": "Extractive approaches generate summaries by selecting salient sentences or phrases from a source text, while abstractive approaches involve a process of paraphrasing or generating sentences to write a summary. Recent work (Liu, 2019; Zhang et al., 2019c ) demonstrates that it is highly beneficial for extractive summarization models to incorporate pretrained language models (LMs) such as BERT (Devlin et al., 2019) into their architectures. However, the performance improvement from the pretrained LMs is known to be relatively small in case of abstractive summarization (Zhang et al., 2019a; Hoang et al., 2019) . This discrepancy may be due to the difference between extractive and abstractive approaches in ways of dealing with the taskthe former classifies whether each sentence to be included in a summary, while the latter generates a whole summary from scratch. In other words, as most of the pre-trained LMs are designed to be of help to the tasks which can be categorized as classification including extractive summarization, they are not guaranteed to be advantageous to abstractive summarization models that should be capable of generating language (Wang and Cho, 2019; Zhang et al., 2019b) . On the other hand, recent studies for abstractive summarization<cite> (Chen and Bansal, 2018</cite>; Hsu et al., 2018; Gehrmann et al., 2018) have attempted to exploit extractive models. Among these, a notable one is<cite> Chen and Bansal (2018)</cite> , in which a sophisticated model called Reinforce-Selected Sentence Rewriting is proposed. The model consists of both an extractor and abstractor, where the extractor picks out salient sentences first from a source article, and then the abstractor rewrites and compresses the extracted sentences into a complete summary. It is further fine-tuned by training the extractor with the rewards derived from sentencelevel ROUGE scores of the summary generated from the abstractor.",
  "y": "background"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_2",
  "x": "Recent work (Liu, 2019; Zhang et al., 2019c ) demonstrates that it is highly beneficial for extractive summarization models to incorporate pretrained language models (LMs) such as BERT (Devlin et al., 2019) into their architectures. However, the performance improvement from the pretrained LMs is known to be relatively small in case of abstractive summarization (Zhang et al., 2019a; Hoang et al., 2019) . This discrepancy may be due to the difference between extractive and abstractive approaches in ways of dealing with the taskthe former classifies whether each sentence to be included in a summary, while the latter generates a whole summary from scratch. In other words, as most of the pre-trained LMs are designed to be of help to the tasks which can be categorized as classification including extractive summarization, they are not guaranteed to be advantageous to abstractive summarization models that should be capable of generating language (Wang and Cho, 2019; Zhang et al., 2019b) . On the other hand, recent studies for abstractive summarization<cite> (Chen and Bansal, 2018</cite>; Hsu et al., 2018; Gehrmann et al., 2018) have attempted to exploit extractive models. Among these, a notable one is<cite> Chen and Bansal (2018)</cite> , in which a sophisticated model called Reinforce-Selected Sentence Rewriting is proposed. The model consists of both an extractor and abstractor, where the extractor picks out salient sentences first from a source article, and then the abstractor rewrites and compresses the extracted sentences into a complete summary. It is further fine-tuned by training the extractor with the rewards derived from sentencelevel ROUGE scores of the summary generated from the abstractor. In this paper, we improve the model of<cite> Chen and Bansal (2018)</cite> , addressing two primary issues.",
  "y": "extends"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_3",
  "x": "It is worth noting that our approach shows large improvements especially on ROUGE-L score which is considered a means of assessing fluency (Narayan et al., 2018) . In addition, our model performs much better than previous work when testing on DUC-2002 dataset, showing better generalization and robustness of our model. Our contributions in this work are three-fold: a novel successful application of pre-trained transformers for abstractive summarization; suggesting a training method to globally optimize sentence selection; achieving the state-of-the-art results on the benchmark datasets, CNN/Daily Mail and New York Times. ---------------------------------- **BACKGROUND** ---------------------------------- **SENTENCE REWRITING** In this paper, we focus on single-document multisentence summarization and propose a neural abstractive model based on the Sentence Rewriting framework<cite> (Chen and Bansal, 2018</cite>; Xu and Dur-rett, 2019) which consists of two parts: a neural network for the extractor and another network for the abstractor. The extractor network is designed to extract salient sentences from a source article.",
  "y": "extends"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_4",
  "x": "The decoder extracts sentences recurrently, producing a distribution over all of the remaining sentence representations excluding those already selected. Since we use the sequential model which selects one sentence at a time step, our decoder can consider the previously selected sentences. This property is needed to avoid selecting sentences that have overlapping information with the sentences extracted already. As the decoder structure is almost the same with the previous work, we convey the equations of<cite> Chen and Bansal (2018)</cite> to avoid confusion, with minor modifications to agree with our notations. Formally, the extraction probability is calculated as: where e t is the output of the glimpse operation: In Equation 3, z t is the hidden state of the LSTM decoder at time t (shown in green in Figure 1 ). All the W and v are trainable parameters. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_5",
  "x": "**ABSTRACTOR NETWORK** The abstractor network approximates f , which compresses and paraphrases an extracted document sentence to a concise summary sentence. We use the standard attention based sequence-tosequence (seq2seq) model (Bahdanau et al., 2015; Luong et al., 2015) with the copying mechanism (See et al., 2017) for handling out-of-vocabulary (OOV) words. Our abstractor is practically identical to the one proposed in<cite> Chen and Bansal (2018)</cite> . ---------------------------------- **TRAINING** In our model, an extractor selects a series of sentences, and then an abstractor paraphrases them. As they work in different ways, we need different training strategies suitable for each of them. Training the abstractor is relatively obvious; maximizing log-likelihood for the next word given the previous ground truth words.",
  "y": "similarities"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_6",
  "x": "To address those issues above, we apply standard policy gradient methods, and we propose a novel training procedure for extractor which guides to the optimal policy in terms of the summary-level ROUGE. As usual in RL for sequence prediction, we pre-train submodules and apply RL to fine-tune the extractor. ---------------------------------- **TRAINING SUBMODULES** Extractor Pre-training Starting from a poor random policy makes it difficult to train the extractor agent to converge towards the optimal policy. Thus, we pre-train the network using cross entropy (CE) loss like previous work (Bahdanau et al., 2017;<cite> Chen and Bansal, 2018)</cite> . However, there is no gold label for extractive summarization in most of the summarization datasets. Hence, we employ a greedy approach (Nallapati et al., 2017) to make the extractive oracles, where we add one sentence at a time incrementally to the summary, such that the ROUGE score of the current set of selected sentences is maximized for the entire ground truth summary. This doesn't guarantee optimal, but it is enough to teach the network to select plausible sentences.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_7",
  "x": "Following<cite> Chen and Bansal (2018)</cite> , we use the Advantage Actor Critic (Mnih et al., 2016) method to train. We add a critic network to estimate a value function V t (D,\u015d 1 , \u00b7 \u00b7 \u00b7 ,\u015d t\u22121 ), which then is used to compute advantage of each action (we will omit the current state (D,\u015d 1 , \u00b7 \u00b7 \u00b7 ,\u015d t\u22121 ) to simplify): where Q t (s i ) is the expected future reward for selecting s i at the current step t. We maximize this advantage with the policy gradient with the where \u03b8 \u03c0 is the trainable parameters of the actor network (original extractor). And the critic is trained to minimize the square loss: where \u03b8 \u03c8 is the trainable parameters of the critic network. ---------------------------------- **EXPERIMENTAL SETUP** ----------------------------------",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_8",
  "x": "On the CNN/Daily Mail and DUC-2002 dataset, we use standard ROUGE-1, ROUGE-2, and ROUGE- L (Lin, 2004) on full length F 1 with stemming as previous work did (Nallapati et al., 2017; See et al., 2017;<cite> Chen and Bansal, 2018)</cite> . On NYT50 dataset, following Durrett et al. (2016) and Paulus et al. (2018) , we used the limited length ROUGE recall metric, truncating the generated summary to the length of the ground truth summary. Table 1 shows the experimental results on CNN/Daily Mail dataset, with extractive models in the top block and abstractive models in the bottom block. For comparison, we list the performance of many recent approaches with ours. ---------------------------------- **RESULTS** ---------------------------------- **CNN/DAILY MAIL** Extractive Summarization As See et al. (2017) showed, the first 3 sentences (lead-3) in an article form a strong summarization baseline in CNN/Daily Mail dataset.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_9",
  "x": "---------------------------------- **CNN/DAILY MAIL** Extractive Summarization As See et al. (2017) showed, the first 3 sentences (lead-3) in an article form a strong summarization baseline in CNN/Daily Mail dataset. Therefore, the very first objective of extractive models is to outperform the simple method which always returns 3 or 4 sentences at the top. However, as Table 2 shows, ROUGE scores of lead baselines and extractors from previous work in Sentence Rewrite framework<cite> (Chen and Bansal, 2018</cite>; Xu and Durrett, 2019) are almost tie. We can easily conjecture that the limited performances of their full model are due to their extractor networks. Our extractor network with BERT (BERT-ext), as a single model, outperforms those models with large margins. Adding reinforcement learning (BERT-ext + RL) gives higher performance, which is competitive with other extractive approaches using pretrained Transformers (see Table 1 ). This shows the effectiveness of our learning method.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_10",
  "x": "Therefore, the very first objective of extractive models is to outperform the simple method which always returns 3 or 4 sentences at the top. However, as Table 2 shows, ROUGE scores of lead baselines and extractors from previous work in Sentence Rewrite framework<cite> (Chen and Bansal, 2018</cite>; Xu and Durrett, 2019) are almost tie. We can easily conjecture that the limited performances of their full model are due to their extractor networks. Our extractor network with BERT (BERT-ext), as a single model, outperforms those models with large margins. Adding reinforcement learning (BERT-ext + RL) gives higher performance, which is competitive with other extractive approaches using pretrained Transformers (see Table 1 ). This shows the effectiveness of our learning method. Abstractive Summarization Our abstractive approaches combine the extractor with the abstractor. The combined model (BERT-ext + abs) without additional RL training outperforms the Sentence Rewrite model<cite> (Chen and Bansal, 2018)</cite> without reranking, showing the effectiveness of our extractor network. With the proposed RL training procedure (BERT-ext + abs + RL), our model exceeds the best model of<cite> Chen and Bansal (2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_11",
  "x": "However, as Table 2 shows, ROUGE scores of lead baselines and extractors from previous work in Sentence Rewrite framework<cite> (Chen and Bansal, 2018</cite>; Xu and Durrett, 2019) are almost tie. We can easily conjecture that the limited performances of their full model are due to their extractor networks. Our extractor network with BERT (BERT-ext), as a single model, outperforms those models with large margins. Adding reinforcement learning (BERT-ext + RL) gives higher performance, which is competitive with other extractive approaches using pretrained Transformers (see Table 1 ). This shows the effectiveness of our learning method. Abstractive Summarization Our abstractive approaches combine the extractor with the abstractor. The combined model (BERT-ext + abs) without additional RL training outperforms the Sentence Rewrite model<cite> (Chen and Bansal, 2018)</cite> without reranking, showing the effectiveness of our extractor network. With the proposed RL training procedure (BERT-ext + abs + RL), our model exceeds the best model of<cite> Chen and Bansal (2018)</cite> . In addition, the result is better than those of all the other abstractive methods exploiting extractive approaches in them (Hsu et al., 2018; <cite>Chen and Bansal, 2018</cite>; Gehrmann et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_12",
  "x": "This shows the effectiveness of our learning method. Abstractive Summarization Our abstractive approaches combine the extractor with the abstractor. The combined model (BERT-ext + abs) without additional RL training outperforms the Sentence Rewrite model<cite> (Chen and Bansal, 2018)</cite> without reranking, showing the effectiveness of our extractor network. With the proposed RL training procedure (BERT-ext + abs + RL), our model exceeds the best model of<cite> Chen and Bansal (2018)</cite> . In addition, the result is better than those of all the other abstractive methods exploiting extractive approaches in them (Hsu et al., 2018; <cite>Chen and Bansal, 2018</cite>; Gehrmann et al., 2018) . Redundancy Control Although the proposed RL training inherently gives training signals that induce the model to avoid redundancy across sentences, there can be still remaining overlaps between extracted sentences. We found that the additional methods reducing redundancies can improve the summarization quality, especially on CNN/Daily Mail dataset. We tried Trigram Blocking (Liu, 2019) for extractor and Reranking<cite> (Chen and Bansal, 2018)</cite> for abstractor, and we empirically found that the reranking only improves the performance. This helps the model to compress the extracted sentences focusing on disjoint information, even if there are some partial overlaps between the sentences.",
  "y": "differences"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_13",
  "x": "Adding reinforcement learning (BERT-ext + RL) gives higher performance, which is competitive with other extractive approaches using pretrained Transformers (see Table 1 ). This shows the effectiveness of our learning method. Abstractive Summarization Our abstractive approaches combine the extractor with the abstractor. The combined model (BERT-ext + abs) without additional RL training outperforms the Sentence Rewrite model<cite> (Chen and Bansal, 2018)</cite> without reranking, showing the effectiveness of our extractor network. With the proposed RL training procedure (BERT-ext + abs + RL), our model exceeds the best model of<cite> Chen and Bansal (2018)</cite> . In addition, the result is better than those of all the other abstractive methods exploiting extractive approaches in them (Hsu et al., 2018; <cite>Chen and Bansal, 2018</cite>; Gehrmann et al., 2018) . Redundancy Control Although the proposed RL training inherently gives training signals that induce the model to avoid redundancy across sentences, there can be still remaining overlaps between extracted sentences. We found that the additional methods reducing redundancies can improve the summarization quality, especially on CNN/Daily Mail dataset. We tried Trigram Blocking (Liu, 2019) for extractor and Reranking<cite> (Chen and Bansal, 2018)</cite> for abstractor, and we empirically found that the reranking only improves the performance.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_14",
  "x": "This search method matches with the best reward from<cite> Chen and Bansal (2018)</cite> . Greedy Search is the same method explained for extractor pre-training in section 4.1. ---------------------------------- **COMBINATION SEARCH SELECTS A SET OF SENTENCES** ---------------------------------- **MODELS** Relevance Readability Total Sentence Rewrite<cite> (Chen and Bansal, 2018)</cite> 56 59 115 BERTSUM (Liu, 2019) 58 60 118 BERT-ext + abs + RL + rerank (ours) 66 61 127 which has the highest summary-level ROUGE-L score, from all the possible combinations of sentences. Due to time constraints, we limited the maximum number of sentences to 5. This method corresponds to our final return in RL training.",
  "y": "similarities"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_15",
  "x": "**MODELS** Relevance Readability Total Sentence Rewrite<cite> (Chen and Bansal, 2018)</cite> 56 59 115 BERTSUM (Liu, 2019) 58 60 118 BERT-ext + abs + RL + rerank (ours) 66 61 127 which has the highest summary-level ROUGE-L score, from all the possible combinations of sentences. Due to time constraints, we limited the maximum number of sentences to 5. This method corresponds to our final return in RL training. Table 3 shows the summary-level ROUGE scores of previously explained methods. We see considerable gaps between Sentence-matching and Greedy Search, while the scores of Greedy Search are close to those of Combination Search. Note that since we limited the number of sentences for Combination Search, the exact scores for it would be higher. The scores can be interpreted to be upper bounds for corresponding training methods. This result supports our training strategy; pretraining with Greedy Search and final optimization with the combinatorial return. Additionally, we experiment to verify the contribution of our training method.",
  "y": "differences"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_16",
  "x": "This method corresponds to our final return in RL training. Table 3 shows the summary-level ROUGE scores of previously explained methods. We see considerable gaps between Sentence-matching and Greedy Search, while the scores of Greedy Search are close to those of Combination Search. Note that since we limited the number of sentences for Combination Search, the exact scores for it would be higher. The scores can be interpreted to be upper bounds for corresponding training methods. This result supports our training strategy; pretraining with Greedy Search and final optimization with the combinatorial return. Additionally, we experiment to verify the contribution of our training method. We train the same model with different training signals; Sentencelevel reward from<cite> Chen and Bansal (2018)</cite> and combinatorial reward from ours. The results are shown in Table 4 . Both with and without reranking, the models trained with the combinatorial reward consistently outperform those trained with the sentence-level reward.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_17",
  "x": "Readability is based on the summarys fluency, grammaticality, and coherence. To evaluate both these criteria, we design a Amazon Mechanical Turk experiment based on ranking method, inspired by Kiritchenko and Mohammad (2017) . We randomly select 20 samples from the CNN/Daily Mail test set and ask the human testers (3 for each sample) to rank summaries (for relevance and readability) produced by 3 different models: our final model, that of<cite> Chen and Bansal (2018)</cite> and that of Liu (2019) . 2, 1 and 0 points were given according to the ranking. R-1 R-2 R-L Extractive First sentences (Durrett et al., 2016) 28.60 17.30 -First k words (Durrett et al., 2016) 35.70 21.60 -Full (Durrett et al., 2016) 42.20 24.90 -BERTSUM (Liu, 2019) 46.66 26.35 42.62 Abstractive Deep Reinforced (Paulus et al., 2018) 42.94 26.02 -Two-Stage BERT (Zhang et al., 2019a) The models were anonymized and randomly shuffled. Following previous work, the input article and ground truth summaries are also shown to the human participants in addition to the three model summaries. From the results shown in Table 5 , we can see that our model is better in relevance compared to others. In terms of readability, there was no noticeable difference. Table 6 gives the results on NYT50 dataset.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_18",
  "x": "There has been a variety of deep neural network models for abstractive document summarization. One of the most dominant structures is the sequence-to-sequence (seq2seq) models with attention mechanism (Rush et al., 2015; Nallapati et al., 2016) . See et al. (2017) introduced Pointer Generator network that implicitly combines the abstraction with the extraction, using copy mechanism (Gu et al., 2016; Zeng et al., 2016) . More recently, there have been several studies that have attempted to improve the performance of the abstractive summarization by explicitly combining them with extractive models. Some notable examples include the use of inconsistency loss (Hsu et al., 2018) , key phrase extraction (Li et al., 2018; Gehrmann et al., 2018) , and sentence extraction with rewriting<cite> (Chen and Bansal, 2018)</cite> . Our model improves Sentence Rewriting with BERT as an extractor and summary-level rewards to optimize the extractor. Reinforcement learning has been shown to be effective to directly optimize a non-differentiable objective in language generation including text summarization (Ranzato et al., 2016; Bahdanau et al., 2017; Paulus et al., 2018; Celikyilmaz et al., 2018; Narayan et al., 2018) . Bahdanau et al. (2017) use actor-critic methods for language generation, using reward shaping (Ng et al., 1999) to solve the sparsity of training signals. Inspired by this, we generalize it to sentence extraction to give per step reward preserving optimality.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_0",
  "x": "Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010) , the recent work of <cite>(Poon and Domingos, 2009</cite> ) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_1",
  "x": "**INTRODUCTION** Statistical approaches to semantic parsing have recently received considerable attention. While some methods focus on predicting a complete formal representation of meaning (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; , others consider more shallow forms of representation (Carreras and M\u00e0rquez, 2005; Liang et al., 2009) . However, most of this research has concentrated on supervised methods requiring large amounts of labeled data. Such annotated resources are scarce, expensive to create and even the largest of them tend to have low coverage (Palmer and Sporleder, 2010) , motivating the need for unsupervised or semi-supervised techniques. Conversely, research in the closely related task of relation extraction has focused on unsupervised or minimally supervised methods (see, for example, (Lin and Pantel, 2001; Yates and Etzioni, 2009) ). These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering (Lin and Pantel, 2001; Banko et al., 2007) . The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations of predicate-argument relations. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010) , the recent work of <cite>(Poon and Domingos, 2009</cite> ) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_2",
  "x": "In this paper, we adopt the above definition of unsupervised semantic parsing and propose a Bayesian non-parametric approach which uses hierarchical Pitman-Yor (PY) processes (Pitman, 2002) to model statistical dependencies between predicate and argument clusters, as well as distributions over syntactic and lexical realizations of each cluster. Our non-parametric model automatically discovers granularity of clustering appropriate for the dataset, unlike the parametric method of<cite> (Poon and Domingos, 2009)</cite> which have to perform model selection and use heuristics to penalize more complex models of semantics. Additional benefits generally expected from Bayesian modeling include the ability to encode prior linguistic knowledge in the form of hyperpriors and the potential for more reliable modeling of smaller datasets. More detailed discussion of relation between the Markov Logic Network (MLN) approach of <cite>(Poon and Domingos, 2009</cite> ) and our non-parametric method is presented in Section 3. Hierarchical Pitman-Yor processes (or their special case, hierarchical Dirichlet processes) have previously been used in NLP, for example, in the context of syntactic parsing (Liang et al., 2007; Johnson et al., 2007) . However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al., 2007) , or the number of adapted productions in the adaptor grammar (Johnson et al., 2007) ) was not very large. In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus <cite>(Poon and Domingos, 2009</cite> ). This suggests that standard inference methods for hierarchical PY processes, such as Gibbs sampling, Metropolis-Hastings (MH) sampling with uniform proposals, or the structured mean-field algorithm, are unlikely to result in efficient inference: for example in standard Gibbs sampling all thousands of alternatives should be considered at each sampling move. Instead, we use a split-merge MH sampling algorithm, which is a standard and efficient inference tool for non-hierarchical PY processes (Jain and Neal, 2000; Dahl, 2003) but has not previously been used in hierarchical setting.",
  "y": "differences"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_3",
  "x": "For example, for a pair of sentences on Figure 1, in addition to inducing predicate-argument structure, they aim to assign expressions \"Steelers\" and \"the Pittsburgh team\" to the same semantic class Steelers, and group expressions \"defeated\" and \"secured the victory over\". Such semantic representation can be useful for entailment or question answering tasks, as an entailment model can abstract away from specifics of syntactic and lexical realization relying instead on the induced semantic representation. For example, the two sentences in Figure 1 have identical semantic representation, and therefore can be hypothesized to be equivalent. From the statistical modeling point of view, joint learning of predicate-argument structure and discovery of semantic clusters of expressions can also be beneficial, because it results in a more compact model of selectional preference, less prone to the data-sparsity problem (Zapirain et al., 2010) . In this respect our model is similar to recent LDA-based models of selectional preference (Ritter et al., 2010; S\u00e9aghdha, 2010) , and can even be regarded as their recursive and non-parametric extension. In this paper, we adopt the above definition of unsupervised semantic parsing and propose a Bayesian non-parametric approach which uses hierarchical Pitman-Yor (PY) processes (Pitman, 2002) to model statistical dependencies between predicate and argument clusters, as well as distributions over syntactic and lexical realizations of each cluster. Our non-parametric model automatically discovers granularity of clustering appropriate for the dataset, unlike the parametric method of<cite> (Poon and Domingos, 2009)</cite> which have to perform model selection and use heuristics to penalize more complex models of semantics. Additional benefits generally expected from Bayesian modeling include the ability to encode prior linguistic knowledge in the form of hyperpriors and the potential for more reliable modeling of smaller datasets. More detailed discussion of relation between the Markov Logic Network (MLN) approach of <cite>(Poon and Domingos, 2009</cite> ) and our non-parametric method is presented in Section 3.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_4",
  "x": "In this paper, we adopt the above definition of unsupervised semantic parsing and propose a Bayesian non-parametric approach which uses hierarchical Pitman-Yor (PY) processes (Pitman, 2002) to model statistical dependencies between predicate and argument clusters, as well as distributions over syntactic and lexical realizations of each cluster. Our non-parametric model automatically discovers granularity of clustering appropriate for the dataset, unlike the parametric method of<cite> (Poon and Domingos, 2009)</cite> which have to perform model selection and use heuristics to penalize more complex models of semantics. Additional benefits generally expected from Bayesian modeling include the ability to encode prior linguistic knowledge in the form of hyperpriors and the potential for more reliable modeling of smaller datasets. More detailed discussion of relation between the Markov Logic Network (MLN) approach of <cite>(Poon and Domingos, 2009</cite> ) and our non-parametric method is presented in Section 3. Hierarchical Pitman-Yor processes (or their special case, hierarchical Dirichlet processes) have previously been used in NLP, for example, in the context of syntactic parsing (Liang et al., 2007; Johnson et al., 2007) . However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al., 2007) , or the number of adapted productions in the adaptor grammar (Johnson et al., 2007) ) was not very large. In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus <cite>(Poon and Domingos, 2009</cite> ). This suggests that standard inference methods for hierarchical PY processes, such as Gibbs sampling, Metropolis-Hastings (MH) sampling with uniform proposals, or the structured mean-field algorithm, are unlikely to result in efficient inference: for example in standard Gibbs sampling all thousands of alternatives should be considered at each sampling move. Instead, we use a split-merge MH sampling algorithm, which is a standard and efficient inference tool for non-hierarchical PY processes (Jain and Neal, 2000; Dahl, 2003) but has not previously been used in hierarchical setting.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_5",
  "x": "However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al., 2007) , or the number of adapted productions in the adaptor grammar (Johnson et al., 2007) ) was not very large. In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus <cite>(Poon and Domingos, 2009</cite> ). This suggests that standard inference methods for hierarchical PY processes, such as Gibbs sampling, Metropolis-Hastings (MH) sampling with uniform proposals, or the structured mean-field algorithm, are unlikely to result in efficient inference: for example in standard Gibbs sampling all thousands of alternatives should be considered at each sampling move. Instead, we use a split-merge MH sampling algorithm, which is a standard and efficient inference tool for non-hierarchical PY processes (Jain and Neal, 2000; Dahl, 2003) but has not previously been used in hierarchical setting. We extend the sampler to include composition-decomposition of syntactic fragments in order to cluster fragments of variables size, as in the example Figure 1 , and also include the argument role-syntax alignment move which attempts to improve mapping between semantic roles and syntactic paths for some fixed predicate. Evaluating unsupervised models is a challenging task. We evaluate our model both qualitatively, examining the revealed clustering of syntactic structures, and quantitatively, on a question answering task. In both cases, we follow <cite>(Poon and Domingos, 2009</cite> ) in using the corpus of biomedical abstracts. Our model achieves favorable results significantly outperforming the baselines, including state-of-theart methods for relation extraction, and achieves scores comparable to those of the MLN model.",
  "y": "uses"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_6",
  "x": "In Sections 5 and 6, we describe our model and the inference method. Section 7 provides both qualitative and quantitative evaluation. Finally, ad-ditional related work is presented in Section 8. ---------------------------------- **SEMANTIC PARSING** In this section, we briefly define the unsupervised semantic parsing task and underlying aspects and assumptions relevant to our model. Unlike <cite>(Poon and Domingos, 2009</cite> ), we do not use the lambda calculus formalism to define our task but rather treat it as an instance of frame-semantic parsing, or a specific type of semantic role labeling (Gildea and Jurafsky, 2002) . The reason for this is two-fold: first, the frame semantics view is more standard in computational linguistics, sufficient to describe induced semantic representation and convenient to relate our method to the previous work. Second, lambda calculus is a considerably more powerful formalism than the predicate-argument structure used in frame semantics, normally supporting quantification and logical connectors (for example, negation and disjunction), neither of which is modeled by our model or in<cite> (Poon and Domingos, 2009)</cite> .",
  "y": "background differences"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_7",
  "x": "This assumption is similar to the adjacency assumption in (Zettlemoyer and Collins, 2005) , though ours may be more appropriate for languages with free or semi-free word order, where syntactic structures are inherently non-projective. Second, we assume that the semantic arguments are local in the dependency tree; that is, one lexical item can be a semantic argument of another one only if they are connected by an arc in the dependency tree. This is a slight simplification of the semantic role labeling problem but one often made. Thus, the argument identification and labeling stages consist of labeling each syntactic arc with a semantic role label. In comparison, the MLN model does not explicitly assume contiguity of lexical items and does not make this directionality assumption but their clustering algorithm uses initialization and clusterization moves such that the resulting model also obeys both of these constraints. Third, as in <cite>(Poon and Domingos, 2009</cite> ), we do not model polysemy as we assume that each syntactic fragment corresponds to a single semantic class. This is not a model assumption and is only used at inference as it reduces mixing time of the Markov chain. It is not likely to be restrictive for the biomedical domain studied in our experiments. As in some of the recent work on learning semantic representations (Eisenstein et al., 2009;<cite> Poon and Domingos, 2009</cite> ), we assume that dependency structures are provided for every sentence.",
  "y": "similarities"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_8",
  "x": "This is not a model assumption and is only used at inference as it reduces mixing time of the Markov chain. It is not likely to be restrictive for the biomedical domain studied in our experiments. As in some of the recent work on learning semantic representations (Eisenstein et al., 2009;<cite> Poon and Domingos, 2009</cite> ), we assume that dependency structures are provided for every sentence. This assumption allows us to construct models of semantics not Markovian within a sequence of words (see for an example a model described in (Liang et al., 2009) ), but rather Markovian within a dependency tree. Though we include generation of the syntactic structure in our model, we would not expect that this syntactic component would result in an accurate syntactic model, even if trained in a supervised way, as the chosen independence assumptions are oversimplistic. In this way, we can use a simple generative story and build on top of the recent success in syntactic parsing. ---------------------------------- **RELATION TO THE MLN APPROACH** The work of <cite>(Poon and Domingos, 2009</cite> ) models joint probability of the dependency tree and its latent semantic representation using Markov Logic Networks (MLNs) (Richardson and Domingos, 2006) , selecting parameters (weights of first-order clauses) to maximize the probability of the observed dependency structures.",
  "y": "similarities"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_9",
  "x": "---------------------------------- **RELATION TO THE MLN APPROACH** The work of <cite>(Poon and Domingos, 2009</cite> ) models joint probability of the dependency tree and its latent semantic representation using Markov Logic Networks (MLNs) (Richardson and Domingos, 2006) , selecting parameters (weights of first-order clauses) to maximize the probability of the observed dependency structures. For each sentence, the MLN induces a Markov network, an undirected graphical model with nodes corresponding to ground atoms and cliques corresponding to ground clauses. The MLN is a powerful formalism and allows for modeling complex interaction between features of the input (syntactic trees) and latent output (semantic representation), however, unsupervised learning of semantics with general MLNs can be prohibitively expensive. The reason for this is that MLNs are undirected models and when learned to maximize likelihood of syntactically annotated sentences, they would require marginalization over semantic representation but also over the entire space of syntactic structures and lexical units. Given the complexity of the semantic parsing task and the need to tackle large datasets, even approximate methods are likely to be infeasible. In order to overcome this problem, <cite>(Poon and Domingos, 2009</cite> ) group parameters and impose local normalization constraints within each group. Given these normalization constraints, and additional structural constraints satisfied by the model, namely that the clauses should be engineered in such a way that they induce treestructured graphs for every sentence, the parameters can be estimated by a variant of the EM algorithm.",
  "y": "motivation background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_10",
  "x": "In order to overcome this problem, <cite>(Poon and Domingos, 2009</cite> ) group parameters and impose local normalization constraints within each group. Given these normalization constraints, and additional structural constraints satisfied by the model, namely that the clauses should be engineered in such a way that they induce treestructured graphs for every sentence, the parameters can be estimated by a variant of the EM algorithm. The class of such restricted MLNs is equivalent to the class of directed graphical models over the same set of random variables corresponding to fragments of syntactic and semantic structure. Given that the above constraints do not directly fit into the MLN methodology, we believe that it is more natural to regard their model as a directed model with an underlying generative story specifying how the semantic structure is generated and how the syntactic parse is drawn for this semantic structure. This view would facilitate understanding what kind of features can easily be integrated into the model, simplify application of non-parametric Bayesian techniques and expedite the use of inference techniques designed specifically for directed models. Our approach makes one step in this direction by proposing a non-parametric version of such generative model. ---------------------------------- **HIERARCHICAL PITMAN-YOR PROCESSES** The central component of our non-parametric Bayesian model are Pitman-Yor (PY) processes, which are a generalization of the Dirichlet processes (DPs) (Ferguson, 1973) .",
  "y": "motivation differences background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_11",
  "x": "4 ---------------------------------- **EMPIRICAL EVALUATION** We induced a semantic representation over a collection of texts and evaluated it by answering questions about the knowledge contained in the corpus. We used the GENIA corpus (Kim et al., 2003) , a dataset of 1999 biomedical abstracts, and a set of questions produced by<cite> (Poon and Domingos, 2009)</cite> . A example question is shown in Figure 3 . All model hyperpriors were set to maximize the posterior, except for w (A) and w (C) , which were set to 1.e \u2212 10 and 1.e \u2212 35, respectively. Inference was run for around 300,000 sampling iterations until the percentage of accepted split-merge moves became lower than 0.05%. Let us examine some of the induced semantic classes (Table 1) realizations have a clear semantic connection.",
  "y": "uses"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_12",
  "x": "The corresponding arguments define the origin of the cells (transgenic mouse, smoker, volunteer, donor, . . . ) . We now turn to the QA task and compare our model (USP-BAYES) with the results of baselines considered in <cite>(Poon and Domingos, 2009</cite> ). The first set of baselines looks for answers by attempting to match a verb and its argument in the question with the input text. The first version (KW) simply returns the rest of the sentence on the other side of the verb, while the second (KW-SYN) uses syntactic information to extract the subject or the object of the verb. Other baselines are based on state-of-the-art relation extraction systems. When the extracted relation and one of the arguments match those in a given question, the second argument is returned as an answer. The systems include TextRunner (TR) (Banko et al., 2007) , RESOLVER (RS) (Yates and Etzioni, 2009) and DIRT (Lin and Pantel, 2001 ). The EX-ACT versions of the methods return answers when they match the question argument exactly, and the SUB versions produce answers containing the question argument as a substring. Similarly to the MLN system (USP-MLN), we generate answers as follows.",
  "y": "uses"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_13",
  "x": "Though all these clusters have clear semantic interpretation (white blood cells, predicates corresponding to changes and cykotines associated with cancer progression, respectively), they appear to be too coarse for the QA method we use in our experiments. Though it is likely that tuning and different heuristics may result in better scores, we chose not to perform excessive tuning, as the evaluation dataset is fairly small. ---------------------------------- **RELATED WORK** There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; ), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model <cite>(Poon and Domingos, 2009</cite> ), another unsupervised method has been proposed in (Goldwasser et al., 2011) . In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap the model. Unsupervised semantic role labeling with a generative model has also been considered (Grenager and Manning, 2006) , however, they do not attempt to discover frames and deal only with isolated predicates.",
  "y": "background"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_0",
  "x": "The currently most sophisticated and most influential approach to distributional semantics employs word embeddings, i.e., low (usually 300-500) dimensional vector word representations of both semantic and syntactic information. Alternative approaches are e.g., graph-based algorithms (Biemann and Riedl, 2013) or ranking functions from information retrieval (Claveau et al., 2014) . The premier example for word embeddings is skip-gram negative sampling, which is part of the word2vec family of algorithms (Mikolov et al., 2013) . The random processes involved in training these embeddings lead to a lack of reliability which is dangerous during interpretationexperiments cannot be repeated without predicting severely different relationships between words Hahn, 2016a, 2017) . Word embeddings based on singular value decomposition (SVD; historically popular in the form of Latent Semantic Analysis (Deerwester et al., 1990) ) are not affected by this problem. Levy et al. (2015) created SVD PPMI after investigating the implicit operations performed while training neural word embeddings (Levy and Goldberg, 2014) . As SVD PPMI performs very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the Digital Humanities<cite> (Hamilton et al., 2016</cite>; Hellrich and Hahn, 2016a) . ---------------------------------- **AUTOMATIC DIACHRONIC SEMANTICS**",
  "y": "background"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_1",
  "x": "The use of statistical methods is getting more and more the status of a commonly shared methodology in diachronic linguistics (see e.g., Curzan (2009)). There exist already several tools for performing statistical analysis on user provided corpora, e.g., WORDSMITH 3 or the UCS TOOLKIT, 4 as well as interactive websites for exploring precompiled corpora, e.g., the \"advanced\" interface for Google Books (Davies, 2014) or DIACOLLO (Jurish, 2015) . Meanwhile, word embeddings and their application to diachronic semantics have become a novel state-of-the-art methodology lacking, however, off-the-shelves analysis tools easy to use for a typically non-technical audience. Most work is centered around word2vec (e.g., Kim et al. (2014) ; Kulkarni et al. (2015) ; Hellrich and Hahn (2016b) ), whereas alternative approaches are rare, e.g., Jo (2016) using GloVe (Pennington et al., 2014) and<cite> Hamilton et al. (2016)</cite> using SVD PPMI . Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (Kim et al., 2014; Kulkarni et al., 2015;<cite> Hamilton et al., 2016)</cite> and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space. This information can then be exploited for automatic (Buechel et al., 2016) or manual (Jo, 2016) interpretation. research. We employ five corpora, including the four largest diachronic corpora of acceptable quality for English and German. The Google Books Ngram Corpus (GB; Michel et al. (2011 ), Lin et al. (2012 ) contains about 6% of all books published between 1500 and 2009 in the form of n-grams (up to pentagrams).",
  "y": "background"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_2",
  "x": "As SVD PPMI performs very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the Digital Humanities<cite> (Hamilton et al., 2016</cite>; Hellrich and Hahn, 2016a) . ---------------------------------- **AUTOMATIC DIACHRONIC SEMANTICS** The use of statistical methods is getting more and more the status of a commonly shared methodology in diachronic linguistics (see e.g., Curzan (2009)). There exist already several tools for performing statistical analysis on user provided corpora, e.g., WORDSMITH 3 or the UCS TOOLKIT, 4 as well as interactive websites for exploring precompiled corpora, e.g., the \"advanced\" interface for Google Books (Davies, 2014) or DIACOLLO (Jurish, 2015) . Meanwhile, word embeddings and their application to diachronic semantics have become a novel state-of-the-art methodology lacking, however, off-the-shelves analysis tools easy to use for a typically non-technical audience. Most work is centered around word2vec (e.g., Kim et al. (2014) ; Kulkarni et al. (2015) ; Hellrich and Hahn (2016b) ), whereas alternative approaches are rare, e.g., Jo (2016) using GloVe (Pennington et al., 2014) and<cite> Hamilton et al. (2016)</cite> using SVD PPMI . Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (Kim et al., 2014; Kulkarni et al., 2015;<cite> Hamilton et al., 2016)</cite> and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space. This information can then be exploited for automatic (Buechel et al., 2016) or manual (Jo, 2016) interpretation.",
  "y": "background"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_3",
  "x": "The resulting models have a size of 32 GB and are available for download on JESEME's Help page. 9 To ensure JESEME's responsiveness, we finally pre-computed similarity (by cosine between word embeddings), as well as context specificity based on PPMI and \u03c7 2 . These values are stored in a POSTGRESQL 10 database, occupying about 60GB of space. Due to both space constraints (scaling with O(n 2 ) for vocabulary size n) and the lower quality of representations for infrequent words, we limited this step to words which were among the 10k most frequent words for all slices of a corpus, resulting in 3,1k -6,5k words per corpus. In accordance with this limit, we also discarded slices with less than 10k (5k for RSC) 7 Parameters were chosen in accordance with Levy et al. (2015) and <cite>Hamilton et al. (2016</cite> words above the minimum frequency threshold used during PPMI and \u03c7 2 calculation, e.g., the 1810s and 1820s COHA slices. Figure 1 illustrates this sequence of processing steps, while Table 1 summarizes the resulting models for each corpus. 5 Website and API JESEME provides both an interactive website and an API for querying the underlying database. Both are implemented with the SPARK 11 framework running inside a JETTY 12 Web server. On JESEME's initial landing page, users can enter a word into a search field and select a corpus.",
  "y": "uses similarities"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_4",
  "x": "On JESEME's initial landing page, users can enter a word into a search field and select a corpus. They are then redirected to the result page, as depicted in Figure 2 . Query words are automatically lowercased or lemmatized, depending on the respective corpus (see Section 4). The result page provides three kinds of graphs, i.e., Similar Words, Typical Context and Relative Frequency. Similar Words depicts the words with the highest similarity relative to the query term for the first and last time slice and how their similarity values changed over time. We follow Kim et al. (2014) in choosing such a visualization, while we refrain from using the two-dimensional projection used in other studies (Kulkarni et al., 2015;<cite> Hamilton et al., 2016)</cite> . We stipulate that the latter could Figure 2 : Screenshot of JESEME's result page when searching for the lexical item \"heart\" in COHA. be potentially misleading by implying a constant meaning of those words used as the background (which are actually positioned by their meaning at a single point in time). Typical Context offers two graphs, one for \u03c7 2 and one for PPMI, arranged in tabs.",
  "y": "differences"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_6",
  "x": "13 Calling conventions are further detailed on JESEME's Help page. 14 13 For example http://jeseme.org/api/ similarity?word1=Tag&word2=Nacht&corpus= dta 14 http://jeseme.org/help.html#api ---------------------------------- **CONCLUSION** We presented JESEME, the Jena Semantic Explorer, an interactive website and REST API for exploring changes in lexical semantics over long periods of time. In contrast to other corpus exploration tools, JESEME is based on cutting-edge word embedding technology (Levy et al., 2015;<cite> Hamilton et al., 2016</cite>; Hahn, 2016a, 2017) and provides access to five popular corpora for the English and German language. JESEME is also the first tool of its kind and under continuous development. Future technical work will add functionality to compare words across corpora which might require a mapping between embeddings (Kulkarni et al., 2015;<cite> Hamilton et al., 2016)</cite> and provide optional stemming routines. Both goals come with an increase in precomputed similarity values and will thus necessitate storage optimizations to ensure long-term availability.",
  "y": "future_work"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_0",
  "x": "Singular value decomposition (SVD) is a common factorization technique and has been explored in feedforward networks [9, 10, 21, 22] and recurrent neural networks (RNN) <cite>[11]</cite> . Neural network quantization refers to compressing the original network by reducing number of bits required to represent weight matrices, and it has been studied for different model architectures [12, 13, 14, 15, 16, 19, 20] . By reducing the bit-width of weights, model size is reduced, and it also brings considerable acceleration via efficient low bit-width arithmetic operations supports available on hardware. For the quantization approach, It is important to fine-tune models with quantized weights to reduce the performance loss with quantized networks. Here we refer quantization with fine-turning as quantization training. 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr\u00e9al, Canada. ---------------------------------- **METHODS** We start by formulating the multi-class audio event detection problem.",
  "y": "background"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_1",
  "x": "The accuracy of AED models have been increased in a large scale in recent years based on deep learning approaches. However, to ensure high performance, those models are of large scale computation and memory intense, which makes it a challenge to deploy for real industrial applications with limited computation resources and memory. Our paper is focused on increasing the computation efficiency for AED models while maintaining their accuracies, so that AED deployment for resource-constraint industrial applications is feasible. Compression of neural networks has been explored in broad context. We focus on two widely used and effective methods for deep models: low-rank matrix factorization and and quantization. Singular value decomposition (SVD) is a common factorization technique and has been explored in feedforward networks [9, 10, 21, 22] and recurrent neural networks (RNN) <cite>[11]</cite> . Neural network quantization refers to compressing the original network by reducing number of bits required to represent weight matrices, and it has been studied for different model architectures [12, 13, 14, 15, 16, 19, 20] . By reducing the bit-width of weights, model size is reduced, and it also brings considerable acceleration via efficient low bit-width arithmetic operations supports available on hardware. For the quantization approach, It is important to fine-tune models with quantized weights to reduce the performance loss with quantized networks.",
  "y": "uses background"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_2",
  "x": "**METHODS** We start by formulating the multi-class audio event detection problem. Given an audio signal I (e.g. log mel-filter bank energies (LFBEs)), the task is to train a model f to predict a multi-hot vector y \u2208 {0, 1} C , with C being the size of event set E and y c being a binary indicator whether event c is present in I. We denote D L = {(I, y)} as the labeled training dataset. Model f is trained using cross-entropy loss: L = \u2212 (I,y)\u2208DL C c=1 {w c y c log f c (I) + (1 \u2212 y c ) log(1 \u2212 f c (I))}, where w c is the penalty of positive mis-classification of class c. Specifically we focus on the RNN-based model in this paper. Compared to CNNs, it is more memory efficient and easier to deploy on devices with constrained resources. Low-rank matrix factorization The factorization of weight matrices is based on the SVD compression of LSTM <cite>[11]</cite> . Let W Quantization training Quantization refers to representing floating-point values with n-bit integers (n < 32). as formulated in 2.",
  "y": "background"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_3",
  "x": "**METHODS** We start by formulating the multi-class audio event detection problem. Given an audio signal I (e.g. log mel-filter bank energies (LFBEs)), the task is to train a model f to predict a multi-hot vector y \u2208 {0, 1} C , with C being the size of event set E and y c being a binary indicator whether event c is present in I. We denote D L = {(I, y)} as the labeled training dataset. Model f is trained using cross-entropy loss: L = \u2212 (I,y)\u2208DL C c=1 {w c y c log f c (I) + (1 \u2212 y c ) log(1 \u2212 f c (I))}, where w c is the penalty of positive mis-classification of class c. Specifically we focus on the RNN-based model in this paper. Compared to CNNs, it is more memory efficient and easier to deploy on devices with constrained resources. Low-rank matrix factorization The factorization of weight matrices is based on the SVD compression of LSTM <cite>[11]</cite> . Let W Quantization training Quantization refers to representing floating-point values with n-bit integers (n < 32). as formulated in 2.",
  "y": "uses background"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_4",
  "x": "As the distribution of weight matrices' eigenvalues can be different across different LSTM layers, we follow the practice of <cite>[11]</cite> to set the same threshold \u03c4 across layers as the fraction of retained singular values, defined as \u03c4 = Table 1 summarizes the results of low-rank matrix factorization compared to our baseline 3-layer LSTM. There is no accuracy degradation when \u03c4 is reduced to 0.6, which we hypothesize to be related to the regularizing effects. Table 2 summarizes the results with quantization compared to our baseline. Post-mortem (PM) refers to the case that quantization is only applied during inference, while quantization training (QT) refers to the case model fine-tuning is further performed on quantized weights. Our quantization training approach outperforms PM significantly for the 4-bit quantization case. We also note the simple PM quantization preserves the accuracy well (3.0% drop in AUC and 2.7% drop in EER) with 8-bit quantization. Finally, we combine both low-rank matrix factorization and quantization training. Its results are summarized in table 3. The attained singular value ratio is fixed to be 0.6, as parameter size and accuracy is well balanced at this point according to table 1.",
  "y": "uses"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_0",
  "x": "Domain-adversarial training is integrated into the optimization of a lipreader based on a stack of feedforward and LSTM (Long Short-Term Memory) recurrent neural networks, yielding an end-to-end trainable system which only requires a very small number of frames of untranscribed target data to substantially improve the recognition accuracy on the target speaker. On pairs of different source and target speakers, we achieve a relative accuracy improvement of around 40% with only 15 to 20 seconds of untranscribed target speech data. On multi-speaker training setups, the accuracy improvements are smaller but still substantial. ---------------------------------- **INTRODUCTION** Lipreading is the process of understanding speech by using solely visual features, i.e. images of the lips of a speaker. In communication between humans, lipreading has a twofold relevance [1] : First, visual cues play a role in spoken conversation [2] ; second, hearing-impaired persons may use lipreading as a means to follow verbal speech. With the success of computer-based speech recognition over the past decades, automatic lipreading has become an active field of research as well, with pioneering work by Petajan [3] , who used lipreading to augment conventional acoustic speech recognition, and Chiou and Hwang [4] , who were the first to perform lipreading without resorting to any acoustic signal at all. Since 2014, lipreading systems have systematically begun to use neural networks at part of the processing pipeline [5, 6] or for end-to-end-training<cite> [7,</cite> 8, 9] .",
  "y": "background"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_1",
  "x": "Lipreading is the process of understanding speech by using solely visual features, i.e. images of the lips of a speaker. In communication between humans, lipreading has a twofold relevance [1] : First, visual cues play a role in spoken conversation [2] ; second, hearing-impaired persons may use lipreading as a means to follow verbal speech. With the success of computer-based speech recognition over the past decades, automatic lipreading has become an active field of research as well, with pioneering work by Petajan [3] , who used lipreading to augment conventional acoustic speech recognition, and Chiou and Hwang [4] , who were the first to perform lipreading without resorting to any acoustic signal at all. Since 2014, lipreading systems have systematically begun to use neural networks at part of the processing pipeline [5, 6] or for end-to-end-training<cite> [7,</cite> 8, 9] . In our previous work <cite>[7]</cite> , we proposed a fully neural network based system, using a stack of fully connected and recurrent (LSTM, Long ShortTerm Memory) [10, 11] neural network layers. The scope of this paper is the introduction of state-of-theart methods for speaker-independent lipreading with neural networks. We evaluate our established system <cite>[7]</cite> in a crossspeaker setting, observing a drastic performance drop on unknown speakers. In order to alleviate the discrepancy between training speakers and unknown test speaker, we use domainadversarial training as proposed by Ganin and Lempitsky [12] : Untranscribed data from the target speaker is used as additional training input to the neural network, with the aim of pushing the network to learn an intermediate data representation which is domain-agnostic, i.e. which does not depend on whether the input data comes from a source speaker or a target speaker. We evaluate our system on a subset of the GRID corpus [13] , which contains extensive data from 34 speakers and is therefore ideal for a systematic evaluation of the proposed method.",
  "y": "similarities"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_2",
  "x": "The scope of this paper is the introduction of state-of-theart methods for speaker-independent lipreading with neural networks. We evaluate our established system <cite>[7]</cite> in a crossspeaker setting, observing a drastic performance drop on unknown speakers. In order to alleviate the discrepancy between training speakers and unknown test speaker, we use domainadversarial training as proposed by Ganin and Lempitsky [12] : Untranscribed data from the target speaker is used as additional training input to the neural network, with the aim of pushing the network to learn an intermediate data representation which is domain-agnostic, i.e. which does not depend on whether the input data comes from a source speaker or a target speaker. We evaluate our system on a subset of the GRID corpus [13] , which contains extensive data from 34 speakers and is therefore ideal for a systematic evaluation of the proposed method. ---------------------------------- **RELATED WORK** Lipreading can be used to complement or augment speech recognition, particularly in the presence of noise [3, 14] , and for purely visual speech recognition [4, 15, 5] . In the latter case, ambiguities due to incomplete information (e.g. about voicing) can be mitigated by augmenting the video stream with ultrasound images of the vocal tract [16] . Visual speech processing is an instance of a Silent Speech interface [17] ; further promising approaches include capturing the movement of the articulators by electric or permanent magnetic articulography [18, 19] , and capturing of muscle activity using electromyography [20, 21, 22, 23] .",
  "y": "similarities"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_3",
  "x": "Mouth tracking is done as a preprocessing step [32, 15, 5] . For a comprehensive review see [33] . Neural networks have early been applied to the Lipreading task [34] , however, they have become widespread only in recent years, with the advent of state-of-the-art learning techniques (and the necessary hardware). The first deep neural network for lipreading was a seven-layer convolutional net as a preprocessing stage for an HMM-based word recognizer [5] . Since then, several end-to-end trainable systems were presented<cite> [7,</cite> 8, 9] . The current state-of-the-art accuracy on the GRID corpus is 3.3% error [9] using a very large set of additional training data; so their result is not directly comparable to ours. In domain adaptation, it is assumed that a learning task exhibits a domain shift between the training (or source) and test (or target) data. This can be mitigated in several ways [35] ; we apply domain-adversarial training [12] , where an intermediate layer in a multi-layer network is driven to learn a representation of the input data which is optimized to be domain-agnostic, i.e. to make it difficult to detect whether an input sample is from the source or the target domain. A great advantage of this approach is the end-to-end trainability of the entire system.",
  "y": "similarities"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_4",
  "x": "This can be mitigated in several ways [35] ; we apply domain-adversarial training [12] , where an intermediate layer in a multi-layer network is driven to learn a representation of the input data which is optimized to be domain-agnostic, i.e. to make it difficult to detect whether an input sample is from the source or the target domain. A great advantage of this approach is the end-to-end trainability of the entire system. For a summary of further approaches to domain adaptation with neural networks, we refer to the excellent overview in [12] . ---------------------------------- **DATA AND PREPROCESSING** We follow the data preprocessing protocol from <cite>[7]</cite> . We use the GRID corpus [13] , which consists of video and audio recordings of 34 speakers (which we name s1 to s34) saying 1000 sentences each. All sentences have a fixed structure: command(4) + color(4) + preposition(4) + letter(25) + digit(10) + adverb(4), for example \"Place red at J 2, please\", where the number of alternative words is given in parentheses. There are 51 distinct words; alternatives are randomly distributed so that context cannot be used for classification.",
  "y": "similarities uses"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_5",
  "x": "We experiment on speakers s1-s19: speakers 1-9 form the development speakers, used to determine optimal parameters; speakers 10-19 are the evaluation speakers, held back until the final evaluation of the systems. The data from each speaker was randomly subdivided into training, validation, and test sets, where the latter two contain five samples of each word, i.e. a total of 51 \u00b7 5 = 255 samples each. The training data is consequently highly unbalanced: For example, each letter from \"a\" to \"z\" appears 30 times, whereas each color appears 240 times. We converted the \"normal\" quality videos (360 \u00d7 288 pixels) to greyscale and extracted 40\u00d740 pixel windows containing the mouth area, as described in <cite>[7]</cite> . The frames were contrastnormalized and z-normalized over the training set, independently for each speaker. Unreadable videos were discarded. All experiments have one dedicated target speaker on which this experiment is evaluated, and one, four, or eight source speakers on which supervised training is performed. Speakers are chosen consecutively, for example, the experiments on four training speakers on the development data are (s1 . . . s4) \u2192 s5, (s2 . . . s5) \u2192 s6, \u00b7 \u00b7 \u00b7 , (s9, s1, s2, s3) \u2192 s4, where \u2192 separates source and target speakers.",
  "y": "uses similarities"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_6",
  "x": "Validation data is used for early stopping, the network is evaluated on the test data. ---------------------------------- **METHODS AND SYSTEM SETUP** The system is based on the lipreading setup from <cite>[7]</cite> , reimplemented in Tensorflow [36] . Raw 40 \u00d7 40 lip images are used as input data, without any further preprocessing except normalization. We stack several fully connected feedforward layers, optionally followed by Dropout [37] , and one LSTM recurrent layer to form a network which is capable of recognizing sequential video data. The final layer is a softmax with 51 word targets. All inner layers use a tanh nonlinearity. During testing, classification is performed on the last frame of an input word, the softmax output on all previous frames is discarded.",
  "y": "differences extends"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_7",
  "x": "Table 1 : Baseline word accuracies on single speakers, averaged over the development set, with standard deviation. Layer types are FC (fully connected feedforward), DP (Dropout), and LSTM, followed by the number of neurons/cells. * marks the (reimplemented and recomputed) best system from <cite>[7]</cite> . ---------------------------------- **EXPERIMENTS AND RESULTS** ---------------------------------- **BASELINE LIPREADER** The first experiment deals with establishing a baseline for our experiments, building on prior work <cite>[7]</cite> . We run the lipreader as a single-speaker system with different topologies, optionally using Dropout (always with 50% dropout ratio) to avoid overfitting the training set.",
  "y": "background"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_8",
  "x": "Table 1 : Baseline word accuracies on single speakers, averaged over the development set, with standard deviation. Layer types are FC (fully connected feedforward), DP (Dropout), and LSTM, followed by the number of neurons/cells. * marks the (reimplemented and recomputed) best system from <cite>[7]</cite> . ---------------------------------- **EXPERIMENTS AND RESULTS** ---------------------------------- **BASELINE LIPREADER** The first experiment deals with establishing a baseline for our experiments, building on prior work <cite>[7]</cite> . We run the lipreader as a single-speaker system with different topologies, optionally using Dropout (always with 50% dropout ratio) to avoid overfitting the training set.",
  "y": "uses similarities"
 },
 {
  "id": "2e95d98d5f9d4d6fc90e3d8453f945_0",
  "x": "This is especially problematic for this field of research since some ESL errors, such as preposition usage, occur at error rates as low as 10%. This means that to collect a corpus of 1,000 preposition errors, an annotator would have to check over 10,000 prepositions. 1 <cite>(Tetreault and Chodorow, 2008b)</cite> challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability. For example, trained raters typically annotate preposition errors with a kappa around 0.60. This low rater reliability has repercussions for system evaluation: Their experiments showed that system precision could vary as much as 10% depending on which rater's judgments they used as the gold standard. For some grammatical errors such as subject-verb agreement, where rules are clearly defined, it may be acceptable to use just one rater. But for usage errors, the rules are less clearly defined and two native speakers can have very different judgments of what is acceptable. One way to address this is by aggregating a multitude of judgments for each preposition and treating this as the gold standard, however such a tactic has been impractical due to time and cost limitations. While annotation is a problem in this field, comparing one system to another has also been a major issue.",
  "y": "background"
 },
 {
  "id": "2e95d98d5f9d4d6fc90e3d8453f945_1",
  "x": "Recently, AMT has been shown to be an effective tool for annotation and evalatuation in NLP tasks ranging from word similarity detection and emotion detection (Snow et al., 2008) to Machine Translation quality evaluation (Callison-Burch, 2009 ). In these cases, a handful of untrained AMT workers (or Turkers) were found to be as effective as trained raters, but with the advantage of being considerably faster and less expensive. Given the success of using AMT in other areas of NLP, we test whether we can leverage it for our work in grammatical error detection, which is the focus of the pilot studies in the next two sections. The presence of a gold standard in the above papers is crucial. In fact, the usability of AMT for text annotation has been demostrated in those studies by showing that non-experts' annotation converges to the gold standard developed by expert annotators. However, in our work we concentrate on tasks where there is no single gold standard, either because there are multiple prepositions that are acceptable in a given context or because the conventions of preposition usage simply do not conform to strict rules. Typically, an early step in developing a preposition or article error detection system is to test the system on well-formed text written by native speakers to see how well the system can predict, or select, the writer's preposition given the context around the preposition. <cite>(Tetreault and Chodorow, 2008b)</cite> showed that trained human raters can achieve very high agreement (78%) on this task. In their work, a rater was shown a sentence with a target preposition replaced with a blank, and the rater was asked to select the preposition that the writer may have used.",
  "y": "background"
 },
 {
  "id": "2e95d98d5f9d4d6fc90e3d8453f945_2",
  "x": "Given the success of using AMT in other areas of NLP, we test whether we can leverage it for our work in grammatical error detection, which is the focus of the pilot studies in the next two sections. The presence of a gold standard in the above papers is crucial. In fact, the usability of AMT for text annotation has been demostrated in those studies by showing that non-experts' annotation converges to the gold standard developed by expert annotators. However, in our work we concentrate on tasks where there is no single gold standard, either because there are multiple prepositions that are acceptable in a given context or because the conventions of preposition usage simply do not conform to strict rules. Typically, an early step in developing a preposition or article error detection system is to test the system on well-formed text written by native speakers to see how well the system can predict, or select, the writer's preposition given the context around the preposition. <cite>(Tetreault and Chodorow, 2008b)</cite> showed that trained human raters can achieve very high agreement (78%) on this task. In their work, a rater was shown a sentence with a target preposition replaced with a blank, and the rater was asked to select the preposition that the writer may have used. We replicate this experiment not with trained raters but with the AMT to answer two research questions: 1. Can untrained raters be as effective as trained 46 raters? 2. If so, how many raters does it take to match trained raters? In the experiment, a Turker was presented with a sentence from Microsoft's Encarta encyclopedia, with one preposition in that sentence replaced with a blank.",
  "y": "extends background"
 },
 {
  "id": "2e95d98d5f9d4d6fc90e3d8453f945_3",
  "x": "**ERROR DETECTION TASK** While the previous results look quite encouraging, the task they are based on, preposition selection in well-formed text, is quite different from, and less challenging than, the task that a system must perform in detecting errors in learner writing. To examine the reliability of Turker preposition error judgments, we ran another experiment in which Turkers were presented with a preposition highlighted in a sentence taken from an ESL corpus, and were in-structed to judge its usage as either correct, incorrect, or the context is too ungrammatical to make a judgment. The set consisted of 152 prepositions in total, and we requested 20 judgments per preposition. Previous work has shown this task to be a difficult one for trainer raters to attain high reliability. For example, <cite>(Tetreault and Chodorow, 2008b)</cite> found kappa between two raters averaged 0.630. Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three trained annotators. Among the trained annotators, inter-kappa agreement ranged from 0.574 to 0.650, for a mean kappa of 0.606. In Figure 2 , kappa is shown for the comparisons of Turker responses to each annotator for samples of various sizes ranging from N = 1 to N = 18.",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_0",
  "x": "---------------------------------- **INTRODUCTION** Readability assessment refers to the task of (automatically) linking a text to the appropriate target audience based on its complexity. A diverse spectrum of potential application domains has been identified for this task in the literature, ranging from the design and evaluation of education materials, to information retrieval, and text simplification. Given the increasing need for learning material adapted to different audiences and the barrier-free access to information required for political and social participation, automatic readability assessment is of immediate social relevance. Accordingly, it has attracted considerable research interest over the last decades, particularly for the assessment of English (Crossley et al., 2011; Chen and Meurers, 2017; Feng et al., 2010) . For German readability assessment, however, little progress has been made in recent years, despite a series of promising results published around the turn of the decade (Vor der Br\u00fcck et al., 2008;<cite> Hancke et al., 2012)</cite> . In particular, German readability research has suffered from the lack of a shared reference corpus and sufficiently comparable corpora for cross-corpus testing of readability models: While for English research, the Common Core corpus consisting of examples from the English Language Arts Standards of the Common Core State Standards, and the WeeklyReader corpus of online news articles have been widely used in studies on English readability and text simplification (Vajjala and Meurers, 2014; Petersen and Ostendorf, 2009; Feng et al., 2010) , there are no comparable resources for German. This is particularly problematic, as over-fitting is a potential issue for classification algorithms, especially given the limited size of the typical data sets.",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_1",
  "x": "**INTRODUCTION** Readability assessment refers to the task of (automatically) linking a text to the appropriate target audience based on its complexity. A diverse spectrum of potential application domains has been identified for this task in the literature, ranging from the design and evaluation of education materials, to information retrieval, and text simplification. Given the increasing need for learning material adapted to different audiences and the barrier-free access to information required for political and social participation, automatic readability assessment is of immediate social relevance. Accordingly, it has attracted considerable research interest over the last decades, particularly for the assessment of English (Crossley et al., 2011; Chen and Meurers, 2017; Feng et al., 2010) . For German readability assessment, however, little progress has been made in recent years, despite a series of promising results published around the turn of the decade (Vor der Br\u00fcck et al., 2008;<cite> Hancke et al., 2012)</cite> . In particular, German readability research has suffered from the lack of a shared reference corpus and sufficiently comparable corpora for cross-corpus testing of readability models: While for English research, the Common Core corpus consisting of examples from the English Language Arts Standards of the Common Core State Standards, and the WeeklyReader corpus of online news articles have been widely used in studies on English readability and text simplification (Vajjala and Meurers, 2014; Petersen and Ostendorf, 2009; Feng et al., 2010) , there are no comparable resources for German. This is particularly problematic, as over-fitting is a potential issue for classification algorithms, especially given the limited size of the typical data sets. To address these issues, we first present two new data sets for German readability assessment in Section 3: a set of German news broadcast subtitles based on the primary German TV news outlet Tagesschau and the children's counterpart Logo!, and a GEO/GEOlino corpus crawled from the educational GEO magazine's web site, a source first identified by <cite>Hancke et al. (2012)</cite> , but double in size.",
  "y": "extends"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_2",
  "x": "There has also been extensive research on the relevance of cohesion and discourse measures for readability assessment that have successfully been employed for proficiency assessment in the CohMetrix project (Crossley et al., 2008; Crossley et al., 2011) . Another example is the work by Feng et al. (2010) , who evaluate which of the typically proposed measures of text readability are most promising by studying their relevance on primary school students reading material. They find language model features and cohesion in terms of entity density to be particularly useful, as well as measures of nouns. Interestingly, they also observe overall sentence length to be more informative than more elaborate syntactic features. While Feng et al. (2010) do not elaborate further on other lexical measures than POS features, Chen and Meurers (2017) conduct an elaborate cross-corpus study on the use of word frequency features for readability assessment. They show, that the typical aggregation of word frequencies across documents are less informative than richer representations including frequency standard deviations. In contrast to English, research on readability assessment for other languages, such as German, is more limited. There was a series of articles on this issue from the late 2000s to the early 2010s that demonstrated the benefits of broad linguistic modeling, in particular the use of morphological complexity measures for languages with rich morphological systems like German (Vor der Br\u00fcck et al., 2008;<cite> Hancke et al., 2012)</cite> , but also Russian (Reynolds, 2016) or French (Fran\u00e7ois and Fairon, 2012) . The readability checker DeLite of Vor der Br\u00fcck et al. (2008) is one of the first more sophisticated approaches that went beyond using simple readability formulas for German.",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_3",
  "x": "While Feng et al. (2010) do not elaborate further on other lexical measures than POS features, Chen and Meurers (2017) conduct an elaborate cross-corpus study on the use of word frequency features for readability assessment. They show, that the typical aggregation of word frequencies across documents are less informative than richer representations including frequency standard deviations. In contrast to English, research on readability assessment for other languages, such as German, is more limited. There was a series of articles on this issue from the late 2000s to the early 2010s that demonstrated the benefits of broad linguistic modeling, in particular the use of morphological complexity measures for languages with rich morphological systems like German (Vor der Br\u00fcck et al., 2008;<cite> Hancke et al., 2012)</cite> , but also Russian (Reynolds, 2016) or French (Fran\u00e7ois and Fairon, 2012) . The readability checker DeLite of Vor der Br\u00fcck et al. (2008) is one of the first more sophisticated approaches that went beyond using simple readability formulas for German. The tool employs morphological, lexical, syntactical, semantic, and discourse measures, which they trained on municipal administration texts rated for their readability by humans in an online readability study involving 500 texts and 300 participant, resulting in overall 3,000 ratings. However, due to the specific nature of the data, the robustness of the approach across genres is unclear. Municipal administration language is so particular that results are unlikely to generalize to educational or literary materials, which are more attractive in first and second language acquisition contexts. Later work by <cite>Hancke et al. (2012)</cite> also combines traditional readability formula measures, such as text or word length, with more sophisticated lexical, syntactic, and language model, and morphological features to assess German readability, but they employ an overall broader and more diverse feature set than DeLite.",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_4",
  "x": "**GEO/GEOLINO** The GEO/GEOlino data set consists of online articles from one of the leading German monthly educational magazines, GEO, and the counterpart for children, GEOlino. 3 They are comparable to the National Geographic magazine and cover a variety of topics ranging from culture and history to technology and nature. <cite>Hancke et al. (2012)</cite> first compiled and analyzed a data set from this web resource. We followed their lead and crawled 8,263 articles from the GEO/GEOlino online archive, almost doubling the size of the original corpus. We removed all material flagged as non-article contents by GEO as well as all articles that contained less than 15 words. We further cleaned our data from crawling artifacts and performed near-duplicate detection with the Simhash algorithm. We then grouped all texts into topic categories based on the subdomains they were published under, following the web page topic structure. 4 Table 1 shows the composition of the corpus in terms of the topic groups.",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_5",
  "x": "**GEO/GEOLINO** The GEO/GEOlino data set consists of online articles from one of the leading German monthly educational magazines, GEO, and the counterpart for children, GEOlino. 3 They are comparable to the National Geographic magazine and cover a variety of topics ranging from culture and history to technology and nature. <cite>Hancke et al. (2012)</cite> first compiled and analyzed a data set from this web resource. We followed their lead and crawled 8,263 articles from the GEO/GEOlino online archive, almost doubling the size of the original corpus. We removed all material flagged as non-article contents by GEO as well as all articles that contained less than 15 words. We further cleaned our data from crawling artifacts and performed near-duplicate detection with the Simhash algorithm. We then grouped all texts into topic categories based on the subdomains they were published under, following the web page topic structure. 4 Table 1 shows the composition of the corpus in terms of the topic groups.",
  "y": "uses"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_6",
  "x": "Both models clearly outperform the baseline of 50.0%. On GEO/GEOlino S , the performance is comparable to the performance observed by <cite>Hancke et al. (2012)</cite> on the original GEO/GEOlino data. 11 As Table 7a shows, erroneous classifications are roughly balanced across both classes, showing that the model does not prefer one class over the other. When training a model using only the 20 most informative measures identified in Study 1, we reach an accuracy of 85.1%, i.e., the additional measures only account only for 3.3%. 12 When testing the models on the Tagesschau/Logo corpus, accuracy increases to 98.8% for both models. The confusion matrix for the model using 400 measures in Table 7b Table 7 : Confusion matrices for testing models with 400 features trained on GEO/GEOlino S . ---------------------------------- **RESULTS AND DISCUSSION** a minor tendency towards classifying Logo! texts as Tagesschau texts, but due to the low number of incorrect classifications this is not conclusive.",
  "y": "similarities"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_7",
  "x": "---------------------------------- **SUMMARY AND OUTLOOK** We presented a study of the difference between German targeting adults and children, as far as we know the most broadly based linguistic complexity analysis to date. We created and analyzed a novel data set compiled from German news subtitles that consists of news broadcasts for adults and children from the same days, ensuring a relatively parallel selection of topics. We compared this with a newly compiled GEO/GEOlino corpus consisting of online articles of two magazines for adults and children by the same publisher discussing the similar topics. Based on these two data sets, we presented within-corpus (10-fold CV) and cross-corpus experiments and built binary classification models of German educational media text readability that perform with very high accuracy across both data sets. The model is based on a broad range of features that are highly informative for both data sets. This model is a valuable contribution since i) it is based on a considerably broader data basis than previous approaches to German readability, and ii) it successfully generalizes across the data sets, illustrating surprising robustness across rather different text types. The approach presented thus extends the state-of-the-art in <cite>Hancke et al. (2012)</cite> in terms of the breadth of features integrated and the accuracy and generalizability of the model -and provides two new data sources for this line of research.",
  "y": "extends"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_0",
  "x": "The duplicate bug reports are manually tagged and associated to the original bug report by either the system manager or software developers. These families of duplicate bug reports form a semi-parallel 1 http://www.bugzilla.org/ Parallel bug reports with a pair of true paraphrases 1: connector extend with a straight line in full screen mode 2: connector show straight line in presentation mode Non-parallel bug reports referring to the same bug 1: Settle language for part of text and spellchecking part of text 2: Feature requested to improve the management of a multi-language document Context-peculiar paraphrases (shown in italics) 1: status bar appear in the middle of the screen 2: maximizing window create phantom status bar in middle of document However, bug reports have characteristics that raise many new challenges. Different from many other parallel corpora, bug reports are noisy. We observe at least three types of noise common in bug reports. First, many bug reports have many spelling, grammatical and sentence structure errors. To address this we extend a suitable stateof-the-art technique that is robust to such corpora, i.e. (<cite>Barzilay and McKeown, 2001</cite>) . Second, many duplicate bug report families contain sentences that are not truly parallel. An example is shown in Table 1 (middle). We handle this by considering lexical similarity between duplicate bug reports.",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_1",
  "x": "To address this we extend a suitable stateof-the-art technique that is robust to such corpora, i.e. (<cite>Barzilay and McKeown, 2001</cite>) . Second, many duplicate bug report families contain sentences that are not truly parallel. An example is shown in Table 1 (middle). We handle this by considering lexical similarity between duplicate bug reports. Third, even if the bug reports are parallel, we find many cases of context-peculiar paraphrases, i.e., a pair of phrases that have the same meaning in a very narrow context. An example is shown in Table 1 (bottom). To address this, we introduce two notions of global context-based score and co-occurrence based score which take into account all good and bad occurrences of the phrases in a candidate paraphrase in the corpus. These scores are then used to identify and remove context-peculiar paraphrases. The contributions of our work are twofold.",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_2",
  "x": "However, most paraphrases used are obtained manually. A recent study using synonyms from WordNet highlights the fact that these are not effective in software engineering tasks due to domain specificity (Sridhara et al., 2008) . Therefore, an automatic way to derive technical paraphrases specific to software engineering is desired. Paraphrases can be extracted from non-parallel corpora using contextual similarity (Lin, 1998) . They can also be obtained from parallel corpora if such data is available (<cite>Barzilay and McKeown, 2001</cite>; Ibrahim et al., 2003) . Recently, there are also a number of studies that extract paraphrases from multilingual corpora (Bannard and CallisonBurch, 2005; Zhao et al., 2008) . The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. Due to this reason, we build our technique on top of <cite>theirs</cite>. The following provides a summary of <cite>their technique</cite>.",
  "y": "background"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_3",
  "x": "Recently, there are also a number of studies that extract paraphrases from multilingual corpora (Bannard and CallisonBurch, 2005; Zhao et al., 2008) . The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. Due to this reason, we build our technique on top of <cite>theirs</cite>. The following provides a summary of <cite>their technique</cite>. Two types of paraphrase patterns are defined: (1) Syntactic patterns which consist of the POS tags of the phrases. For example, the paraphrases \"a VGA monitor\" and \"a monitor\" are represented as \"DT 1 JJ NN 2 \" \u2194 \"DT 1 NN 2 \", where the subscripts denote common words. (2) Contextual patterns which consist of the POS tags before and after the phrases. For example, the contexts \"in the middle of\" and \"in middle of\" in Table 1 (bottom) are represented as \" During pre-processing, the parallel corpus is aligned to give a list of parallel sentence pairs.",
  "y": "background"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_4",
  "x": "The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. Due to this reason, we build our technique on top of <cite>theirs</cite>. The following provides a summary of <cite>their technique</cite>. Two types of paraphrase patterns are defined: (1) Syntactic patterns which consist of the POS tags of the phrases. For example, the paraphrases \"a VGA monitor\" and \"a monitor\" are represented as \"DT 1 JJ NN 2 \" \u2194 \"DT 1 NN 2 \", where the subscripts denote common words. (2) Contextual patterns which consist of the POS tags before and after the phrases. For example, the contexts \"in the middle of\" and \"in middle of\" in Table 1 (bottom) are represented as \" During pre-processing, the parallel corpus is aligned to give a list of parallel sentence pairs. The sentences are then processed by a POS tagger and a chunker.",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_5",
  "x": "Paraphrases can be extracted from non-parallel corpora using contextual similarity (Lin, 1998) . They can also be obtained from parallel corpora if such data is available (<cite>Barzilay and McKeown, 2001</cite>; Ibrahim et al., 2003) . Recently, there are also a number of studies that extract paraphrases from multilingual corpora (Bannard and CallisonBurch, 2005; Zhao et al., 2008) . The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. Due to this reason, we build our technique on top of <cite>theirs</cite>. The following provides a summary of <cite>their technique</cite>. Two types of paraphrase patterns are defined: (1) Syntactic patterns which consist of the POS tags of the phrases. For example, the paraphrases \"a VGA monitor\" and \"a monitor\" are represented as \"DT 1 JJ NN 2 \" \u2194 \"DT 1 NN 2 \", where the subscripts denote common words. (2) Contextual patterns which consist of the POS tags before and after the phrases.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_6",
  "x": "The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. <cite>The authors</cite> first used identical words and phrases as seeds to find and score contextual patterns.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_7",
  "x": "An example is shown in Table 1 (middle). This kind of sentence pairs should not be regarded as parallel. To address this problem, we take a heuristic approach and only select sentence pairs that have strong similarities. Our similarity score is based on the number of common words, bigrams and trigrams shared between two parallel sentences. We use a threshold of 5 to filter out non-parallel sentences. Global Context-Based Scoring Our contextbased paraphrase scoring method is an extension of (<cite>Barzilay and McKeown, 2001</cite> ) described in Sec. 2. Parallel bug reports are usually noisy. At times, some words might be detected as paraphrases incidentally due to the noise. In (<cite>Barzilay and McKeown, 2001</cite> ), a paraphrase is reported as long as there is a single good supporting pair of sentences.",
  "y": "extends"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_8",
  "x": "At times, some words might be detected as paraphrases incidentally due to the noise. In (<cite>Barzilay and McKeown, 2001</cite> ), a paraphrase is reported as long as there is a single good supporting pair of sentences. Although <cite>this</cite> works well for a relatively clean parallel corpus considered in their work, i.e., novels, <cite>this</cite> does not work well for bug reports. Consider the context-peculiar example in Table 1 (bottom). For a context-peculiar para-phrase, there can be many sentences containing the pair of phrases but very few support them to be a paraphrase. We develop a technique to offset this noise by computing a global context-based score for two phrases being a paraphrase over all their parallel occurrences. This is defined by the following formula: where n is the number of parallel bug reports with the two phrases occurring in parallel, and s i is the score for the i'th occurrence. s i is computed as follows: 1.",
  "y": "background"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_9",
  "x": "In (<cite>Barzilay and McKeown, 2001</cite> ), a paraphrase is reported as long as there is a single good supporting pair of sentences. Although <cite>this</cite> works well for a relatively clean parallel corpus considered in their work, i.e., novels, <cite>this</cite> does not work well for bug reports. Consider the context-peculiar example in Table 1 (bottom). For a context-peculiar para-phrase, there can be many sentences containing the pair of phrases but very few support them to be a paraphrase. We develop a technique to offset this noise by computing a global context-based score for two phrases being a paraphrase over all their parallel occurrences. This is defined by the following formula: where n is the number of parallel bug reports with the two phrases occurring in parallel, and s i is the score for the i'th occurrence. s i is computed as follows: 1. We compute the set of patterns with affixed pattern scores based on (<cite>Barzilay and McKeown, 2001</cite> ).",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_10",
  "x": "s i is computed as follows: 1. We compute the set of patterns with affixed pattern scores based on (<cite>Barzilay and McKeown, 2001</cite> ). 2. For the i'th parallel occurrence of the pair of phrases we want to score, we try to find a pattern that matches the occurrence and assign the pattern score to the pair of phrases as s i . If no such pattern exists, we set s i to 0. By taking the average of s i as the global score for a pair of phrases, we do not rely much on a single s i and can therefore prevent context-peculiar paraphrases to some degree. Co-occurrence-Based Scoring We also consider another global co-occurrence-based score that is commonly used for finding collocations. A general observation is that noise tends to appear in random but random things do not occur in the same way often. It is less likely for randomly paired words or paraphrases to co-occur together many times. To compute the likelihood of two phrases occurring together, we use the following commonly used co-occurrence-based score:",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_11",
  "x": "The expression P (w 1 , w 2 ) refers to the probability of a pair of phrases w 1 and w 2 appearing together. It is estimated based on the proportion of the corpus containing both w 1 and w 2 in parallel. Similarly, P (w 1 ) and P (w 2 ) each corresponds to the probability of w 1 and w 2 appearing respectively. We normalize the S c score to the range of 0 to 1 by dividing it with the size of the corpus. Holistic Solution We employ the parallel sentence selection as a pre-processing step, and merge co-occurrence-based scoring with global contextbased scoring. For each parallel sentence pairs, a chunker is used to get chunks from each sentence. All possible pairings of chunks are then formed. This set of chunk pairs are later fed to the method in (<cite>Barzilay and McKeown, 2001</cite> ) to produce a set of patterns with affixed scores. With this we compute our global-context based scores.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_12",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. These patterns are later used to select paraphrases. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases. This number is too small for practical purposes. Therefore, we reduce the threshold to get more paraphrases. For each threshold in the range of 0.45-0.95 (step size: 0.05), we extract paraphrases and compute the corresponding precision. In our approach, we first form chunk pairs from the 5,935 pairs of parallel sentences and then use the <cite>baseline approach</cite> at a low threshold to ob-tain patterns.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_13",
  "x": "Finally, we extract duplicate bug report pairs by pairing each two members of each group. We get in total 53,363 duplicate bug report pairs. As the first step, we employ parallel sentence selection, described in Sec. 3, to remove nonparallel duplicate bug report pairs. After this step, we find 5,935 parallel duplicate bug report pairs. Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. These patterns are later used to select paraphrases. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_14",
  "x": "We get in total 53,363 duplicate bug report pairs. As the first step, we employ parallel sentence selection, described in Sec. 3, to remove nonparallel duplicate bug report pairs. After this step, we find 5,935 parallel duplicate bug report pairs. Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. These patterns are later used to select paraphrases. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases. This number is too small for practical purposes.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_15",
  "x": "After this step, we find 5,935 parallel duplicate bug report pairs. Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. These patterns are later used to select paraphrases. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases. This number is too small for practical purposes. Therefore, we reduce the threshold to get more paraphrases. For each threshold in the range of 0.45-0.95 (step size: 0.05), we extract paraphrases and compute the corresponding precision.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_16",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. In our approach, we first form chunk pairs from the 5,935 pairs of parallel sentences and then use the <cite>baseline approach</cite> at a low threshold to ob-tain patterns.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_17",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. These patterns are later used to select paraphrases. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases. This number is too small for practical purposes. Therefore, we reduce the threshold to get more paraphrases. For each threshold in the range of 0.45-0.95 (step size: 0.05), we extract paraphrases and compute the corresponding precision. In our approach, we first form chunk pairs from the 5,935 pairs of parallel sentences and then use the <cite>baseline approach</cite> at a low threshold to ob-tain patterns.",
  "y": "motivation extends"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_18",
  "x": "We get in total 53,363 duplicate bug report pairs. As the first step, we employ parallel sentence selection, described in Sec. 3, to remove nonparallel duplicate bug report pairs. After this step, we find 5,935 parallel duplicate bug report pairs. Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. These patterns are later used to select paraphrases. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases. This number is too small for practical purposes.",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_19",
  "x": "These patterns are later used to select paraphrases. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases. This number is too small for practical purposes. Therefore, we reduce the threshold to get more paraphrases. For each threshold in the range of 0.45-0.95 (step size: 0.05), we extract paraphrases and compute the corresponding precision. In our approach, we first form chunk pairs from the 5,935 pairs of parallel sentences and then use the <cite>baseline approach</cite> at a low threshold to ob-tain patterns. Using these patterns we compute the global context-based scores S g . We also compute the co-occurrence scores S c . We rank and extract top-k paraphrases based on these scores.",
  "y": "extends uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_20",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. From the figure we can see that our holistic approach using global-context score to rank and co-occurrence score to filter (i.e., Rk-S g +Ft-S c ) has higher precision than the <cite>baseline approach</cite> (i.e., <cite>BL</cite>) in all ks.",
  "y": "differences"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_21",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. Interestingly, the graph shows that using only one of the scores alone (i.e., Rk-S g and Rk-S c ) does not result in a significantly higher precision than the <cite>baseline approach</cite>.",
  "y": "similarities differences"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_0",
  "x": "Even in the monolingual IR, they cannot bridge the lexical gap, being incapable of semantic generalization [6] . A solution is to resort to structured real-valued semantic representations, that is, text embeddings [2, 6, 10] : these representations allow to generalize over the vocabularies observed in labelled data, and hence offer additional retrieval evidence and mitigate the ubiquitous problem of data sparsity. Their usefulness has been proven for monolingual [15] and cross-lingual ad-hoc IR models [21] . Besides the embedding-based CLIR paradigms, other approaches to bridging the lexical gap for CLIR exist. 1) Full-blown Machine Translation (MT) systems are employed to translate either queries or documents [8, 9] , but these require huge amounts of parallel data, while such resources are still scarce for many language pairs and domains. 2) The lexical chasm can be crossed by grounding queries and documents in an external multilingual knowledge source (e.g., Wikipedia or BabelNet) [4, 20] . However, the concept coverage is limited for resource-lean languages, and all content not present in a knowledge base is effectively ignored by a CLIR system. Bilingual text embeddings, while displaying a wider applicability and versatility than the two other paradigms, still suffer from one important limitation: a bilingual supervision signal is required to induce shared cross-lingual semantic spaces. This supervision takes form of sentence-aligned parallel data [5] , pre-built word translation pairs [11,<cite> 19]</cite> or document-aligned comparable data [21] .",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_1",
  "x": "The proposed unsupervised CLIR models rely on the existence of a shared cross-lingual word embedding space in which all vocabulary terms of both languages are placed. We first outline three methods for the shared space induction, with a focus on the unsupervised method. We then explain in detail the query and document representations as well as the ranking functions of our CLIR models. ---------------------------------- **CROSS-LINGUAL WORD VECTOR SPACES** For our proposed CLIR models, we investigate cross-lingual embedding spaces produced with state-of-the-art representative methods requiring different amount and type of bilingual supervision: 1) document-aligned comparable data [21] , 2) word translation pairs <cite>[19]</cite> ; and 3) no bilingual data at all [3] . ---------------------------------- **CROSS-LINGUAL EMBEDDINGS FROM COMPARABLE DOCUMENTS (CL-CD).** The BWE Skip-Gram (BWESG) model from Vuli\u0107 and Moens [21] exploits large document-aligned comparable corpora (e.g., Wikipedia).",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_2",
  "x": "BWESG first creates a merged corpus of bilingual pseudo-documents by intertwining pairs of available comparable documents. Then it applies a standard monolingual log-linear Skip-Gram model with negative sampling (SGNS) [10] on the merged corpus in which words have bilingual contexts instead of monolingual ones. ---------------------------------- **CROSS-LINGUAL EMBEDDINGS FROM WORD TRANSLATION PAIRS (CL-WT).** This class of models [1, 11,<cite> 19]</cite> focuses on learning the projections (i.e., mappings) between independently trained monolingual embedding spaces. Let { S w i } V S i =1 , S w i \u2208 R ds be the monolingual word embedding space of the source language L S with V S vectors, and { T w i } V T i =1 , T w i \u2208 R dt the monolingual space for the target language L T containing V T vectors; ds and dt are the respective space dimensionalities. The models learn a parametrized mapping function f ( |\u03b8) that projects the source language vectors into the target space: . The projection parameters \u03b8 are learned using the training set of K word translation pairs: , typically via second-order stochastic optimisation techniques.",
  "y": "background"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_3",
  "x": "The models learn a parametrized mapping function f ( |\u03b8) that projects the source language vectors into the target space: . The projection parameters \u03b8 are learned using the training set of K word translation pairs: , typically via second-order stochastic optimisation techniques. According to the comparative evaluation from [18] , all projectionbased methods for inducing cross-lingual embedding spaces perform similarly. We therefore opt for the recent model of Smith et al. <cite>[19]</cite> to serve as a baseline, due to its competitive performance, large coverage, and readily available implementation. 2 Technically, the method of Smith et al. <cite>[19]</cite> learns two projection functions f S ( S |\u03b8 S ) and f S ( T |\u03b8 T ), projecting the source and target monolingual embedding spaces, respectively, to the new shared space. ---------------------------------- **CROSS-LINGUAL EMBEDDINGS WITHOUT BILINGUAL SUPERVISION (CL-UNSUP).** Most recently, Conneau et al. [3] have proposed an adversarial learning-based model in order to automatically, in a fully unsupervised fashion, create word translation pairs that can then be used to learn the same projection functions f S and f T as in the model of Smith et al. <cite>[19]</cite> . Let X be the set of all monolingual word embeddings from the source language, and Y the set of all target language embeddings.",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_4",
  "x": "The models learn a parametrized mapping function f ( |\u03b8) that projects the source language vectors into the target space: . The projection parameters \u03b8 are learned using the training set of K word translation pairs: , typically via second-order stochastic optimisation techniques. According to the comparative evaluation from [18] , all projectionbased methods for inducing cross-lingual embedding spaces perform similarly. We therefore opt for the recent model of Smith et al. <cite>[19]</cite> to serve as a baseline, due to its competitive performance, large coverage, and readily available implementation. 2 Technically, the method of Smith et al. <cite>[19]</cite> learns two projection functions f S ( S |\u03b8 S ) and f S ( T |\u03b8 T ), projecting the source and target monolingual embedding spaces, respectively, to the new shared space. ---------------------------------- **CROSS-LINGUAL EMBEDDINGS WITHOUT BILINGUAL SUPERVISION (CL-UNSUP).** Most recently, Conneau et al. [3] have proposed an adversarial learning-based model in order to automatically, in a fully unsupervised fashion, create word translation pairs that can then be used to learn the same projection functions f S and f T as in the model of Smith et al. <cite>[19]</cite> . Let X be the set of all monolingual word embeddings from the source language, and Y the set of all target language embeddings.",
  "y": "background"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_5",
  "x": "The models learn a parametrized mapping function f ( |\u03b8) that projects the source language vectors into the target space: . The projection parameters \u03b8 are learned using the training set of K word translation pairs: , typically via second-order stochastic optimisation techniques. According to the comparative evaluation from [18] , all projectionbased methods for inducing cross-lingual embedding spaces perform similarly. We therefore opt for the recent model of Smith et al. <cite>[19]</cite> to serve as a baseline, due to its competitive performance, large coverage, and readily available implementation. 2 Technically, the method of Smith et al. <cite>[19]</cite> learns two projection functions f S ( S |\u03b8 S ) and f S ( T |\u03b8 T ), projecting the source and target monolingual embedding spaces, respectively, to the new shared space. ---------------------------------- **CROSS-LINGUAL EMBEDDINGS WITHOUT BILINGUAL SUPERVISION (CL-UNSUP).** Most recently, Conneau et al. [3] have proposed an adversarial learning-based model in order to automatically, in a fully unsupervised fashion, create word translation pairs that can then be used to learn the same projection functions f S and f T as in the model of Smith et al. <cite>[19]</cite> . Let X be the set of all monolingual word embeddings from the source language, and Y the set of all target language embeddings.",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_6",
  "x": "We therefore opt for the recent model of Smith et al. <cite>[19]</cite> to serve as a baseline, due to its competitive performance, large coverage, and readily available implementation. 2 Technically, the method of Smith et al. <cite>[19]</cite> learns two projection functions f S ( S |\u03b8 S ) and f S ( T |\u03b8 T ), projecting the source and target monolingual embedding spaces, respectively, to the new shared space. ---------------------------------- **CROSS-LINGUAL EMBEDDINGS WITHOUT BILINGUAL SUPERVISION (CL-UNSUP).** Most recently, Conneau et al. [3] have proposed an adversarial learning-based model in order to automatically, in a fully unsupervised fashion, create word translation pairs that can then be used to learn the same projection functions f S and f T as in the model of Smith et al. <cite>[19]</cite> . Let X be the set of all monolingual word embeddings from the source language, and Y the set of all target language embeddings. In the first, adversarial learning step, they jointly learn (1) the projection matrix W that maps one embedding space to the other and (2) the parameters of the discriminator model which, given an embedding vector (either W x where x \u2208 X , or \u2208 Y ) needs to predict whether it is an original vector from the target embedding space ( ),nor a vector from the source embedding space mapped via projection W to the target embedding space (W x). The discriminator model is a multi-layer perceptron network. In the second step, the projection matrix W trained with adversarial objective is used to find the mutual nearest neighbors between the two vocabularies -this set of automatically obtained word translation pairs becomes a synthetic training set for the refined projection functions f S and f T computed via the SVD-based method similar to the previously described model of Smith et al. <cite>[19]</cite> . ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_7",
  "x": "In other words, the query q = {t 5 By effectively transforming a CLIR task into a monolingual IR task, we can apply any of the traditional IR ranking functions designed for sparse text representations. We opt for the ubiquitous query likelihood model [17] , smoothing the unigram language model of individual documents with the unigram language model of the entire collection, using the Dirichlet smoothing scheme [23] : ---------------------------------- **EXPERIMENTAL SETUP** Language Pairs and Training Data. We experiment with three language pairs of varying degree of similarity: English (EN) -{Dutch (NL), Italian (IT), Finnish (FI)}. 6 We use precomputed monolingual T vectors [2] (available online) 7 as monolingual word embeddings required by CL-WT and CL-UNSUP embedding models. For the CL-CD embeddings, the BWESG model trains on full documentaligned Wikipedias 8 using SGNS with suggested parameters from prior work [22] : 15 negative samples, global decreasing learning rate is .025, subsampling rate is 1e \u2212 4, window size is 16. The CL-WT embeddings of Smith et al. <cite>[19]</cite> use 10K translation pairs obtained from Google Translate to learn the linear mapping functions. The CL-UNSUP training setup closely follows the default setup of Conneau et al. [3] : we refer the reader to the original 4 Note that with both variants of BWE-Agg, we effectively ignore both query and document terms that are not represented in the cross-lingual embedding space.",
  "y": "differences"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_8",
  "x": "We speculate that this is due to the compounding phenomenon in word formation, which is present in NL, but is not a property of EN and IT. The reported performance on bilingual lexicon extraction (BLE) using cross-lingual embedding spaces is also lower for EN-NL compared to EN-IT (see, e.g., <cite>[19]</cite> ). We observe the same pattern (4-5% lower BLE performance for EN-NL than for EN-IT) with the CL-UNSUP embedding spaces. The weighted variant of BWE-Agg (BWE-Agg-IDF) outperforms the simpler non-weighted summation model (BWE-Agg-Add) across the board. These results suggest that the common IR assumption about document-specific terms being more important than the terms occurring collection-wide is also valid for constructing dense document representations by summing word embeddings. ---------------------------------- **CONCLUSION** We have presented a fully unsupervised CLIR framework that leverages unsupervised cross-lingual word embeddings induced solely on the basis of monolingual corpora. We have shown the ability of our models to retrieve relevant content cross-lingually without any bilingual data at all, by reporting competitive performance on standard CLEF CLIR evaluation data for three test language pairs.",
  "y": "similarities"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_0",
  "x": "We train five state-of-the-art neural network models on different datasets and show that each one of these fail to generalize outside of the respective benchmark. In light of these results we conclude that the current neural network models are not able to generalize in capturing the semantics of natural language inference, but seem to be overfitting to the specific dataset. ---------------------------------- **INTRODUCTION** Natural Language Inference (NLI) has attracted considerable interest in the NLP community and, recently, a large number of neural network-based systems have been proposed to deal with the task. These approaches can be usually categorized into: a) sentence encoding models, and b) other neural network models. Both of them have been very successful, with the state of the art on the SNLI and MultiNLI datasets being 90.1% (Kim et al., 2018) and 86.7% (Devlin et al., 2018) respectively. However, a big question w.r.t to these systems is their ability to generalize outside the specific datasets they are trained and tested on. Recently, <cite>Glockner et al. (2018)</cite> have shown that state-of-the-art NLI systems break considerably easily when instead of tested on the original SNLI test set, they are tested on a test set which Preprint.",
  "y": "motivation"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_1",
  "x": "However, a big question w.r.t to these systems is their ability to generalize outside the specific datasets they are trained and tested on. Recently, <cite>Glockner et al. (2018)</cite> have shown that state-of-the-art NLI systems break considerably easily when instead of tested on the original SNLI test set, they are tested on a test set which Preprint. Work in progress. is constructed by taking premises from the training set and creating several hypotheses from them by changing at most one word within the premise. The results show a very significant drop in accuracy for three of the four systems. The system that was more difficult to break and had the less loss in accuracy was the system by Chen et al. (2018) which utilizes external knowledge taken from WordNet (Miller, 1995) . In this paper we show that NLI systems that have been very successful in specific NLI benchmarks, fail to generalize when trained on a specific NLI datset and then tested across different NLI benchmarks. The results we get are in line with <cite>Glockner et al. (2018)</cite> , showing that the generalization capability of the individual NLI systems is very limited, but, what is more, they further show the only system that was less prone to breaking in <cite>Glockner et al. (2018)</cite> , breaks in the experiments we have conducted as well. ---------------------------------- **RELATED WORK**",
  "y": "similarities differences extends"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_2",
  "x": "The results show a very significant drop in accuracy for three of the four systems. The system that was more difficult to break and had the less loss in accuracy was the system by Chen et al. (2018) which utilizes external knowledge taken from WordNet (Miller, 1995) . In this paper we show that NLI systems that have been very successful in specific NLI benchmarks, fail to generalize when trained on a specific NLI datset and then tested across different NLI benchmarks. The results we get are in line with <cite>Glockner et al. (2018)</cite> , showing that the generalization capability of the individual NLI systems is very limited, but, what is more, they further show the only system that was less prone to breaking in <cite>Glockner et al. (2018)</cite> , breaks in the experiments we have conducted as well. ---------------------------------- **RELATED WORK** The ability of NLI systems to generalize and related skepticism has been raised in a recent paper by <cite>Glockner et al. (2018)</cite> . There, the authors show that the generalization capabilities of stateof-the-art NLI systems, in cases where some kind of external lexical knowledge is needed, drops dramatically when the SNLI test set is replaced by a test set where the premise and the hypothesis are otherwise identical except for at most one word. The results show a very significant drop in accuracy. recognize the generalization problem that comes with training on datasets like SNLI, which tend to be homogeneous with linguistic variation.",
  "y": "motivation background"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_3",
  "x": "We also selected one model involving a pretrained language model, namely ESIM + ELMo . All of the models perform well on the SNLI dataset, reaching near stateof-the-art accuracy in the sentence encoding and the other category respectively. KIM is particularly interesting in this context as it performed significantly better than other models in the Breaking NLI experiment conducted by <cite>Glockner et al. (2018)</cite> . For BiLSTM-max we used the Adam optimizer (Kingma and Ba, 2014) and a learning rate of 5e-4. The learning rate was decreased by the factor of 0.2 after each epoch if the model did not improve. We used a batch size of 64. Dropout of 0.1 was used between the layers of the multilayer perceptron classifier, except before the last layer. The models were evaluated with the development data after each epoch and training was stopped if the development loss increased for more than 3 epochs. The model with the highest development accuracy was selected for testing.",
  "y": "background"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_4",
  "x": "This might also explain why the drop in accuracy for all of the four models is lowest when the models are trained on MultiNLI and tested on SNLI. It is also very surprising that the model with biggest drop in accuracy was ESIM + ELMo which includes a pretrained ELMo language model. ESIM + ELMo did, however, get the highest accuracy of 69.1% in this experiment. All the models perform almost equally poorly across all the experiments. Both BiLSTM-max and HBMP have an average drop in accuracy of 24.4 points, while the average for KIM is 25.5 and for ESIM + ELMo 25.6. ESIM has the highest average drop of 27.0 points. In contrast to the findings of <cite>Glockner et al. (2018)</cite> , utilizing external knowledge did not improve the model's generalization capability, as KIM performed equally poorly across all dataset combinations. Also including a pretrained language model did not improve the results significantly. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_5",
  "x": "We experimented with five state-of-the-art models covering both sentence encoding approaches and cross-sentence attention models. For all the systems, the accuracy drops between 7.9-33.7 points (the average drop being 25.4 points), when testing with a test set drawn from a separate corpus from that of the training data, as compared to when the test and training data are splits from the same corpus. Our findings, together with the previous negative findings e.g. by <cite>Glockner et al. (2018)</cite> and Gururangan et al. (2018) , indicate that the current state-of-the-art neural network models fail to capture the semantics of NLI in a way that will enable them to generalize across different NLI situations. The results indicate two issues to be taken into consideration: a) using datasets involving a fraction of what NLI is, will fail when tested in datasets that are testing for a slightly different definition. This is evident when we move from the SNLI to the SICK dataset. b) NLI is to some extent also genre/context dependent. Training on SNLI and testing on MultiNLI gives worse results than vice versa. This can be seen as an indication that training on multiple genres helps. However, this is still not enough given that, even in case of training on MultiNLI and testing on SNLI, accuracy drops significantly.",
  "y": "similarities"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_6",
  "x": "**CONCLUSION** In this paper we have shown that neural network models for NLI fail to generalize across different NLI benchmarks. We experimented with five state-of-the-art models covering both sentence encoding approaches and cross-sentence attention models. For all the systems, the accuracy drops between 7.9-33.7 points (the average drop being 25.4 points), when testing with a test set drawn from a separate corpus from that of the training data, as compared to when the test and training data are splits from the same corpus. Our findings, together with the previous negative findings e.g. by <cite>Glockner et al. (2018)</cite> and Gururangan et al. (2018) , indicate that the current state-of-the-art neural network models fail to capture the semantics of NLI in a way that will enable them to generalize across different NLI situations. The results indicate two issues to be taken into consideration: a) using datasets involving a fraction of what NLI is, will fail when tested in datasets that are testing for a slightly different definition. This is evident when we move from the SNLI to the SICK dataset. b) NLI is to some extent also genre/context dependent. Training on SNLI and testing on MultiNLI gives worse results than vice versa.",
  "y": "future_work"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_0",
  "x": "The question of how to evaluate parser output has naturally already arisen in earlier work on parsing English. As discussed by Lin (1995) and others, the PARSEVAL evaluation typically used to analyze the performance of statistical parsing models has many drawbacks. Bracketing evaluation may count a single error multiple times and does not differentiate between errors that significantly affect the interpretation of the sentence and those that are less crucial. It also does not allow for evaluation of particular syntactic structures or provide meaningful information about where the parser is failing. In addition, and most directly relevant for this paper, PARSE-VAL scores are difficult to compare across syntactic annotation schemes (Carroll et al., 2003) . At the same time, previous research on PCFG parsing using treebank training data present PAR-SEVAL measures in comparing the parsing performance for different languages and annotation schemes, reporting a number of striking differences. For example, Levy and Manning (2003) , K\u00fcbler (2005) , and <cite>K\u00fcbler et al. (2006)</cite> highlight the significant effect of language properties and annotation schemes for German and Chinese treebanks. In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004;<cite> K\u00fcbler et al., 2006)</cite> . Previous work has compared the similar Negra and Tiger corpora of German to the very different T\u00fcBa-D/Z corpus.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_1",
  "x": "For example, Levy and Manning (2003) , K\u00fcbler (2005) , and <cite>K\u00fcbler et al. (2006)</cite> highlight the significant effect of language properties and annotation schemes for German and Chinese treebanks. In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004;<cite> K\u00fcbler et al., 2006)</cite> . Previous work has compared the similar Negra and Tiger corpora of German to the very different T\u00fcBa-D/Z corpus. <cite>K\u00fcbler et al. (2006)</cite> compares the Negra and T\u00fcBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure. 1 Using the Stanford Parser (Klein and Manning, 2002) , which employs a factored PCFG and dependency model, they claim that the model trained on T\u00fcBa-D/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical function evaluations. Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from <cite>K\u00fcbler et al. (2006)</cite> . 2 In recent related work, Rehbein and van Genabith (2007a) demonstrate using the Tiger and T\u00fcBa-D/Z 1 The evaluation is based only on the grammatical function; it does not identify the dependency pair that it labels. 2 While the focus of <cite>K\u00fcbler et al. (2006)</cite> is on comparing parsing results across corpora, Dubey (2004) focuses on improving parsing for Negra, including corpus-specific enhancements leading to better results. This difference in focus and additional differences in experimental setup mean that a finegrained comparison of the results is inappropriate -the relevant point here is that the gap between the results (23% for subjects, 35% for accusative objects) warrants further attention in the context of comparing parsing results across corpora.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_2",
  "x": "Previous work has compared the similar Negra and Tiger corpora of German to the very different T\u00fcBa-D/Z corpus. <cite>K\u00fcbler et al. (2006)</cite> compares the Negra and T\u00fcBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure. 1 Using the Stanford Parser (Klein and Manning, 2002) , which employs a factored PCFG and dependency model, they claim that the model trained on T\u00fcBa-D/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical function evaluations. Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from <cite>K\u00fcbler et al. (2006)</cite> . 2 In recent related work, Rehbein and van Genabith (2007a) demonstrate using the Tiger and T\u00fcBa-D/Z 1 The evaluation is based only on the grammatical function; it does not identify the dependency pair that it labels. 2 While the focus of <cite>K\u00fcbler et al. (2006)</cite> is on comparing parsing results across corpora, Dubey (2004) focuses on improving parsing for Negra, including corpus-specific enhancements leading to better results. This difference in focus and additional differences in experimental setup mean that a finegrained comparison of the results is inappropriate -the relevant point here is that the gap between the results (23% for subjects, 35% for accusative objects) warrants further attention in the context of comparing parsing results across corpora. corpora of German that PARSEVAL is inappropriate for comparisons of the output of PCFG parsers trained on different treebank annotation schemes because PARSEVAL scores are affected by the ratio of terminal to non-terminal nodes. A dependencybased evaluation on triples of the form word-POShead shows better results for the parser trained on Tiger even though the much lower PARSEVAL scores, if meaningful, would predict that the output for Tiger is of lower quality.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_3",
  "x": "Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from <cite>K\u00fcbler et al. (2006)</cite> . 2 In recent related work, Rehbein and van Genabith (2007a) demonstrate using the Tiger and T\u00fcBa-D/Z 1 The evaluation is based only on the grammatical function; it does not identify the dependency pair that it labels. 2 While the focus of <cite>K\u00fcbler et al. (2006)</cite> is on comparing parsing results across corpora, Dubey (2004) focuses on improving parsing for Negra, including corpus-specific enhancements leading to better results. This difference in focus and additional differences in experimental setup mean that a finegrained comparison of the results is inappropriate -the relevant point here is that the gap between the results (23% for subjects, 35% for accusative objects) warrants further attention in the context of comparing parsing results across corpora. corpora of German that PARSEVAL is inappropriate for comparisons of the output of PCFG parsers trained on different treebank annotation schemes because PARSEVAL scores are affected by the ratio of terminal to non-terminal nodes. A dependencybased evaluation on triples of the form word-POShead shows better results for the parser trained on Tiger even though the much lower PARSEVAL scores, if meaningful, would predict that the output for Tiger is of lower quality. However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalismindependent evaluation of parsers (e.g., Clark and Curran, 2007) . 3 Addressing these issues, we resolve the apparent discrepancy between <cite>K\u00fcbler et al. (2006)</cite> and Dubey (2004) and establish a firm grammatical function comparison of Negra and T\u00fcBa-D/Z. We also extend the evaluation to a labeled dependency evaluation based on grammatical relations for both corpora. Such an evaluation, which abstracts away from the specifics of the annotation schemes, shows that, in contrast to the claims made in <cite>K\u00fcbler et al. (2006)</cite> , the parsing results for PCFG parsers trained on these heterogeneous corpora are very similar.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_4",
  "x": "<cite>K\u00fcbler et al. (2006)</cite> compares the Negra and T\u00fcBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure. 1 Using the Stanford Parser (Klein and Manning, 2002) , which employs a factored PCFG and dependency model, they claim that the model trained on T\u00fcBa-D/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical function evaluations. Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from <cite>K\u00fcbler et al. (2006)</cite> . 2 In recent related work, Rehbein and van Genabith (2007a) demonstrate using the Tiger and T\u00fcBa-D/Z 1 The evaluation is based only on the grammatical function; it does not identify the dependency pair that it labels. 2 While the focus of <cite>K\u00fcbler et al. (2006)</cite> is on comparing parsing results across corpora, Dubey (2004) focuses on improving parsing for Negra, including corpus-specific enhancements leading to better results. This difference in focus and additional differences in experimental setup mean that a finegrained comparison of the results is inappropriate -the relevant point here is that the gap between the results (23% for subjects, 35% for accusative objects) warrants further attention in the context of comparing parsing results across corpora. corpora of German that PARSEVAL is inappropriate for comparisons of the output of PCFG parsers trained on different treebank annotation schemes because PARSEVAL scores are affected by the ratio of terminal to non-terminal nodes. A dependencybased evaluation on triples of the form word-POShead shows better results for the parser trained on Tiger even though the much lower PARSEVAL scores, if meaningful, would predict that the output for Tiger is of lower quality. However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalismindependent evaluation of parsers (e.g., Clark and Curran, 2007) .",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_5",
  "x": "1 Using the Stanford Parser (Klein and Manning, 2002) , which employs a factored PCFG and dependency model, they claim that the model trained on T\u00fcBa-D/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical function evaluations. Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from <cite>K\u00fcbler et al. (2006)</cite> . 2 In recent related work, Rehbein and van Genabith (2007a) demonstrate using the Tiger and T\u00fcBa-D/Z 1 The evaluation is based only on the grammatical function; it does not identify the dependency pair that it labels. 2 While the focus of <cite>K\u00fcbler et al. (2006)</cite> is on comparing parsing results across corpora, Dubey (2004) focuses on improving parsing for Negra, including corpus-specific enhancements leading to better results. This difference in focus and additional differences in experimental setup mean that a finegrained comparison of the results is inappropriate -the relevant point here is that the gap between the results (23% for subjects, 35% for accusative objects) warrants further attention in the context of comparing parsing results across corpora. corpora of German that PARSEVAL is inappropriate for comparisons of the output of PCFG parsers trained on different treebank annotation schemes because PARSEVAL scores are affected by the ratio of terminal to non-terminal nodes. A dependencybased evaluation on triples of the form word-POShead shows better results for the parser trained on Tiger even though the much lower PARSEVAL scores, if meaningful, would predict that the output for Tiger is of lower quality. However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalismindependent evaluation of parsers (e.g., Clark and Curran, 2007) . 3 Addressing these issues, we resolve the apparent discrepancy between <cite>K\u00fcbler et al. (2006)</cite> and Dubey (2004) and establish a firm grammatical function comparison of Negra and T\u00fcBa-D/Z. We also extend the evaluation to a labeled dependency evaluation based on grammatical relations for both corpora.",
  "y": "motivation"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_6",
  "x": "2 While the focus of <cite>K\u00fcbler et al. (2006)</cite> is on comparing parsing results across corpora, Dubey (2004) focuses on improving parsing for Negra, including corpus-specific enhancements leading to better results. This difference in focus and additional differences in experimental setup mean that a finegrained comparison of the results is inappropriate -the relevant point here is that the gap between the results (23% for subjects, 35% for accusative objects) warrants further attention in the context of comparing parsing results across corpora. corpora of German that PARSEVAL is inappropriate for comparisons of the output of PCFG parsers trained on different treebank annotation schemes because PARSEVAL scores are affected by the ratio of terminal to non-terminal nodes. A dependencybased evaluation on triples of the form word-POShead shows better results for the parser trained on Tiger even though the much lower PARSEVAL scores, if meaningful, would predict that the output for Tiger is of lower quality. However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalismindependent evaluation of parsers (e.g., Clark and Curran, 2007) . 3 Addressing these issues, we resolve the apparent discrepancy between <cite>K\u00fcbler et al. (2006)</cite> and Dubey (2004) and establish a firm grammatical function comparison of Negra and T\u00fcBa-D/Z. We also extend the evaluation to a labeled dependency evaluation based on grammatical relations for both corpora. Such an evaluation, which abstracts away from the specifics of the annotation schemes, shows that, in contrast to the claims made in <cite>K\u00fcbler et al. (2006)</cite> , the parsing results for PCFG parsers trained on these heterogeneous corpora are very similar. ---------------------------------- **THE CORPORA USED**",
  "y": "extends"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_7",
  "x": "corpora of German that PARSEVAL is inappropriate for comparisons of the output of PCFG parsers trained on different treebank annotation schemes because PARSEVAL scores are affected by the ratio of terminal to non-terminal nodes. A dependencybased evaluation on triples of the form word-POShead shows better results for the parser trained on Tiger even though the much lower PARSEVAL scores, if meaningful, would predict that the output for Tiger is of lower quality. However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalismindependent evaluation of parsers (e.g., Clark and Curran, 2007) . 3 Addressing these issues, we resolve the apparent discrepancy between <cite>K\u00fcbler et al. (2006)</cite> and Dubey (2004) and establish a firm grammatical function comparison of Negra and T\u00fcBa-D/Z. We also extend the evaluation to a labeled dependency evaluation based on grammatical relations for both corpora. Such an evaluation, which abstracts away from the specifics of the annotation schemes, shows that, in contrast to the claims made in <cite>K\u00fcbler et al. (2006)</cite> , the parsing results for PCFG parsers trained on these heterogeneous corpora are very similar. ---------------------------------- **THE CORPORA USED** As motivated in the introduction, the work discussed in this paper is based on two German corpora, Negra and T\u00fcBa-D/Z, which differ significantly in the syntactic representations used -thereby offering an interesting test bed for investigating the influence of an annotation scheme on the parsers trained. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_8",
  "x": "**DATA PREPARATION** Following <cite>K\u00fcbler et al. (2006)</cite> , only sentences with fewer than 35 words were used, which results in 20,002 sentences for Negra and 21,365 sentences for T\u00fcBa-D/Z. Because punctuation is not attached within the sentence in the corpus annotation, punctuation was removed. To be able to train PCFG parsing models, it is necessary to convert the syntax graphs encoding trees with discontinuities in Negra into traditional syntax trees. Around 30% of sentences in Negra contain at least one discontinuity. To remove discontinuities, we used the conversion program included with the Negra corpus annotation tools (Brants and Plaehn, 2000) , the same tool used in <cite>K\u00fcbler et al. (2006)</cite> , which raises non-head elements to a higher tree until there are no more discontinuities. For example, for the discontinuous tree with a fronted object we saw in Figure 1 , the PP containing the fronted NP Dieser Meinung is raised to become a daughter of the top S node. 4 Additionally, the edge labels used in both corpora need to be folded into the node labels to become a part of context-free grammar rules used by a PCFG parser. In the Penn Treebank-style versions of the corpora appropriate for training a PCFG parser, each edge label is joined with the phrase or POS label on the phrase or word immediately below it. Both corpora include edge labels above all phrases and words.",
  "y": "uses"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_9",
  "x": "To be able to train PCFG parsing models, it is necessary to convert the syntax graphs encoding trees with discontinuities in Negra into traditional syntax trees. Around 30% of sentences in Negra contain at least one discontinuity. To remove discontinuities, we used the conversion program included with the Negra corpus annotation tools (Brants and Plaehn, 2000) , the same tool used in <cite>K\u00fcbler et al. (2006)</cite> , which raises non-head elements to a higher tree until there are no more discontinuities. For example, for the discontinuous tree with a fronted object we saw in Figure 1 , the PP containing the fronted NP Dieser Meinung is raised to become a daughter of the top S node. 4 Additionally, the edge labels used in both corpora need to be folded into the node labels to become a part of context-free grammar rules used by a PCFG parser. In the Penn Treebank-style versions of the corpora appropriate for training a PCFG parser, each edge label is joined with the phrase or POS label on the phrase or word immediately below it. Both corpora include edge labels above all phrases and words. However the flatter structures in Negra result in 39 different edge labels on words while T\u00fcBa-D/Z has only 5. Unlike <cite>K\u00fcbler et al. (2006)</cite> , which ignored edge labels on words, we incorporate all edge labels present in both corpora.",
  "y": "uses"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_10",
  "x": "However the flatter structures in Negra result in 39 different edge labels on words while T\u00fcBa-D/Z has only 5. Unlike <cite>K\u00fcbler et al. (2006)</cite> , which ignored edge labels on words, we incorporate all edge labels present in both corpora. As a consequence of this, providing a parser with perfect lexical tags would also provide the edge label for that word. T\u00fcBa-D/Z does not annotate grammatical functions other than HD on words, but Negra includes many grammatical functions on words. Including edge labels in the perfect lexical tags would artificially boost the results of a grammatical function evaluation for Negra since it amounts to providing the correct grammatical function for the 38% of arguments in Negra that are single words. To avoid this problem, we introduced nonbranching phrasal nodes into Negra to prevent the correct grammatical function label from being provided with the perfect lexical tag in the cases of single-word arguments, which are mostly bare nouns and pronouns. We added phrasal nodes above all single-word subject, accusative object, dative object, and genitive object 5 arguments, with the category of the inserted phrase depending on the POS tag on the word. The introduced phrasal node is given the word's original grammatical function label; the grammatical function label of the word itself becomes NK for NPs and HD for APs and VPs. In total, 14,580 nodes were inserted into Negra in this way.",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_11",
  "x": "**DEPENDENCY EVALUATION** Complementing the issue of the ratio of terminals to non-terminals raised in the last section, one can question whether counting all brackets in the sentence equally, as done by the PARSEVAL metric, provides a good measure of how accurately the basic functor-argument structure of the sentence has been captured in a parse. Thus, it is useful to per-7 Our experimental setup is designed to support a comparison between Negra and T\u00fcBa-D/Z for the three evaluation metrics and is intended to be comparable to the setup of <cite>K\u00fcbler et al. (2006)</cite> . For Negra, Dubey (2004) explores a range of parsing models and the corpus preparation he uses differs from the one discussed in this paper so that a discussion of his results is beyond the scope of the corpus comparison in this paper. 8 Scores were calculated using evalb. form an evaluation based on the grammatical function labels that are important for determining the functor-argument structure of the sentence: subjects, accusative objects, and dative objects. 9 The first step in an evaluation of functor-argument structure is to identify whether an argument bears the correct grammatical function label. ---------------------------------- **GRAMMATICAL FUNCTION LABEL EVALUATION**",
  "y": "similarities"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_12",
  "x": "For Negra, Dubey (2004) explores a range of parsing models and the corpus preparation he uses differs from the one discussed in this paper so that a discussion of his results is beyond the scope of the corpus comparison in this paper. 8 Scores were calculated using evalb. form an evaluation based on the grammatical function labels that are important for determining the functor-argument structure of the sentence: subjects, accusative objects, and dative objects. 9 The first step in an evaluation of functor-argument structure is to identify whether an argument bears the correct grammatical function label. ---------------------------------- **GRAMMATICAL FUNCTION LABEL EVALUATION** <cite>K\u00fcbler et al. (2006)</cite> present the results shown in Table 3 for the parsing performance of the unlexicalized model of the Stanford Parser (Klein and Manning, 2002) . In this grammatical function label evaluation, T\u00fcBa-D/Z outperforms Negra for subjects, accusative objects, and dative objects based on an evaluation of phrasal arguments. <cite>K\u00fcbler et al. (2006)</cite> Note that this grammatical function label evaluation is restricted to labels on phrases; grammatical function labels on words are ignored in training and testing.",
  "y": "uses"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_14",
  "x": "9 The first step in an evaluation of functor-argument structure is to identify whether an argument bears the correct grammatical function label. ---------------------------------- **GRAMMATICAL FUNCTION LABEL EVALUATION** <cite>K\u00fcbler et al. (2006)</cite> present the results shown in Table 3 for the parsing performance of the unlexicalized model of the Stanford Parser (Klein and Manning, 2002) . In this grammatical function label evaluation, T\u00fcBa-D/Z outperforms Negra for subjects, accusative objects, and dative objects based on an evaluation of phrasal arguments. <cite>K\u00fcbler et al. (2006)</cite> Note that this grammatical function label evaluation is restricted to labels on phrases; grammatical function labels on words are ignored in training and testing. This results in an unbalanced comparison between Negra and T\u00fcBa-D/Z since, as discussed in section 2, T\u00fcBa-D/Z includes unary-branching phrases above all single-word arguments whereas Negra does not. In effect, single-word arguments in Negra -mainly pronouns and bare nouns -are not considered in the evaluation from <cite>K\u00fcbler et al. (2006)</cite> . The result is thus a comparison of multiword arguments in Negra to both single-and multiword arguments in T\u00fcBa-D/Z. Recall from section 3.1 that this is not a minor difference: single-word arguments account for 38% of subjects, accusative objects, and dative objects in Negra.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_15",
  "x": "In this grammatical function label evaluation, T\u00fcBa-D/Z outperforms Negra for subjects, accusative objects, and dative objects based on an evaluation of phrasal arguments. <cite>K\u00fcbler et al. (2006)</cite> Note that this grammatical function label evaluation is restricted to labels on phrases; grammatical function labels on words are ignored in training and testing. This results in an unbalanced comparison between Negra and T\u00fcBa-D/Z since, as discussed in section 2, T\u00fcBa-D/Z includes unary-branching phrases above all single-word arguments whereas Negra does not. In effect, single-word arguments in Negra -mainly pronouns and bare nouns -are not considered in the evaluation from <cite>K\u00fcbler et al. (2006)</cite> . The result is thus a comparison of multiword arguments in Negra to both single-and multiword arguments in T\u00fcBa-D/Z. Recall from section 3.1 that this is not a minor difference: single-word arguments account for 38% of subjects, accusative objects, and dative objects in Negra. As discussed in the data preparation section, Negra was modified for our experiment so as not to provide the parser with the grammatical function labels for single word phrases as part of the perfect tags provided. This evaluation handles multiple categories of arguments, not just NPs, so it focuses solely on the grammatical function labels, ignoring the phrasal categories. For example, in Negra an NP-OA in a parse is considered a correct accusative object even if the OA label in the gold standard has the category MPN. The results are shown in Table 4 In contrast to the results for NP grammatical functions of <cite>K\u00fcbler et al. (2006)</cite> we saw in Table 3 , Negra and T\u00fcBa-D/Z perform quite similarly overall, with Negra slightly outperforming T\u00fcBa-D/Z for all types of arguments.",
  "y": "uses"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_16",
  "x": "This results in an unbalanced comparison between Negra and T\u00fcBa-D/Z since, as discussed in section 2, T\u00fcBa-D/Z includes unary-branching phrases above all single-word arguments whereas Negra does not. In effect, single-word arguments in Negra -mainly pronouns and bare nouns -are not considered in the evaluation from <cite>K\u00fcbler et al. (2006)</cite> . The result is thus a comparison of multiword arguments in Negra to both single-and multiword arguments in T\u00fcBa-D/Z. Recall from section 3.1 that this is not a minor difference: single-word arguments account for 38% of subjects, accusative objects, and dative objects in Negra. As discussed in the data preparation section, Negra was modified for our experiment so as not to provide the parser with the grammatical function labels for single word phrases as part of the perfect tags provided. This evaluation handles multiple categories of arguments, not just NPs, so it focuses solely on the grammatical function labels, ignoring the phrasal categories. For example, in Negra an NP-OA in a parse is considered a correct accusative object even if the OA label in the gold standard has the category MPN. The results are shown in Table 4 In contrast to the results for NP grammatical functions of <cite>K\u00fcbler et al. (2006)</cite> we saw in Table 3 , Negra and T\u00fcBa-D/Z perform quite similarly overall, with Negra slightly outperforming T\u00fcBa-D/Z for all types of arguments. These results also form a clear contrast to the PARSEVAL results we saw in Table 2 . Contrary to the finding in <cite>K\u00fcbler et al. (2006)</cite> , the PAR-SEVAL evaluation does not echo the grammatical function label evaluation.",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_17",
  "x": "The results for the labeled dependency evaluation are shown in Table 5 . The parser trained on Negra outperforms the one trained on T\u00fcBa-D/Z for all types of arguments. ---------------------------------- **DISCUSSION OF RESULTS** Comparing PARSEVAL scores for a parser trained on the Negra and the T\u00fcBa-D/Z corpus with a grammatical function and a labeled dependency evalua-10 However, some strings labeled as S and VP do not contain a head and thus lack a daughter with a HD function label. 11 The relative numbers of instances where a lexical head is not found are comparable for Negra and T\u00fcBa-D/Z. Heads are not found for approximately 4% of subjects, 1% of accusative objects, and 1% of dative objects. These instances are frequently due to elision of the verb in headlines and coordinated clauses. Table 5 : Labeled Dependency Evaluation tion, we confirm that the PARSEVAL scores do not correlate with the scores in the other two evaluations, which given their closeness to the semantic functor argument structure make meaningful targets for evaluating parsers. Shifting the focus to the grammatical function evaluation, we showed that a grammatical function evaluation based on phrasal arguments as provided by <cite>K\u00fcbler et al. (2006)</cite> is inadequate for comparing parsers trained on the Negra and T\u00fcBa-D/Z corpora.",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_18",
  "x": "Comparing PARSEVAL scores for a parser trained on the Negra and the T\u00fcBa-D/Z corpus with a grammatical function and a labeled dependency evalua-10 However, some strings labeled as S and VP do not contain a head and thus lack a daughter with a HD function label. 11 The relative numbers of instances where a lexical head is not found are comparable for Negra and T\u00fcBa-D/Z. Heads are not found for approximately 4% of subjects, 1% of accusative objects, and 1% of dative objects. These instances are frequently due to elision of the verb in headlines and coordinated clauses. Table 5 : Labeled Dependency Evaluation tion, we confirm that the PARSEVAL scores do not correlate with the scores in the other two evaluations, which given their closeness to the semantic functor argument structure make meaningful targets for evaluating parsers. Shifting the focus to the grammatical function evaluation, we showed that a grammatical function evaluation based on phrasal arguments as provided by <cite>K\u00fcbler et al. (2006)</cite> is inadequate for comparing parsers trained on the Negra and T\u00fcBa-D/Z corpora. By introducing non-branching phrase nodes above single-word arguments in Negra, it is possible to provide a balanced comparison for the grammatical function label evaluation between Negra and T\u00fcBa-D/Z on both phrasal and single-word arguments. The models trained on both corpora perform very similarly in the grammatical function evaluation, in contrast to the claims in <cite>K\u00fcbler et al. (2006)</cite> . When the grammatical function label evaluation is extended into a labeled dependency evaluation by finding the verbal head to complete the labeled dependency triple, the parser trained on Negra outperforms that trained on T\u00fcBa-D/Z. The more significant drop in results for T\u00fcBa-D/Z compared to the grammatical function label evaluation may be due to the fact that a verbal lexical head in T\u00fcBa-D/Z is not in the same local tree as its dependents, whereas it is in Negra. The presence of intervening topological field nodes in T\u00fcBa-D/Z may make it difficult for the parser to consistently identify the elements of the dependency triple across several subtrees.",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_19",
  "x": "An evaluation on arguments of verbs is just a first step in working towards a more complete labeled dependency evaluation. Because Negra and T\u00fcBa-D/Z do not have parallel uses of many grammatical function labels beyond arguments of verbs, a more detailed evaluation on more types of dependency relations will require a complex dependency conversion method to provide comparable results. Since previous work on head-lexicalized parsing models for German has focused on PARSEVAL evaluations, it would also be useful to perform a labeled dependency evaluation to determine what effect head lexicalization has on particular constructions for the parsers. Because of the concerns discussed in the previous section and the difference in which types of clauses have marked heads in Negra and T\u00fcBa-D/Z, the effect of head lexicalization on the parsing results may differ for the two corpora. ---------------------------------- **CONCLUSION** Addressing the general question of how to compare parsing results for different annotation schemes, we revisited the comparison of PCFG parsing results for the Negra and T\u00fcBa-D/Z corpora. We show that these different annotation schemes lead to very significant differences in PARSEVAL scores for unlexicalized PCFG parsing models, but grammatical function label and labeled dependency evaluations for arguments of verbs show that this difference does not carry over to measures which are relevant to the semantic functor-argument structure. In contrast to <cite>K\u00fcbler et al. (2006)</cite> a grammatical function evaluation on subjects, accusative objects, and dative objects establishes that Negra and T\u00fcBa-D/Z perform similarly when all types of words and phrases appearing as arguments are taken into consideration.",
  "y": "differences"
 },
 {
  "id": "3816a122d7f0847c01415fadef2d3d_0",
  "x": "Along with the independent development of parsing techniques for individual grammar formalisms, some of them have been adapted to other formalisms (Schabes et al., 1988; van Noord, 1994; Yoshida et al., 1999; Torisawa et al., 2000) . However, these realizations sometimes exhibit quite different performance in each grammar formalism (Yoshida et al., 1999; ). If we could identify an algorithmic difference that causes performance difference, it would reveal advantages and disadvantages of the different realizations. This should also allow us to integrate the advantages of the realizations into one generic parsing technique, which yields the further advancement of the whole parsing community. In this paper, we compare CFG filtering techniques for LTAG<cite> (Harbusch, 1990</cite>; Poller and Becker, 1998) and HPSG (Torisawa et al., 2000; Kiefer and Krieger, 2000) , following an approach to parsing comparison among different grammar formalisms ). The key idea of the approach is to use strongly equivalent grammars, which generate equivalent parse results for the same input, obtained by a grammar conversion as demonstrated by . The parsers with CFG filtering predict possible parse trees by a CFG approximated from a given grammar. Comparison of those parsers are interesting because effective CFG filters allow us to bring the empirical time complexity of the parsers close to that of CFG parsing. Investigating the difference between the ways of context-free (CF) approximation of LTAG and HPSG will thereby enlighten a way of further optimization for both techniques.",
  "y": "uses"
 },
 {
  "id": "3816a122d7f0847c01415fadef2d3d_1",
  "x": "We performed a comparison between the existing CFG filtering techniques for LTAG (Poller and Becker, 1998) and HPSG (Torisawa et al., 2000) , using strongly equivalent grammars obtained by converting LTAGs extracted from the Penn Treebank (Marcus et al., 1993) into HPSG-style. We compared the parsers with respect to the size of the approximated CFG and its effectiveness as a filter. ---------------------------------- **BACKGROUND** In this section, we introduce a grammar conversion ) and CFG filtering<cite> (Harbusch, 1990</cite>; Poller and Becker, 1998; Torisawa et al., 2000; Kiefer and Krieger, 2000) . ---------------------------------- **GRAMMAR CONVERSION** The grammar conversion consists of a conversion of LTAG elementary trees to HPSG lexical entries and an emulation of substitution and adjunction by We can perform a comparison between LTAG and HPSG parsers using strongly equivalent grammars obtained by the above conversion. This is because strongly equivalent grammars can be a substitute for the same grammar in different grammar formalisms.",
  "y": "uses"
 },
 {
  "id": "3816a122d7f0847c01415fadef2d3d_2",
  "x": "In phase 1, the parser first predicts possible parse trees using the approximated CFG, and then filters out irrelevant edges by a top-down traversal starting from roots of successful context-free derivations. In phase 2, it then eliminates invalid parse trees by using constraints in the given grammar. We call the remaining edges that are used for the phase 2 parsing essential edges. The parsers with CFG filtering used in our experiments follow the above parsing strategy, but are different in the way the CF approximation and the elimination of impossible parse trees in phase 2 are performed. In the following sections, we briefly describe the CF approximation and the elimination of impossible parse trees in each realization. ---------------------------------- **CF APPROXIMATION OF LTAG** In CFG filtering techniques for LTAG<cite> (Harbusch, 1990</cite>; Poller and Becker, 1998) , every branching of elementary trees in a given grammar is extracted as a CFG rule as shown in Figure 1 . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "3816a122d7f0847c01415fadef2d3d_3",
  "x": "A generated feature structure in the approximation process thus corresponds to the whole unprocessed portion of a canonical elementary tree. This implies that successful context-free derivations obtained by CFG TNT basically involve elementary trees in which all substitution and adjunction have succeeded. However, CFG PB (also a CFG produced by the other work<cite> (Harbusch, 1990)</cite> ) cannot avoid generating invalid parse trees that connect two lo-cal structures where adjunction takes place between them. We measured with G 2-21 the proportion of the number of ok-prop between two node numbers of nodes that take adjunction and its success rate. It occupied 87% of the total number of ok-prop and its success rate was only 22%. These results suggest that the global contexts in a given grammar is essential to obtain an effective CFG filter. It should be noted that the above investigation also tells us another way of CF approximation of LTAG. We first define a unique way of tree traversal such as head-corner traversal (van Noord, 1994) on which we can perform a sequential application of substitution and adjunction. We then recursively apply substitution and adjunction on that traversal to an elementary tree and a generated tree structure.",
  "y": "uses motivation"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_0",
  "x": "Text adventure games, in which players must make sense of the world through text descriptions and declare actions through natural language, can provide a stepping stone toward more realworld environments where agents must communicate to understand the state of the world and affect change in the world. Despite the steadily increasing body of research on text-adventure games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Tao et al., 2018;<cite> Ammanabrolu and Riedl, 2019)</cite> , and in addition to the ubiquity of deep reinforcement learning applications (Parisotto et al., 2016; Zambaldi et al., 2019) , teaching an agent to play text-adventure games remains a challenging task. Learning a control policy for a text-adventure game requires a significant amount of exploration, resulting in training runs that take hundreds of thousands of simulations (Narasimhan et al., 2015;<cite> Ammanabrolu and Riedl, 2019)</cite> . One reason that text-adventure games require so much exploration is that most deep reinforcement learning algorithms are trained on a task without a real prior. In essence, the agent must learn everything about the game from only its interactions with the environment. Yet, text-adventure games make ample use of commonsense knowledge (e.g., an axe can be used to cut wood) and genre themes (e.g., in a horror or fantasy game, a coffin is likely to contain a vampire or other undead monster). This is in addition to the challenges innate to the text-adventure game itself-games are puzzleswhich results in inefficient training. Ammanabrolu and Riedl (2019) developed a reinforcement learning agent that modeled the text environment as a knowledge graph and achieved state-of-the-art results on simple text-adventure games provided by the TextWorld environment. They observed that a simple form of transfer from very similar games greatly improved policy training time.",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_1",
  "x": "Text adventure games, in which players must make sense of the world through text descriptions and declare actions through natural language, can provide a stepping stone toward more realworld environments where agents must communicate to understand the state of the world and affect change in the world. Despite the steadily increasing body of research on text-adventure games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Tao et al., 2018;<cite> Ammanabrolu and Riedl, 2019)</cite> , and in addition to the ubiquity of deep reinforcement learning applications (Parisotto et al., 2016; Zambaldi et al., 2019) , teaching an agent to play text-adventure games remains a challenging task. Learning a control policy for a text-adventure game requires a significant amount of exploration, resulting in training runs that take hundreds of thousands of simulations (Narasimhan et al., 2015;<cite> Ammanabrolu and Riedl, 2019)</cite> . One reason that text-adventure games require so much exploration is that most deep reinforcement learning algorithms are trained on a task without a real prior. In essence, the agent must learn everything about the game from only its interactions with the environment. Yet, text-adventure games make ample use of commonsense knowledge (e.g., an axe can be used to cut wood) and genre themes (e.g., in a horror or fantasy game, a coffin is likely to contain a vampire or other undead monster). This is in addition to the challenges innate to the text-adventure game itself-games are puzzleswhich results in inefficient training. Ammanabrolu and Riedl (2019) developed a reinforcement learning agent that modeled the text environment as a knowledge graph and achieved state-of-the-art results on simple text-adventure games provided by the TextWorld environment. They observed that a simple form of transfer from very similar games greatly improved policy training time.",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_2",
  "x": "Specifically, we explore transfer learning at multiple levels and across different dimensions. We first look at the effects of playing a text-adventure game given a strong prior in the form of a knowledge graph extracted from generalized textual walk-throughs of interactive fiction as well as those made specifically for a given game. Next, we explore the transfer of control policies in deep Q-learning (DQN) by pre-training portions of a deep Q-network using question-answering and by DQN-to-DQN parameter transfer between games. We evaluate these techniques on two different sets of human authored and computer generated games, demonstrating that our transfer learning methods enable us to learn a higher-quality control policy faster. ---------------------------------- **BACKGROUND AND RELATED WORK** Text-adventure games, in which an agent must interact with the world entirely through natural language, provide us with two challenges that have proven difficult for deep reinforcement learning to solve (Narasimhan et al., 2015; Haroush et al., 2018;<cite> Ammanabrolu and Riedl, 2019)</cite> : (1) The agent must act based only on potentially incomplete textual descriptions of the world around it. The world is thus partially observable, as the agent does not have access to the state of the world at any stage. (2) the action space is combinatorially large-a consequence of the agent having to declare commands in natural language.",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_3",
  "x": "(2) the action space is combinatorially large-a consequence of the agent having to declare commands in natural language. These two problems together have kept commercial text adventure games out of the reach of existing deep reinforcement learning methods, especially given the fact that most of these methods attempt to train on a particular game from scratch. Text-adventure games can be treated as partially observable Markov decision processes (POMDPs). This can be represented as a 7-tuple of S, T, A, \u2126, O, R, \u03b3 : the set of environment states, conditional transition probabilities between states, words used to compose text commands, observations, conditional observation probabilities, the reward function, and the discount factor respectively . Multiple recent works have explored the challenges associated with these games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Tao et al., 2018;<cite> Ammanabrolu and Riedl, 2019)</cite> . Narasimhan et al. (2015) introduce the LSTM-DQN, which learns to score the action verbs and corresponding objects separately and then combine them into a single action. He et al. (2016) propose the Deep Reinforcement Relevance Network that consists of separate networks to encode state and action information, with a final Q-value for a state-action pair that is computed between a pairwise interaction function between these. Haroush et al. (2018) present the Action Elimination Network (AEN), which restricts actions in a state to the top-k most likely ones, using the emulator's feedback. Hausknecht et al. (2019) design an agent that uses multiple modules to identify a general set of game play rules for text games across various domains.",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_4",
  "x": "He et al. (2016) propose the Deep Reinforcement Relevance Network that consists of separate networks to encode state and action information, with a final Q-value for a state-action pair that is computed between a pairwise interaction function between these. Haroush et al. (2018) present the Action Elimination Network (AEN), which restricts actions in a state to the top-k most likely ones, using the emulator's feedback. Hausknecht et al. (2019) design an agent that uses multiple modules to identify a general set of game play rules for text games across various domains. None of these works study how to transfer policies between different text-adventure games in any depth and so there exists a gap between the two bodies of work. Transferring policies across different textadventure games requires implicitly learning a mapping between the games' state and action spaces. The more different the domain of the two games, the harder this task becomes. Previous work <cite>(Ammanabrolu and Riedl, 2019)</cite> introduced the use of knowledge graphs and questionanswering pre-training to aid in the problems of partial observability and a combinatorial action space. This work made use of a system called TextWorld that uses grammars to generate a series of similar (but not exact same) games. An oracle was used to play perfect games and the traces were used to pre-train portions of the agent's network responsible for encoding the observations, graph, and actions.",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_5",
  "x": "The rest of this section describes KG-DQN in detail and summarizes our modifications. 1 For each step that the agent takes, it automatically extracts a set of RDF triples from the received observation through the use of OpenIE (Angeli et al., 2015) in addition to a few rules to account for the regularities of text-adventure games. The graph itself is more or less a map of the world, with information about objects' affordances and attributes linked to the rooms that they are place in in a map. The graph also makes a distinction with respect to items that are in the agent's possession or in their immediate surrounding environment. We make minor modifications to the rules used in<cite> Ammanabrolu and Riedl (2019)</cite> to better achieve such a graph in general interactive fiction environments. The agent also has access to all actions accepted by the game's parser, following Narasimhan et al. (2015) . For general interactive fiction environments, we develop our own method to extract this information. This is done by extracting a set of templates accepted by the parser, with the objects or noun phrases in the actions replaces with a OBJ tag. An example of such a template is \"place OBJ in OBJ\".",
  "y": "extends"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_6",
  "x": "We make minor modifications to the rules used in<cite> Ammanabrolu and Riedl (2019)</cite> to better achieve such a graph in general interactive fiction environments. The agent also has access to all actions accepted by the game's parser, following Narasimhan et al. (2015) . For general interactive fiction environments, we develop our own method to extract this information. This is done by extracting a set of templates accepted by the parser, with the objects or noun phrases in the actions replaces with a OBJ tag. An example of such a template is \"place OBJ in OBJ\". These OBJ tags are then filled in by looking at all possible objects in the given vocabulary for the game. This action space is of the order of A = O(|V | \u00d7 |O| 2 ) where V is the number of action verbs, and O is the number of distinct objects in the world that the agent can interact with. As this is too large a space for a RL agent to effectively explore, the knowledge graph is used to prune this space by ranking actions based on their presence in the current knowledge graph and the relations between the objects in the graph as in <cite>(Ammanabrolu and Riedl, 2019)</cite> The architecture for the deep Q-network consists of two separate neural networks-encoding state and action separately-with the final Q-value for a state-action pair being the result of a pairwise interaction function between the two (Figure 1 ). We train with a standard DQN training loop; the policy is determined by the Q-value of a particular state-action pair, which is updated using the Bellman equation (Sutton and Barto, 2018) :",
  "y": "uses similarities"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_7",
  "x": "We use the parameters of the questionanswering system to pre-train portions of the deep Q-network for a different game within in the same domain. The portions that are pre-trained are the same parts of the architecture as in<cite> Ammanabrolu and Riedl (2019)</cite> . This game is referred to as the source task. The seeding of the knowledge graph is not strictly necessary but given that state-of-theart DRL agents cannot complete real games, this makes the agent more effective at the source task. We then transfer the knowledge and skills acquired from playing the source task to another game from the same genre-the target task. The parameters of the deep Q-network trained on the source game are used to initialize a new deep Qnetwork for the target task. All the weights indicated in the architecture of KG-DQN as shown in Fig. 1 are transferred. Unlike Rusu et al. (2016), we do not freeze the parameters of the deep Qnetwork trained on the source task nor use the two networks to jointly make decisions but instead just use it to initialize the parameters of the target task deep Q-network. This is done to account for the fact that although graph embeddings can be transferred between games, the actual graph extracted from a game is non-transferable due to differences in structure between the games.",
  "y": "similarities uses"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_8",
  "x": "We observe that in both of these domains, the complexity of the game increases steadily from the game used for the question-answering system to the target and then source task games. We perform ablation tests within each domain, mainly testing the effects of transfer from seeding, oracle-based question-answering, and sourceto-target parameter transfer. Additionally, there are a couple of extra dimensions of ablations that we study, specific to each of the domains and explained below. All experiments are run three times using different random seeds. For all the experiments we report metrics known to be important for transfer learning tasks (Taylor and Stone, 2009; Narasimhan et al., 2017) : average reward collected in the first 50 episodes (init. reward), average reward collected for 50 episodes after convergence (final reward), and number of steps taken to finish the game for 50 episodes after convergence (steps). For the metrics tested after convergence, we set = 0.1 following both Narasimhan et al. (2015) and<cite> Ammanabrolu and Riedl (2019)</cite> . We use similar hyperparameters to those reported in <cite>(Ammanabrolu and Riedl, 2019)</cite> for training the KG-DQN with action pruning, with the main difference being that we use 100 dimensional word embeddings instead of 50 dimensions for the horror genre. ---------------------------------- **SLICE OF LIFE EXPERIMENTS**",
  "y": "uses"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_9",
  "x": "Additionally, there are a couple of extra dimensions of ablations that we study, specific to each of the domains and explained below. All experiments are run three times using different random seeds. For all the experiments we report metrics known to be important for transfer learning tasks (Taylor and Stone, 2009; Narasimhan et al., 2017) : average reward collected in the first 50 episodes (init. reward), average reward collected for 50 episodes after convergence (final reward), and number of steps taken to finish the game for 50 episodes after convergence (steps). For the metrics tested after convergence, we set = 0.1 following both Narasimhan et al. (2015) and<cite> Ammanabrolu and Riedl (2019)</cite> . We use similar hyperparameters to those reported in <cite>(Ammanabrolu and Riedl, 2019)</cite> for training the KG-DQN with action pruning, with the main difference being that we use 100 dimensional word embeddings instead of 50 dimensions for the horror genre. ---------------------------------- **SLICE OF LIFE EXPERIMENTS** TextWorld uses a grammar to generate similar games. Following<cite> Ammanabrolu and Riedl (2019)</cite>, we use TextWorld's \"home\" theme to generate the games for the question-answering system.",
  "y": "similarities differences"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_10",
  "x": "---------------------------------- **SLICE OF LIFE EXPERIMENTS** TextWorld uses a grammar to generate similar games. Following<cite> Ammanabrolu and Riedl (2019)</cite>, we use TextWorld's \"home\" theme to generate the games for the question-answering system. TextWorld is a framework that uses a grammar to randomly generate game worlds and quests. This framework also gives us information such as instructions on how to finish the quest, and a list of actions that can be performed at each step based on the current world state. We do not let our agent access this additional solution information or admissible actions list. Given the relatively small quest length for TextWorld games-games can be completed in as little as 5 steps-we generate 50 such games and partition them into train and test sets in a 4:1 ratio. The traces are generated on the training set, and the question-answering system is evaluated on the test set.",
  "y": "uses"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_0",
  "x": "Furthermore, we obtain a considerable improvement on the Movie Review data set, reporting an accuracy of 93.3%, which represents an absolute gain of 10% over the stateof-the-art approach. ---------------------------------- **INTRODUCTION** In recent years, word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have had a huge impact in natural language processing (NLP) and related fields, being used in many tasks including sentiment analysis (Dos Santos and Gatti, 2014; Fu et al., 2018) , information retrieval (Clinchant and Perronnin, 2013; Ye et al., 2016) and word sense disambiguation (Bhingardive et al., 2015; Chen et al., 2014; Iacobacci et al., 2016) , among many others. Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations Cheng et al., 2018; Clinchant and Perronnin, 2013; Conneau et al., 2017; Cozma et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Kiros et al., 2015; Kusner et al., 2015; Le and Mikolov, 2014; Shen et al., 2018; Torki, 2018; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 . Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (Mitchell and Lapata, 2010) , it seems that more complex approaches usually yield better performance (Cheng et al., 2018; Conneau et al., 2017; Cozma et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Kiros et al., 2015; Torki, 2018; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 . To this end, we propose a simple yet effective approach for aggregating word embeddings into document embeddings. Our approach is inspired by the Vector of Locally-Aggregated Descriptors (VLAD) (J\u00e9gou et al., 2010 (J\u00e9gou et al., , 2012 used in computer vision to efficiently represent images for various image classification and retrieval tasks. To our knowledge, we are the first to adapt and use VLAD in the text domain.",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_1",
  "x": "**INTRODUCTION** In recent years, word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have had a huge impact in natural language processing (NLP) and related fields, being used in many tasks including sentiment analysis (Dos Santos and Gatti, 2014; Fu et al., 2018) , information retrieval (Clinchant and Perronnin, 2013; Ye et al., 2016) and word sense disambiguation (Bhingardive et al., 2015; Chen et al., 2014; Iacobacci et al., 2016) , among many others. Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations Cheng et al., 2018; Clinchant and Perronnin, 2013; Conneau et al., 2017; Cozma et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Kiros et al., 2015; Kusner et al., 2015; Le and Mikolov, 2014; Shen et al., 2018; Torki, 2018; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 . Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (Mitchell and Lapata, 2010) , it seems that more complex approaches usually yield better performance (Cheng et al., 2018; Conneau et al., 2017; Cozma et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Kiros et al., 2015; Torki, 2018; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 . To this end, we propose a simple yet effective approach for aggregating word embeddings into document embeddings. Our approach is inspired by the Vector of Locally-Aggregated Descriptors (VLAD) (J\u00e9gou et al., 2010 (J\u00e9gou et al., , 2012 used in computer vision to efficiently represent images for various image classification and retrieval tasks. To our knowledge, we are the first to adapt and use VLAD in the text domain. Our document-level representation is constructed as follows. First, we apply a pre-trained word embedding model, such as GloVe (Pennington et al., 2014) , on all the words from a set of training documents in order to obtain a set of training word vectors.",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_2",
  "x": "Thus, VLAWE is robust to vocabulary distribution gaps between training and test, which can appear when the training set is particularly smaller or from a different domain. Certainly, the robustness holds as long as the word embeddings are pretrained on a very large set of documents, e.g. the entire Wikipedia. We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier, namely Support Vector Machines (SVM), and show that it is useful for a diverse set of text classification tasks. We consider five benchmark data sets: Reuters-21578 (Lewis, 1997) , RT-2k (Pang and Lee, 2004) , MR (Pang and Lee, 2005) , TREC (Li and Roth, 2002) and Subj (Pang and Lee, 2004) . We compare VLAWE with recent stateof-the-art methods Cheng et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 , demonstrating the effectiveness of our approach. Furthermore, we obtain a considerable improvement on the Movie Review (MR) data set, surpassing the state-of-the-art approach of Cheng et al. (2018) by almost 10%. The rest of the paper is organized as follows. We present related works on learning documentlevel representations in Section 2. We describe the Vector of Locally-Aggregated Word Embeddings in Section 3.",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_3",
  "x": "---------------------------------- **RELATED WORK** There are various works Cheng et al., 2018; Conneau et al., 2017; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Kusner et al., 2015; Le and Mikolov, 2014; Clinchant and Perronnin, 2013; Shen et al., 2018; Torki, 2018; Zhao et al., 2015; Zhou et al., 2018 ) that propose to build effective sentence-level or document-level representations based on word embeddings. While most of these approaches are based on deep learning (Cheng et al., 2018; Conneau et al., 2017; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Zhao et al., 2015; Zhou et al., 2018) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (Clinchant and Perronnin, 2013) . The relationship between the bag-of-visual-words, Fisher Vectors and VLAD is discussed in (J\u00e9gou et al., 2012) . The discussion can be transferred to describe the relantionship of our work and the closely-related works of and Clinchant and Perronnin (2013) . ---------------------------------- **METHOD** The Vector of Locally-Aggregated Descriptors (VLAD) (J\u00e9gou et al., 2010 (J\u00e9gou et al., , 2012 was introduced in computer vision to efficiently represent images for various image classification and retrieval tasks.",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_4",
  "x": "We describe the Vector of Locally-Aggregated Word Embeddings in Section 3. We present experiments and results on various text classification tasks in Section 4. Finally, we draw our conclusion in Section 5. ---------------------------------- **RELATED WORK** There are various works Cheng et al., 2018; Conneau et al., 2017; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Kusner et al., 2015; Le and Mikolov, 2014; Clinchant and Perronnin, 2013; Shen et al., 2018; Torki, 2018; Zhao et al., 2015; Zhou et al., 2018 ) that propose to build effective sentence-level or document-level representations based on word embeddings. While most of these approaches are based on deep learning (Cheng et al., 2018; Conneau et al., 2017; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Zhao et al., 2015; Zhou et al., 2018) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (Clinchant and Perronnin, 2013) . The relationship between the bag-of-visual-words, Fisher Vectors and VLAD is discussed in (J\u00e9gou et al., 2012) . The discussion can be transferred to describe the relantionship of our work and the closely-related works of and Clinchant and Perronnin (2013) .",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_5",
  "x": "**EVALUATION AND IMPLEMENTATION DETAILS** In the experiments, we used the pre-trained word embeddings computed with the GloVe toolkit provided by Pennington et al. (2014) . The pre-trained GloVe model contains 300-dimensional vectors for 2.2 million words and phrases. Most of the steps required for building the VLAWE representation, such as the k-means clustering and the randomized forest of k-d trees, are implemented using the VLFeat library (Vedaldi and Fulkerson, 2008) . We set the number of clusters (size of the codebook) to k = 10, leading to a VLAWE representation of k \u00b7 d = 10 \u00b7 300 = 3000 components. Similar to J\u00e9gou et al. (2012) , we set \u03b1 = 0.5 for the power normalization step in Equation (4), which consistently leads to near-optimal results on all data sets. In the learning stage, we employ the Support Vector Machines (SVM) implementation provided by LibSVM (Chang and Lin, 2011 Table 1 : Performance results (in %) of our approach (VLAWE) versus several state-of-the-art methods Cheng et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 on the Reuters-21578, RT-2k, MR, TREC and Subj data sets. The top three results on each data set are highlighted in red, green and blue, respectively. Best viewed in color.",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_6",
  "x": "For optimal results, the VLAWE representation is combined with the BOSWE representation , which is based on the PQ kernel Popescu, 2013, 2015b) . We follow the same evaluation procedure as Kiros et al. (2015) and<cite> Hill et al. (2016)</cite> , using 10-fold cross-validation when a train and test split is not pre-defined for a given data set. As evaluation metrics, we employ the micro-averaged F 1 measure for the Reuters-21578 data set and the standard classification accuracy for the RT-2k, the MR, the TREC and the Subj data sets, in order to fairly compare with the related art. ---------------------------------- **RESULTS** We compare VLAWE with several state-of-theart methods Cheng et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 as well as two baseline methods, namely the average of word embeddings and the standard bag-of-words (BOW). The corresponding results are presented in Table 1 . First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (Le and Mikolov, 2014;<cite> Hill et al., 2016)</cite> . In most cases, our improvements over the baselines are higher than 5%.",
  "y": "similarities uses"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_7",
  "x": "set the SVM regularization parameter to C = 1 in all our experiments. In the SVM, we use the linear kernel. For optimal results, the VLAWE representation is combined with the BOSWE representation , which is based on the PQ kernel Popescu, 2013, 2015b) . We follow the same evaluation procedure as Kiros et al. (2015) and<cite> Hill et al. (2016)</cite> , using 10-fold cross-validation when a train and test split is not pre-defined for a given data set. As evaluation metrics, we employ the micro-averaged F 1 measure for the Reuters-21578 data set and the standard classification accuracy for the RT-2k, the MR, the TREC and the Subj data sets, in order to fairly compare with the related art. ---------------------------------- **RESULTS** We compare VLAWE with several state-of-theart methods Cheng et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 as well as two baseline methods, namely the average of word embeddings and the standard bag-of-words (BOW). The corresponding results are presented in Table 1 .",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_8",
  "x": "For optimal results, the VLAWE representation is combined with the BOSWE representation , which is based on the PQ kernel Popescu, 2013, 2015b) . We follow the same evaluation procedure as Kiros et al. (2015) and<cite> Hill et al. (2016)</cite> , using 10-fold cross-validation when a train and test split is not pre-defined for a given data set. As evaluation metrics, we employ the micro-averaged F 1 measure for the Reuters-21578 data set and the standard classification accuracy for the RT-2k, the MR, the TREC and the Subj data sets, in order to fairly compare with the related art. ---------------------------------- **RESULTS** We compare VLAWE with several state-of-theart methods Cheng et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 as well as two baseline methods, namely the average of word embeddings and the standard bag-of-words (BOW). The corresponding results are presented in Table 1 . First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (Le and Mikolov, 2014;<cite> Hill et al., 2016)</cite> . In most cases, our improvements over the baselines are higher than 5%.",
  "y": "differences"
 },
 {
  "id": "3ebfa05038431571701a7199163832_0",
  "x": "[12, 13, 14] have used this database to provide speech recognition systems with robustness to dysarthria. [15] trains various linear classifiers on TORGO and the NKI CCRT corpus [16] to detect dysarthria. More recently, [17] has trained fully connected neural networks to classify the severity of the disease, using TORGO and the UASPEECH [18] database. All these models are trained on standard low-level features. In this work we show that dysarthria detection benefits significantly from learning directly from the raw waveform. Previous work has explored learnable alternatives to speech features that rely on a similar computation to spectral representations [19, 20, 21, <cite>22,</cite> 5] . These approaches learn convolutions that are then passed through a non-linearity, eventually a pooling operator and then a log compression to replicate the dynamic range compression typically performed on spectrograms or mel-filterbanks. This compression function remains fixed and is chosen beforehand, which could impact the final performance, as various compression functions including logarithm, cubic root, or 10th root have been previously showed to perform better depending on the task (see Table 2 of [23] ). A second fixed component is the meanvariance normalization of speech features.",
  "y": "background"
 },
 {
  "id": "3ebfa05038431571701a7199163832_1",
  "x": "These approaches learn convolutions that are then passed through a non-linearity, eventually a pooling operator and then a log compression to replicate the dynamic range compression typically performed on spectrograms or mel-filterbanks. This compression function remains fixed and is chosen beforehand, which could impact the final performance, as various compression functions including logarithm, cubic root, or 10th root have been previously showed to perform better depending on the task (see Table 2 of [23] ). A second fixed component is the meanvariance normalization of speech features. [5] integrates this normalization into the neural architecture, but keeps it fixed during training. [24] introduces a computational block, the Per Channel Energy Normalization (PCEN) that can learn a compression and a normalization factor per channel, and can be integrated into a neural network on top of speech features. It has since then been used in production speech recognition systems [25] . In this work, we start from an attention-based model on mel-filterbanks, which already outperforms an equivalent model trained on low-level descriptors (LLDs). Our experiments show that by training a PCEN block on top of mel-filterbanks or replacing them by learnable time-domain filterbanks from<cite> [22]</cite> , we get a gain in accuracy around 10% in absolute when training an identical neural network for dysarthria detection. Finally, by combining time-domain filterbanks and PCEN we propose the first audio frontend that can learn features, compression and normalization jointly with a neural network using backpropagation.",
  "y": "uses"
 },
 {
  "id": "3ebfa05038431571701a7199163832_2",
  "x": "As the first step of our computational pipeline, we use TimeDomain filterbanks from<cite> [22]</cite> . Time-Domain filterbanks are neural network layers that take the raw waveform as input. They can be initialized to replicate mel-filterbanks, and then learnt for the task at hand. The standard computation of melfilterbanks relies on passing a spectrogram through a bank of frequency domain filters. More formally, the n th melfilterbank of a signal in t is: where is the waveform windowed with an Hanning function \u03c6 centered in t, (\u03c8 n ) n=1...N the N melfilters andf denotes the Fourier transform of f . [26] shows that these coefficient can be approximated in the time domain by the following computation, referred as the first order scattering transform: where (\u03d5 n ) n=1...N are Gabor wavelets defined in<cite> [22]</cite> such that |\u03c6 n | 2 \u2248 |\u03c8 n | 2 .<cite> [22]</cite> shows that this computation can be implemented as neural network layers, referred as TimeDomain filterbanks (TD-filterbanks). The waveform goes through a complex-valued convolution, a modulus operator and the a convolution with a lowpass-filter (the squared hanning window) that performs the decimation.",
  "y": "uses"
 },
 {
  "id": "3ebfa05038431571701a7199163832_3",
  "x": "They can be initialized to replicate mel-filterbanks, and then learnt for the task at hand. The standard computation of melfilterbanks relies on passing a spectrogram through a bank of frequency domain filters. More formally, the n th melfilterbank of a signal in t is: where is the waveform windowed with an Hanning function \u03c6 centered in t, (\u03c8 n ) n=1...N the N melfilters andf denotes the Fourier transform of f . [26] shows that these coefficient can be approximated in the time domain by the following computation, referred as the first order scattering transform: where (\u03d5 n ) n=1...N are Gabor wavelets defined in<cite> [22]</cite> such that |\u03c6 n | 2 \u2248 |\u03c8 n | 2 .<cite> [22]</cite> shows that this computation can be implemented as neural network layers, referred as TimeDomain filterbanks (TD-filterbanks). The waveform goes through a complex-valued convolution, a modulus operator and the a convolution with a lowpass-filter (the squared hanning window) that performs the decimation. When not combined with PCEN, a log-compression is added on top of TD-filterbanks after adding 1 to their absolute value to avoid numerical issues. Table 1 shows the detailed layers.",
  "y": "uses background"
 },
 {
  "id": "3ebfa05038431571701a7199163832_4",
  "x": "Following<cite> [22]</cite> , the first 1D convolution filters are initialized with Gabor wavelets, to replicate mel-filterbanks, and are then learnt at the same time as the rest of the model. The second convolution layer is kept fixed as a squared hanning window to perform lowpass filtering. ---------------------------------- **PER CHANNEL ENERGY NORMALIZATION** Per Channel Energy Normalization (PCEN) is a learnable component introduced in [24] which computes parametrized normalization and compression. It replaces the log-compression and the mean-variance normalization. With E(t, f ) the value of the feature f at time t, the computation of PCEN is: M (t, f ) is a moving average of the feature f along the time axis, defined as: \u03b1 controls the strength of the normalization, the exponent r (typically in [0, 1]) defines the slope of the compression curve, s sets the spread of the moving average, and is a small scalar used to avoid division by zero.",
  "y": "uses"
 },
 {
  "id": "3fe979e570992b79c8656ab6cb34fb_0",
  "x": "**INTRODUCTION** We describe a computational verb lexicon called VerbNet which utilizes Levin verb classes (Levin, 1993) to systematically construct lexical entries. We have used Lexicalized Tree Adjoining Grammar (LTAG) (Joshi, 1985; Schabes, 1990) to capture the syntax associated with each verb class, and have added semantic predicates. We also show how regular extensions of verb meaning can be achieved through the adjunction of particular syntactic phrases. We base these regular extensions on intersective Levin classes, a fine-grained variation on Levin classes, as a source of semantic components associated with specific adjuncts <cite>(Dang et al., 1998)</cite> . Whereas previous research on tying semantics to Levin classes (Dorr, 1997) has not explicitly implemented the close relation between syntax and semantics hypothesized by Levin, our lexical resource combines traditional lexical semantic information, such as thematic roles and semantic predicates, with syntactic frames and selectional restrictions. In order to increase the utility of VerbNet, we also include links to entries in WordNet, which is one of the most widely used online lexical databases in Natural Language Processing applications. ---------------------------------- **LEVIN CLASSES AND WORDNET**",
  "y": "uses background"
 },
 {
  "id": "3fe979e570992b79c8656ab6cb34fb_1",
  "x": "The core meaning of this verb class is exertion of force. Adjunction of a path PP implying motion modifies membership of these verbs to the Carry class. Push/Pull verbs can appear in the conative construction, which emphasizes their forceful semantic component and ability to express an attempted action where any result that might be associated with the verb is not necessarily achieved; Carry verbs (used with a goal or directional phrase) cannot take the conative alternation because this would conflict with the causation of motion which is the intrinsic meaning of the class <cite>(Dang et al., 1998)</cite> . Palmer et al. (1999) and Bleam et al. (1998) also defined compositional semantics for classes of verbs implemented in FB-LTAG, but they represented general semantic components (e.g., motion, manner) as features on the nodes of the trees. Our use of separate logical forms gives a more detailed semantics for the sentence, so that for an event involving motion, it is possible to know not only that the event has a motion semantic component, but also which entity is actually in motion. ---------------------------------- **CONCLUSION** We have presented a class-based approach to building a verb lexicon that makes explicit the close association between syntax and semantics, as postulated by Levin. By using verb classes we capture generalizations about verb behavior and reduce not only the effort needed to construct the lexicon, but also the likelihood that errors are introduced when adding new verbs.",
  "y": "background"
 },
 {
  "id": "40d73d5fc22686c13a14946946dd18_0",
  "x": "Recently, several neural mechanisms have been used to train end-to-end SRL models that do not require task-specific feature engineering as the traditional SRL models do. Zhou and Xu (2015) introduced the first deep end-to-end model for SRL using a stacked Bi-LSTM network with a conditional random field (CRF) as the top layer. He et al. (2017) simplified their architecture using a highway Bi-LSTM network. More recently, <cite>Tan et al. (2018)</cite> replaced the common recurrent architecture with a self-attention network, directly capturing relationships between tokens regardless of their distance, resulting in better results and faster training. The work in deep end-to-end SRL has focused heavily on applying deep learning advances without considering the multilingual aspect. However, language-specific characteristics and the available amount of training data highly influence the optimal model structure. DAMESRL facilitates exploration and fair evaluation of new SRL models for different languages by providing flexible neural model construction on different modeling levels, the handling of various input and output formats, and clear output visualization. Beyond the existing state-of-the-art models (Zhou and Xu, 2015; He et al., 2017; <cite>Tan et al., 2018</cite> ), we exploit character-level modeling, beneficial when considering multiple languages. To demonstrate the merits of easy cross-lingual exploration and evaluation of model structures for SRL provided by DAMESRL, we report performance of several distinct models integrated into our framework for English, German and Arabic, as they have very different linguistic characteristics.",
  "y": "background"
 },
 {
  "id": "40d73d5fc22686c13a14946946dd18_1",
  "x": "He et al. (2017) simplified their architecture using a highway Bi-LSTM network. More recently, <cite>Tan et al. (2018)</cite> replaced the common recurrent architecture with a self-attention network, directly capturing relationships between tokens regardless of their distance, resulting in better results and faster training. The work in deep end-to-end SRL has focused heavily on applying deep learning advances without considering the multilingual aspect. However, language-specific characteristics and the available amount of training data highly influence the optimal model structure. DAMESRL facilitates exploration and fair evaluation of new SRL models for different languages by providing flexible neural model construction on different modeling levels, the handling of various input and output formats, and clear output visualization. Beyond the existing state-of-the-art models (Zhou and Xu, 2015; He et al., 2017; <cite>Tan et al., 2018</cite> ), we exploit character-level modeling, beneficial when considering multiple languages. To demonstrate the merits of easy cross-lingual exploration and evaluation of model structures for SRL provided by DAMESRL, we report performance of several distinct models integrated into our framework for English, German and Arabic, as they have very different linguistic characteristics. by w p . Here, words outside argument spans have the tag O, and words at the beginning and inside of argument spans with role r have the tags B r and I r , respectively.",
  "y": "background"
 },
 {
  "id": "40d73d5fc22686c13a14946946dd18_2",
  "x": "DAMESRL also provides an HTML format that can be directly visualized in the web browser (as in Fig. 2 ). ---------------------------------- **MODEL CONSTRUCTION MODULES** As can be seen in Fig. 1 , the framework divides model construction in four phases: (I) word representation, (II) sentence representation, (III) output modeling, and (IV) inference. Phase I: The word representation of a word w i consist of three optional concatenated components: a word-embedding, a Boolean indicating if w i is the predicate of the semantic frame (w p ), and a character representation. DAMESRL provides a Bi-LSTM network to learn character-level word representations helping for languages where important SRL cues are given through inflections, such as case markings in German and Arabic. Despite the foreseen importance, character-level embeddings have not been used in previous work (Zhou and Xu, 2015; He et al., 2017; <cite>Tan et al., 2018</cite>) . Phase II: As core sequence representation component, users can choose between a self-attention encoding (<cite>Tan et al., 2018</cite>) , a regular Bi-LSTM (Hochreiter and Schmidhuber, 1997) or a highway Bi-LSTM (Zhang et al., 2016; He et al., 2017) . Phase III: To compute model probabilities, users can choose a regular softmax, or a linear chain CRF as proposed by (Zhou and Xu, 2015) , which can be useful for languages where word order is an important SRL cue, such as English, or when less training data is available (shown in Section 4).",
  "y": "background"
 },
 {
  "id": "40d73d5fc22686c13a14946946dd18_3",
  "x": "DAMESRL provides a Bi-LSTM network to learn character-level word representations helping for languages where important SRL cues are given through inflections, such as case markings in German and Arabic. Despite the foreseen importance, character-level embeddings have not been used in previous work (Zhou and Xu, 2015; He et al., 2017; <cite>Tan et al., 2018</cite>) . Phase II: As core sequence representation component, users can choose between a self-attention encoding (<cite>Tan et al., 2018</cite>) , a regular Bi-LSTM (Hochreiter and Schmidhuber, 1997) or a highway Bi-LSTM (Zhang et al., 2016; He et al., 2017) . Phase III: To compute model probabilities, users can choose a regular softmax, or a linear chain CRF as proposed by (Zhou and Xu, 2015) , which can be useful for languages where word order is an important SRL cue, such as English, or when less training data is available (shown in Section 4). Phase IV: The inference phase provides two options for label inference from the computed model probabilities including greedy prediction and Viterbi decoding. ---------------------------------- **EXPERIMENTS 4.1 SETTINGS** To evaluate our framework, and show the benefits of choosing certain model components, we construct five models: HLstm, Char, CRFm, Att, and CharAtt, whose configurations are shown in Tab. 1.",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_0",
  "x": "****SPMRL'13 SHARED TASK SYSTEM: THE CADIM ARABIC DEPENDENCY PARSER**** **ABSTRACT** We describe the submission from the Columbia Arabic & Dialect Modeling group (CADIM) for the Shared Task at the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL'2013). We participate in the Arabic Dependency parsing task for predicted POS tags and features. Our system is based on <cite>Marton et al. (2013)</cite> . ---------------------------------- **INTRODUCTION** In this paper, we discuss the system that the Columbia Arabic & Dialect Modeling group (CADIM) submitted to the 2013 Shared Task on Parsing Morphologically Rich Languages (Seddah et al., 2013) . We used a system for Arabic dependency parsing which we had previously developed, but retrained it on the training data splits used in this task.",
  "y": "uses"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_1",
  "x": "We first summarize our previous work (Section 2). We then discuss our submission and the results (Section 3). ---------------------------------- **APPROACH** In this section, we summarize <cite>Marton et al. (2013)</cite> . We first present some background information on Arabic morphology and then discuss our methodology and main results. We present our best performing set of features, which we also use in our SPMRL'2013 submission. ---------------------------------- **BACKGROUND**",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_2",
  "x": "For example, the DET and STATE features alone both help parsing because they help identify the idafa construction (the modificiation of a nominal by a genitive noun phrase), but they are redundant with each other and the DET feature is more helpful since it also helps with adjectival modification of nouns. Finally, the accuracy of automatically predicting the feature values (ratio of correct predictions out of all predictions) of course affects the value of a feature on unseen text. Even if relevant and non-redundant, a feature may be hard to predict with sufficient accuracy by current technology, in which case it will be of little or no help for parsing, even if helpful when its gold values are provided. The CASE feature is very relevant and not redundant, but it cannot be predicted with high accuracy and overall it is not useful. Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors. It has been shown previously that if the relevant morphological features in assignment configurations can be recognized well enough, then they contribute to parsing accuracy. For example, modeling CASE in Czech improves Czech parsing (Collins et al., 1999) : CASE is relevant, not redundant, and can be predicted with sufficient accuracy. However, it had been more difficult showing that agreement morphology helps parsing, with negative results for dependency parsing in several languages Eryigit et al., 2008; Nivre, 2009) . In contrast to these negative results, <cite>Marton et al. (2013)</cite> showed positive results for using agreement morphology for Arabic.",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_3",
  "x": "For example, modeling CASE in Czech improves Czech parsing (Collins et al., 1999) : CASE is relevant, not redundant, and can be predicted with sufficient accuracy. However, it had been more difficult showing that agreement morphology helps parsing, with negative results for dependency parsing in several languages Eryigit et al., 2008; Nivre, 2009) . In contrast to these negative results, <cite>Marton et al. (2013)</cite> showed positive results for using agreement morphology for Arabic. ---------------------------------- **METHODOLOGY** In <cite>Marton et al. (2013)</cite> , we investigated morphological features for dependency parsing of Modern Standard Arabic (MSA). The goal was to find a set of relevant, accurate and non-redundant features. <cite>We</cite> used both the MaltParser (Nivre, 2008) and the Easy-First Parser (Goldberg and Elhadad, 2010) . Since the Easy-First Parser performed better, we use it in all experiments reported in this paper.",
  "y": "motivation"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_5",
  "x": "---------------------------------- **METHODOLOGY** In <cite>Marton et al. (2013)</cite> , we investigated morphological features for dependency parsing of Modern Standard Arabic (MSA). The goal was to find a set of relevant, accurate and non-redundant features. <cite>We</cite> used both the MaltParser (Nivre, 2008) and the Easy-First Parser (Goldberg and Elhadad, 2010) . Since the Easy-First Parser performed better, we use it in all experiments reported in this paper. For MSA, the space of possible morphological features is quite large. We determined which morphological features help by performing a search through the feature space. In order to do this, we separated part-of-speech (POS) from the morphological features.",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_6",
  "x": "In contrast to these negative results, <cite>Marton et al. (2013)</cite> showed positive results for using agreement morphology for Arabic. ---------------------------------- **METHODOLOGY** In <cite>Marton et al. (2013)</cite> , we investigated morphological features for dependency parsing of Modern Standard Arabic (MSA). The goal was to find a set of relevant, accurate and non-redundant features. <cite>We</cite> used both the MaltParser (Nivre, 2008) and the Easy-First Parser (Goldberg and Elhadad, 2010) . Since the Easy-First Parser performed better, we use it in all experiments reported in this paper. For MSA, the space of possible morphological features is quite large. We determined which morphological features help by performing a search through the feature space.",
  "y": "uses"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_7",
  "x": "In <cite>Marton et al. (2013)</cite> , we showed that by modeling the functional morphology rather than the form-based morphology, we obtain a further increase in parsing performance (2013) test (old split) 81.0 84.0 92.7 Table 2 : Results of our system on Shared Task test data, Gold Tokenization, Predicted Morphological Tags; and for reference also on the data splits used in our previous work <cite>(Marton et al., 2013)</cite> ; \"\u2264 70\" refers to the test sentences with 70 or fewer words. Training Set Test Set Labeled Tedeval Score Unlabeled Tedeval Score 5K (SPMRL'2013) test \u2264 70 86.4 89.9 All (SPMRL'2013) test \u2264 70 87.8 90.8 Table 3 : Results of our system on on Shared Task test data, Predicted Tokenization, Predicted Morphological Tags; \"\u2264 70\" refers to the test sentences with 70 or fewer words (again, both when using gold and when using predicted POS and morphological features). We also showed that for parsing with predicted POS and morphological features, training on a combination of gold and predicted POS and morphological feature values outperforms the alternative training scenarios. ---------------------------------- **BEST PERFORMING FEATURE SET** The best performing set of features on non-gold input, obtained in <cite>Marton et al. (2013)</cite> , are shown in Table 1 . The features are clustered into three types. \u2022 First is part-of-speech, represented using a \"core\" 12-tag set. \u2022 Second are the inflectional morphological features: determiner clitic, person and functional gender and number.",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_8",
  "x": "Training Set Test Set Labeled Tedeval Score Unlabeled Tedeval Score 5K (SPMRL'2013) test \u2264 70 86.4 89.9 All (SPMRL'2013) test \u2264 70 87.8 90.8 Table 3 : Results of our system on on Shared Task test data, Predicted Tokenization, Predicted Morphological Tags; \"\u2264 70\" refers to the test sentences with 70 or fewer words (again, both when using gold and when using predicted POS and morphological features). We also showed that for parsing with predicted POS and morphological features, training on a combination of gold and predicted POS and morphological feature values outperforms the alternative training scenarios. ---------------------------------- **BEST PERFORMING FEATURE SET** The best performing set of features on non-gold input, obtained in <cite>Marton et al. (2013)</cite> , are shown in Table 1 . The features are clustered into three types. \u2022 First is part-of-speech, represented using a \"core\" 12-tag set. \u2022 Second are the inflectional morphological features: determiner clitic, person and functional gender and number. \u2022 Third are the rationality (humanness) feature, which participates in morphosyntactic agreement in Arabic (Alkuhlani and Habash, 2011) , and a form of the lemma, which abstract over all inflectional morphology.",
  "y": "uses"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_9",
  "x": "---------------------------------- **DATA PREPARATION** The data split used in the shared task is different from the data split we used in <cite>(Marton et al., 2013)</cite> , so we retrained our models on the new splits (Diab et al., 2013) . The data released for the Shared Task showed inconsistent availability of lemmas across gold and predicted input, so we used the ALMOR analyzer (Habash, 2007) with the SAMA databases (Graff et al., 2009 ) to determine a lemma given the word form and the provided (gold or predicted) POS tags. In addition to the lemmas, the ALMOR analyzer also provides morphological features in the feature-value representation our approach requires. Finally, we ran our existing converter (Alkuhlani and Habash, 2012) over this representation to obtain functional number and gender, as well as the rationality feature. 3 For simplicity reasons, we used the MLE:W2+CATiB model (Alkuhlani and Habash, 2012) , which was the best performing model on seen words, as opposed to the combination system that used a syntactic component with better results on unseen words. We did not perform Alif or Ya normalization on the data. We trained two models: one on 5,000 sentences of training data and one on the entire training data.",
  "y": "differences"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_10",
  "x": "We did not perform Alif or Ya normalization on the data. We trained two models: one on 5,000 sentences of training data and one on the entire training data. ---------------------------------- **RESULTS** Our performance in the Shared Task for Arabic Dependency, Gold Tokenization, Predicted Tags, is shown in Table 2 . Our performance in the Shared Task for Arabic Dependency, Predicted Tokenization, Predicted Tags, is shown in Table 3 . For predicted tokenization, only the IMS/Szeged system which uses system combination (Run 2) outperformed our parser on all measures; our parser performed better than all other single-parser systems. For gold tokenization, our system is the second best single-parser system after the IMS/Szeged single system (Run 1). For gold tokenization and predicted morphology (Table 2) , we also give the performance reported in our previous work <cite>(Marton et al., 2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_11",
  "x": "We did not perform Alif or Ya normalization on the data. We trained two models: one on 5,000 sentences of training data and one on the entire training data. ---------------------------------- **RESULTS** Our performance in the Shared Task for Arabic Dependency, Gold Tokenization, Predicted Tags, is shown in Table 2 . Our performance in the Shared Task for Arabic Dependency, Predicted Tokenization, Predicted Tags, is shown in Table 3 . For predicted tokenization, only the IMS/Szeged system which uses system combination (Run 2) outperformed our parser on all measures; our parser performed better than all other single-parser systems. For gold tokenization, our system is the second best single-parser system after the IMS/Szeged single system (Run 1). For gold tokenization and predicted morphology (Table 2) , we also give the performance reported in our previous work <cite>(Marton et al., 2013)</cite> .",
  "y": "differences"
 },
 {
  "id": "41bd8c692ac513b8a9cabbd5aafbda_0",
  "x": "Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, 9 ]. The word2vec <cite>[10]</cite> is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather ---------------------------------- **** knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, 5, 6] . How to extract useful information from unannotated large scale corpus has been a research issue.",
  "y": "background"
 },
 {
  "id": "41bd8c692ac513b8a9cabbd5aafbda_1",
  "x": "Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather ---------------------------------- **** knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, 5, 6] . How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, 9] . The word2vec <cite>[10]</cite> is among the most widely used word embedding models today.",
  "y": "background"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_0",
  "x": "---------------------------------- **INTRODUCTION** PredPatt 1 <cite>(White et al., 2016</cite> ) is a pattern-based framework for predicate-argument extraction. It defines a set of interpretable, extensible and non-lexicalized patterns based on Universal Dependencies (UD) (de Marneffe et al., 2014) , and extracts predicates and arguments through these manual patterns. Figure 1 shows the predicates and arguments extracted by PredPatt from the sentence: \"Chris, the designer, wants to launch a new brand.\" The underlying predicate-argument structure constructed by PredPatt is a directed graph, where a special dependency ARG is built between a predicate head token and its arguments' head tokens, and the original UD relations are retained within predicate phrases and argument phrases. For example, Figure 2 shows the directed graph for the predicate-argument extraction (1) and (2) in Figure 1 . Compared to other existing systems for predicate-argument extraction (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015) , the use of manual language-agnostic patterns on UD makes PredPatt a well-founded component across languages. Additionally, the underlying structure constructed by PredPatt has been shown to be a well-formed syntax-semantics interface for NLP tasks: Zhang et al. (2016) utilizes PredPatt to extract possibilistic propositions in automatic common-sense inference generation. White et al. (2016) uses PredPatt to help augmenting data with Universal Decompositional Semantics.",
  "y": "background"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_1",
  "x": "Compared to other existing systems for predicate-argument extraction (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015) , the use of manual language-agnostic patterns on UD makes PredPatt a well-founded component across languages. Additionally, the underlying structure constructed by PredPatt has been shown to be a well-formed syntax-semantics interface for NLP tasks: Zhang et al. (2016) utilizes PredPatt to extract possibilistic propositions in automatic common-sense inference generation. White et al. (2016) uses PredPatt to help augmenting data with Universal Decompositional Semantics. Zhang et al. (2017) adapts PredPatt to data generation for cross-lingual open information extraction. However, the evaluation of PredPatt has been restricted to manually-checked extractions over a small set of sentences<cite> (White et al., 2016)</cite> , which lacks gold annotations to conduct an objective and reproducible evaluation, and inhibits the updates of patterns in PredPatt. Chris , the designer , wants to launch a new brand . In this work, we aim to conduct a large-scale and reproducible evaluation of PredPatt by introducing a large set of gold annotations gathered from PropBank (Palmer et al., 2005) . We leverage these gold annotations to improve PredPatt and compare it with other prominent systems. The evaluation results demonstrate that we make a promising improvement on PredPatt, and it significantly outperforms other comparing systems.",
  "y": "motivation background"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_2",
  "x": "Zhang et al. (2017) adapts PredPatt to data generation for cross-lingual open information extraction. However, the evaluation of PredPatt has been restricted to manually-checked extractions over a small set of sentences<cite> (White et al., 2016)</cite> , which lacks gold annotations to conduct an objective and reproducible evaluation, and inhibits the updates of patterns in PredPatt. Chris , the designer , wants to launch a new brand . In this work, we aim to conduct a large-scale and reproducible evaluation of PredPatt by introducing a large set of gold annotations gathered from PropBank (Palmer et al., 2005) . We leverage these gold annotations to improve PredPatt and compare it with other prominent systems. The evaluation results demonstrate that we make a promising improvement on PredPatt, and it significantly outperforms other comparing systems. The scripts for creating gold annotations and evaluation are available at: https: //github.com/hltcoe/PredPatt/tree/master/eval ---------------------------------- **CREATING GOLD ANNOTATIONS**",
  "y": "motivation extends"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_3",
  "x": "PredPatt is a pattern-based system, comprising an extensible set of clean, interpretable linguistic patterns over UD parses. By analyzing PredPatt extractions in comparison with gold annotations (Sec. 2), we are able to refine and improve PredPatt's pattern set. From the auto-converted gold annotations, we create a held-out set by randomly sampling 10% sentences from EWT. We then update the existing PredPatt patterns and introduce new patterns by analyzing PredPatt annotations on the held-out set. PredPatt extracts predicates and arguments in four stages<cite> (White et al., 2016)</cite> : (1) predicate and argument root identification, (2) argument resolution, (3) predicate and argument phrase extraction, and (4) optional post-processing. We analyze PredPatt extraction in each of these stages on the held-out set, and make 19 improvements to PredPatt patterns. Due to lack of space, we only highlight one improvement for each stage below. Fixed-MWE-pred: The UD version 2.0 introduces a new dependency relation fixed for identifying fixed function-word \"multiword expressions\" (MWEs). To accommodate this new feature, we add patterns to identify the MWE predicate and its argument.",
  "y": "background"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_4",
  "x": "We then update the existing PredPatt patterns and introduce new patterns by analyzing PredPatt annotations on the held-out set. PredPatt extracts predicates and arguments in four stages<cite> (White et al., 2016)</cite> : (1) predicate and argument root identification, (2) argument resolution, (3) predicate and argument phrase extraction, and (4) optional post-processing. We analyze PredPatt extraction in each of these stages on the held-out set, and make 19 improvements to PredPatt patterns. Due to lack of space, we only highlight one improvement for each stage below. Fixed-MWE-pred: The UD version 2.0 introduces a new dependency relation fixed for identifying fixed function-word \"multiword expressions\" (MWEs). To accommodate this new feature, we add patterns to identify the MWE predicate and its argument. As shown in Figure 3 , the predicate root in this case is the dependent of fixed that is tagged as a verb (i.e., \"opposed\"); the root of its argument is the token which indirectly governs the predicate root via the case and fixed relation (i.e., \"one\"). Please use this new file as opposed to the one I sent earlier . Cut-complex-pred: The existing patterns take clausal complements (ccomp and xcomp) as predicatives of complex predicates in the argument resolution stage, where the arguments of the clausal complement will be merged into the argument set of their head predicate.",
  "y": "extends background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_0",
  "x": "**ABSTRACT** Self-attention networks (SANs) have drawn increasing interest due to their high parallelization in computation and flexibility in modeling dependencies. SANs can be further enhanced with multi-head attention by allowing the model to attend to information from different representation subspaces. In this work, we propose novel convolutional self-attention networks, which offer SANs the abilities to 1) strengthen dependencies among neighboring elements, and 2) model the interaction between features extracted by multiple attention heads. Experimental results of machine translation on different language pairs and model settings show that our approach outperforms both the strong Transformer baseline and other existing models on enhancing the locality of SANs. Comparing with prior studies, the proposed model is parameter free in terms of introducing no more parameters. ---------------------------------- **INTRODUCTION** Self-attention networks (SANs) (Parikh et al., 2016; Lin et al., 2017) have shown promising empirical results in various natural language processing (NLP) tasks, such as machine translation <cite>(Vaswani et al., 2017)</cite> , natural language inference (Shen et al., 2018a) , and acoustic modeling (Sperber et al., 2018) .",
  "y": "background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_1",
  "x": "SANs can be further enhanced with multi-head attention by allowing the model to attend to information from different representation subspaces. In this work, we propose novel convolutional self-attention networks, which offer SANs the abilities to 1) strengthen dependencies among neighboring elements, and 2) model the interaction between features extracted by multiple attention heads. Experimental results of machine translation on different language pairs and model settings show that our approach outperforms both the strong Transformer baseline and other existing models on enhancing the locality of SANs. Comparing with prior studies, the proposed model is parameter free in terms of introducing no more parameters. ---------------------------------- **INTRODUCTION** Self-attention networks (SANs) (Parikh et al., 2016; Lin et al., 2017) have shown promising empirical results in various natural language processing (NLP) tasks, such as machine translation <cite>(Vaswani et al., 2017)</cite> , natural language inference (Shen et al., 2018a) , and acoustic modeling (Sperber et al., 2018) . One appealing strength of SANs lies in their ability to capture dependencies regardless of distance by explicitly attending to all the elements. In addition, the performance of SANs can be improved by multi-head attention <cite>(Vaswani et al., 2017)</cite> , which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace.",
  "y": "background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_2",
  "x": "Thus, the proposed model allows each head to interact local features with its adjacent subspaces at attention time. We expect that the interaction across different subspaces can further improve the performance of SANs. We evaluate the effectiveness of the proposed model on three widely-used translation tasks: WMT14 English-to-German, WMT17 Chineseto-English, and WAT17 Japanese-to-English. Experimental results demonstrate that our approach consistently improves performance over the strong TRANSFORMER model <cite>(Vaswani et al., 2017)</cite> across language pairs. Comparing with previous work on modeling locality for SANs (e.g. Shaw et al., 2018; Sperber et al., 2018) , our model boosts performance on both translation quality and training efficiency. 2 Multi-Head Self-Attention Networks SANs produce representations by applying attention to each pair of tokens from the input sequence, regardless of their distance. Vaswani et al. (2017) found it is beneficial to capture different contextual features with multiple individual attention functions. Given an input sequence X = {x 1 , . . . , x I } \u2208 R I\u00d7d , the model first transforms it into queries Q, keys K, and values V:",
  "y": "differences"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_3",
  "x": "Comparing with previous work on modeling locality for SANs (e.g. Shaw et al., 2018; Sperber et al., 2018) , our model boosts performance on both translation quality and training efficiency. 2 Multi-Head Self-Attention Networks SANs produce representations by applying attention to each pair of tokens from the input sequence, regardless of their distance. Vaswani et al. (2017) found it is beneficial to capture different contextual features with multiple individual attention functions. Given an input sequence X = {x 1 , . . . , x I } \u2208 R I\u00d7d , the model first transforms it into queries Q, keys K, and values V: where where ATT(\u00b7) is an attention model (Bahdanau et al., 2015;<cite> Vaswani et al., 2017)</cite> that retrieves the keys K h with the query q h i . The final output representation O is the concatenation of outputs generated by multiple attention models: 3 Approach",
  "y": "background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_4",
  "x": "In the context of distance-aware SANs, Shaw et al. (2018) introduced relative position encoding to consider the relative distances between sequence elements. While they modeled locality from position embedding, we improve locality modeling from revising attention scope. To make a fair comparison, we re-implemented the above approaches under a same framework. Empirical results on machine translation tasks show the superiority of our approach in both translation quality and training efficiency. Multi-Head Attention Multi-head attention mechanism <cite>(Vaswani et al., 2017)</cite> employs different attention heads to capture distinct features (Raganato and Tiedemann, 2018) . Along this direction, Shen et al. (2018a) explicitly used multiple attention heads to model different dependencies of the same word pair, and Strubell et al. (2018) employed different attention heads to capture different linguistic features. Li et al. (2018) introduced disagreement regularizations to encourage the diversity among attention heads. Inspired by recent successes on fusing information across layers (Dou et al., 2018 , proposed to aggregate information captured by different attention heads. Based on these findings, we model interactions among attention heads to exploit the richness of local properties distributed in different heads.",
  "y": "background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_5",
  "x": "We conducted experiments with the Transformer model <cite>(Vaswani et al., 2017)</cite> on English\u21d2German (En\u21d2De), Chinese\u21d2English (Zh\u21d2En) and Japanese\u21d2English (Ja\u21d2En) translation tasks. For the En\u21d2De and Zh\u21d2En tasks, the models were trained on widely-used WMT14 and WMT17 corpora, consisting of around 4.5 and 20.62 million sentence pairs, respectively. Concerning Ja\u21d2En, we used the first two sections of WAT17 corpus as the training data, which consists of 2M sentence pairs. To reduce the vocabulary size, all the data were tokenized and segmented into subword symbols using byte-pair encoding (Sennrich et al., 2016) with 32K merge operations. Following Shaw et al. (2018) , we incorporated the proposed model into the encoder, which is a stack of 6 SAN layers. Prior studies revealed that modeling locality in lower layers can achieve better performance (Shen et al., 2018b; Yu et al., 2018; , we applied our approach to the lowest three layers of the encoder. About configurations of NMT models, we used the Base and Big settings same as<cite> Vaswani et al. (2017)</cite> , and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens. ---------------------------------- **EFFECTS OF WINDOW/AREA SIZE**",
  "y": "uses"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_6",
  "x": "To reduce the vocabulary size, all the data were tokenized and segmented into subword symbols using byte-pair encoding (Sennrich et al., 2016) with 32K merge operations. Following Shaw et al. (2018) , we incorporated the proposed model into the encoder, which is a stack of 6 SAN layers. Prior studies revealed that modeling locality in lower layers can achieve better performance (Shen et al., 2018b; Yu et al., 2018; , we applied our approach to the lowest three layers of the encoder. About configurations of NMT models, we used the Base and Big settings same as<cite> Vaswani et al. (2017)</cite> , and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens. ---------------------------------- **EFFECTS OF WINDOW/AREA SIZE** We first investigated the effects of window size (1D-CSANs) and area size (2D-CSANs) on En\u21d2De validation set, as plotted in Figure 2 . For 1D-CSANs, the local size with 11 is superior to other settings. This is consistent with Luong et al. (2015) who found that 10 is the best window size in their local attention experiments.",
  "y": "similarities"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_0",
  "x": "**INTRODUCTION** Recurrent neural networks (RNNs), in particular Long Short-Term Memory networks (LSTMs), have become a dominant tool in natural language processing. While LSTMs appear to be a natural choice for modeling sequential data, recently a class of non-recurrent models (Gehring et al., 2017; Vaswani et al., 2017) have shown competitive performance on sequence modeling. Gehring et al. (2017) propose a fully convolutional sequence-tosequence model that achieves state-of-the-art performance in machine translation. Vaswani et al. (2017) introduce Transformer networks that do not use any convolution or recurrent connections while obtaining the best translation performance. These non-recurrent models are appealing due to their highly parallelizable computations on modern GPUs. But do they have the same ability to exploit hierarchical structures implicitly in comparison to RNNs? In this work, we provide a first answer to this question. Our interest here is the ability of capturing hierarchical structure without being equipped with explicit structural representations <cite>(Bowman et al., 2015b</cite>; Tran et al., 2016; Linzen et al., 2016) . We choose Transformer as a non-recurrent model to study in this paper.",
  "y": "uses"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_1",
  "x": "We now proceed to measure both models' ability to learn hierarchical structure with a set of controlled experiments. ---------------------------------- **TASKS** We choose two tasks to study in this work: (1) subject-verb agreement, and (2) logical inference. The first task was proposed by Linzen et al. (2016) to test the ability of recurrent neural networks to capture syntactic dependencies in natural language. The second task was introduced by<cite> Bowman et al. (2015b)</cite> to compare tree-based recursive neural networks against sequence-based recurrent networks with respect to their ability to exploit hierarchical structures to make accurate inferences. The choice of tasks here is important to ensure that both models have to exploit hierarchical structural features (Jia and Liang, 2017) . 4 Subject-Verb Agreement Linzen et al. (2016) propose the task of predicting number agreement between subject and verb in naturally occurring English sentences as a proxy for the ability of LSTMs to capture hierarchical structure in natural language. We use the dataset provided by Linzen et al. (2016) and follow their experimental protocol of training each model using either (a) a general language model, i.e., next word prediction objective, and (b) an explicit supervision objective, i.e., predicting the number of the verb given its sentence history.",
  "y": "uses"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_2",
  "x": "Specifically, for each attention head at each layer of the FAN, we compute the percentage of 2 We note that our LSTM results are better than those in Linzen et al. (2016) . Also surprising is that the language model objective yields higher accuracies than the number prediction objective. We believe this may be due to better model optimization and to the embedding-output layer weight sharing, but we leave a thorough investigation to future work. times the subject is the most attended word among all words in the history. Figure 3 shows the results for all cases where the model made the correct prediction. While it is hard to interpret the exact role of attention for different heads and at different layers, we find that some of the attention heads at the higher layers ( 2 h1, 3 h0) frequently point to the subject with an accuracy that decreases linearly with the distance between subject and verb. ---------------------------------- **LOGICAL INFERENCE** In this task, we choose the artificial language introduced by<cite> Bowman et al. (2015b)</cite> .",
  "y": "uses"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_3",
  "x": "Despite the simplicity of the language, this task is not trivial. To correctly classify logical relations, the model must learn nested structures as well as the scope of logical operators. We verify the difficulty of the task by training three bag-of-words models followed by sum/average/max-pooling. The best of the three models achieve less than 59% accuracy on the logical inference versus 77% on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015a) . This shows that the SNLI task can be largely solved by exploiting shallow features without understanding the underlying linguistic structures, which has also been pointed out by recent work (Glockner et al., 2018; Gururangan et al., 2018) . Concurrently to our work Evans et al. (2018) proposed an alternative data set for logical inference and also found that a FAN model underperformed various other architectures including LSTMs. ---------------------------------- **MODELS** We follow the general architecture proposed in<cite> (Bowman et al., 2015b)</cite> : Premise and hypothesis sentences are encoded by fixed-size vectors.",
  "y": "uses"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_4",
  "x": "---------------------------------- **MODELS** We follow the general architecture proposed in<cite> (Bowman et al., 2015b)</cite> : Premise and hypothesis sentences are encoded by fixed-size vectors. These two vectors are then concatenated and fed to a 3-layer feed-forward neural network with ReLU nonlinearities to perform 7-way classification of the logical relation. The LSTM architecture used in this experiment is similar to that of<cite> Bowman et al. (2015b)</cite> . We simply take the last hidden state of the top LSTM layer as a fixed-size vector representation of the sentence. Here, we use a 2-layer LSTM with skip connections. The FAN maps a sentence x of length n to H = [h 1 , . . . , h n ] \u2208 R d\u00d7n . To obtain a fixedsize representation z, we use a self-attention layer with two trainable queries q 1 , q 2 \u2208 R 1\u00d7d :",
  "y": "similarities"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_5",
  "x": "---------------------------------- **RESULTS** Following the experimental protocol of<cite> Bowman et al. (2015b)</cite> , the data is divided into 13 bins based on the number of logical operators. Both FANs and LSTMs are trained on samples with at most n logical operators and tested on all bins. Figure 4 shows the result of the experiments with n \u2264 6 and n \u2264 12. We see that FANs and LSTMs perform similarly when trained on the whole dataset (Figure 4a ). However when trained on a subset of the data (Figure 4b) , LSTMs obtain better accuracies on similar examples (n \u2264 6) and generalize better on longer examples (6 < n \u2264 12). ---------------------------------- **DISCUSSION AND CONCLUSION**",
  "y": "uses"
 },
 {
  "id": "45d804ec30d20bd7e484c3bbd8399f_0",
  "x": "To that end, researchers have investigated the linguistic knowledge that these models learn by analyzing BERT (Goldberg, 2018; Lin et al., 2019) directly or training probing classifiers on the contextualized embeddings or attention heads of BERT (Tenney et al., 2019b,a; Hewitt and Manning, 2019) . BERT and RoBERTa, as Transformer models (Vaswani et al., 2017) , compute the hidden representation of all the attention heads at each layer for each token by attending to all the token representations in the preceding layer. In this work, we investigate the hypothesis that BERTstyle models use at least some of their attention heads to track syntactic dependency relationships between words. We use two dependency relation extraction methods to extract dependency relations from each self-attention heads of BERT and RoBERTa. The first method-maximum attention weight (MAX)-designates the word with the highest incoming attention weight as the parent, and is meant to identify specialist heads that track specific dependencies like obj (in the style of <cite>Clark et al., 2019)</cite> . The second-maximum spanning tree (MST)-computes a maximum spanning tree over the attention matrix, and is meant to identify generalist heads that can form complete, syntactically informative dependency trees. We analyze the extracted dependency relations and trees to investigate whether the attention heads of these models track syntactic dependencies significantly better than chance or baselines, and what type of dependency relations they learn best. In contrast to probing models (Adi et al., 2017; Conneau et al., 2018) , our methods require no further training. In prior work,<cite> Clark et al. (2019)</cite> find that some heads of BERT exhibit the behavior of some dependency relation types, though they do not perform well at all types of relations in general.",
  "y": "uses background"
 },
 {
  "id": "45d804ec30d20bd7e484c3bbd8399f_1",
  "x": "We use two dependency relation extraction methods to extract dependency relations from each self-attention heads of BERT and RoBERTa. The first method-maximum attention weight (MAX)-designates the word with the highest incoming attention weight as the parent, and is meant to identify specialist heads that track specific dependencies like obj (in the style of <cite>Clark et al., 2019)</cite> . The second-maximum spanning tree (MST)-computes a maximum spanning tree over the attention matrix, and is meant to identify generalist heads that can form complete, syntactically informative dependency trees. We analyze the extracted dependency relations and trees to investigate whether the attention heads of these models track syntactic dependencies significantly better than chance or baselines, and what type of dependency relations they learn best. In contrast to probing models (Adi et al., 2017; Conneau et al., 2018) , our methods require no further training. In prior work,<cite> Clark et al. (2019)</cite> find that some heads of BERT exhibit the behavior of some dependency relation types, though they do not perform well at all types of relations in general. We are able to replicate their results on BERT using our MAX method. In addition, we also perform a similar analysis on BERT models fine-tuned on natural language understanding tasks as well as RoBERTa. Our experiments suggest that there are particular attention heads of BERT and RoBERTa that encode certain dependency relation types such as nsubj, obj with substantially higher accuracy than our baselines-a randomly initialized Transformer and relative positional baselines.",
  "y": "motivation extends background"
 },
 {
  "id": "45d804ec30d20bd7e484c3bbd8399f_2",
  "x": "In contrast, Raganato and Tiedemann (2018) train a Transformer-based machine translation model on different language pairs and extract the maximum spanning tree algorithm from the attention weights of the encoder for each layer and head individually. They find that the best dependency score is not significantly higher than a right-branching tree baseline. Voita et al. (2019) find the most confident attention heads of the Transformer NMT encoder based on a heuristic of the concentration of attention weights on a single token, and find that these heads mostly attend to relative positions, syntactic relations, and rare words. Additionally, researchers have investigated the syntactic knowledge that BERT learns by analyzing the contextualized embeddings (Warstadt et al., 2019a) and attention heads of BERT<cite> (Clark et al., 2019)</cite> . Goldberg (2018) analyzes the contextualized embeddings of BERT by computing language model surprisal on subject-verb agreement and shows that BERT learns significant knowledge of syntax. Tenney et al. (2019b) introduce a probing classifier for evaluating syntactic knowledge in BERT and show that BERT encodes syntax more than semantics. Hewitt and Manning (2019) train a structural probing model that maps the hidden representations of each token to an inner-product space that corresponds to syntax tree distance. They show that the learned spaces of strong models such as BERT and ELMo (Peters et al., 2018) are better for reconstructing dependency trees compared to baselines. Clark et al. (2019) train a probing classifier on the attentionheads of BERT and show that BERT's attention heads capture substantial syntactic information.",
  "y": "background"
 },
 {
  "id": "45d804ec30d20bd7e484c3bbd8399f_3",
  "x": "Both methods operate on the attention weight matrix W \u2208 (0, 1) T \u00d7T for a given head at a given layer, where T is the number of tokens in the sequence, and the rows and columns correspond to the attending and attended tokens respectively (such that each row sums to 1). Method 1: Maximum Attention Weights (MAX) Given a token A in a sentence, a selfattention mechanism is designed to assign high attention weights on tokens that have some kind of relationship with token A (Vaswani et al., 2017) . Therefore, for a given token A, a token B that has the highest attention weight with respect to the token A should be related to token A. Our aim is to investigate whether this relation maps to a universal dependency relation. We assign a relation (w i , w j ) between word w i and w j if j = argmax W [i] for each row (that corresponds to a word in attention matrix) i in attention matrix W . Based on this simple strategy, we extract relations for all sentences in our evaluation datasets. This method is similar to<cite> Clark et al. (2019)</cite> , and attempts to recover individual arcs between words; the relations extracted using this method need not form a valid tree, or even be fully connected, and the resulting edge directions may or may not match the canonical directions. Hence, we evaluate the resulting arcs individually and ignore their direction. After extracting dependency relations from all heads at all layers, we take the maximum UUAS over all relations types. ----------------------------------",
  "y": "similarities extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_0",
  "x": "Many core linguistic phenomena that one would like to model in general-purpose sentence representations depend on syntactic structure (Chomsky, 1965; Everaert et al., 2015) . Despite the fact that none of the aforementioned architectures have explicit syntactic structural representations, there is some evidence that these models can approximate such structure-dependent phenomena under certain conditions (Gulordava et al., 2018; McCoy et al., 2018; Linzen et al., 2016; Bowman et al., 2015) , in addition to their widespread success in practical tasks. The recently introduced BERT model (Devlin et al., 2018) , which is based on transformers, achieves state-of-the-art results on eleven natural language processing tasks. In this work, we assess BERT's ability to learn structure-dependent linguistic phenomena of agreement relations. To test whether BERT is sensitive to agreement relations, we use the cloze test (Taylor, 1953 , also called the \"masked language model\" objective), in which we mask out one of two words in an agreement relation and ask BERT to predict the masked word, one of the two tasks on which BERT is initially trained. <cite>Goldberg (2019)</cite> adapted the experimental setup of Linzen et al. (2016) , Gulordava et al. (2018) and Marvin and Linzen (2018) to use the cloze test to assess BERT's sensitivity to number agreement in English subject-verb agreement relations. The results showed that the single-language BERT model performed surprisingly well at this task (above 80% accuracy in all experiments), even when there were multiple \"distractors\" in the sentence (other nouns that differed from the subject in number). This suggests that BERT is actually learning to approximate structure-dependent computation, and not simply relying on flawed heuristics. However, English subject-verb agreement is a rather restricted phenomenon, with the majority of verbs having only two inflected forms and only one morphosyntactic feature (number) involved.",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_1",
  "x": "<cite>Goldberg (2019)</cite> adapted the experimental setup of Linzen et al. (2016) , Gulordava et al. (2018) and Marvin and Linzen (2018) to use the cloze test to assess BERT's sensitivity to number agreement in English subject-verb agreement relations. The results showed that the single-language BERT model performed surprisingly well at this task (above 80% accuracy in all experiments), even when there were multiple \"distractors\" in the sentence (other nouns that differed from the subject in number). This suggests that BERT is actually learning to approximate structure-dependent computation, and not simply relying on flawed heuristics. However, English subject-verb agreement is a rather restricted phenomenon, with the majority of verbs having only two inflected forms and only one morphosyntactic feature (number) involved. To what extent does<cite> Goldberg's (2019)</cite> result hold for subject-verb agreement in other languages, including more morphologically rich ones, as well as for other types of agreement relations? Building on<cite> Goldberg's (2019)</cite> work, we expand the experiment to 26 languages and four types of agreement relations, which include more challenging examples. In Section 2, we define what is meant by agreement relations and outline the particular agreement relations under study. Section 3 introduces our newly curated cross-linguistic dataset of agreement relations, while section 4 discusses our experimental setup. We report the results of our experiments in section 5.",
  "y": "motivation"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_2",
  "x": "To what extent does<cite> Goldberg's (2019)</cite> result hold for subject-verb agreement in other languages, including more morphologically rich ones, as well as for other types of agreement relations? Building on<cite> Goldberg's (2019)</cite> work, we expand the experiment to 26 languages and four types of agreement relations, which include more challenging examples. In Section 2, we define what is meant by agreement relations and outline the particular agreement relations under study. Section 3 introduces our newly curated cross-linguistic dataset of agreement relations, while section 4 discusses our experimental setup. We report the results of our experiments in section 5. All data and code are available at https://github.com/ geoffbacon/does-bert-agree. ---------------------------------- **STRUCTURE-DEPENDENT AGREEMENT RELATIONS** Agreement phenomena are an important and cross-linguistically common property of natural languages, and as such have been extensively studied in syntax and morphology (Corbett, 2006) .",
  "y": "extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_3",
  "x": "French, for example, employs all four types of agreement relations. Examples are given in (2)- (5). The subject and verb in (2) agree for number, while the noun and determiner in (3), the noun and attributive adjective in (4) and the subject and predicated adjective in (5) agree for both number and gender. (2) Les cl\u00e9s de la porte se trouvent sur la table. 'The keys to the door are on the table.' 'The keys to the door are broken. ' Previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject-verb agreement in number<cite> (Goldberg, 2019</cite>; Gulordava et al., 2018; Linzen et al., 2016) . In our work, we study all four types of agreement relations and all four features discussed above. Moreover, previous work using any method to assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model (Hewitt and Manning, 2019;<cite> Goldberg, 2019</cite>; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; Clark et al., 2019) . We expand this line of work to 26 languages.",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_4",
  "x": "'The keys to the door are on the table.' 'The keys to the door are broken. ' Previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject-verb agreement in number<cite> (Goldberg, 2019</cite>; Gulordava et al., 2018; Linzen et al., 2016) . In our work, we study all four types of agreement relations and all four features discussed above. Moreover, previous work using any method to assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model (Hewitt and Manning, 2019;<cite> Goldberg, 2019</cite>; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; Clark et al., 2019) . We expand this line of work to 26 languages. Not all languages in our sample exhibit all four types of agreement nor use all four features examined, but they all exhibit at least one of the agreement types involving at least one of the features. ---------------------------------- **DATA** Our study requires two types of data.",
  "y": "uses"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_5",
  "x": "Languages with richer inflectional morphology tend to display more agreement types and involve more features. French, for example, employs all four types of agreement relations. Examples are given in (2)- (5). The subject and verb in (2) agree for number, while the noun and determiner in (3), the noun and attributive adjective in (4) and the subject and predicated adjective in (5) agree for both number and gender. (2) Les cl\u00e9s de la porte se trouvent sur la table. 'The keys to the door are on the table.' 'The keys to the door are broken. ' Previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject-verb agreement in number<cite> (Goldberg, 2019</cite>; Gulordava et al., 2018; Linzen et al., 2016) . In our work, we study all four types of agreement relations and all four features discussed above. Moreover, previous work using any method to assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model (Hewitt and Manning, 2019;<cite> Goldberg, 2019</cite>; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; Clark et al., 2019) .",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_6",
  "x": "Moreover, previous work using any method to assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model (Hewitt and Manning, 2019;<cite> Goldberg, 2019</cite>; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; Clark et al., 2019) . We expand this line of work to 26 languages. Not all languages in our sample exhibit all four types of agreement nor use all four features examined, but they all exhibit at least one of the agreement types involving at least one of the features. ---------------------------------- **DATA** Our study requires two types of data. First, we need sentences containing agreement relations. We mask out one of the words in the agreement relation and ask BERT to predict the masked word. We are interested in BERT's ability to predict words that respect the agreement relation, that is, words which share the morphosyntactic features of the word with which it agrees.",
  "y": "extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_7",
  "x": "**EXPERIMENT** Our experiment is designed to measure BERT's ability to model syntactic structure. Our experimental set up is an adaptation of that of <cite>Goldberg (2019)</cite> . As in previous work, we mask one word involved in an agreement relation and ask BERT to predict it. <cite>Goldberg (2019)</cite> , following Linzen et al. (2016) , considered a correct prediction to be one in which the masked word receives a higher probability than other inflected forms of the lemma. For example, when dogs is masked, a correct response gives more probability to dogs than dog. This evaluation leaves open the possibility that selectional restrictions or frequency are responsible for the results rather than sensitivity to syntactic structure (Gulordava et al., 2018) . To remove this possibility, we take into account all words of the same part-of-speech as the masked word. Concretely, we consider a correct prediction to be one in which the average probability of all possible correct words is higher than that of all incorrect words.",
  "y": "extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_8",
  "x": "Our experiment is designed to measure BERT's ability to model syntactic structure. Our experimental set up is an adaptation of that of <cite>Goldberg (2019)</cite> . As in previous work, we mask one word involved in an agreement relation and ask BERT to predict it. <cite>Goldberg (2019)</cite> , following Linzen et al. (2016) , considered a correct prediction to be one in which the masked word receives a higher probability than other inflected forms of the lemma. For example, when dogs is masked, a correct response gives more probability to dogs than dog. This evaluation leaves open the possibility that selectional restrictions or frequency are responsible for the results rather than sensitivity to syntactic structure (Gulordava et al., 2018) . To remove this possibility, we take into account all words of the same part-of-speech as the masked word. Concretely, we consider a correct prediction to be one in which the average probability of all possible correct words is higher than that of all incorrect words. By \"correct words\", we mean words with the exact same feature values and the same part of speech as the masked word.",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_9",
  "x": "**EXPERIMENT** Our experiment is designed to measure BERT's ability to model syntactic structure. Our experimental set up is an adaptation of that of <cite>Goldberg (2019)</cite> . As in previous work, we mask one word involved in an agreement relation and ask BERT to predict it. <cite>Goldberg (2019)</cite> , following Linzen et al. (2016) , considered a correct prediction to be one in which the masked word receives a higher probability than other inflected forms of the lemma. For example, when dogs is masked, a correct response gives more probability to dogs than dog. This evaluation leaves open the possibility that selectional restrictions or frequency are responsible for the results rather than sensitivity to syntactic structure (Gulordava et al., 2018) . To remove this possibility, we take into account all words of the same part-of-speech as the masked word. Concretely, we consider a correct prediction to be one in which the average probability of all possible correct words is higher than that of all incorrect words.",
  "y": "differences"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_10",
  "x": "The average example in our cloze data is evaluated using 1,468 words, compared with 2 in <cite>Goldberg (2019)</cite> . Following <cite>Goldberg (2019)</cite>, we use the pretrained BERT models from the original authors 2 , but through the PyTorch implementation. 3 <cite>Goldberg (2019)</cite> showed that in his experiments the base BERT model performed better than the larger model, so we restrict our attention to the base model. For English, we use the model trained only on English data, whereas for all other languages we use the multilingual model. ---------------------------------- **RESULTS** Overall, BERT performs well on our experimental task, suggesting that it is able to model syntactic structure. BERT was correct in 94.3% of all cloze examples. This high performance is found across all four types of agreement relations.",
  "y": "differences"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_11",
  "x": "To remove this possibility, we take into account all words of the same part-of-speech as the masked word. Concretely, we consider a correct prediction to be one in which the average probability of all possible correct words is higher than that of all incorrect words. By \"correct words\", we mean words with the exact same feature values and the same part of speech as the masked word. By \"incorrect words\", we mean words of the same part of speech as the masked word but that differ from the masked word with respect to at least one feature value. We ignore cloze examples in which there are fewer than 10 possible correct and 10 incorrect answers in our feature data. The average example in our cloze data is evaluated using 1,468 words, compared with 2 in <cite>Goldberg (2019)</cite> . Following <cite>Goldberg (2019)</cite>, we use the pretrained BERT models from the original authors 2 , but through the PyTorch implementation. 3 <cite>Goldberg (2019)</cite> showed that in his experiments the base BERT model performed better than the larger model, so we restrict our attention to the base model. For English, we use the model trained only on English data, whereas for all other languages we use the multilingual model.",
  "y": "uses"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_12",
  "x": "Following <cite>Goldberg (2019)</cite>, we use the pretrained BERT models from the original authors 2 , but through the PyTorch implementation. 3 <cite>Goldberg (2019)</cite> showed that in his experiments the base BERT model performed better than the larger model, so we restrict our attention to the base model. For English, we use the model trained only on English data, whereas for all other languages we use the multilingual model. ---------------------------------- **RESULTS** Overall, BERT performs well on our experimental task, suggesting that it is able to model syntactic structure. BERT was correct in 94.3% of all cloze examples. This high performance is found across all four types of agreement relations. Figure 1 shows that BERT performed above 90% accuracy in each type.",
  "y": "uses"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_13",
  "x": "As a complementary approach, Clark et al. (2019) studied the attention mechanism of BERT, finding clear correlates with interpretable linguistic structures such as direct objects, and suggest that BERT's success is due in part to its syntactic awareness. However, by subjecting it to existing psycholinguistic tasks, Ettinger (2019) found that BERT fails in its ability to understand negation. In concurrent work, van Schijndel et al. (forthcoming) show that BERT does not consistently outperform LSTM-based models on English subjectverb agreement tasks. ---------------------------------- **CONCLUSIONS & FUTURE WORK** Core linguistic phenomena depend on syntactic structure. Yet current state-of-the-art models in language representations, such as BERT, do not have explicit syntactic structural representations. Previous work by <cite>Goldberg (2019)</cite> showed that BERT captures English subject-verb number agreement well despite this lack of explicit structural representation. We replicated this result using a different evaluation methodology that addresses shortcomings in the original methodology and expanded the study to 26 languages.",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_14",
  "x": "---------------------------------- **CONCLUSIONS & FUTURE WORK** Core linguistic phenomena depend on syntactic structure. Yet current state-of-the-art models in language representations, such as BERT, do not have explicit syntactic structural representations. Previous work by <cite>Goldberg (2019)</cite> showed that BERT captures English subject-verb number agreement well despite this lack of explicit structural representation. We replicated this result using a different evaluation methodology that addresses shortcomings in the original methodology and expanded the study to 26 languages. Our study further broadened existing work by considering the most cross-linguistically common agreement types as well as the most common morphosyntactic features. The main result of this expansion into more languages, types and features is that BERT, without explicit syntactic structure, is still able to capture syntax-sensitive agreement patterns well. However, our analysis highlights an important qualification of this result.",
  "y": "extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_15",
  "x": [
   "Our study further broadened existing work by considering the most cross-linguistically common agreement types as well as the most common morphosyntactic features. The main result of this expansion into more languages, types and features is that BERT, without explicit syntactic structure, is still able to capture syntax-sensitive agreement patterns well. However, our analysis highlights an important qualification of this result. We showed that BERT's ability to model syntaxsensitive agreement relations decreases slightly as the dependency becomes longer range, and as the number of distractors increases. We release our new curated cross-linguistic datasets and code in the hope that it is useful to future research that may probe why this pattern appears. The experimental setup we used has some known limitations. First, in certain languages some of the cloze examples we studied contain redundant information. Even when one word from an agreement relation is masked out, other cues remain in the sentence (e.g. when masking out the noun for a French attributive adjective agreement relation, number information is still available from the determiner). To counter this in future work, we plan to run our experiment twice, masking out the controller and then the target."
  ],
  "y": "differences"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_0",
  "x": "In this work, we define multigranular ngrams as basic units for explanation, and organize all ngrams into a hierarchical structure, so that shorter ngrams can be reused while computing longer ngrams. We leverage a tree-structured LSTM to learn a contextindependent representation for each unit via parameter sharing. Experiments on medical disease classification show that our model is more accurate, efficient and compact than BiL-STM and CNN baselines. More importantly, our model can extract intuitive multi-granular evidence to support its predictions. ---------------------------------- **INTRODUCTION** Increasingly complex neural networks have achieved highly competitive results for many NLP tasks (Vaswani et al., 2017; Devlin et al., 2018) , but they prevent human experts from understanding how and why a prediction is made. Understanding how a prediction is made can be very important for certain domains, such as the medical domain. Recent research has started to investigate models with self-explaining capability, i.e. extracting evidence to support their final predictions (Li et al., 2015; Lei et al., 2016;<cite> Lin et al., 2017</cite>; Mullenbach et al., 2018) .",
  "y": "background"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_1",
  "x": "---------------------------------- **INTRODUCTION** Increasingly complex neural networks have achieved highly competitive results for many NLP tasks (Vaswani et al., 2017; Devlin et al., 2018) , but they prevent human experts from understanding how and why a prediction is made. Understanding how a prediction is made can be very important for certain domains, such as the medical domain. Recent research has started to investigate models with self-explaining capability, i.e. extracting evidence to support their final predictions (Li et al., 2015; Lei et al., 2016;<cite> Lin et al., 2017</cite>; Mullenbach et al., 2018) . For example, in order to make diagnoses based on the medical report in Table 1 , the highlighted symptoms may be extracted as evidence. Two methods have been proposed on how to jointly provide highlights along with classification. (1) an extraction-based method (Lei et al., 2016) , which first extracts evidences from the original text and then makes a prediction solely based on the extracted evidences; (2) an attentionbased method <cite>(Lin et al., 2017</cite>; Mullenbach et al., 2018) , which leverages the self-attention mechaMedical Report: The patient was admitted to the Neurological Intensive Care Unit for close observation. She was begun on heparin anticoagulated carefully secondary to the petechial bleed .",
  "y": "background"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_2",
  "x": "She was started on antibiotics for possible aspiration pneumonia . Her chest xray showed retrocardiac effusion . She had some bleeding after nasogastric tube insertion . Diagnoses: Cerebral artery occlusion; Unspecified essential hypertension; Atrial fibrillation; Diabetes mellitus. nism to show the importance of basic units (words or ngrams) through their attention weights. However, previous work has several limitations. Lin et al. (2017) , for example, take single words as basic units, while meaningful information is usually carried by multi-word phrases. For instance, useful symptoms in Table 1 , such as \"bleeding after nasogastric tube insertion\", are larger than a single word. Another issue of<cite> Lin et al. (2017)</cite> is that their attention model is applied on the representation vectors produced by an LSTM.",
  "y": "motivation background"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_3",
  "x": "Her chest xray showed retrocardiac effusion . She had some bleeding after nasogastric tube insertion . Diagnoses: Cerebral artery occlusion; Unspecified essential hypertension; Atrial fibrillation; Diabetes mellitus. nism to show the importance of basic units (words or ngrams) through their attention weights. However, previous work has several limitations. Lin et al. (2017) , for example, take single words as basic units, while meaningful information is usually carried by multi-word phrases. For instance, useful symptoms in Table 1 , such as \"bleeding after nasogastric tube insertion\", are larger than a single word. Another issue of<cite> Lin et al. (2017)</cite> is that their attention model is applied on the representation vectors produced by an LSTM. Each LSTM output contains more than just the information of that position, thus the real range for the highlighted position is unclear.",
  "y": "motivation background"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_4",
  "x": "Our work leverages the attention-based selfexplaining method<cite> (Lin et al., 2017)</cite> , as shown in Figure 1 . First, our text encoder ( \u00a73) formulates an input text into a list of basic units, learning a vector representation for each, where the basic units can be words, phrases, or arbitrary ngrams. Then, the attention mechanism is leveraged over all basic units, and sums up all unit representations based on the attention weights {\u03b1 1 , ..., \u03b1 n }. Eventually, the attention weight \u03b1 i will be used to reveal how important a basic unit h i is. The last prediction layer takes the fixed-length text representation t as input, and makes the final prediction. ---------------------------------- **BASELINES:** We compare two types of baseline text encoders in Figure 1 . (1)<cite> Lin et al. (2017)</cite> (BiLSTM), which formulates single word positions as basic units, and computes the vector h i for the i-th word position with a BiLSTM; (2) Extension of Mullenbach et al. (2018) (CNN) . The original model in (Mullenbach et al., 2018) only utilizes 4-grams.",
  "y": "uses"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_5",
  "x": "**BASELINES:** We compare two types of baseline text encoders in Figure 1 . (1)<cite> Lin et al. (2017)</cite> (BiLSTM), which formulates single word positions as basic units, and computes the vector h i for the i-th word position with a BiLSTM; (2) Extension of Mullenbach et al. (2018) (CNN) . The original model in (Mullenbach et al., 2018) only utilizes 4-grams. Here we extend this model to take all unigrams, bigrams, and up to n-grams as the basic units. For a fair comparison, both our approach and the baselines share the same architecture, and the only difference is the text encoder used. volving seven pathogens , discusses pitfalls of routine blood cultures and examines the role of the laboratory in microbiologic diagnosis . Structures for a sentence w 1 w 2 w 3 w 4 , where each node corresponds to a phrase or ngram. ---------------------------------- **MULTI-GRANULAR TEXT ENCODER**",
  "y": "uses"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_0",
  "x": "Section 5 is the conclusion. ---------------------------------- **RELATED WORK** Automating word sense disambiguation tasks based on annotated corpora have been proposed. Examples of supervised learning methods for WSD appear in [2] [3] [4] , [7] [8] . The learning algorithms applied including: decision tree, decisionlist [15] , neural networks [7] , na\u00efve Bayesian learning ( [5] , <cite>[11]</cite> ) and maximum entropy [10] . Among these leaning methods, the most important issue is what features will be used to construct the classifier. It is common in WSD to use contextual information that can be found in the neighborhood of the ambiguous word in training data ( [6] , [16] [17] [18] ). It is generally true that when words are used in the same sense, they have similar context and co-occurrence information [13] .",
  "y": "uses"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_1",
  "x": "It is generally true that when words are used in the same sense, they have similar context and co-occurrence information [13] . It is also generally true that the nearby context words of an ambiguous word give more effective patterns and features values than those far from it [12] . The existing methods consider features selection for context representation including both local and topic features where local features refer to the information pertained only to the given context and topical features are statistically obtained from a training corpus. Most of the recent works for English corpus including [7] and [8] , which combine both local and topical information in order to improve their performance. An interesting study on feature selection for Chinese [10] has considered topical features as well as local collocational, syntactic, and semantic features using the maximum entropy model. In Dang's [10] work, collocational features refer to the local PoS information and bi-gram co-occurrences of words within 2 positions of the ambiguous word. A useful result from this work based on (about one million words) the tagged People's Daily News shows that adding more features from richer levels of linguistic information such as PoS tagging yielded no significant improvement (less than 1%) over using only the bi-gram co-occurrences information. Another similar study for Chinese <cite>[11]</cite> is based on the Naive Bayes classifier model which has taken into consideration PoS with position information and bi-gram templates in the local context. The system has a reported 60.40% in both precision and recall based on the SENSEVAL-3 Chinese training data.",
  "y": "background"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_2",
  "x": "In Dang's [10] work, collocational features refer to the local PoS information and bi-gram co-occurrences of words within 2 positions of the ambiguous word. A useful result from this work based on (about one million words) the tagged People's Daily News shows that adding more features from richer levels of linguistic information such as PoS tagging yielded no significant improvement (less than 1%) over using only the bi-gram co-occurrences information. Another similar study for Chinese <cite>[11]</cite> is based on the Naive Bayes classifier model which has taken into consideration PoS with position information and bi-gram templates in the local context. The system has a reported 60.40% in both precision and recall based on the SENSEVAL-3 Chinese training data. Even though in both approaches, statistically significant bi-gram co-occurrence information is used, they are not necessarily true collocations. For example, in the express \" \", the bi-grams in their system are ( , , , Some bi-grams such as may have higher frequency but may introduce noise when considering it as features in disambiguating the sense \"human| \" and \"symbol| \" like in the example case of \" \". In our system, we do not rely on co-occurrence information. Instead, we utilize true collocation information ( , ) which fall in the window size of (-5, +5) as fea-tures and the sense of \"human| \" can be decided clearly using this features.",
  "y": "background differences"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_3",
  "x": "These features are extracted form the 60MB human sense-tagged People's Daily News with segmentation information. ---------------------------------- **TOPICAL CONTEXTUAL FEATURES** Niu <cite>[11]</cite> proved in his experiments that Na\u00efve Bayes classifier achieved best disambiguation accuracy with small topical context window size (< 10 words). We follow their method and set the contextual window size as 10 in our system. Each of the Chinese words except the stop words inside the window range will be considered as one topical feature. Their frequencies are calculated over the entire corpus with respect to each sense of an ambiguous word w. The sense definitions are obtained from HowNet. ---------------------------------- **LOCAL COLLOCATION FEATURES**",
  "y": "background"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_4",
  "x": "**TOPICAL CONTEXTUAL FEATURES** Niu <cite>[11]</cite> proved in his experiments that Na\u00efve Bayes classifier achieved best disambiguation accuracy with small topical context window size (< 10 words). We follow their method and set the contextual window size as 10 in our system. Each of the Chinese words except the stop words inside the window range will be considered as one topical feature. Their frequencies are calculated over the entire corpus with respect to each sense of an ambiguous word w. The sense definitions are obtained from HowNet. ---------------------------------- **LOCAL COLLOCATION FEATURES** We chose collocations as the local features. A collocation is a recurrent and conventional fixed expression of words which holds syntactic and semantic relations [21] .",
  "y": "uses background"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_5",
  "x": "\" (\"local college\"), or \" \" (\"local university\"). The sense of ambiguous words can be uniquely determined in these two types of collocations, therefore are the collocations applied in our system. The sources of the collocations will be explained in Section 4.1. In both Niu <cite>[11]</cite> and Dang's [10] work, topical features as well as the so called collocational features were used. However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features. However, bi-gram co-occurrences only indicate statistical significance which may not actually satisfy the conceptual definition of collocations. Thus instead of using co-occurrences of bigrams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD. The local features in our system make use of the collocations using the template (w i , w) within a window size of ten (where i = \u00b1 5). For example, \" \" (\"Government departments and local government commanded that\") fits the bi-gram collocation template (w, w 1 ) with the value of ( ).",
  "y": "background"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_6",
  "x": "The sources of the collocations will be explained in Section 4.1. In both Niu <cite>[11]</cite> and Dang's [10] work, topical features as well as the so called collocational features were used. However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features. However, bi-gram co-occurrences only indicate statistical significance which may not actually satisfy the conceptual definition of collocations. Thus instead of using co-occurrences of bigrams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD. The local features in our system make use of the collocations using the template (w i , w) within a window size of ten (where i = \u00b1 5). For example, \" \" (\"Government departments and local government commanded that\") fits the bi-gram collocation template (w, w 1 ) with the value of ( ). During the training and the testing processes, the counting of frequency value of the collocation feature will be increased by 1 if a collocation containing the ambiguous word occurs in a sentence. To have a good analysis on collocation features, we have also developed an algorithm using lonely adjacent bi-gram as locals features(named Sys-adjacent bi-gram as locals features(named System A) and another using collocation as local features(named System B).",
  "y": "differences background"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_0",
  "x": "However, such an approach does not scale to complex natural language tasks involving rich vocabularies and wide language variability. Another strategy is to employ fast sampling techniques (Ahmed et al., 2012; Ravi, 2013) or incorporate deep learning models with graph learning like (Bui et al., 2017 (Bui et al., , 2018 , which result in large models but have proven to be extremely powerful for complex language understanding tasks like response completion (Pang and Ravi, 2012) and Smart Reply (Kannan et al., 2016) . In this paper, we propose Self-Governing Neural Networks (SGNNs) inspired by projection networks (Ravi, 2017) . SGNNs are on-device deep learning models learned via embedding-free projection operations. We employ a modified version of the locality sensitive hashing (LSH) to reduce input dimension from millions of unique words/features to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017;<cite> Ortega and Vu, 2017)</cite> . The main contributions of the paper are: \u2022 Novel Self-Governing Neural Networks (SGNNs) for on-device deep learning for short text classification.",
  "y": "motivation"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_1",
  "x": "Table 1 summarizes dataset statistics. We use the train, validation and test splits as defined in (Lee and Dernoncourt, 2016;<cite> Ortega and Vu, 2017)</cite> . ---------------------------------- **EXPERIMENTAL SETUP** We setup our experimental evaluation, as follows: given a classification task and a dataset, we generate an on-device model. The size of the model can be configured (by adjusting the projection matrix P) to fit in the memory footprint of the device, i.e. a phone has more memory compared to a smart watch. For each classification task, we report Accuracy on the test set. ---------------------------------- **HYPERPARAMETER AND TRAINING**",
  "y": "uses"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_2",
  "x": "**BASELINES** We compare our model against a majority class baseline and Naive Bayes classifier (Lee and Dernoncourt, 2016) . Our model significantly outperforms both baselines by 12 to 35% absolute. ---------------------------------- **COMPARISON AGAINST STATE-OF-ART METHODS** We also compare our performance against prior work using HMMs (Stolcke et al., 2000) and recent deep learning methods like CNN (Lee and Dernoncourt, 2016) , RNN (Khanpour et al., 2016) and RNN with gated attention (Tran et al., 2017) . To the best of our knowledge, (Lee and Dernoncourt, 2016; Ortega and Vu, 2017; Tran et al., 2017) are the latest approaches in dialog act classification, which also reported on the same data splits. Therefore, we compare our research against these works. According to <cite>(Ortega and Vu, 2017)</cite> , prior work by (Ji and Bilmes, 2006) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly.",
  "y": "background"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_3",
  "x": "To the best of our knowledge, (Lee and Dernoncourt, 2016; Ortega and Vu, 2017; Tran et al., 2017) are the latest approaches in dialog act classification, which also reported on the same data splits. Therefore, we compare our research against these works. According to <cite>(Ortega and Vu, 2017)</cite> , prior work by (Ji and Bilmes, 2006) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly. For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work. This is very impressive given that we work with very small memory footprint and we do not rely on pre-trained word embeddings. Our study also shows that the proposed method is very effective for such natural language tasks compared to more complex neural network architectures such as deep CNN (Lee and Dernoncourt, 2016) and RNN variants (Khanpour et al., 2016;<cite> Ortega and Vu, 2017)</cite> . We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing lowdimensional semantic text representations that are useful for text classification applications. ---------------------------------- **DISCUSSION ON MODEL SIZE AND INFERENCE**",
  "y": "differences"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_5",
  "x": "This amounts to a huge savings in storage and computation cost wrt FLOPs (floating point operations per second). ---------------------------------- **CONCLUSION** We proposed Self-Governing Neural Networks for on-device short text classification. Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods (Lee and Dernoncourt, 2016; Khanpour et al., 2016;<cite> Ortega and Vu, 2017)</cite> . We introduced a compression technique that effectively captures low-dimensional semantic representation and produces compact models that significantly save on storage and computational cost. Our approach does not rely on pre-trained embeddings and efficiently computes the projection vectors on the fly. In the future, we are interested in extending this approach to more natural language tasks. For instance, we built a multilingual SGNN model for customer feedback classification (Liu et al., 2017) and obtained 73% on Japanese, close to best performing system on the challenge (Plank, 2017) .",
  "y": "differences"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_0",
  "x": "Furthermore, the vectors obtained for sentences of different grammatical structures live in different vector spaces: sentences with intransitive verbs live in the same space as context vectors, denoted by N , sentences with transitive verbs in N 2 = N \u2297 N , and sentences with ditransitive verbs in N 3 . A direct consequence of this instantiation is that one cannot compare meanings of sentences unless they have the same grammatical structure. In this work we outline a solution to the above problems by instantiating the sentence space to be the same space as one in which context vectors live, namely we stipulate that S = N . As a result of this decision, we become able to compare lexical meanings of words with compositional meanings of phrases and sentences. We show how the theoretical computations of Coecke et al. (2010) instantiate in this concrete setting, and how the Frobenius Algebras, originating from group theory (Frobenius, 1903) and later extended to vector spaces (Coecke et al., 2008) , allow us to not only represent meanings of words with complex roles, such as verbs, adjectives, and prepositions, in an intuitive relational manner, but also to stay faithful to their original linguistic types. Equally as importantly, this model enables us to realize the concrete computations in lower dimensional spaces, thus reduce the space complexity of the implementation. We experiment in two different tasks with promising results: First, we repeat the disambiguation experiment of <cite>Grefenstette and Sadrzadeh (2011a)</cite> for transitive verbs. Then we proceed to a novel task: We use The Oxford Junior Dictionary (Sansome et al., 2000) , Oxford Concise School Dictionary (Hawkins et al., 2004) , and WordNet in order to derive a set of term/definition pairs, measure the similarity of each term with every definition, and use this measurement to classify the definitions to specific terms. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_1",
  "x": "---------------------------------- **INSTANTIATING THE SENTENCE SPACE** The categorical framework of Coecke et al. (2010) is abstract in the sense that it does not prescribe concrete guidelines for constructing tensors for meanings of words with special roles such as verbs or adjectives. Even more importantly, it does not specify the exact form of the sentence space S, leaving these details as open questions for the implementor. ---------------------------------- **STIPULATING S = N \u2297 N** The work of <cite>Grefenstette and Sadrzadeh (2011a)</cite> was the first large-scale practical implementation of this framework for intransitive and transitive sentences, and thus a first step towards providing some concrete answers to these questions. Following ideas from formal semantics that verbs are actually relations, the authors argue that the distributional meaning of a verb is a weighted relation representing the extent according to which the verb is related to its subjects and objects. In vector spaces, these relations are represented by linear maps, equivalent to matrices for the case of binary relations and to tensors for relations of arity n. Hence transitive verbs can be represented by matrices created by structurally mixing and summing up all the contexts (subject and object pairs) in which the verb appears.",
  "y": "background"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_2",
  "x": "---------------------------------- **STIPULATING S = N** The work presented in this paper stems from the observation that the theory does not impose a special choice of sentence space, in particular it does not impose that tensors for S should have ranks greater than 1. Hence we stipulate that S = N and show how this instantiation works by performing the computations on the example transitive sentence 'dogs chase cats'. Take \u2212 \u2212 \u2192 do g and \u2212\u2192 cat be the context vectors for the subject and the object, both living in N as prescribed by their types. As any vector, these can be expressed as weighted sums of their basis vectors, that is, . By putting everything together, the meaning of the sentence is calculated as follows; this result lives in N , since it is a weighted sum over \u2212 \u2192 n j : An important consequence of our design decision is that it enables us to reduce the space complexity of the implementation from \u0398(d n )<cite> (Grefenstette and Sadrzadeh, 2011a)</cite> to \u0398(d), making the problem much more tractable. What remains to be solved is a theoretical issue, that in practice the meaning of relational words such as 'chase' as calculated by Equation 5 is a matrix living in N 2 -however, the mathematical framework above prescribes that it should be a rank-3 tensor in N 3 .",
  "y": "differences extends"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_3",
  "x": "We also remind to the reader that the relational method for constructing a tensor for the meaning of a verb<cite> (Grefenstette and Sadrzadeh, 2011a)</cite> provides us with a matrix in N 2 . In order to embed this in N 3 , as required by the categorical framework, we apply a \u03c3 : N 2 \u2192 N 3 map to it. Now the Frobenius operation \u03c3 gives us some options for the form of the resulting tensor, which are presented below: CPSBJ The first option is to copy the \"row\" dimension of the matrix which, according to Equation 5, corresponds to the subject. In Part (a) below we see how \u03c3 transforms the verb this way. Once substituted in Equation 1, we obtain the interaction in Part (b). Linear algebraically, the \u03c3 map transforms the matrix of the verb in the way depicted on the right: CPOBJ Our other option is to copy the \"column\" dimension of the matrix, i.e. the object dimension (the corresponding \u03c3 map again on the right): Geometrically, we can think of these two options as different ways for \"diagonally\" placing a plane into a cube.",
  "y": "similarities"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_4",
  "x": "The diagrams provide us a direct way to simplify the calculations involved, since they suggest a closed form formula for each case. Taking as an example the diagram of the copy-subject method, we see that: (a) the object interacts with the verb; (b) the result of this interaction serves as input for the \u03c3 function; (c) one wire of the output of \u03c3 interacts with the object, while the other branch delivers the result. In terms of linear algebra, this corresponds to the computation \u03c3(ver b \u00d7 \u2212\u2192 o b j) \u00d7 \u2212 \u2192 s b j (where \u00d7 denotes matrix multiplication), which is equivalent to the following: where the symbol denotes component-wise multiplication and \u00d7 is matrix multiplication. Similarly, the meaning of a transitive sentence for the copy-object case is given by: We should bring to the reader's attention the fact that equipped with the above closed forms we do not need to create or manipulate rank-3 tensors at any point of the computation, something that would cause high computational overhead. Furthermore, note that the nesting problem of <cite>Grefenstette and Sadrzadeh (2011a)</cite> does not arise here, since the linguistic and concrete types are the same. ---------------------------------- **EXPERIMENTS**",
  "y": "differences"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_5",
  "x": "where the symbol denotes component-wise multiplication and \u00d7 is matrix multiplication. Similarly, the meaning of a transitive sentence for the copy-object case is given by: We should bring to the reader's attention the fact that equipped with the above closed forms we do not need to create or manipulate rank-3 tensors at any point of the computation, something that would cause high computational overhead. Furthermore, note that the nesting problem of <cite>Grefenstette and Sadrzadeh (2011a)</cite> does not arise here, since the linguistic and concrete types are the same. ---------------------------------- **EXPERIMENTS** We train our vectors from a lemmatised version of the British National Corpus (BNC), following closely the parameters of the setting described in Mitchell and Lapata (2008) , later used by <cite>Grefenstette and Sadrzadeh (2011a)</cite> . Specifically, we use the 2000 most frequent words as the basis for our vector space; this single space will serve as a semantic space for both nouns and sentences. The weights of the vectors are set to the ratio of the probability of the context word given the target word to the probability of the context word overall.",
  "y": "similarities uses"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_6",
  "x": "---------------------------------- **DISAMBIGUATION** We first test our models against the disambiguation task for transitive sentences described in <cite>Grefenstette and Sadrzadeh (2011a)</cite> . The goal is to assess how well a model can discriminate between the different senses of an ambiguous verb, given the context (subject and object) of that verb. The entries of this dataset consist of a target verb, a subject, an object, and a landmark verb used for the comparison. One such entry for example is, \"write, pupil, name, spell\". A good compositional model should be able to understand that the sentence \"pupil write name\" is closer to the sentence \"pupil spell name\" than, for example, to \"pupil publish name\". On the other hand, given the context \"writer, book\" these results should be reversed. The dataset contains 200 such entries with verbs from CELEX, hence 400 sentences.",
  "y": "similarities uses"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_7",
  "x": "On the other hand, given the context \"writer, book\" these results should be reversed. The dataset contains 200 such entries with verbs from CELEX, hence 400 sentences. The evaluation of this experiment is performed by calculating Spearman's \u03c1 correlation against the judgements of 25 human evaluators. As our baselines we use an additive (ADDTV) and a multiplicative (MULTP) model, where the meaning of a sentence is computed by adding and point-wise multiplying, respectively, the context vectors of its words. The results are shown in Table 1 . The most successful S = N model for this task is the copyobject model, which is performing really close to the original relational model of <cite>Grefenstette and Sadrzadeh (2011a)</cite> , with the difference to be statistically insignificant. This is a promising result, since it suggests that the lower-dimensional new model performs similarly with the richer structure of the old model for transitive sentences, while at the same time allows generalisation to even more complex sentences 1 . More importantly, note that the categorical models are the only ones that respect the word order and grammatical structure of sentences; a feature completely dismissed in the simple multiplicative model. ----------------------------------",
  "y": "differences extends"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_8",
  "x": "To our knowledge, this is the first time a compositional distributional model is tested for its ability to match words with phrases. Our dataset consists of 112 terms (72 nouns and 40 verbs) and their main definitions, extracted from The Oxford Junior Dictionary (Sansome et al., 2000) . For each term, and in order to get a richer dataset, we added two more definitions that expressed the same or an 1 The original relational model of <cite>Grefenstette and Sadrzadeh (2011a)</cite> with S = N 2 , provided a \u03c1 of 0.21. When computed with our program with the exact same parameters (without embedding them in the S = N model), we obtained a \u03c1 of 0.195. The differences between both of these and our best model are statistically insignificant. In Grefenstette and Sadrzadeh (2011b) , a direct non-relational model was used to compute verb matrices; this provided a \u03c1 of 0.28. However, as explained by the authors themselves, this method is not general and for instance cannot be used for intransitive verbs. alternative meaning, using the entries from the Oxford Concise School Dictionary (Hawkins et al., 2004) or by paraphrasing with the WordNet synonyms of the words in the definitions.",
  "y": "differences extends"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_0",
  "x": "Various efforts have been made to alleviate the latent variable collapse issue. Bowman et al. (2016) uses KL annealing, where a variable weight is added to the KL term in the cost function at training time. Yang et al. (2017) discovered that there is a trade-off between the contextual capacity of the decoder and effective use of encoding information, and developed a dilated CNN as decoder which can vary the amount of conditioning context. They also introduced a loss clipping strategy in order to make the model more robust. Xu and Durrett (2018) addressed the problem by replacing the standard normal distribution for the prior with the von Mises-Fisher (vMF) distribution. With vMF, the KL loss only depends on the concentration parameter which is fixed during training and testing, and hence results in a constant KL loss. In a more recent work, Dieng et al. (2019) avoided latent variable collapse by including skip connections in the generative model, where the skip connections enforce strong links between the latent variables and the likelihood function. Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss (Bowman et al., 2016; , or resort to designing more sophisticated model structures<cite> (Yang et al., 2017</cite>; Xu and Durrett, 2018; Dieng et al., 2019) . In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse.",
  "y": "background"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_1",
  "x": "In a more recent work, Dieng et al. (2019) avoided latent variable collapse by including skip connections in the generative model, where the skip connections enforce strong links between the latent variables and the likelihood function. Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss (Bowman et al., 2016; , or resort to designing more sophisticated model structures<cite> (Yang et al., 2017</cite>; Xu and Durrett, 2018; Dieng et al., 2019) . In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse. In contrast to existing VAE-RNN models for text modelling which merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation for all hidden states of the RNN encoder. Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures. We evaluate our model against several strong baselines which apply VAE for text modelling (Bowman et al., 2016; <cite>Yang et al., 2017</cite>; Xu and Durrett, 2018) . We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation dataset (Novikova et al., 2017) . Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation. The code for our model is available online 2 .",
  "y": "uses"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_2",
  "x": "Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss (Bowman et al., 2016; , or resort to designing more sophisticated model structures<cite> (Yang et al., 2017</cite>; Xu and Durrett, 2018; Dieng et al., 2019) . In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse. In contrast to existing VAE-RNN models for text modelling which merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation for all hidden states of the RNN encoder. Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures. We evaluate our model against several strong baselines which apply VAE for text modelling (Bowman et al., 2016; <cite>Yang et al., 2017</cite>; Xu and Durrett, 2018) . We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation dataset (Novikova et al., 2017) . Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation. The code for our model is available online 2 . ----------------------------------",
  "y": "uses differences"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_3",
  "x": "the recognition model) and P \u03b8 (x|z) the decoder (a.k.a. the generative model). Both encoder and decoder are implemented via neural networks. As proved in (Kingma and Welling, 2013) , optimising the marginal log likelihood is essentially equivalent to maximising L(\u03b8, \u03c6; x), i.e., the evidence lower bound (ELBO), which consists of two terms. The first term is the expected reconstruction error indicating how well the model can reconstruct data given a latent variable. The the second term is the KL divergence of the approximate posterior from prior, i.e., a regularisation pushing the learned posterior to be as close to the prior as possible. ---------------------------------- **VARIATIONAL AUTOENDODER WITH HOLISTIC REGULARISATION** In this section, we discuss the technical details of the proposed holistic regularisation VAE (HR-VAE) model, a general architecture which can effectively mitigate the KL vanishing phenomenon. Our model design is motivated by one noticeable defect shared by the VAE-RNN based models in previous works (Bowman et al., 2016; <cite>Yang et al., 2017</cite>; Xu and Durrett, 2018; Dieng et al., 2019) .",
  "y": "motivation"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_4",
  "x": "The Adam optimiser (Kingma and Ba, 2014) was used for training with an initial learning rate of 0.0001. Each utterance in a mini-batch was padded to the maximum length for that batch, and the maximum batch-size allowed is 128. ---------------------------------- **BASELINES** We compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base 3 : A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue (Bowman et al., 2016) ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder<cite> (Yang et al., 2017)</cite> ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (Xu and Durrett, 2018) . the decoder needs to predict the entire sequence with only the help of the given latent variable z. In this way, a high-quality representation abstracting the information of the input sentence is much needed for the decoder, and hence enforcing z to learn the required information. Overall performance.",
  "y": "background"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_5",
  "x": "We represent input data with 512-dimensional word2vec embeddings (Mikolov et al., 2013) . We set the dimension of the hidden layers of both encoder and decoder to 256. The Adam optimiser (Kingma and Ba, 2014) was used for training with an initial learning rate of 0.0001. Each utterance in a mini-batch was padded to the maximum length for that batch, and the maximum batch-size allowed is 128. ---------------------------------- **BASELINES** We compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base 3 : A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue (Bowman et al., 2016) ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder<cite> (Yang et al., 2017)</cite> ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (Xu and Durrett, 2018) . the decoder needs to predict the entire sequence with only the help of the given latent variable z.",
  "y": "uses background"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_0",
  "x": "Moreover, this pointer traverses the source word from left to right in order to generate the inflected form. All that motivated the hard attention model of (Aharoni and Goldberg, 2017) , which outperformed the soft attention approaches. The key feature of this model is that it predicts not only the output word, but also the alignment between source and target using an additional step symbol which shifts the pointer to the next symbol. This model was further improved by<cite> (Makarov et al., 2017)</cite> , whose system was the winner of Sigmorphon 2017 evaluation campaign (Cotterell et al., 2017) . The approach of Makarov et al. was especially successful in low and medium resource setting, while in high resource setting it achieves an impressive accuracy of over 95% 1 . Does it mean that no further research is required and hard attention method equipped with copy mechanism is the final solution for automatic inflection problem? Actually, not, since the quality of the winning approach was much lower on medium (about 85%) and low (below 50%) datasets. This lower quality is easy to explain since in low resource setting the system might even see no examples of the required form 2 or observe just one or two inflection pairs which do not cover all possible paradigms for this particular form. For example, Russian verbs has several tens of variants to produce the +Pres+Sg+1 form.",
  "y": "background"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_1",
  "x": "For example, Russian verbs has several tens of variants to produce the +Pres+Sg+1 form. Consequently, to improve the inflection accuracy the system should extract more information from the whole language, not only the instances of the given form. This task is easier for agglutinative languages with regular inflection paradigm: to predict, say, the +Pres+Sg+1 form in Turkish, the system has just to observe several singular verb form (not necessarily of the first person) to extract the singular suffix and several first person form (of any number and tense). In presence of fusion, like in Russian and other Slavonic languages, the decomposition is not that easy or even impossible. However, this decomposition is already realised in model of<cite> (Makarov et al., 2017)</cite> since the grammatical features are treated as a list of atomic elements, not as entire label. A new source of information about the whole language are the laws of its phonetics. For example, to detect the vowel in the suffix of the Turkish verb one do not need to observe any verbs at all, but to extract the vowel harmony patterns from the inflection of nouns. A natural way to capture the phonetic patterns are character language models. They were already applied to the problem of inflection in (Sorokin, 2016) and produced a strong boost over the baseline system.",
  "y": "background"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_2",
  "x": "This task is easier for agglutinative languages with regular inflection paradigm: to predict, say, the +Pres+Sg+1 form in Turkish, the system has just to observe several singular verb form (not necessarily of the first person) to extract the singular suffix and several first person form (of any number and tense). In presence of fusion, like in Russian and other Slavonic languages, the decomposition is not that easy or even impossible. However, this decomposition is already realised in model of<cite> (Makarov et al., 2017)</cite> since the grammatical features are treated as a list of atomic elements, not as entire label. A new source of information about the whole language are the laws of its phonetics. For example, to detect the vowel in the suffix of the Turkish verb one do not need to observe any verbs at all, but to extract the vowel harmony patterns from the inflection of nouns. A natural way to capture the phonetic patterns are character language models. They were already applied to the problem of inflection in (Sorokin, 2016) and produced a strong boost over the baseline system. The work of Sorokin used simple ngram models, however, neural language models (Tran et al., 2016) has shown their superiority over earlier approaches for various tasks. Summarizing, our approach was to enrich the model of<cite> (Makarov et al., 2017</cite> ) with the language model component.",
  "y": "extends"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_3",
  "x": "We expected to improve performance especially in low and medium resource setting, however, our approach does not have clear advantages: our joint system is only slightly ahead the baseline system of<cite> (Makarov et al., 2017)</cite> for most of the languages. We conclude that the language model job is already executed by the decoder. However, given the vitality of language model approach in other areas of modern NLP (Peters et al., 2018), we describe our attempts in detail to give other researchers the ideas for future work in this direction. ---------------------------------- **MODEL STRUCTURE** ---------------------------------- **BASELINE MODEL** As the state-of-the-art baseline we choose the model of Makarov et al.<cite> (Makarov et al., 2017)</cite> , the winner of previous Sigmorphon Shared Task. This system is based on earlier work of Aharoni and Goldberg (Aharoni and Goldberg, 2017) .",
  "y": "differences"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_4",
  "x": "**MODEL STRUCTURE** ---------------------------------- **BASELINE MODEL** As the state-of-the-art baseline we choose the model of Makarov et al.<cite> (Makarov et al., 2017)</cite> , the winner of previous Sigmorphon Shared Task. This system is based on earlier work of Aharoni and Goldberg (Aharoni and Goldberg, 2017) . We briefly describe the structure of baseline model (we call it AGM-model further) and refer the reader to these two papers for more information. AGMmodel consists of encoder and decoder, where an encoder is just a bidirectional LSTM. Each element of the input sequence contains a 0-1 encoding of a current letter and two LSTMs traverse this sequence in opposite directions. After encoding, each element of obtained sequence contains information about current letter and its context.",
  "y": "uses"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_5",
  "x": "On i-th step the decoder takes a concatenation of 3 vectors: x j -the j-th element in the output of the encoder,f = W f eat f -the embedding of the grammatical feature vector and g i = W emb y i\u22121 -the embedding of previous output symbol. The feature vector is obtained as 0/1-encoding of the list of grammatical features. We actually take the concatenation of output vectors for d \u2265 1 previous output symbols as y i\u22121 , in our experiments d was set to 4. On each step the decoder produces a vector z i as output and propagates updated hidden state vector h i to the next timestep. z i is then passed to a two-layer perceptron with ReLU activation on the intermediate layer and softmax activation on the output layer, which produces the output distribution p i over output letters, formally: If y i is the index of step symbol, we move the pointer to the next input letter. We also use the copy gate from<cite> (Makarov et al., 2017)</cite> : since the neural network copies the vast majority of its symbols, the output distribution p i is obtained as a weighted sum of singleton distribution which outputs current input symbol and the preliminary distribution p i specified above. The weight \u03c3 i is the output of another one-layer perceptron: ----------------------------------",
  "y": "uses"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_6",
  "x": "The size of the training dataset was 10000 words in the high subset 4 , 1000 in medium and 100 in low. The dataset also contained a development set containing 1000 instances most of the time, for all languages we used this subset as validation data. Overall, there were 86 languages in the high setting, 102 in medium and 103 in low. ---------------------------------- **RESULTS AND DISCUSSION** We submitted three systems, one replicating the algorithm of<cite> (Makarov et al., 2017)</cite> , the second equipped with language models. The third one used only the language models: we extracted all possible abstract inflection paradigms for a given set of grammatical features and created a set of possible candidate forms applying all paradigms to the lemma. For example, consider the word \u0434\u0435\u043b\u0430\u0442\u044c and paradigms 1+\u0430\u0442\u044c#1+\u0435\u0442, 1+\u0430\u0442\u044c#1+\u0438\u0442, 1+\u044c#1 and 1+\u0447\u044c#1+\u0436\u0435\u0442; the first three produce the forms \u0434\u0435\u043b\u0430\u0435\u0442, \u0434\u0435\u043b\u0438\u0442, \u0434\u0435\u043b\u0430\u0442, while the fourth yields nothing since the given word does not end in -\u0447\u044c. Then all these forms are ranked using sum of logarithmic probabilities from forward and backward language models.",
  "y": "uses"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_0",
  "x": "Neural machine translation (NMT) is a recent approach in MT which learns patterns between source and target language corpora to produce text translations using deep neural networks (Sutskever et al., 2014) . One downside of models trained with human generated corpora is that social biases present in the data are learned. This is shown when training word embeddings, a vector representation of words, in news sets with crowd-sourcing evaluation to quantify the presence of biases, such as gender bias, in those representation<cite> (Bolukbasi et al., 2016)</cite> . This can affect downstream applications (Zhao et al., 2018a) and are at risk of being amplified (Zhao et al., 2017) . The objective of this work is to study the presence of gender bias in MT and give insight on the impact of debiasing in such systems. An example of this gender bias is the word \"friend\" in the English sentence \"She works in a hospital, my friend is a nurse\" would be correctly translated to \"amiga\" (feminine of friend) in Spanish, while \"She works in a hospital, my friend is a doctor\" would be incorrectly translated to \"amigo\" (masculine of friend) in Spanish. We consider that this translation contains gender bias since it ignores the fact that, for both cases, \"friend\" is a female and translates by focusing on the occupational stereotypes, i.e. translating doctor as male and nurse as female. The main contribution of this study is providing progress on the recent detected problem which is gender bias in MT (Prates et al., 2018) . The progress towards reducing gender bias in MT is made in two directions: first, we define a framework to experiment, detect and evaluate gender bias in MT for a particular task; second, we propose to use debiased word embeddings techniques in the MT system to reduce the detected bias.",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_1",
  "x": "While there are many techniques for extracting word embeddings, in this work we are using Global Vectors, or GloVe (Pennington et al., 2014) . Glove is an unsupervised method for learning word embeddings. This count-based method, uses statistical information of word occurrences from a given corpus to train a vector space for which each vector is related to a word and their values describes their semantic relations. ---------------------------------- **DEBIASING WORD EMBEDDINGS** The presence of biases in word embeddings has aroused as a topic of discussion about fairness. More specifically, gender stereotypes are learned from human generated corpora as shown by<cite> (Bolukbasi et al., 2016)</cite> . Several debiasing approaches have been proposed. Debiaswe is a postprocess method for debiasing previously generated embeddings<cite> (Bolukbasi et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_2",
  "x": "**DEBIASING WORD EMBEDDINGS** The presence of biases in word embeddings has aroused as a topic of discussion about fairness. More specifically, gender stereotypes are learned from human generated corpora as shown by<cite> (Bolukbasi et al., 2016)</cite> . Several debiasing approaches have been proposed. Debiaswe is a postprocess method for debiasing previously generated embeddings<cite> (Bolukbasi et al., 2016)</cite> . GN-GloVe is a method for generating gender neutral embeddings (Zhao et al., 2018b) . The main ideas behind these algorithms are described next. Debiaswe<cite> (Bolukbasi et al., 2016</cite> ) is a postprocess method for debiasing word embeddings. It consists of two main parts: First the direction of the embeddings where the bias is present is identified.",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_3",
  "x": "Several debiasing approaches have been proposed. Debiaswe is a postprocess method for debiasing previously generated embeddings<cite> (Bolukbasi et al., 2016)</cite> . GN-GloVe is a method for generating gender neutral embeddings (Zhao et al., 2018b) . The main ideas behind these algorithms are described next. Debiaswe<cite> (Bolukbasi et al., 2016</cite> ) is a postprocess method for debiasing word embeddings. It consists of two main parts: First the direction of the embeddings where the bias is present is identified. Second, the gender neutral words in this direction are neutralized to zero and also equalizes the sets by making the neutral word equidistant to the remaining ones in the set. The disadvantage of the first part of the process is that it can remove valuable information in the embed-dings for semantic relations between words with several meanings that are not related to the bias being treated. (Zhao et al., 2018b) is an algorithm for learning gender netural word embeddings models.",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_4",
  "x": "The dimension of the vectors is settled to 512 as standard and kept through all the experiments in this study. The parameter values for training the word embedding models with GloVe and GN-GloVe methods are listed in Table 3 . Debiaswe<cite> (Bolukbasi et al., 2016</cite> ) is a debiasing post-process performed on trained embeddings. Instead of having parameters for learning the representation it uses a set of words to define the gender direction and to neutralize and equalize the bias from the word vectors. Three set of words are used in the Debiaswe algorithm. One set of ten pairs of words such as woman-man, girl-boy, she-he... are used to define the gender direction. Another set of 218 genderspecific words such as aunt, uncle, wife, husband... are used for learning a larger set of genderspecific words. Finally, a set of crowd-sourced male-female equalization pairs such as dad-mom, boy-girl, granpa-grandma... that represent gender direction are equalized in the algorithm. For the Spanish side, the sets are adapted for the task and slightly modified to avoid unclear words from the English language or unnecessary repetitions.",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_5",
  "x": "We studied the impact of gender debiasing on neural machine translation. We trained sets of word embeddings with the standard GloVe algorithm. Then, we debiased the embeddings using Debiaswe<cite> (Bolukbasi et al., 2016)</cite> and also trained its gender neutral version with GN-GloVe (Zhao et al., 2018b) . We used all these different models on the Transformer (Vaswani et al., 2017) . Experiments were reported on using these word embeddings on both the encoder and decoder sides, or only the encoder side. The models were evaluated using the BLEU metric on the standard task of the WMT newstest2013 test set. BLEU performance was similar when using bias and debiased models and even improved when using the latter. To study of the bias for the translations of the models, we developed a specific test set. This set consists of sentences that includes context of the gender of the ambiguous \"friend\" in the English-to-Spanish translation.",
  "y": "uses"
 },
 {
  "id": "4bc5fc3bccb704b9978b294ffb07de_0",
  "x": "**ABSTRACT** Given the current growth in research and related emerging technologies in machine learning and deep learning, it is timely to introduce this tutorial to a large number of researchers and practitioners who are attending COLING 2018 and working on statistical models, deep neural networks, sequential learning and natural language understanding. To the best of our knowledge, there is no similar tutorial presented in previous ACL/COLING/EMNLP/NAACL. This three-hour tutorial will concentrate on a wide range of theories and applications and systematically present the recent advances in deep Bayesian and sequential learning which are impacting the communities of computational linguistics, human language technology and machine learning for natural language processing. ---------------------------------- **TUTORIAL DESCRIPTION** This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (Saon and Chien, 2012; Chan et al., 2016) to document summarization (Chang and Chien, 2009 ), text classification (Blei et al., 2003; Zhang et al., 2015) , text segmentation (Chien and Chueh, 2012) , information extraction (Narasimhan et al., 2016) , image caption generation (Vinyals et al., 2015; Xu et al., 2015) , sentence generation (Li et al., 2016b) , dialogue control (Zhao and Eskenazi, 2016; <cite>Li et al., 2016a</cite>) , sentiment classification, recommendation system, question answering (Sukhbaatar et al., 2015) and machine translation , to name a few. Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model. The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.",
  "y": "uses background"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_0",
  "x": "Transition-based parsers (Nivre, 2008) are extremely popular because of their high accuracy and speed. Inspired by the greedy neural network transition-based parser of Chen and Manning (2014) , Weiss et al. (2015) and Zhou et al. (2015) concurrently developed structured neural network parsers that use beam search and achieve state-of-the-art accuracies for English dependency parsing. 1 While very successful, these parsers have made use only of a small fraction of the rich options provided inside the transition-based framework: for example, all of these parsers use virtually identical atomic features and the arcstandard transition system. In this paper we extend this line of work and introduce two new types of features that significantly improve parsing performance: (1) a set-valued (i.e., bag-of-words style) feature for each word's morphological attributes, and (2) a weighted set-valued feature for each word's k-best POS tags. These features can be integrated naturally as atomic inputs to the embedding layer of the network and the model can learn arbitrary conjunctions with all other features through the hidden layers. In contrast, integrating such features into a model with discrete features requires nontrivial manual tweaking. For example,<cite> Bohnet and Nivre (2012)</cite> had to carefully discretize the real-valued POS tag score in order to combine it with the other discrete binary features in their system. Additionally, we also experiment with different transition systems, most notably the integrated parsing and part-of-speech (POS) tagging system of<cite> Bohnet and Nivre (2012)</cite> and also the swap system of Nivre (2009) . We evaluate our parser on the CoNLL '09 shared task dependency treebanks, as well as on two English setups, achieving the best published numbers in many cases.",
  "y": "background"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_1",
  "x": "In contrast, integrating such features into a model with discrete features requires nontrivial manual tweaking. For example,<cite> Bohnet and Nivre (2012)</cite> had to carefully discretize the real-valued POS tag score in order to combine it with the other discrete binary features in their system. Additionally, we also experiment with different transition systems, most notably the integrated parsing and part-of-speech (POS) tagging system of<cite> Bohnet and Nivre (2012)</cite> and also the swap system of Nivre (2009) . We evaluate our parser on the CoNLL '09 shared task dependency treebanks, as well as on two English setups, achieving the best published numbers in many cases. ---------------------------------- **MODEL** In this section, we review the baseline model, and then introduce the features (which are novel) and the transition systems (taken from existing work) that we propose as extensions. We measure the impact of each proposed change on the development sets of the multi-lingual CoNLL '09 shared task treebanks (Haji\u010d et al., 2009) . For details on our experimental setup, see Section 3.",
  "y": "similarities uses"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_2",
  "x": "We define the feature group morph as the matrix X morph such that, for where N f is the number of morphological features active on the token indexed by f . In other words, we embed a bag of features into a shared embedding space by averaging the individual feature embeddings. k-best Tags. The non-linear network models of Weiss et al. (2015) and Chen and Manning (2014) embed the 1-best tag, according to a first-stage tagger, for a select set of tokens for any configuration. Inspired by the work of<cite> Bohnet and Nivre (2012)</cite> , we embed the set of top tags according to a first-stage tagger. Specifically, we define the feature group ktags as the matrix X ktags such that, for where P(POS = v | f ) is the marginal probability that the token indexed by f has the tag indexed by v, according to the first-stage tagger. Results.",
  "y": "similarities uses"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_3",
  "x": "Adding the k-best tag feature (+morph +ktags) provides modest gains (and modest losses), peaking at 0.54% LAS for Spanish. This feature proves more beneficial in the integrated transition system, discussed in the next section. We note the ease with which we can obtain these gains in a multilayer embedding framework, without the need for any hand-tuning. ---------------------------------- **INTEGRATING PARSING AND TAGGING** While past work on neural network transitionbased parsing has focused exclusively on the arcstandard transition system, it is known that better results can often be obtained with more sophisticated transition systems that have a larger set of possible actions. The integrated arc-standard transition system of<cite> Bohnet and Nivre (2012)</cite> allows the parser to participate in tagging decisions, rather than being forced to treat the tagger's tags as given, as in the arc-standard system. It does this by replacing the shift action in the arc-standard system with an action shift p , which, aside from shifting the top token on the buffer also assigns it one of the k best POS tags from a first-stage tagger. We also experiment with the swap action of Nivre (2009) , which allows reordering of the tokens in the input sequence.",
  "y": "uses"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_4",
  "x": "Whether punctuation is included in the evaluation is specified in each subsection. We use 1024 units in all hidden layers, a choice made based on the development set. We found network sizes to be of critical importance for the accuracy of our models. For example, LAS improvements can be as high as 0.98% in CoNLL'09 German when increasing the size of the two hidden layers from 200 to 1024. We use B = 16 or B = 32 based on the development set performance per language. For ease of experimentation, we deviate from<cite> Bohnet and Nivre (2012)</cite> and use a single unstructured beam, rather than separate beams for POS tag and parse differences. We train our neural networks on the standard training sets only, except for initializing with word embeddings generated by word2vec and using cluster features in our POS tagger. Unlike Weiss et al. (2015) we train our model only on the treebank training set and do not use tri-training, which can likely further improve the results. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_5",
  "x": "It is worth pointing out that Gesmundo et al. (2009) is itself a neural net parser. Our models achieve higher labeled accuracy than the winning systems in the shared task in all languages. Additionally, our pipelined neural network parser always outperforms its linear counterpart, an in-house reimplementation of the system of Zhang and Nivre (2011) , as well as the more recent and highly accurate parsers of Zhang and McDonald (2014) and Lei et al. (2014 again outperforms its linear counterpart<cite> (Bohnet and Nivre, 2012)</cite> , however, in some cases the addition of graph-based and cluster features<cite> (Bohnet and Nivre, 2012</cite> )+G+C can lead to even better results. The improvements in POS tagging (Table  2 ) range from 0.3% for English to 1.4% absolute for Chinese and are always higher for the neural network models compared to the linear models. ---------------------------------- **ENGLISH WSJ** We experiment on English using the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) , with standard train/test splits. We convert the constituency trees to Stanford style dependencies (De Marneffe et al., 2006 ) using version 3.3.0 of the converter. We use predicted POS tags and exclude punctuation from the evaluation, as is standard for English.",
  "y": "differences"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_6",
  "x": "The improvements in POS tagging (Table  2 ) range from 0.3% for English to 1.4% absolute for Chinese and are always higher for the neural network models compared to the linear models. ---------------------------------- **ENGLISH WSJ** We experiment on English using the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) , with standard train/test splits. We convert the constituency trees to Stanford style dependencies (De Marneffe et al., 2006 ) using version 3.3.0 of the converter. We use predicted POS tags and exclude punctuation from the evaluation, as is standard for English. Results. The results shown in Table 4 , we find that our full model surpasses, to our knowledge, all previously reported supervised parsing models for the Stanford dependency conversions. It surpasses its linear analog, the work of<cite> Bohnet and Nivre (2012)</cite> on Stanford Dependencies UAS by 0.9% UAS and by 1.14% LAS.",
  "y": "differences"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_0",
  "x": "<cite>Zhang et al. (2017)</cite> utilize adversarial training to obtain cross-lingual word embeddings without any parallel data. However, their performance is still significantly worse than supervised methods. <cite>Zhang et al. (2017)</cite> apply adversarial training to align monolingual word vector spaces with no supervision.",
  "y": "background"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_1",
  "x": "In this paper, we propose an iterative method to train our models. Basically, we first initialize W \u00b5t to be identity matrix and train \u03c6 s and \u03b8 s on the source side. After convergence, we freeze W \u00b5s , and then train \u03c6 t and \u03b8 t in the target side. The pseudo-code for this process is shown in Algorithm 1. It should be noted that there is no variance once completing training. Our experiments could be divided into two parts. In the first part, we conduct experiments on smallscale datasets and our main baseline is <cite>Zhang et al. (2017)</cite> . In the second part, we combine our model with several advanced techniques and we compare our model with Conneau et al. (2018) ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_2",
  "x": "In the second part, we combine our model with several advanced techniques and we compare our model with Conneau et al. (2018) ---------------------------------- **SMALL-SCALE DATASETS** In this section, our experiments focus on smallscale datasets and our main baseline model is adversarial autoencoder<cite> (Zhang et al., 2017)</cite> . For justice, we use the same model selection strategy with <cite>Zhang et al. (2017)</cite> , i.e. we choose the model whose sum of reconstruction loss and classification accuracy is the least. The source and target word embeddings would be first mapped into the latent space. For each source word embedding x, it would be first transformed into z x . The the its k nearest target embeddings would be retrieved and be compared against the entry in a ground truth bilingual lexicon. Performance is measured by top-1 accuracy.",
  "y": "similarities"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_3",
  "x": "In this section, our experiments focus on smallscale datasets and our main baseline model is adversarial autoencoder<cite> (Zhang et al., 2017)</cite> . For justice, we use the same model selection strategy with <cite>Zhang et al. (2017)</cite> , i.e. we choose the model whose sum of reconstruction loss and classification accuracy is the least. The source and target word embeddings would be first mapped into the latent space. For each source word embedding x, it would be first transformed into z x . The the its k nearest target embeddings would be retrieved and be compared against the entry in a ground truth bilingual lexicon. Performance is measured by top-1 accuracy. ---------------------------------- **EXPERIMENTS ON CHINESE-ENGLISH DATASET** For this set of experiments, we use the same data as <cite>Zhang et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_4",
  "x": "The baseline models are MonoGiza system (Dou et al., 2015) , translation matrix (TM) (Mikolov et al., 2013) , isometric alignment (IA) (Zhang et al., 2016b) and adversarial training approach<cite> (Zhang et al., 2017)</cite> . Table 2 summarizes the performance of baseline models and our approach. The results of baseline models are cited from <cite>Zhang et al. (2017)</cite> . As we can see from the table, our model could achieve superior performance compared with other baseline models. ---------------------------------- **EXPERIMENTS ON OTHER LANGUAGE PAIRS DATASETS** We also conduct experiments on Spanish-English and Italian-English language pairs. Again, we use the same dataset with <cite>Zhang et al. (2017)</cite> . and the statistics are shown in The experimental results are shown in Table 4 . Because Spanish, Italian and English are closely related languages, the accuracy would be higher than the Chinese-English dataset.",
  "y": "similarities uses"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_5",
  "x": "**EXPERIMENTS ON CHINESE-ENGLISH DATASET** For this set of experiments, we use the same data as <cite>Zhang et al. (2017)</cite> . The statistics of the final training data is given in Table 1 . We use Chinese-English Translation Lexicon Version 3.0 (LDC2002L27) as our ground truth bilingual lexicon for evaluation. The baseline models are MonoGiza system (Dou et al., 2015) , translation matrix (TM) (Mikolov et al., 2013) , isometric alignment (IA) (Zhang et al., 2016b) and adversarial training approach<cite> (Zhang et al., 2017)</cite> . Table 2 summarizes the performance of baseline models and our approach. The results of baseline models are cited from <cite>Zhang et al. (2017)</cite> . As we can see from the table, our model could achieve superior performance compared with other baseline models. ----------------------------------",
  "y": "uses differences"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_6",
  "x": "We use Chinese-English Translation Lexicon Version 3.0 (LDC2002L27) as our ground truth bilingual lexicon for evaluation. The baseline models are MonoGiza system (Dou et al., 2015) , translation matrix (TM) (Mikolov et al., 2013) , isometric alignment (IA) (Zhang et al., 2016b) and adversarial training approach<cite> (Zhang et al., 2017)</cite> . Table 2 summarizes the performance of baseline models and our approach. The results of baseline models are cited from <cite>Zhang et al. (2017)</cite> . As we can see from the table, our model could achieve superior performance compared with other baseline models. ---------------------------------- **EXPERIMENTS ON OTHER LANGUAGE PAIRS DATASETS** We also conduct experiments on Spanish-English and Italian-English language pairs. Again, we use the same dataset with <cite>Zhang et al. (2017)</cite> . and the statistics are shown in The experimental results are shown in Table 4 .",
  "y": "similarities uses"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_7",
  "x": "Our model is able to outperform baseline model in this setting. ---------------------------------- **MODEL** Accuracy ( ---------------------------------- **LARGE-SCALE DATASETS** In this section, we integrate our method with Conneau et al. (2018) , whose method improves <cite>Zhang et al. (2017)</cite> by more sophiscated refinement procedure and validation criterion. We replace their first step, namely the adversarial training step, with our model. Basically, we first map the source and target embeddings into the latent space using our algorithm, and then fine-tune the identity mapping in the latent space with the closed-form Procrustes solution.",
  "y": "extends differences"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_0",
  "x": "**INTRODUCTION** Natural language processing problems frequently involve scenarios in which a pair or group of related sentences need to be aligned to each other, establishing links between their common words or phrases. For instance, most approaches for natural language inference (NLI) rely on alignment techniques to establish the overlap between the given premise and a hypothesis before determining if the former entails the latter. Such monolingual alignment techniques are also frequently employed in systems for paraphrase generation, multi-document summarization, sentence fusion and question answering. Previous work<cite> (MacCartney et al., 2008)</cite> has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006) . However, MANLI's use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, <cite>MacCartney et al. (2008)</cite> employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation. In this paper, we propose an exact decoding technique for MANLI that retrieves the globally optimal alignment for a sentence pair given some parameters. Our approach is based on integer linear programming (ILP) and can leverage optimized general-purpose LP solvers to recover exact solutions.",
  "y": "background"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_1",
  "x": "This ILP-based decoding strategy permits us to consider syntacticallyinformed constraints on alignments which significantly increase the precision of the model. ---------------------------------- **INTRODUCTION** Natural language processing problems frequently involve scenarios in which a pair or group of related sentences need to be aligned to each other, establishing links between their common words or phrases. For instance, most approaches for natural language inference (NLI) rely on alignment techniques to establish the overlap between the given premise and a hypothesis before determining if the former entails the latter. Such monolingual alignment techniques are also frequently employed in systems for paraphrase generation, multi-document summarization, sentence fusion and question answering. Previous work<cite> (MacCartney et al., 2008)</cite> has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006) . However, MANLI's use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, <cite>MacCartney et al. (2008)</cite> employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation.",
  "y": "motivation"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_2",
  "x": "---------------------------------- **INTRODUCTION** Natural language processing problems frequently involve scenarios in which a pair or group of related sentences need to be aligned to each other, establishing links between their common words or phrases. For instance, most approaches for natural language inference (NLI) rely on alignment techniques to establish the overlap between the given premise and a hypothesis before determining if the former entails the latter. Such monolingual alignment techniques are also frequently employed in systems for paraphrase generation, multi-document summarization, sentence fusion and question answering. Previous work<cite> (MacCartney et al., 2008)</cite> has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006) . However, MANLI's use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, <cite>MacCartney et al. (2008)</cite> employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation. In this paper, we propose an exact decoding technique for MANLI that retrieves the globally optimal alignment for a sentence pair given some parameters.",
  "y": "background"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_3",
  "x": "This ILP-based decoding strategy permits us to consider syntacticallyinformed constraints on alignments which significantly increase the precision of the model. ---------------------------------- **INTRODUCTION** Natural language processing problems frequently involve scenarios in which a pair or group of related sentences need to be aligned to each other, establishing links between their common words or phrases. For instance, most approaches for natural language inference (NLI) rely on alignment techniques to establish the overlap between the given premise and a hypothesis before determining if the former entails the latter. Such monolingual alignment techniques are also frequently employed in systems for paraphrase generation, multi-document summarization, sentence fusion and question answering. Previous work<cite> (MacCartney et al., 2008)</cite> has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006) . However, MANLI's use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, <cite>MacCartney et al. (2008)</cite> employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation.",
  "y": "motivation"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_4",
  "x": "Alignment is an integral part of statistical MT (Vogel et al., 1996; Och and Ney, 2003; Liang et al., 2006) but the task is often substantively different from monolingual alignment, which poses unique challenges depending on the application<cite> (MacCartney et al., 2008)</cite> . Outside of NLI, prior research has also explored the task of monolingual word align-ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002) . ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009 ). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. ---------------------------------- **THE MANLI ALIGNER** Our alignment system is structured identically to MANLI<cite> (MacCartney et al., 2008)</cite> and uses the same phrase-based alignment representation. An alignment E between two fragments of text T 1 and T 2 is represented by a set of edits {e 1 , e 2 , . . .}, each belonging to one of the following types: \u2022 INS and DEL edits covering unaligned words in T 1 and T 2 respectively \u2022 SUB and EQ edits connecting a phrase in T 1 to a phrase in T 2 .",
  "y": "background motivation"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_5",
  "x": "MANLI was trained and evaluated on a corpus of human-generated alignment annotations produced by Microsoft Research (Brockett, 2007) for inference problems from the second Recognizing Textual Entailment (RTE2) challenge (Bar-Haim et al., 2006) . The corpus consists of a development set and test set that both feature 800 inference problems, each of which consists of a premise, a hypothesis and three independently-annotated human alignments. In our experiments, we merge the annotations using majority rule in the same manner as <cite>MacCartney et al. (2008)</cite> . ---------------------------------- **FEATURES** A MANLI alignment is scored as a sum of weighted feature values over the edits that it contains. Features encode the type of edit, the size of the phrases involved in SUB edits, whether the phrases are constituents and their similarity (determined by leveraging various lexical resources). Additionally, contextual features note the similarity of neighboring words and the relative positions of phrases while a positional distortion feature accounts for the difference between the relative positions of SUB edit phrases in their respective sentences. Our implementation uses the same set of features as <cite>MacCartney et al. (2008)</cite> with some minor changes: we use a shallow parser (Daum\u00e9 and Marcu, 2005) for detecting constituents and employ only string similarity and WordNet for determining semantic relatedness, forgoing NomBank and the distributional similarity resources used in the original MANLI implementation.",
  "y": "uses"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_6",
  "x": "ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009 ). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. ---------------------------------- **THE MANLI ALIGNER** Our alignment system is structured identically to MANLI<cite> (MacCartney et al., 2008)</cite> and uses the same phrase-based alignment representation. An alignment E between two fragments of text T 1 and T 2 is represented by a set of edits {e 1 , e 2 , . . .}, each belonging to one of the following types: \u2022 INS and DEL edits covering unaligned words in T 1 and T 2 respectively \u2022 SUB and EQ edits connecting a phrase in T 1 to a phrase in T 2 . EQ edits are a specific case of SUB edits that denote a word/lemma match; we refer to both types as SUB edits in this paper. Every token in T 1 and T 2 participates in exactly one edit.",
  "y": "uses"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_7",
  "x": "In our experiments, we merge the annotations using majority rule in the same manner as <cite>MacCartney et al. (2008)</cite> . ---------------------------------- **FEATURES** A MANLI alignment is scored as a sum of weighted feature values over the edits that it contains. Features encode the type of edit, the size of the phrases involved in SUB edits, whether the phrases are constituents and their similarity (determined by leveraging various lexical resources). Additionally, contextual features note the similarity of neighboring words and the relative positions of phrases while a positional distortion feature accounts for the difference between the relative positions of SUB edit phrases in their respective sentences. Our implementation uses the same set of features as <cite>MacCartney et al. (2008)</cite> with some minor changes: we use a shallow parser (Daum\u00e9 and Marcu, 2005) for detecting constituents and employ only string similarity and WordNet for determining semantic relatedness, forgoing NomBank and the distributional similarity resources used in the original MANLI implementation. ---------------------------------- **PARAMETER INFERENCE**",
  "y": "uses extends"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_8",
  "x": "**PARAMETER INFERENCE** Feature weights are learned using the averaged structured perceptron algorithm (Collins, 2002) , an intuitive structured prediction technique. We deviate from <cite>MacCartney et al. (2008)</cite> and do not introduce L2 normalization of weights during learning as this could have an unpredictable effect on the averaged parameters. For efficiency reasons, we parallelize the training procedure using iterative parameter mixing (McDonald et al., 2010) in our experiments. ---------------------------------- **DECODING** The decoding problem is that of finding the highestscoring alignment under some parameter values for the model. MANLI's phrase-based representation makes decoding more complex because the segmentation of T 1 and T 2 into phrases is not known beforehand. Every pair of phrases considered for inclusion in an alignment must adhere to some consistent segmentation so that overlapping edits and uncovered words are avoided.",
  "y": "differences extends"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_9",
  "x": "This expresses the score of an alignment as the sum of scores of edits that are present in it, i.e., edits e that have x e = 1. In order to address the phrase segmentation issue discussed in \u00a73.4, we merely need to add linear constraints ensuring that every token participates in exactly one edit. Introducing the notation e \u227a t to indicate that edit e covers token t in one of its phrases, this constraint can be encoded as: highest-scoring alignment under w. A similar approach is employed by DeNero and Klein (2008) for finding optimal phrase-based alignments for MT. ---------------------------------- **ALIGNMENT EXPERIMENTS** For evaluation purposes, we compare the performance of approximate search decoding against exact ILP-based decoding on a reimplementation of MANLI as described in \u00a73. All models are trained on the development section of the Microsoft Research RTE2 alignment corpus (cf. \u00a73.1) using the training parameters specified in <cite>MacCartney et al. (2008)</cite> . Aligner performance is determined by counting aligned token pairs per problem and macro-averaging over all problems.",
  "y": "uses"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_10",
  "x": "We first observe that our reimplemented version of MANLI improves over the results reported in <cite>MacCartney et al. (2008)</cite> , gaining 2% in precision, 1% in recall and 2-3% in the fraction of alignments that exactly matched human annotations. We attribute at least some part of this gain to our modified parameter inference (cf. \u00a73.3) which avoids normalizing the structured perceptron weights and instead adheres closely to the algorithm of Collins (2002) . Although exact decoding improves alignment performance over the approximate search approach, the gain is marginal and not significant. This seems to indicate that the simulated annealing search strategy is fairly effective at avoiding local maxima and finding the highest-scoring alignments. Table 2 contains the results from timing alignment tasks over various corpora on the same machine using the models trained as per \u00a74.1. We observe a twenty-fold improvement in performance with ILPbased decoding. It is important to note that the specific implementations being compared 2 may be responsible for the relative speed of decoding. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_11",
  "x": "Initial experiments with a feature that considers the similarity of dependency heads of tokens in an edit (similar to MANLI's contextual features that look at preceding and following words) yielded some improvement over the baseline models; however, this did not perform as well as the simple constraints described above. Specific features that approximate soft variants of these constraints could also be devised but this was not explored here. In addition to the NLI applications considered in this work, we have also employed the MANLI alignment technique to tackle alignment problems that are not inherently asymmetric such as the sentence fusion problems from McKeown et al. (2010) . Although the absence of asymmetric alignment features affects performance marginally over the RTE2 dataset, all the performance gains exhibited by exact decoding with constraints appear to be preserved in symmetric settings. ---------------------------------- **CONCLUSION** We present a simple exact decoding technique as an alternative to approximate search-based decoding in MANLI that exhibits a twenty-fold improvement in runtime performance in our experiments. In addition, we propose novel syntactically-informed constraints to increase precision. Our final system improves over the results reported in <cite>MacCartney et al. (2008)</cite> by about 4.5% in precision and 1% in recall, with a large gain in the number of perfect alignments over the test corpus.",
  "y": "differences"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_12",
  "x": "Most compellingly, the number of perfect alignments produced by the system increases significantly when compared to the unconstrained models from Table 1 (a relative increase of 35% on the test corpus). ---------------------------------- **DISCUSSION** The results of our evaluation indicate that exact decoding via ILP is a robust and efficient technique for solving alignment problems. Furthermore, the incorporation of simple constraints over a dependency parse can help to shape more accurate alignments. An examination of the alignments produced by our system reveals that many remaining errors can be tackled by the use of named-entity recognition and better paraphrase corpora; this was also noted by <cite>MacCartney et al. (2008)</cite> with regard to the original MANLI system. In addition, stricter constraints that enforce the alignment of syntactically-related tokens (rather than just their inclusion in the solution) may also yield performance gains. Although MANLI's structured prediction approach to the alignment problem allows us to encode preferences as features and learn their weights via the structured perceptron, the decoding constraints used here can be used to establish dynamic links between alignment edits which cannot be determined a priori. The interaction between the selection of soft features for structured prediction and hard constraints for decoding is an interesting avenue for further research on this task.",
  "y": "similarities future_work"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_0",
  "x": "Varis and Bojar (2017) implement and compare two multisource architectures: In the first setup, they use a single encoder with concatenation of src and mt sentences, and in the second setup, they use two character-level encoders for mt and src, separately, along with a character-level decoder. The initial state of this decoder is a weighted combination of the final states of the two encoders. Intuitively, such an integration of sourcelanguage information in APE should be useful in conveying the context information to improve the APE performance. To provide the awareness of errors in mt originating from src, the transformer architecture <cite>(Vaswani et al., 2017)</cite> , which is built solely upon attention mechanisms (Bahdanau et al., 2015) , makes it possible to model dependencies without regard to their distance in the input or output sequences and also captures global dependencies between input and output (for our case src, mt, and pe). The transformer architecture replaces recurrence and convolutions by using positional encodings on both the input and output sequences. The encoder and decoder both use multi-head (facilitating parallel computations) self-attention to compute representations of their corresponding inputs, and also compute multi-head vanilla-attentions between encoder and decoder representations. Our APE system extends this transformer-based NMT architecture <cite>(Vaswani et al., 2017)</cite> by using two encoders, a joint encoder, and a single decoder. Our model concatenates two separate selfattention-based encoders (enc src and enc mt ) and passes this sequence through another self-attended joint encoder (enc src,mt ) to ensure capturing dependencies between src and mt. Finally, this joint encoder is fed to the decoder which follows a similar architecture as described in<cite> Vaswani et al. (2017)</cite> .",
  "y": "background"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_1",
  "x": "Our APE system extends this transformer-based NMT architecture <cite>(Vaswani et al., 2017)</cite> by using two encoders, a joint encoder, and a single decoder. Our model concatenates two separate selfattention-based encoders (enc src and enc mt ) and passes this sequence through another self-attended joint encoder (enc src,mt ) to ensure capturing dependencies between src and mt. Finally, this joint encoder is fed to the decoder which follows a similar architecture as described in<cite> Vaswani et al. (2017)</cite> . The entire model is optimized as a single end-to-end transformer network. 2 Transformer-Based Multi-Source APE MT errors originating from the input source sentences suggest that APE systems should leverage information from both the src and mt, instead of considering mt in isolation. This can help the model to disambiguate corrections applied at every time step. Generating the pe output from mt is greatly facilitated by the availability of src. To achieve benefits from both single-source (mt \u2192 pe) and multi-source ({src, mt} \u2192 pe) APEs, our primary submission in the WMT 2018 shared task is an ensemble of these two models. Transformer-based models are currently providing state-of-the-art performance in MT; hence, we want to explore a similar architecture for this year's APE task.",
  "y": "differences extends"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_2",
  "x": "The encoder and decoder both use multi-head (facilitating parallel computations) self-attention to compute representations of their corresponding inputs, and also compute multi-head vanilla-attentions between encoder and decoder representations. Our APE system extends this transformer-based NMT architecture <cite>(Vaswani et al., 2017)</cite> by using two encoders, a joint encoder, and a single decoder. Our model concatenates two separate selfattention-based encoders (enc src and enc mt ) and passes this sequence through another self-attended joint encoder (enc src,mt ) to ensure capturing dependencies between src and mt. Finally, this joint encoder is fed to the decoder which follows a similar architecture as described in<cite> Vaswani et al. (2017)</cite> . The entire model is optimized as a single end-to-end transformer network. 2 Transformer-Based Multi-Source APE MT errors originating from the input source sentences suggest that APE systems should leverage information from both the src and mt, instead of considering mt in isolation. This can help the model to disambiguate corrections applied at every time step. Generating the pe output from mt is greatly facilitated by the availability of src. To achieve benefits from both single-source (mt \u2192 pe) and multi-source ({src, mt} \u2192 pe) APEs, our primary submission in the WMT 2018 shared task is an ensemble of these two models.",
  "y": "similarities uses"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_3",
  "x": "Our single-source model (SS) is based on an encoder-decoder-based transformer architecture <cite>(Vaswani et al., 2017)</cite> . Transformer models can replace sequence-aligned recurrence entirely and follow three types of multi-head attention: encoder-decoder attention (also known as vanilla Figure 1 : Multi-source transformer-based APE attention), encoder self-attention, and masked decoder self-attention. Since for multi-head attention each head uses different linear transformations, it can learn these separate relationships in parallel, thereby improving learning time. ---------------------------------- **MULTI-SOURCE TRANSFORMER FOR APE ({SRC, MT} \u2192 PE)** For our multi-source model (MS), we propose a novel joint transformer model (cf. Figure 1) , which combines the encodings of src and mt and attends over a combination of both sequences while generating the post-edited sentence. Apart from enc src and enc mt , each of which is equivalent to the original transformer's encoder <cite>(Vaswani et al., 2017)</cite> , we use a joint encoder with an equivalent architecture, to maintain the homogeneity of the transformer model. For this, we extend<cite> Vaswani et al. (2017)</cite> by introducing an additional identical encoding block by which both the enc src and the enc mt encoders communicate with the decoder. Our multi-source neural APE computes intermediate states enc src and enc mt for the two encoders, enc src,mt for their combination, and dec pe for the decoder in sequence-to-sequence modeling.",
  "y": "extends"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_4",
  "x": "(mt \u2192 pe) Our single-source model (SS) is based on an encoder-decoder-based transformer architecture <cite>(Vaswani et al., 2017)</cite> . Transformer models can replace sequence-aligned recurrence entirely and follow three types of multi-head attention: encoder-decoder attention (also known as vanilla Figure 1 : Multi-source transformer-based APE attention), encoder self-attention, and masked decoder self-attention. Since for multi-head attention each head uses different linear transformations, it can learn these separate relationships in parallel, thereby improving learning time. ---------------------------------- **MULTI-SOURCE TRANSFORMER FOR APE ({SRC, MT} \u2192 PE)** For our multi-source model (MS), we propose a novel joint transformer model (cf. Figure 1) , which combines the encodings of src and mt and attends over a combination of both sequences while generating the post-edited sentence. Apart from enc src and enc mt , each of which is equivalent to the original transformer's encoder <cite>(Vaswani et al., 2017)</cite> , we use a joint encoder with an equivalent architecture, to maintain the homogeneity of the transformer model. For this, we extend<cite> Vaswani et al. (2017)</cite> by introducing an additional identical encoding block by which both the enc src and the enc mt encoders communicate with the decoder.",
  "y": "differences extends"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_5",
  "x": "For this, we extend<cite> Vaswani et al. (2017)</cite> by introducing an additional identical encoding block by which both the enc src and the enc mt encoders communicate with the decoder. Our multi-source neural APE computes intermediate states enc src and enc mt for the two encoders, enc src,mt for their combination, and dec pe for the decoder in sequence-to-sequence modeling. One self-attended encoder for src maps s = (s 1 , s 2 , ..., s k ) into a sequence of continuous representations, enc src = (e 1 , e 2 , ..., e k ), and a second encoder for mt, m = (m 1 , m 2 , ..., m l ), returns another sequence of continuous representations, enc mt = (e \u2032 1 , e \u2032 2 , ..., e \u2032 l ). The self-attended joint encoder (cf. Figure 1 ) then receives the concatenation of enc src and enc mt , enc concat = [enc src , enc mt ] as an input, and passes it through the stack of 6 layers, with residual connections, a self-attention and a position-wise fully connected feed-forward neural network. As a result, the joint encoder produces a final representation (enc src,mt ) for both src and mt. Self-attention at this point provides the advantage of aggregating information from all of the words, including src and mt, and successively generates a new representation per word informed by the entire src and mt context. The decoder generates the pe output in sequence, dec pe = (p 1 , p 2 , . . . , p n ), one word at a time from left to right by attending previously generated words as well as the final representations (enc src,mt ) generated by the encoder. ---------------------------------- **ENSEMBLE**",
  "y": "differences extends"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_6",
  "x": "Each layer again consists of two sub-layers and a residual connection (He et al., 2016) around each of the two sublayers. During training, we employ label smoothing of value \u03f5 ls = 0.1. The output dimension produced by all sub-layers and embedding layers is defined as d model = 256. All dropout values in the network are set to 0.2. Each encoder and decoder contains a fully connected feed-forward network having dimensionality d model = 256 for the input and output and dimensionality d f f = 1024 for the inner layer. This is a similar setting to<cite> Vaswani et al. (2017)</cite> 's C \u2212 model 1 . For the scaled dotproduct attention, the input consists of queries and keys of dimension d k , and values of dimension d v . As multi-head attention parameters, we employ h = 8 for parallel attention layers, or heads. For each of these we use a dimensional-",
  "y": "similarities"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_7",
  "x": "As multi-head attention parameters, we employ h = 8 for parallel attention layers, or heads. For each of these we use a dimensional- For optimization, we use the Adam optimizer (Kingma and Ba, 2015) with \u03b2 1 = 0.9, \u03b2 2 = 0.98 and \u03f5 = 10 \u22129 . The learning rate is varied throughout the training process, first increasing linearly for the first training steps warmup steps = 4000 and then adjusted as described in <cite>(Vaswani et al., 2017)</cite> . At training time, the batch size is set to 32 samples, with a maximum sentence length of 80 subwords, and a vocabulary of the 50K most frequent subwords. After each epoch, the training data is shuffled. For encoding the word order, our model uses learned positional embeddings (Gehring et al., 2017) , since<cite> Vaswani et al. (2017)</cite> reported nearly identical results to sinusoidal encodings. After finishing training, we save the 5 best checkpoints saved at each epoch. Finally, we use a single model obtained by averaging the last 5 checkpoints.",
  "y": "similarities"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_8",
  "x": "For optimization, we use the Adam optimizer (Kingma and Ba, 2015) with \u03b2 1 = 0.9, \u03b2 2 = 0.98 and \u03f5 = 10 \u22129 . The learning rate is varied throughout the training process, first increasing linearly for the first training steps warmup steps = 4000 and then adjusted as described in <cite>(Vaswani et al., 2017)</cite> . At training time, the batch size is set to 32 samples, with a maximum sentence length of 80 subwords, and a vocabulary of the 50K most frequent subwords. After each epoch, the training data is shuffled. For encoding the word order, our model uses learned positional embeddings (Gehring et al., 2017) , since<cite> Vaswani et al. (2017)</cite> reported nearly identical results to sinusoidal encodings. After finishing training, we save the 5 best checkpoints saved at each epoch. Finally, we use a single model obtained by averaging the last 5 checkpoints. During decoding, we perform greedy-search-based decoding. We follow a similar hyper-parameter setup for mt \u2192 pe.",
  "y": "similarities"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_9",
  "x": "In this paper, we investigated a novel transformerbased multi-source APE approach that jointly attends over a combination of src and mt to capture dependencies between the two. This architecture yields statistically significant improvements over single-source transformer-based models. An ensemble of both variants increases the performance further. For the PBSMT task, the baseline MT system was outperformed by 3.2 BLEU points, while the NMT baseline remains 0.51 BLEU points better than our APE approach on the 2018 test set. In the future, we will investigate if the performance of each system can be improved by using a different hyper-parameter setup. Unfortunately, we could not test either the 'big' or the 'base' hyper-parameter configuration in<cite> Vaswani et al. (2017)</cite> due to unavailable computing resources at the time of submission. As additional future work, we would like to explore whether using re-ranking and ensembling of different neural APEs helps to improve the performance further. Moreover, we will incorporate word-level quality estimation features of mt into the encoding layer. Lastly, we will evaluate if our model indeed is able to better handle word order errors and to capture longrange dependencies, as we expect.",
  "y": "differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_0",
  "x": "To promote the study for multi-hop RC over multiple documents, two data sets are recently proposed: WIKIHOP (Welbl et al., 2018) and HotpotQA . These two data sets require multi-hop reasoning over multiple supporting documents to find the answer. In Figure 1 , we show an excerpt from one sample in WIKIHOP development set to illustrate the need for multi-hop reasoning. Two types of approaches have been proposed on the multi-hop multi-document RC problem. The first is based on previous neural RC models. The earliest attempt in (Dhingra et al., 2018) concatenated all supporting documents and designed a recurrent layer to explicitly exploit the skip connections between entities given automatically gener-ated coreference annotations. Adding this layer to the neural RC models improved performance on multi-hop tasks. Recently, an attention based system<cite> (Zhong et al., 2019)</cite> utilizing both documentlevel and entity-level information achieved stateof-the-art results on WIKIHOP data set, proving that techniques like co-attention and self-attention widely employed in single-document RC tasks are also useful in multi-document RC tasks. The second type of research work is based on graph neural networks (GNN) for multi-hop reasoning.",
  "y": "background"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_1",
  "x": "Adding this layer to the neural RC models improved performance on multi-hop tasks. Recently, an attention based system<cite> (Zhong et al., 2019)</cite> utilizing both documentlevel and entity-level information achieved stateof-the-art results on WIKIHOP data set, proving that techniques like co-attention and self-attention widely employed in single-document RC tasks are also useful in multi-document RC tasks. The second type of research work is based on graph neural networks (GNN) for multi-hop reasoning. The study in Song et al. (2018) In this paper, we propose a new method to solve the multi-hop RC problem across multiple documents. Inspired by the success of GNN based methods (Song et al., 2018; De Cao et al., 2018) for multi-hop RC, we introduce a new type of graph, called Heterogeneous Document-Entity (HDE) graph. Our proposed HDE graph has the following advantages: \u2022 Instead of graphs with single type of nodes (Song et al., 2018; De Cao et al., 2018) , the HDE graph contains different types of queryaware nodes representing different granularity levels of information. Specifically, instead of only entity nodes as in (Song et al., 2018; De Cao et al., 2018) , we include nodes corresponding to candidates, documents and entities. In addition, following the success of Coarse-grain Fine-grain Coattention (CFC) network<cite> (Zhong et al., 2019)</cite> , we apply both co-attention and self-attention to learn queryaware node representations of candidates, documents and entities;",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_2",
  "x": "\u2022 Instead of graphs with single type of nodes (Song et al., 2018; De Cao et al., 2018) , the HDE graph contains different types of queryaware nodes representing different granularity levels of information. Specifically, instead of only entity nodes as in (Song et al., 2018; De Cao et al., 2018) , we include nodes corresponding to candidates, documents and entities. In addition, following the success of Coarse-grain Fine-grain Coattention (CFC) network<cite> (Zhong et al., 2019)</cite> , we apply both co-attention and self-attention to learn queryaware node representations of candidates, documents and entities; \u2022 The HDE graph enables rich information interaction among different types of nodes thus facilitate accurate reasoning. Different types of nodes are connected with different types of edges to highlight the various structural information presented among query, document and candidates. Through ablation studies, we show the effectiveness of our proposed HDE graph for multi-hop multi-document RC task. Evaluated on the blind test set of WIKIHOP, our proposed end-to-end trained single neural model beats the current stateof-the-art results in<cite> (Zhong et al., 2019)</cite> 1 , without using pretrained contextual ELMo embedding (Peters et al., 2018) . ---------------------------------- **RELATED WORK**",
  "y": "differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_3",
  "x": "**RELATED WORK** The study presented in this paper is directly related to existing research on multi-hop reading comprehension across multiple documents (Dhingra et al., 2018; Song et al., 2018; De Cao et al., 2018;<cite> Zhong et al., 2019</cite>; Kundu et al., 2018) . The method presented in this paper is similar to previous studies using GNN for multi-hop reasoning (Song et al., 2018; De Cao et al., 2018) . Our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information. The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model<cite> (Zhong et al., 2019)</cite> because they show the effectiveness of attention mechanisms. Our model is very different from the other two studies (Dhingra et al., 2018; Kundu et al., 2018) : these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs. Besides these studies, our work is also related to the following research directions. Multi-hop RC: There exist several different data sets that require reasoning in multiple steps in literature, for example bAbI (Weston et al., 2015) , MultiRC (Khashabi et al., 2018) and OpenBookQA (Mihaylov et al., 2018) . A lot of systems have been proposed to solve the multi-hop RC problem with these data sets (Sun et al., 2018; Wu et al., 2019) .",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_4",
  "x": "The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model<cite> (Zhong et al., 2019)</cite> because they show the effectiveness of attention mechanisms. Our model is very different from the other two studies (Dhingra et al., 2018; Kundu et al., 2018) : these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs. Besides these studies, our work is also related to the following research directions. Multi-hop RC: There exist several different data sets that require reasoning in multiple steps in literature, for example bAbI (Weston et al., 2015) , MultiRC (Khashabi et al., 2018) and OpenBookQA (Mihaylov et al., 2018) . A lot of systems have been proposed to solve the multi-hop RC problem with these data sets (Sun et al., 2018; Wu et al., 2019) . However, these data sets require multi-hop reasoning over multiple sentences or multiple common knowledge while the problem we want to solve in this paper requires collecting evidences across multiple documents. GNN for NLP: Recently, there is considerable amount of interest in applying GNN to NLP tasks and great success has been achieved. For exam-ple, in neural machine translation, GNN has been employed to integrate syntactic and semantic information into encoders (Bastings et al., 2017; Marcheggiani et al., 2018) ; applied GNN to relation extraction over pruned dependency trees; the study by Yao et al. (2018) employed GNN over a heterogeneous graph to do text classification, which inspires our idea of the HDE graph; Liu et al. (2018) proposed a new contextualized neural network for sequence learning by leveraging various types of non-local contextual information in the form of information passing over GNN. These studies are related to our work in the sense that we both use GNN to improve the information interaction over long context or across documents.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_5",
  "x": "For example, the entity \"get ready\" in query and two entities \"Mase\" and \"Sean Combs\" co-occur in the 2nd support document, and both \"Mase\" and \"Sean Combs\" can lead to the correct answer \"bad boy records\". Based on this observation, we propose to extract mentions of both query subject s and candidates C q from documents. We will show later that by including mentions of query subject the performance can be improved. We use simple exact match strategy (De Cao et al., 2018;<cite> Zhong et al., 2019)</cite> to find the locations of mentions of query subject and candidates, i.e. we need the start and end positions of each mention. Each mention is treated as an entity. Then, representations of entities can be taken out from the i-th document encoding H i s . We denote an entity's representation as M \u2208 R lm\u00d7h where l m is the length of the entity. Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (Seo et al., 2016; Xiong et al., 2016) , and recently was applied to multiple-hop reading comprehension<cite> (Zhong et al., 2019)</cite> . Coattention enables the model to combine learned query contextual information attended by document and document contextual information attended by query, with inputs of one query and one document.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_6",
  "x": "We will show later that by including mentions of query subject the performance can be improved. We use simple exact match strategy (De Cao et al., 2018;<cite> Zhong et al., 2019)</cite> to find the locations of mentions of query subject and candidates, i.e. we need the start and end positions of each mention. Each mention is treated as an entity. Then, representations of entities can be taken out from the i-th document encoding H i s . We denote an entity's representation as M \u2208 R lm\u00d7h where l m is the length of the entity. Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (Seo et al., 2016; Xiong et al., 2016) , and recently was applied to multiple-hop reading comprehension<cite> (Zhong et al., 2019)</cite> . Coattention enables the model to combine learned query contextual information attended by document and document contextual information attended by query, with inputs of one query and one document. We follow the implementation of coattention in<cite> (Zhong et al., 2019)</cite> . We use the co-attention between a query and a supporting document for illustration.",
  "y": "background"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_7",
  "x": "We will show later that by including mentions of query subject the performance can be improved. We use simple exact match strategy (De Cao et al., 2018;<cite> Zhong et al., 2019)</cite> to find the locations of mentions of query subject and candidates, i.e. we need the start and end positions of each mention. Each mention is treated as an entity. Then, representations of entities can be taken out from the i-th document encoding H i s . We denote an entity's representation as M \u2208 R lm\u00d7h where l m is the length of the entity. Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (Seo et al., 2016; Xiong et al., 2016) , and recently was applied to multiple-hop reading comprehension<cite> (Zhong et al., 2019)</cite> . Coattention enables the model to combine learned query contextual information attended by document and document contextual information attended by query, with inputs of one query and one document. We follow the implementation of coattention in<cite> (Zhong et al., 2019)</cite> . We use the co-attention between a query and a supporting document for illustration.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_8",
  "x": "where denotes matrix transpose. Each entry of the matrix A i qs indicates how related two words are, one from the query and one from the document. For simplification, in later context, we ignore the superscript i which indicates the operation on the i-th document. Next we derive the attention context of the query and document as follows: sof tmax(\u00b7) denotes column-wise normalization. We further encode the co-attended document context using a bidirectional RNN f with GRU: The final co-attention context is the columnwise concatenation of C s and D s : We expect S ca carries query-aware contextual information of supporting documents as shown by <cite>Zhong et al. (2019)</cite> . The same co-attention module can also be applied to query and candidates, and query and entities (as shown in Figure 2 ) to get C ca and E ca . Note that we do not do coattention between query and entities corresponding to query subject because query subject is already a part of the query.",
  "y": "uses differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_9",
  "x": "The same co-attention module can also be applied to query and candidates, and query and entities (as shown in Figure 2 ) to get C ca and E ca . Note that we do not do coattention between query and entities corresponding to query subject because query subject is already a part of the query. To keep the dimensionality consistent, we apply a single-layer multi-layer perceptron (MLP) with tanh activation function to increase the dimension of the query subject entities to 2h. Self-attentive pooling: while co-attention yields a query-aware contextual representation of documents, self-attentive pooling is designed to convert the sequential contextual representation to a fixed dimensional non-sequential feature vector by selecting important query-aware information<cite> (Zhong et al., 2019)</cite> . Self-attentive pooling summarizes the information presented in the coattention output by calculating a score for each word in the sequence. The scores are normalized and a weighted sum based pooling is applied to the sequence to get a single feature vector as the summarization of the input sequence. Formally, the self-attention module can be formulated as the following operations given S ca as input: where M LP (\u00b7) is a two-layer MLP with tanh as activation function. Similarly, after self-attentive pooling, we can get c sa and e sa for each candidate and entity. Our context encoding module is different from the one used in <cite>Zhong et al. (2019)</cite> in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_10",
  "x": "To keep the dimensionality consistent, we apply a single-layer multi-layer perceptron (MLP) with tanh activation function to increase the dimension of the query subject entities to 2h. Self-attentive pooling: while co-attention yields a query-aware contextual representation of documents, self-attentive pooling is designed to convert the sequential contextual representation to a fixed dimensional non-sequential feature vector by selecting important query-aware information<cite> (Zhong et al., 2019)</cite> . Self-attentive pooling summarizes the information presented in the coattention output by calculating a score for each word in the sequence. The scores are normalized and a weighted sum based pooling is applied to the sequence to get a single feature vector as the summarization of the input sequence. Formally, the self-attention module can be formulated as the following operations given S ca as input: where M LP (\u00b7) is a two-layer MLP with tanh as activation function. Similarly, after self-attentive pooling, we can get c sa and e sa for each candidate and entity. Our context encoding module is different from the one used in <cite>Zhong et al. (2019)</cite> in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model. 2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while <cite>Zhong et al. (2019)</cite> first do self-attention on entity word sequences to get a sequence of entity vectors in each documents.",
  "y": "differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_11",
  "x": "To keep the dimensionality consistent, we apply a single-layer multi-layer perceptron (MLP) with tanh activation function to increase the dimension of the query subject entities to 2h. Self-attentive pooling: while co-attention yields a query-aware contextual representation of documents, self-attentive pooling is designed to convert the sequential contextual representation to a fixed dimensional non-sequential feature vector by selecting important query-aware information<cite> (Zhong et al., 2019)</cite> . Self-attentive pooling summarizes the information presented in the coattention output by calculating a score for each word in the sequence. The scores are normalized and a weighted sum based pooling is applied to the sequence to get a single feature vector as the summarization of the input sequence. Formally, the self-attention module can be formulated as the following operations given S ca as input: where M LP (\u00b7) is a two-layer MLP with tanh as activation function. Similarly, after self-attentive pooling, we can get c sa and e sa for each candidate and entity. Our context encoding module is different from the one used in <cite>Zhong et al. (2019)</cite> in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model. 2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while <cite>Zhong et al. (2019)</cite> first do self-attention on entity word sequences to get a sequence of entity vectors in each documents.",
  "y": "differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_13",
  "x": "The performance on development set is measured after each training epoch, and the model with the highest accuracy is saved and submitted to be evaluated on the blind test set. We will make our code publicly available after the review process. ---------------------------------- **RESULTS** In Table 1 , we show the results of the our proposed HDE graph based model on both development and test set and compare it with previously published results. We show that our proposed HDE graph based model improves the state-of-the-art accuracy on development set from 67.1% (Kundu et al., 2018) to 68.1%, on the blind test set from 70.6%<cite> (Zhong et al., 2019)</cite> to 70.9%. Compared to two previous studies using GNN for multi-hop reading comprehension (Song et al., 2018; De Cao et al., 2018) , our model surpasses them by a large margin even though we do not use better pre-trained contextual embedding ELMo (Peters et al., 2018) . ---------------------------------- **ABLATION STUDIES**",
  "y": "differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_0",
  "x": "Cultural transmission of language occurs when one group of agents passes their language to a new group of agents, e.g. parents who teach their children to speak as they do. Of the many design features which make human language unique, cultural transmission is important partially because it allows language itself to change over time via cultural evolution (Tomasello, 1999; Christiansen & Kirby, 2003a) . This helps explain how a modern language like English emerged from some proto-language, an \"almost language\" precursor lacking the functionality of modern languages. Figure 1 . We introduce cultural transmission into language emergence between neural agents. The starting point of our study is the goal oriented dialogue task of <cite>Kottur et al. (2017)</cite> , summarized in Fig. 2 . During learning we periodically replace some agents with new ones (gray agents). These new agents do not know any language, but instead of creating one they learn it from older agents. This creates generations of language that become more compositional over time.",
  "y": "uses"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_1",
  "x": "This kind of structure helps give human language the ability to express infinitely many concepts using finitely many elements, and to generalize in obviously correct ways despite a dearth of training examples (Lake & Baroni, 2018) . For example, an agent who understands blue square and purple triangle should also understand purple square without directly experiencing it; we use this sort of generalization to measure compositionality. Existing work has investigated conditions under which compositional languages emerge between neural agents in simple environments (Mordatch & Abbeel, 2018;<cite> Kottur et al., 2017)</cite> , but it only investigates how language changes within a generation. Simulating cultural transmission, the iterated learning model (Kirby et al., 2008; Kirby, 2001; Kirby et al., 2014) has found that generational dynamics cause compositional language to emerge using experiments in simulation (Kirby, 2001 ) and with human subjects (Kirby et al., 2008) . In this model, language is directly but incompletely transmitted (taught) to one generation of agents from the previous generation. Because learning is incomplete and biased, the student language may differ from the teacher language. With the right learning and transmission mechanisms, a noncompositional language becomes compositional after many generations. This is cast as a trade-off between expressivity and compressibility, where a language must be expressive enough to differentiate between all possible meanings (e.g., objects) and compressible enough to be learned (Kirby et al., 2015) . The explanation is so prominent that it was somewhat surprising when it was recently found that other factors can cause enough compressibility pressure to get compositional language emergence without generational transmission (Raviv et al., 2018) .",
  "y": "motivation background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_2",
  "x": "If some of the agents in the environment already know a language like English then the other agents can indirectly learn that language. But even this is expensive because it requires some agent that already knows a language. On the other end of the spectrum, and most relevant to us, some work has found that languages will emerge to enable communication-centric tasks to be solved without direct or even indirect language supervision (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017; Das et al., 2017b) . Unlike regimes where agents are trained to learn an existing language, languages that emerge in this sort of setting are not necessarily easy to understand for a human. Even attempts to translate these emerged languages for other agents are not completely successful (Andreas et al., 2017) , possibly because the target languages can't express the same concepts as the source languages. This desire for structure motivates the previously mentioned work on compositional language emergence in neural agents<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018; Choi et al., 2018) . In this paper, we study the following question -what are the conditions that lead to the emergence of a compositional language? Our key finding is evidence that cultural transmission leads to more compositional language in deep reinforcement learning agents, as it does in evolutionary linguistics. The starting point for our investigation is the recent work of <cite>Kottur et al. (2017)</cite> , which investigates compositionality using a cooperative reference game between two agents. Instead of using the same set of agents throughout training, we replace (re-initialize) some subset of them periodically.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_3",
  "x": "Our key finding is evidence that cultural transmission leads to more compositional language in deep reinforcement learning agents, as it does in evolutionary linguistics. The starting point for our investigation is the recent work of <cite>Kottur et al. (2017)</cite> , which investigates compositionality using a cooperative reference game between two agents. Instead of using the same set of agents throughout training, we replace (re-initialize) some subset of them periodically. The resulting knowledge gap makes it easier for the new agent to learn from the older agents than to create a new language. In this way our approach introduces cultural transmission and thus a compressibility pressure, causing compositionality to emerge. One difference between our approach and evolutionary methods applied elsewhere in deep learning (Stanley & Miikkulainen, 2002; Stanley et al., 2009; Real et al., 2017) is that we emulate cultural evolution instead of biological evolution. In biological evolution agents change from generation to generation while in cultural evolution the language itself evolves, so the same agent can have different languages at different times. Agents can directly benefit from evolutionary innovations throughout their lifetime instead of only at the beginning. Our approach is also different from iterated learning because our version of cultural transmission is implicit instead of explicit.",
  "y": "uses"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_4",
  "x": "Q-bot starts by observing A-bot's previous response m t\u22121 A , its view of the world x Q (e.g., task), and its previous memory h t\u22121 Q , then outputs memory h t Q summarizing its current view of the dialog and utters message m A ). After the conversation, Q-bot tries to solve the given task by predicting u (e.g., corresponding to red square) as a function of its observation and final memory:\u00fb = U (x Q , h T Q ). Both agents are rewarded if both attributes are correct. As in <cite>Kottur et al. (2017)</cite> , we implement Q, A, and U as neural networks. Our model is trained to maximize the reward using policy gradients (Williams, 1992) . Unlike an approach supervised by human dialogues, nothing orients the agents toward specific meanings for specific words, so they must create their own appropriately grounded language to solve the task. ---------------------------------- **1**",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_5",
  "x": "In <cite>Kottur et al. (2017)</cite> it was used to generate a somewhat compositional language given Algorithm 1: Training with Replacement and Multiple Agents Policy gradient update w.r.t. both Q-bot and A-bot parameters appropriate agent and vocabulary configurations. ---------------------------------- **LANGUAGE EMERGENCE WITH CULTURAL TRANSMISSION** Here we add cultural transmission to neural dialog models by considering an implicit model of cultural transmission. Implicit cultural transmission does not use word-level supervision, as opposed to explicit cultural transmission in which students are told which words refer to which objects. In implicit cultural transmission shared language emerges from shared goals. We develop an implicit model of cultural transmission 2 that periodically replaces agents.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_6",
  "x": "Both agents are rewarded if both attributes are correct. As in <cite>Kottur et al. (2017)</cite> , we implement Q, A, and U as neural networks. Our model is trained to maximize the reward using policy gradients (Williams, 1992) . Unlike an approach supervised by human dialogues, nothing orients the agents toward specific meanings for specific words, so they must create their own appropriately grounded language to solve the task. ---------------------------------- **1** This approach-summarized in the black lines (4-9) of Algorithm 1-is our starting point. In <cite>Kottur et al. (2017)</cite> it was used to generate a somewhat compositional language given Algorithm 1: Training with Replacement and Multiple Agents Policy gradient update w.r.t.",
  "y": "uses background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_7",
  "x": "In <cite>Kottur et al. (2017)</cite> there is only one pair of agents (N Q = N A = 1) so we cannot replace both agents at the same round because all existing language would be lost. Instead, we consider two strategies that only replace one bot at a time: -Random. Sample Q-bot or A-bot uniformly -Alternate. Alternate between Q-bot and A-bot Multi Agent. Q-bot and A-bot have different roles due to the asymmetry in information, so they use different parts of the language. Replacing A-bot (alt. Q-bot) means Q-bot (alt. A-bot) has to remember what A-bot says or else knowledge about that part of the language is lost. If A-bot was replaced but other A-bots speaking the same language were present then Q-bot would have incentive to remember the original language because of the other bots, preventing language loss. Furthermore, a Multi Agent environment may add compressibility pressure (Raviv et al., 2018) as bots have more to remember if there is any difference between the languages of their conversational partners.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_8",
  "x": "**NEURAL QUESTION ANSWERING DETAILS** Task Description. As in <cite>Kottur et al. (2017)</cite> , our world contains objects with 3 attributes (shape, size, color) such that each attribute has 4 possible values. Objects are represented 'symbolically' as 3-hot vectors and not rendered as RGB images. Evaluation with Compositional Dataset. The explicit annotation of independent properties like shape and color allows compositionality to be tested, a necessarily domain specific evaluation. Certain combinations of attributes (e.g., purple square) are held out of the training set while ensuring that at least one instance of each attribute is present (e.g., at least one purple thing and one square thing). If the language created by interaction between agents can identify the held out instances (e.g., it has unique words for purple square which both agents understand) then it is compositional. This is simply measured by accuracy on the test set.",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_9",
  "x": "Certain combinations of attributes (e.g., purple square) are held out of the training set while ensuring that at least one instance of each attribute is present (e.g., at least one purple thing and one square thing). If the language created by interaction between agents can identify the held out instances (e.g., it has unique words for purple square which both agents understand) then it is compositional. This is simply measured by accuracy on the test set. Previous work also measures generalization to held out compositions of attributes to measure compositionality<cite> (Kottur et al., 2017</cite>; Kirby et al., 2015) . Unlike <cite>Kottur et al. (2017)</cite> , we use a slightly harder version of their dataset which aligns better with the goal of compositional language. For a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. This disallows opportunities for non-compositional generalization. Without this constraint, agents could generalize perfectly using words for attribute pairs like \"red triangle\" and \"filled star\" instead of words for \"red,\" \"triangle,\" \"filled,\" and \"star.\" The drop in accuracy 5 between test and validation (which does not hold out attribute pairs) is roughly 20 points. Architecture and Training.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_10",
  "x": "If the language created by interaction between agents can identify the held out instances (e.g., it has unique words for purple square which both agents understand) then it is compositional. This is simply measured by accuracy on the test set. Previous work also measures generalization to held out compositions of attributes to measure compositionality<cite> (Kottur et al., 2017</cite>; Kirby et al., 2015) . Unlike <cite>Kottur et al. (2017)</cite> , we use a slightly harder version of their dataset which aligns better with the goal of compositional language. For a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. This disallows opportunities for non-compositional generalization. Without this constraint, agents could generalize perfectly using words for attribute pairs like \"red triangle\" and \"filled star\" instead of words for \"red,\" \"triangle,\" \"filled,\" and \"star.\" The drop in accuracy 5 between test and validation (which does not hold out attribute pairs) is roughly 20 points. Architecture and Training. Our A-bots and Q-bots have the same architecture and hyperparameter variations as in <cite>Kottur et al. (2017)</cite> , but with our cultural transmission training procedure and some other differences identified below.",
  "y": "differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_11",
  "x": "This disallows opportunities for non-compositional generalization. Without this constraint, agents could generalize perfectly using words for attribute pairs like \"red triangle\" and \"filled star\" instead of words for \"red,\" \"triangle,\" \"filled,\" and \"star.\" The drop in accuracy 5 between test and validation (which does not hold out attribute pairs) is roughly 20 points. Architecture and Training. Our A-bots and Q-bots have the same architecture and hyperparameter variations as in <cite>Kottur et al. (2017)</cite> , but with our cultural transmission training procedure and some other differences identified below. Like <cite>Kottur et al. (2017)</cite> , our hyperparameter variations consider the number of vocab words Q-bot (V Q ) and A-bot (V A ) may utter and whether or not A-bot has memory between dialog rounds. The memoryless version of A-bot simply sets h t A = 0 between each round of dialog. This means A-bot cannot represent which attributes it has already communicated. When there are too many vocab words available there is less pressure to develop a compositional language because for every new object there is always an unused sequence of words which isn't too similar to existing words, an effect also noticed elsewhere (Mordatch & Abbeel, 2018; Nowak et al., 2000) . We add one setting where A-bot has no memory yet the number of vocab words is still overcomplete to help understand and disentangle these two factors.",
  "y": "similarities differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_12",
  "x": "Without this constraint, agents could generalize perfectly using words for attribute pairs like \"red triangle\" and \"filled star\" instead of words for \"red,\" \"triangle,\" \"filled,\" and \"star.\" The drop in accuracy 5 between test and validation (which does not hold out attribute pairs) is roughly 20 points. Architecture and Training. Our A-bots and Q-bots have the same architecture and hyperparameter variations as in <cite>Kottur et al. (2017)</cite> , but with our cultural transmission training procedure and some other differences identified below. Like <cite>Kottur et al. (2017)</cite> , our hyperparameter variations consider the number of vocab words Q-bot (V Q ) and A-bot (V A ) may utter and whether or not A-bot has memory between dialog rounds. The memoryless version of A-bot simply sets h t A = 0 between each round of dialog. This means A-bot cannot represent which attributes it has already communicated. When there are too many vocab words available there is less pressure to develop a compositional language because for every new object there is always an unused sequence of words which isn't too similar to existing words, an effect also noticed elsewhere (Mordatch & Abbeel, 2018; Nowak et al., 2000) . We add one setting where A-bot has no memory yet the number of vocab words is still overcomplete to help understand and disentangle these two factors. Specifically, we consider the following settings:",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_13",
  "x": "Specifically, we consider the following settings: All agents are trained for E = 5000 epochs with a batch size of 1000 (so 1 batch per epoch) and the Adam (Kingma & Ba, 2015) optimizer with learning rate 0.01. In the Multi Agent setting we use N A = N Q = 5. To decide when to stop we measure validation set accuracy averaged over all Q-bot-Abot pairs and choose the first population whose validation accuracy did not improve for 200k epochs. 7 This differs from <cite>Kottur et al. (2017)</cite> , which stopped once train accuracy reached 100%. Furthermore, we do not mine negatives for each training batch. ---------------------------------- **BASELINES. TWO BASELINES HELP VERIFY OUR APPROACH:** -No Replacement.",
  "y": "differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_14",
  "x": "Never replace any agent (i.e., Algorithm 1 without blue lines). -Replace All. Always replace every agent (i.e., with B = all agents at line 11 of Algorithm 1). Comparing to the No Replacement baseline establishes the main result by measuring the difference replacement makes. However, each time lines 11 and 12 of Algorithm 1 are executed there is one more chance of getting a lucky random initialization. Since the No Replacement baseline never does this it has a smaller chance of running in to one such lucky agent. Thus we compare to the Replace All baseline, which has the greatest chance of seeing a lucky initialization and thereby ensures that gains over the No Replacement baseline 6 This is slightly different from Small Vocab in<cite> (Kottur et al., 2017)</cite> . 7 There are few objects in the environment, so each batch contains all objects and is an entire epoch. are not simply due to luck.",
  "y": "differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_15",
  "x": "The Multi Agent No Replacement policy is only better than the Single Agent No Replacement policy for the Memoryless models (p \u2264 0.05), and not otherwise. It is somewhat surprising that this difference is not stronger since having multiple agents in the environment is one of the factors found to lead to compositionality without using cultural (more specifically, generational) transmission in (Raviv et al., 2018) . Variations in replacement strategy do not appear to significantly affect outcomes. The Single Agent Random/Alternate replacement strategies are usually not significantly different than each other (p \u2264 0.05). The same is true for the Multi Agent Uniform Random/Epsilon Greedy/Oldest strategies. Significant differences only occur in the Overcomplete setting, where Single Agent Alternate outperforms Multi Agent Uniform Random, and in a few Small Vocab settings. This suggests that while some agent replacement needs to occur, it does not much matter whether agents with worse language are replaced or whether there is a pool of similarly typed agents to remember knowledge lost from older generations. The main factor is that new agents learn in the presence of others who already know a language. Test set accuracies (with standard deviations) are reported against our new harder dataset using models similar to those in<cite> (Kottur et al., 2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_16",
  "x": "The models considered in <cite>Kottur et al. (2017)</cite> were ordered, from best to worse, as: Memoryless + Small Vocab > Small Vocab > Overcomplete. Our trends tend to agree with that conclusion though the differences are smaller-mainly comparing the Memoryless + Small Vocab model to others in cultural transmission settings. Only in the Oldest setting are the differences all significant enough to completely establish the above ordering. This agrees with factors noted elsewhere<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018; Nowak et al., 2000) . Removing memory sometimes hurts. Removing memory always makes a significant difference (p \u2264 0.05) to Small Vocab models and only sometimes makes a difference for Overcomplete models. When it is significant, it helps Small Vocab models and hurts Overcomplete models. As the Memoryless + Overcomplete setting has not been reported before, these results suggest that the relationship between inter-round memory and compositionality is not clear. Overall, these results show that adding cultural transmission to neural dialog agents improves the compositional generalization of the languages learned by those agents in a way complementary to other priors.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_17",
  "x": "The models considered in <cite>Kottur et al. (2017)</cite> were ordered, from best to worse, as: Memoryless + Small Vocab > Small Vocab > Overcomplete. Our trends tend to agree with that conclusion though the differences are smaller-mainly comparing the Memoryless + Small Vocab model to others in cultural transmission settings. Only in the Oldest setting are the differences all significant enough to completely establish the above ordering. This agrees with factors noted elsewhere<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018; Nowak et al., 2000) . Removing memory sometimes hurts. Removing memory always makes a significant difference (p \u2264 0.05) to Small Vocab models and only sometimes makes a difference for Overcomplete models. When it is significant, it helps Small Vocab models and hurts Overcomplete models. As the Memoryless + Overcomplete setting has not been reported before, these results suggest that the relationship between inter-round memory and compositionality is not clear. Overall, these results show that adding cultural transmission to neural dialog agents improves the compositional generalization of the languages learned by those agents in a way complementary to other priors.",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_18",
  "x": "The main factor is that new agents learn in the presence of others who already know a language. Test set accuracies (with standard deviations) are reported against our new harder dataset using models similar to those in<cite> (Kottur et al., 2017)</cite> . Our variations on cultural transmission outperform the baselines (lighter two green and lighter two blue bars) where language does not change over generations. Cultural transmission is complementary with other factors that encourage compositionality. The models considered in <cite>Kottur et al. (2017)</cite> were ordered, from best to worse, as: Memoryless + Small Vocab > Small Vocab > Overcomplete. Our trends tend to agree with that conclusion though the differences are smaller-mainly comparing the Memoryless + Small Vocab model to others in cultural transmission settings. Only in the Oldest setting are the differences all significant enough to completely establish the above ordering. This agrees with factors noted elsewhere<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018; Nowak et al., 2000) . Removing memory sometimes hurts.",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_19",
  "x": "An important piece of the explanation of linguistic structure is the iterated learning model (Kirby et al., 2014; Kirby, 2001; Kirby et al., 2008) described in Section 1. This model focuses on the cultural transmission and bottlenecks that restrict how languages can be learned, showing compositional language emerges in computational (Kirby, 2001; Christiansen & Kirby, 2003b; Smith et al., 2003) and human (Kirby et al., 2008; Cornish et al., 2009; Scott-Phillips & Kirby, 2010) experiments. Even though cultural transmission may aid the emergence of compositionality, recent results in evolutionary linguistics (Raviv et al., 2018) and deep learning<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018) show cultural transmission may not be necessary for compositionality to emerge. While existing work in deep learning has focused on biases that encourage compositionality, it has not considered settings where language is permitted to evolve as it is passed down over generations of agents. We consider such a setting because of its potential to complement our existing understanding. Language Emergence in Deep Learning. Recent work in deep learning has increasingly focused on multi-agent environments where deep agents learn to accomplish goals (possibly cooperative or competitive) by interacting appropriately with the environment and each other. Some of this work has shown that deep agents will develop their own language where none exists initially if driven by a task which requires communication (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017) . Most relevant is similar work which focuses on conditions under which compositional language emerges as deep agents learn to cooperate (Mordatch & Abbeel, 2018;<cite> Kottur et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_20",
  "x": "Recent work in deep learning has increasingly focused on multi-agent environments where deep agents learn to accomplish goals (possibly cooperative or competitive) by interacting appropriately with the environment and each other. Some of this work has shown that deep agents will develop their own language where none exists initially if driven by a task which requires communication (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017) . Most relevant is similar work which focuses on conditions under which compositional language emerges as deep agents learn to cooperate (Mordatch & Abbeel, 2018;<cite> Kottur et al., 2017)</cite> . Both Mordatch & Abbeel (2018) and <cite>Kottur et al. (2017)</cite> find that limiting the vocabulary size so that there aren't too many more words than there are objects to refer to encourages compositionality, which follows earlier results in evolutionary linguistics (Nowak et al., 2000) . Follow up work has continued to investigate the emergence of compositional language among neural agents, mainly focusing on perceptual as opposed to symbolic input and how the structure of the input relates to the tendency for compositional language to emerge (Choi et al., 2018; Havrylov & Titov, 2017; Lazaridou et al., 2018) . Other work investigating emergent translation has shown that Multi Agent interaction leads to better translation (Lee et al., 2018) , but they do not measure compositionality. Cultural Evolution and Neural Nets. Some work has considered the evolution of ideas by cultural transmission using neural agents. Most recently, Bengio (2012) considers what benefit cultural evolution might bestow on neural agents.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_21",
  "x": "Both Mordatch & Abbeel (2018) and <cite>Kottur et al. (2017)</cite> find that limiting the vocabulary size so that there aren't too many more words than there are objects to refer to encourages compositionality, which follows earlier results in evolutionary linguistics (Nowak et al., 2000) . Follow up work has continued to investigate the emergence of compositional language among neural agents, mainly focusing on perceptual as opposed to symbolic input and how the structure of the input relates to the tendency for compositional language to emerge (Choi et al., 2018; Havrylov & Titov, 2017; Lazaridou et al., 2018) . Other work investigating emergent translation has shown that Multi Agent interaction leads to better translation (Lee et al., 2018) , but they do not measure compositionality. Cultural Evolution and Neural Nets. Some work has considered the evolution of ideas by cultural transmission using neural agents. Most recently, Bengio (2012) considers what benefit cultural evolution might bestow on neural agents. The main idea is that culturally transmitted ideas may provide an important mechanism for escaping local minima in large complex models. Experiments in G\u00fcl\u00e7ehre & Bengio (2016) followed up on that idea and supported part of the hypothesis by showing that supervision of intermediate representations allows a more complex toy task to be learned. Unlike our work, these experiments use direct language supervision provided by the designed environment rather than indirect and implicit supervision provided by other agents.",
  "y": "background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_0",
  "x": "**ABSTRACT** Recently, translation scholars have made some general claims about translation properties. Some of these are source language independent while others are not. <cite>Koppel and Ordan (2011)</cite> performed empirical studies to validate both types of properties using English source texts and other texts translated into English. Obviously, corpora of this sort, which focus on a single language, are not adequate for claiming universality of translation properties. In this paper, we are validating both types of translation properties using original and translated texts from six European languages. ---------------------------------- **INTRODUCTION** Even though it is content words that are semantically rich, function words also play an important role in a text.",
  "y": "background motivation"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_1",
  "x": "Recently, scholars in this area identified several properties of the translation process with the aid of corpora (Baker, 1993; Baker, 1996; Olohan, 2001; Laviosa, 2002; Hansen, 2003; Pym, 2005) . These properties are subsumed under four keywords: explicitation, simplification, normalization and levelling out. They focus on the general effects of the translation process. Toury (1995) has a different theory from these. That is, a translated text will carry some fingerprints of its source language. Recently, Pastor et al. (2008) and Ilisei et al. (2009; have provided empirical evidence of simplification translation properties using a comparable corpus of Spanish. <cite>Koppel and Ordan (2011)</cite> perform empirical studies to validate both theories, using a subcorpus extracted from the Europarl (Koehn, 2005) and IHT corpora<cite> (Koppel and Ordan, 2011)</cite> . They used a comparable corpus of original English and English translated from five other European languages. In addition, original English and English translated from Greek and Korean was also used in their experiment. They have found that a translated text contains both source language dependent and independent features. Obviously, corpora of this sort, which focus on a single language (e.g., English), are not adequate for claiming the universal validity of translation properties. Different languages (and language families) have different linguistic properties.",
  "y": "motivation background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_2",
  "x": "The focus of their works is to investigate the simplification relation that was proposed by (Baker, 1996) . In total, 21 quantitative features (e.g. a number of different POS, average sentence length, the parse-tree depth etc.) were used where, nine (9) of them are able to grasp the simplification translation property. <cite>Koppel and Ordan (2011)</cite> have built a classifier that can identify the correct source of the translated text (given different possible source languages). They have built another classifier which can identify source text and translated text. Furthermore, they have shown that the degree of difference between two translated texts, translated from two different languages into the same target language reflects, the degree of difference of the source languages. They have gained impressive results for both of the tasks. However, the limitation of this study is that they only used a corpus of English original text and English text translated from various European languages. A list of 300 function words (Pennebaker et al., 2001 ) was used as feature vector for these classifications. Popescu (2011) uses string kernels (Lodhi et al., 2002) to study translation properties.",
  "y": "motivation background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_3",
  "x": "A classifier was built to classify English original texts and English translated texts from French and German books that were written in the nineteenth century. The p-spectrum normalized kernel was used for the experiment. The system works on a character level rather than on a word level. The system performs poorly when the source language of the training corpus is different from the one of the test corpus. We can not compare our findings directly with <cite>Koppel and Ordan (2011)</cite> even though we use text from the same corpus and similar techniques. The English language is not considered for this study due to unavailability of English translations for some languages included in this work. Furthermore, instead of the list of 300 function words used by <cite>Koppel and Ordan (2011)</cite> , we used the 100 most frequent words for each candidate language. ---------------------------------- **DATA**",
  "y": "similarities"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_4",
  "x": "The system works on a character level rather than on a word level. The system performs poorly when the source language of the training corpus is different from the one of the test corpus. We can not compare our findings directly with <cite>Koppel and Ordan (2011)</cite> even though we use text from the same corpus and similar techniques. The English language is not considered for this study due to unavailability of English translations for some languages included in this work. Furthermore, instead of the list of 300 function words used by <cite>Koppel and Ordan (2011)</cite> , we used the 100 most frequent words for each candidate language. ---------------------------------- **DATA** The field of translation studies lacks a multilingual corpus that can be used to validate translation properties proposed by translation scholars. There are many multilingual corpora available used for different NLP applications.",
  "y": "differences"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_5",
  "x": "---------------------------------- **DATA** The field of translation studies lacks a multilingual corpus that can be used to validate translation properties proposed by translation scholars. There are many multilingual corpora available used for different NLP applications. A customized version of the Europarl corpus (Islam and Mehler, 2012 ) is freely available for corpus-based translation studies. However, this corpus is not suitable for the experiment we are performing here. We extract a suitable corpus from the Europarl corpus in a way similar to Lembersky et al. (2011) and <cite>Koppel and Ordan (2011)</cite> . Our target is to extract texts that are translated from and to the languages considered here. We trust the source language marker that has been put by the respective translator, as did Lembersky et al.(2011) and <cite>Koppel and Ordan (2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_6",
  "x": "The field of translation studies lacks a multilingual corpus that can be used to validate translation properties proposed by translation scholars. There are many multilingual corpora available used for different NLP applications. A customized version of the Europarl corpus (Islam and Mehler, 2012 ) is freely available for corpus-based translation studies. However, this corpus is not suitable for the experiment we are performing here. We extract a suitable corpus from the Europarl corpus in a way similar to Lembersky et al. (2011) and <cite>Koppel and Ordan (2011)</cite> . Our target is to extract texts that are translated from and to the languages considered here. We trust the source language marker that has been put by the respective translator, as did Lembersky et al.(2011) and <cite>Koppel and Ordan (2011)</cite> . To experiment with stylistic differences in translated text, a list of function words and their ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_7",
  "x": "curacy. In this experiment the value of N is 100. The randomly generated training sets contain 80% of the data while the remaining data is used as a test set. To evaluate the classification results, we use standard F-Score and Accuracy measures. ---------------------------------- **SOURCE LANGUAGE IDENTIFICATION** In this experiment, our goal is to validate the translation properties postulated by Toury (1995) . He stated that a translated text inherits some fingerprints from the source language. The experimental result of <cite>Koppel and Ordan (2011)</cite> shows that text translated into English holds this property.",
  "y": "background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_8",
  "x": "If it does not hold for a single language then it might be claimed that this translation property is not universal. In order to train a classifier, we use texts translated into the same language from different source languages. Table 1 shows the statistics of the corpus used for source language identification experiments. Later, each corpus is divided into a number of chunks (see Table 2 ). Each chunk contains at least seven sentences. Our hypothesis is again similar to <cite>Koppel and Ordan (2011)</cite> , that is, if the classifier's accuracy is close to 20%, then we cannot say that there is an interference effect in translated text. If the classifier's accuracy is close to 100% then our conclusion will be that interference effects exist in translated text. Table 3 and Table 4 show the evaluation results. Table 4 : Source language identification evaluation (Accuracy)",
  "y": "similarities"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_9",
  "x": "According to Baker (1993; 1996) , Olohan (2001) , Lavisoa (2002) , Hansen (2003) , and Pym (2005) there are some general properties of translations that are responsible for the difference between these two text types. Some of these properties are source and target language independent. According to their findings, a translated text will be similar to another translated text but will be different from a source text. In the past, researchers have used comparable corpora to validate these translation properties (Baroni and Bernardini, 2006; Pastor et al., 2008; Ilisei et al., 2009; Ilisei et al., 2010;<cite> Koppel and Ordan, 2011)</cite> . Most of them used comparable corpora for two-class classification, distinguishing translated texts from the original texts. Only<cite> Koppel and Ordan (Koppel and Ordan, 2011)</cite> used English texts translated from multiple source languages. We perform similar experiments only for six European languages as shown in Table 1 . In this experiment, the translated text in our training and test set will be a combination of all languages other than the target language. For example: when the original class contains original texts (source) in German, then the translation class contains texts that are translated German texts, translated from French, Dutch, Spanish, Polish, and Czech texts.",
  "y": "background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_10",
  "x": "In this experiment, the translated text in our training and test set will be a combination of all languages other than the target language. For example: when the original class contains original texts (source) in German, then the translation class contains texts that are translated German texts, translated from French, Dutch, Spanish, Polish, and Czech texts. Each class contains 200 chunks of texts, where as the translated class has 40 chunks from each of the source languages. The source language texts are extracted for the corresponding languages in a similar way from the Europarl corpus. <cite>Koppel and Ordan (2011)</cite> received the highest accuracy (96.7%) among all works noted above. The training and test data are generated in similar ways as in our previous experiment. That is, 80% of the data is randomly extracted for training and the rest of the data is used for testing. Expected F-Scores are calculated from 100 samples. Table 5 shows the evaluation results.",
  "y": "background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_11",
  "x": "That is, 80% of the data is randomly extracted for training and the rest of the data is used for testing. Expected F-Scores are calculated from 100 samples. Table 5 shows the evaluation results. Even though the classifier for German achieves around 99% accuracy, we cannot compare the result with<cite> Koppel and Ordan (Koppel and Ordan, 2011)</cite> as the amount of chunks for the classes are different. The classifiers for other languages also display very high accuracy. The result of Table 5 shows that general translation properties exist for all languages used in this experiment. ---------------------------------- **DISCUSSION** The results show that training a classifier based on the 100 most frequent words of a language is sufficient to obtain interpretable results.",
  "y": "differences"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_12",
  "x": "Expected F-Scores are calculated from 100 samples. Table 5 shows the evaluation results. Even though the classifier for German achieves around 99% accuracy, we cannot compare the result with<cite> Koppel and Ordan (Koppel and Ordan, 2011)</cite> as the amount of chunks for the classes are different. The classifiers for other languages also display very high accuracy. The result of Table 5 shows that general translation properties exist for all languages used in this experiment. ---------------------------------- **DISCUSSION** The results show that training a classifier based on the 100 most frequent words of a language is sufficient to obtain interpretable results. We find our results to be compatible with <cite>Koppel and Ordan (2011)</cite> who used 300 function words.",
  "y": "similarities"
 },
 {
  "id": "511c17a6cb6bd74e0216c3d50eb9c0_0",
  "x": "We also provide a tentative categorial grammar account of the words in the sentences, based largely on previous work on English. ---------------------------------- **INTRODUCTION** Scottish Gaelic (usually hereafter Gaelic) is a Celtic language, rather closely related to Irish, with around 59,000 speakers as of the last UK census in 2011. As opposed to the situation for Irish Gaelic (Lynn et al., 2012a;<cite> Lynn et al., 2012b</cite>; Lynn et al., 2013; Lynn et al., 2014) there are no treebanks or tagging schemes for Scottish Gaelic, although there are machine-readable dictionaries and databases available from Sabhal M\u00f2r Ostaig. A single paper in the ACL Anthology (Kessler, 1995) mentions Scottish Gaelic in the context of computational dialectology of Irish. There is also an LREC workshop paper (Scannell, 2006 ) on machine translation between Irish and Scottish Gaelic. Elsewhere in the Celtic languages, Welsh has an LFG grammar (Mittendorf and Sadler, 2005) but no treebanks. For Breton there is a small amount of work on morphological analysis and Constraint-Grammar-based machine translation (Tyers, 2010) .",
  "y": "background"
 },
 {
  "id": "511c17a6cb6bd74e0216c3d50eb9c0_1",
  "x": "This is clearly less than ideal, however, the guidelines are available along with the corpus and we hope to be able to get the input of a native speaker, not least for interannotator studies. We use the CoNLL-X format (Buchholz and Marsi, 2006) , leaving the POS and projective dependency fields empty and store the categorial grammar type under field 6, FEATS. ---------------------------------- **DEPENDENCY SCHEME** There are four dependency schemes that we consulted while preparing the corpus. The initial inspiration was provided by the C&C parser (Curran et al., 2007) , which in addition to providing categorial grammar derivations for sentences provides a dependency structure in the GR (Grammatical Representation) scheme due to (Briscoe and Carroll, 2000; Briscoe and Carroll, 2002) . This contains 23 types and was developed originally for parser evaluation. Another popular scheme is the Stanford Dependency scheme (de Marneffe and Manning, 2008; de Marneffe and Manning, 2013) , which is more finely-grained with over twice the number of dependency types to deal specifically with noisy data and to make it more accessible to non-linguists building information extraction applications. A very important scheme is the Dublin scheme for Irish (Lynn et al., 2012a;<cite> Lynn et al., 2012b</cite>; Lynn et al., 2013) , which is of a similar size to the Stanford scheme, but the reason for its size relative to GR is that it includes a large number of dependencies intended to handle grammatical features found in Irish but not in English.",
  "y": "background"
 },
 {
  "id": "511c17a6cb6bd74e0216c3d50eb9c0_2",
  "x": "It does not usually equate two NPs, with an exception we will come to. In the Dublin scheme the prepositional phrase headed by ag in T\u00e1 s\u00e9 ag iascaireacht (\"He is fishing.\") is treated as being an externally-controlled complement of T\u00e1 (Gaelic tha) and we carry this analysis over into Scottish Gaelic where this is the most common way of expressing the present tense. Figure 1 demonstrates this, where dhachaigh is a non-clausal modifier of dol, the verbal noun for \"to go\". Is can be used as the copula between two NPs, and to express psychological states such as liking and preference. To say \"I am a teacher\", the Gaelic is 'S e tidsear a th' annam. This, at least on the surface, equates pronoun e, with a noun described by a relative clause including the verb bi. Fig. 1 shows our dependency tree for this. Note that this is different from the scheme in <cite>Lynn et al. (2012b)</cite> because of a difference between the two languages. They treat the analogous sentence Is tusa an m\u00fainteoir \"You are the teacher\" as having a subject, \"the teacher\", and a clausal predicate, tusa, \"you indeed\".",
  "y": "differences"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_0",
  "x": "However, very often, handcrafted dictionaries do not contain definitions of expressions that are rarely used or newly created. Ultimately, we may need to read through the entire document or even search the web to find other occurances of the expression (global context) so that we can guess its meaning. Can machines help us do this work? Ni and Wang (2017) have proposed a task of generating a definition for a phrase given its local context. However, they follow the strict assumption that the target phrase is newly emerged and there is only a single local context available for the phrase, which makes the task of generating an accurate and coherent definition difficult (perhaps as difficult as a human comprehending the phrase itself). On the other hand, <cite>Noraset et al. (2017)</cite> attempted to generate a definition of a word from an embedding induced from massive text (which can be seen as global context). This is followed by Gadetsky et al. (2018) that refers to a local context to disambiguate polysemous words by choosing relevant dimensions of their word embeddings. Al-though these research efforts revealed that both local and global contexts are useful in generating definitions, none of these studies exploited both contexts directly to describe unknown phrases. In this study, we tackle the task of describing (defining) a phrase when given its local and global contexts.",
  "y": "background"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_1",
  "x": "However, they follow the strict assumption that the target phrase is newly emerged and there is only a single local context available for the phrase, which makes the task of generating an accurate and coherent definition difficult (perhaps as difficult as a human comprehending the phrase itself). On the other hand, <cite>Noraset et al. (2017)</cite> attempted to generate a definition of a word from an embedding induced from massive text (which can be seen as global context). This is followed by Gadetsky et al. (2018) that refers to a local context to disambiguate polysemous words by choosing relevant dimensions of their word embeddings. Al-though these research efforts revealed that both local and global contexts are useful in generating definitions, none of these studies exploited both contexts directly to describe unknown phrases. In this study, we tackle the task of describing (defining) a phrase when given its local and global contexts. We present LOG-CaD, a neural description generator (Figure 1 ) to directly solve this task. Given an unknown phrase without sense definitions, our model obtains a phrase embedding as its global context by composing word embeddings while also encoding the local context. The model therefore combines both pieces of information to generate a natural language description. Considering various applications where we need definitions of expressions, we evaluated our method with four datasets including WordNet<cite> (Noraset et al., 2017)</cite> for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slang, and a newlycreated Wikipedia dataset for entities.",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_2",
  "x": "To verify this idea, we propose to incorporate both local and global contexts to describe an unknown phrase. Figure 1 shows an illustration of our LOG-CaD model. Similarly to the standard encoder-decoder model with attention (Bahdanau et al., 2015; Luong and Manning, 2016) , it has a context encoder and a description decoder. The challenge here is that the decoder needs to be conditioned not only on the local context, but also on its global context. To incorporate the different types of contexts, we propose to use a gate function similar to <cite>Noraset et al. (2017)</cite> to dynamically control how the global and local contexts influence the description. ---------------------------------- **PROPOSED MODEL** Local & global context encoders We first describe how to model local and global contexts. Given a sentence X and a phrase X trg , a bidirectional LSTM (Gers et al., 1999) encoder generates a sequence of continuous vectors H = {h 1 \u00b7 \u00b7 \u00b7 , h I } as",
  "y": "similarities"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_3",
  "x": "---------------------------------- **USE OF CHARACTER INFORMATION** In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following<cite> (Noraset et al., 2017)</cite> . Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following <cite>Noraset et al. (2017)</cite>, we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg . ---------------------------------- **GATE FUNCTION TO CONTROL LOCAL & GLOBAL CONTEXTS** In order to capture the interaction between the local and global contexts, we adopt a GATE(\u00b7) function (Eq. (7)) which is similar to <cite>Noraset et al. (2017)</cite> . The GATE(\u00b7) function updates the LSTM output s t to s t depending on the global context x trg , local context d t , and character-level information c trg as where \u03c3(\u00b7), and ; denote the sigmoid function, element-wise multiplication, and vector concatenation, respectively.",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_4",
  "x": "In what follows, we explain each equation in detail. Attention on local context Considering the fact that the local context can be relatively long (e.g., around 20 words on average in our Wikipedia dataset introduced in Section 4), it is hard for the decoder to focus on important words in local contexts. In order to deal with this problem, the ATTENTION(\u00b7) function in Eq. (5) decides which words in the local context X to focus on at each time step. d t is computed with an attention mechanism (Luong and Manning, 2016) as where U h and U s are matrices that map the encoder and decoder hidden states into a common space, respectively. ---------------------------------- **USE OF CHARACTER INFORMATION** In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following<cite> (Noraset et al., 2017)</cite> . Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following <cite>Noraset et al. (2017)</cite>, we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg .",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_5",
  "x": "---------------------------------- **GATE FUNCTION TO CONTROL LOCAL & GLOBAL CONTEXTS** In order to capture the interaction between the local and global contexts, we adopt a GATE(\u00b7) function (Eq. (7)) which is similar to <cite>Noraset et al. (2017)</cite> . The GATE(\u00b7) function updates the LSTM output s t to s t depending on the global context x trg , local context d t , and character-level information c trg as where \u03c3(\u00b7), and ; denote the sigmoid function, element-wise multiplication, and vector concatenation, respectively. W * and b * are weight matrices and bias terms, respectively. Here, the update gate z t controls how much the original hidden state s t is to be changed, and the reset gate r t controls how much the information from f t contributes to word generation at each time step. ---------------------------------- **WIKIPEDIA DATASET**",
  "y": "similarities"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_6",
  "x": "**EXPERIMENTS** We evaluate our method by applying it to describe words in WordNet 5 (Miller, 1995) and Oxford Dictionary, 6 phrases in Urban Dictionary 7 and Wikipedia/Wikidata. 8 For all of these datasets, a given word or phrase has an inventory of senses with corresponding definitions and usage examples. These definitions are regarded as groundtruth descriptions. Datasets To evaluate our model on the word description task on WordNet, we followed <cite>Noraset et al. (2017)</cite> and extracted data from WordNet using the dict-definition 9 toolkit. Each entry in the data consists of three elements: (1) a word, (2) its definition, and (3) a usage example of the Table 2 : Domains, expressions to be described, and the coverage of pre-trained embeddings of the expressions to be described. word. We split this dataset to obtain Train, Validation, and Test sets. If a word has multiple definitions/examples, we treat them as different entries.",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_7",
  "x": "The only difference between our dataset and theirs is that we extract the tuples only if the words have their usage examples in WordNet. Since not all entries in WordNet have usage examples, our dataset is a small subset of <cite>Noraset et al. (2017)</cite> . In addition to WordNet, we use the Oxford Dictionary following Gadetsky et al. (2018) , the Urban Dictionary following Ni and Wang (2017) and our Wikipedia dataset described in the previous section. Table 1 and Table 2 show the properties and statistics of the four datasets, respectively. To simulate a situation in a real application where we might not have access to global context for the target phrases, we did not train domainspecific word embeddings on each dataset. Instead, for all of the four datasets, we use the same Table 3 : Hyperparameters of the models pre-trained CBOW 10 vectors trained on Google news corpus as global context following previous work (Noraset et al., 2017; Gadetsky et al., 2018) . If the expression to be described consists of multiple words, its phrase embedding is calculated by simply summing up all the CBOW vectors of words in the phrase, such as \"sonic\" and \"boom.\" (See Figure 1) . If pre-trained CBOW embeddings are unavailable, we instead use a special [UNK] vector (which is randomly initialized with a uniform distribution) as word embeddings. Note that our pre-trained embeddings only cover 26.79% of the words in the expressions to be described in our Wikipedia dataset, while it covers all words in WordNet dataset (See Table 2 ).",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_8",
  "x": "Instead, for all of the four datasets, we use the same Table 3 : Hyperparameters of the models pre-trained CBOW 10 vectors trained on Google news corpus as global context following previous work (Noraset et al., 2017; Gadetsky et al., 2018) . If the expression to be described consists of multiple words, its phrase embedding is calculated by simply summing up all the CBOW vectors of words in the phrase, such as \"sonic\" and \"boom.\" (See Figure 1) . If pre-trained CBOW embeddings are unavailable, we instead use a special [UNK] vector (which is randomly initialized with a uniform distribution) as word embeddings. Note that our pre-trained embeddings only cover 26.79% of the words in the expressions to be described in our Wikipedia dataset, while it covers all words in WordNet dataset (See Table 2 ). Even if no reliable word embeddings are available, all models can capture the character information through character-level CNNs (See Figure 1) . ---------------------------------- **MODELS WE IMPLEMENTED FOUR METHODS: (1)** Global<cite> (Noraset et al., 2017)</cite> , (2) Local (Ni and Wang, 2017) with CNN, (3) I-Attention (Gadetsky et al., 2018) , and our proposed model, (4) LOGCaD. The Global model is our reimplementation of the best model (S + G + CH) in <cite>Noraset et al. (2017)</cite> . It can access the global context of a phrase to be described, but has no ability to read the local context.",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_9",
  "x": "However, the target of paraphrase acquisition are words/phrases with no specified context. Although a few studies (Connor and Roth, 2007; Max, 2009; Max et al., 2012) consider subsentential (context-sensitive) paraphrases, they do not intend to obtain a definition-like description as a paraphrase of a word. Recently, <cite>Noraset et al. (2017)</cite> introduced a task of generating a definition sentence of a word from its pre-trained embedding. Since their task does not take local contexts of words as inputs, their method cannot generate an appropriate definition for a polysemous word for a specific context. To cope with this problem, Gadetsky et al. (2018) proposed a definition generation method that works with polysemous words in dictionaries. They presented a model that utilizes local context to filter out the unrelated meanings from a pre-trained word embedding in a specific context. While their method use local context for disambiguating the meanings that are mixed up in word embeddings, the information from local contexts cannot be utilized if the pre-trained embeddings are unavailable or unreliable. On the other hand, our method can fully utilize the local context through an attentional mechanism, even if the reliable word embeddings are unavailable. The most related work to this paper is Ni and Wang (2017) .",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_0",
  "x": "Conventional studies concentrate on the area of multilingual acoustic modeling by the contextdependent deep neural network hidden Markov models (CD-DNN-HMM) [6] . The hidden layers of DNN in CD-DNN-HMM can be thought of complicated feature transformation through multiple layers of nonlinearity, which can be used to extract universal feature transformation from multilingual datasets [1] . Among the CD-DNN-HMM based approaches, the architecture of SHL-MDNN [1] , in which the hidden layers are shared across multiple languages while the softmax layers are language dependent, is a significant progress in the area of multilingual ASR. These shared hidden layers and language dependent softmax layers of SHL-MDNN are optimized jointly by multilingual datasets. SHL-MLSTM [5] further explores long short-term memory (LSTM) [7] with residual learning as the shared hidden layer instead of DNN and achieves better results than SHL-MDNN. Although these models achieve encouraging results on multilingual ASR tasks, a hand-designed language-specific pronunciation lexicon must be employed. This severely limits their application on low-resource languages, which may have not a well-designed pronunciation lexicon. Recent researches on sequence-to-sequence attention-based models try to remove this dependency on the pronunciation lexicon [8,<cite> 9,</cite> 10] . Chiu et al. shows that attention-based encoder-decoder architecture, namely listen, attend, and spell (LAS), achieves a new stateof-the-art WER on a 12500 hour English voice search task using the word piece models (WPM) [10] .",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_1",
  "x": "Chiu et al. shows that attention-based encoder-decoder architecture, namely listen, attend, and spell (LAS), achieves a new stateof-the-art WER on a 12500 hour English voice search task using the word piece models (WPM) [10] . Our previous work <cite>[9]</cite> demonstrates that the lexicon independent models can outperform lexicon dependent models on Mandarin Chinese ASR tasks by the ASR Transformer and the character based model establishes a new state-of-the-art character error rate (CER) on HKUST datasets. Since the acoustic, pronunciation and language model are integrated into a single neural network by sequence-to-sequence attention-based models, it makes them very suitable for multilingual ASR. In this paper, we concentrate on multilingual ASR on low-resource languages. Building on our work <cite>[9]</cite> , we employ sub-words generated by byte pair encoding (BPE) [11] as the multilingual modeling unit, which do not need any pronunciation lexicon. The ASR Transformer is chosen to be the basic architecture of sequence-to-sequence attention-based model<cite> [9,</cite> 12] . To alleviate the problem of few training data on low-resource languages, a well-trained ASR Transformer from a high-resource language is adopted as the initial model rather than random initialization, whose softmax layer is replaced by the language-specific softmax layer. We then look at incorporating language information into the model by inserting the language symbol at the beginning or at the end of the original sub-words sequence [13] under the condition of language information being known during training. A comparison with SHL-MLSTM [5] with residual learning is investigated on CALL-HOME datasets with 6 languages.",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_2",
  "x": "Although these models achieve encouraging results on multilingual ASR tasks, a hand-designed language-specific pronunciation lexicon must be employed. This severely limits their application on low-resource languages, which may have not a well-designed pronunciation lexicon. Recent researches on sequence-to-sequence attention-based models try to remove this dependency on the pronunciation lexicon [8,<cite> 9,</cite> 10] . Chiu et al. shows that attention-based encoder-decoder architecture, namely listen, attend, and spell (LAS), achieves a new stateof-the-art WER on a 12500 hour English voice search task using the word piece models (WPM) [10] . Our previous work <cite>[9]</cite> demonstrates that the lexicon independent models can outperform lexicon dependent models on Mandarin Chinese ASR tasks by the ASR Transformer and the character based model establishes a new state-of-the-art character error rate (CER) on HKUST datasets. Since the acoustic, pronunciation and language model are integrated into a single neural network by sequence-to-sequence attention-based models, it makes them very suitable for multilingual ASR. In this paper, we concentrate on multilingual ASR on low-resource languages. Building on our work <cite>[9]</cite> , we employ sub-words generated by byte pair encoding (BPE) [11] as the multilingual modeling unit, which do not need any pronunciation lexicon. The ASR Transformer is chosen to be the basic architecture of sequence-to-sequence attention-based model<cite> [9,</cite> 12] .",
  "y": "differences extends"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_3",
  "x": "The ASR Transformer is chosen to be the basic architecture of sequence-to-sequence attention-based model<cite> [9,</cite> 12] . To alleviate the problem of few training data on low-resource languages, a well-trained ASR Transformer from a high-resource language is adopted as the initial model rather than random initialization, whose softmax layer is replaced by the language-specific softmax layer. We then look at incorporating language information into the model by inserting the language symbol at the beginning or at the end of the original sub-words sequence [13] under the condition of language information being known during training. A comparison with SHL-MLSTM [5] with residual learning is investigated on CALL-HOME datasets with 6 languages. Experimental results reveal that the multilingual ASR Transformer with the language symbol at the end performs better and can obtain relatively 10.5% average WER reduction compared to SHL-MLSTM with residual learning. We go on to show that, assuming the language information being known during training and testing, about relatively 12.4% average WER reduction can be observed compared to SHL-MLSTM with residual learning through giving the language symbol as the sentence start token. The rest of the paper is organized as follows. After an overview of the related work in Section 2, Section 3 describes the proposed method in detail. We then show experimental results in Section 4 and conclude this work in Section 5.",
  "y": "differences extends"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_4",
  "x": "**SYSTEM OVERVIEW 3.1. ASR TRANSFORMER MODEL ARCHITECTURE** The ASR Transformer architecture used in this work is the same as our work<cite> [9,</cite> 12] which is shown in Figure 1 . It stacks multihead attention (MHA) [17] and position-wise, fully connected layers for both the encode and decoder. The encoder is composed of a stack of N identical layers. Each layer has two sublayers. The first is a MHA, and the second is a position-wise fully connected feed-forward network. Residual connections are employed around each of the two sub-layers, followed by a layer normalization. The decoder is similar to the encoder except inserting a third sub-layer to perform a MHA over the output of the encoder stack. To prevent leftward information flow and preserve the auto-regressive property in the decoder, the self-attention sub-layers in the decoder mask out all values corresponding to illegal connections.",
  "y": "similarities uses"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_5",
  "x": "The following six languages are used: Mandarin (MA), English (EN), Japanese (JA), Arabic (AR), German (GE) and Spanish (SP). We follow the Kaldi [19] recipe to process CALLHOME datasets 2 . The detailed information is listed below in Table 2 . We train the ASR Transformer with a given number of epochs, so validation sets are not employed in this paper. All experiments are conducted using 80-dimensional log-Mel filterbank features, computed with a 25ms window and shifted every 10ms. The features are normalized via mean subtraction and variance normalization on the speaker basis. Similar to [20, 21] , at the current frame t, these features are stacked with 3 frames to the left and downsampled to a 30ms frame rate. We generate more training data by linearly scaling the audio lengths by factors of 0.9 and 1.1 [22] , since it is always beneficial for training the ASR Transformer <cite>[9]</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_6",
  "x": "We generate more training data by linearly scaling the audio lengths by factors of 0.9 and 1.1 [22] , since it is always beneficial for training the ASR Transformer <cite>[9]</cite> . ---------------------------------- **MODEL AND TRAINING DETAILS** We perform our experiments on the big model (D1024-H16)<cite> [9,</cite> 17] of the ASR Transformer. Table 3 lists our experimental parameters. The Adam algorithm [23] with gradient clipping and warmup is used for optimization. During training, label smoothing of value ls = 0.1 is employed [24] . After trained, the last 20 checkpoints are averaged to make the performance more stable [17] . At the beginning we train the ASR Transformer on English data with a random initialization, but the result is poor although the CE loss looks good.",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_7",
  "x": "**MODEL AND TRAINING DETAILS** We perform our experiments on the big model (D1024-H16)<cite> [9,</cite> 17] of the ASR Transformer. Table 3 lists our experimental parameters. The Adam algorithm [23] with gradient clipping and warmup is used for optimization. During training, label smoothing of value ls = 0.1 is employed [24] . After trained, the last 20 checkpoints are averaged to make the performance more stable [17] . At the beginning we train the ASR Transformer on English data with a random initialization, but the result is poor although the CE loss looks good. We propose that one reason for the poor performance could be the training data is too few but the parameters of the ASR Transformer are relatively large which is about 230M in this work. To compensate the lack of training data on low-resource languages, a well-trained ASR Transformer with a CER of 26.64% on HKUST dataset, a corpus of Mandarin Chinese conversational telephone speech, is adopted from our work <cite>[9]</cite> .",
  "y": "uses"
 },
 {
  "id": "52af1f5378194ccdb7c8f755a6ae34_0",
  "x": "We start from a baseline joint model that performs the tasks of named entity recognition (NER) and relation extraction at once. Previously proposed models (summarized in Section 2) exhibit several issues that the neural network-based baseline approach (detailed in Section 3.1) overcomes: (i) our model uses automatically extracted features without the need of external parsers nor manually extracted features (see Gupta et al. (2016) ; Miwa and Bansal (2016) ; Li et al. (2017) ), (ii) all entities and the corresponding relations within the sentence are extracted at once, instead of examining one pair of entities at a time (see <cite>Adel and Sch\u00fctze (2017)</cite> ), and (iii) we model relation extraction in a multi-label setting, allowing multiple relations per entity (see Katiyar and Cardie (2017) ; Bekoulis et al. (2018a) ). The core contribution of the paper is the use of AT as an extension in the training procedure for the joint extraction task (Section 3.2). To evaluate the proposed AT method, we perform a large scale experimental study in this joint task (see Section 4), using datasets from different contexts (i.e., news, biomedical, real estate) and languages (i.e., English, Dutch). We use a strong baseline that outperforms all previous models that rely on automatically extracted features, achieving state-of-the-art performance (Section 5). Compared to the baseline model, applying AT during training leads to a consistent additional increase in joint extraction effectiveness. ---------------------------------- **RELATED WORK** Joint entity and relation extraction: Joint models (Li and Ji, 2014; Miwa and Sasaki, 2014) that are based on manually extracted features have been proposed for performing both the named entity recognition (NER) and relation extraction subtasks at once.",
  "y": "differences"
 },
 {
  "id": "52af1f5378194ccdb7c8f755a6ae34_1",
  "x": "**RELATED WORK** Joint entity and relation extraction: Joint models (Li and Ji, 2014; Miwa and Sasaki, 2014) that are based on manually extracted features have been proposed for performing both the named entity recognition (NER) and relation extraction subtasks at once. These methods rely on the availability of NLP tools (e.g., POS taggers) or manually designed features leading to additional complexity. Neural network methods have been exploited to overcome this feature design issue and usually involve RNNs and CNNs (Miwa and Bansal, 2016; Zheng et al., 2017) . Specifically, Miwa and Bansal (2016) as well as Li et al. (2017) apply bidirectional tree-structured RNNs for different contexts (i.e., news, biomedical) to capture syntactic information (using external dependency parsers). Gupta et al. (2016) propose the use of various manually extracted features along with RNNs. <cite>Adel and Sch\u00fctze (2017)</cite> solve the simpler problem of entity classification (EC, assuming entity boundaries are given), instead of NER, and they replicate the context around the entities, feeding entity pairs to the relation extraction layer. Katiyar and Cardie (2017) investigate RNNs with attention without taking into account that relation labels are not mutually exclusive. Finally, Bekoulis et al. (2018a) use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part.",
  "y": "background"
 },
 {
  "id": "52af1f5378194ccdb7c8f755a6ae34_2",
  "x": "where\u03b8 is a copy of the current model parameters. Since Eq. (2) is intractable in neural networks, we use the approximation proposed in Goodfellow et al. (2015) defined as: \u03b7 adv = g/ g , with g = \u2207 w L JOINT (w;\u03b8), where is a small bounded norm treated as a hyperparameter. Similar to Yasunaga et al. (2018) , we set to be \u03b1 \u221a D (where D is the dimension of the embeddings). We train on the mixture of original and adversarial examples, so the final loss is computed as: L JOINT (w;\u03b8) + L JOINT (w + \u03b7 adv ;\u03b8). ---------------------------------- **EXPERIMENTAL SETUP** We evaluate our models on four datasets, using the code as available from our github codebase. 1 Specifically, we follow the 5-fold crossvalidation defined by Miwa and Bansal (2016) for the ACE04 (Doddington et al., 2004) dataset. For the CoNLL04 (Roth and Yih, 2004 ) EC task (assuming boundaries are given), we use the same splits as in Gupta et al. (2016) ; <cite>Adel and Sch\u00fctze (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "52af1f5378194ccdb7c8f755a6ae34_3",
  "x": "Compared to Miwa and Bansal (2016) , who rely on NLP tools, the baseline performs within a reasonable margin (less than 1%) on the joint task. On the other hand, Li et al. (2017) use the same model for the ADE biomedical dataset, where we report a 2.5% overall improvement. This indicates that NLP tools are not always accurate for various contexts. For the CoNLL04 dataset, we use two evaluation settings. We use the relaxed evaluation similar to Gupta et al. (2016) ; <cite>Adel and Sch\u00fctze (2017)</cite> on the EC task. The baseline model outperforms the state-of-the-art models that do not rely on manually extracted features (>4% improvement for both tasks), since we directly model the whole sentence, instead of just considering pairs of entities. Moreover, compared to the model of Gupta et al. (2016) that relies on complex features, the baseline model performs within a margin of 1% in terms of overall F 1 score. We also report NER results on the same dataset and improve overall F 1 score with \u223c1% compared to Miwa and Sasaki (2014) , indicating that our automatically extracted features are more informative than the hand-crafted ones. These automatically extracted features exhibit their performance improvement mainly due to the shared LSTM layer that learns to automatically generate feature representations of entities and their corresponding relations within a single model.",
  "y": "similarities"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_0",
  "x": "Recently, methods for cross-lingual transfer have appeared as a promising avenue for overcoming this hurdle for both part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011) and syntactic dependency parsing (Hwa et al., 2005; Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; , * Work primarily carried out while at Google, NY. 2012). While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods (Klein and Manning, 2004 ) and weakly supervised methods (Naseem et al., 2010; Berg-Kirkpatrick and Klein, 2010) . A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related languages. This idea was first explored for weakly supervised learning (Cohen and Smith, 2009; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010) and recently by <cite>Naseem et al. (2012)</cite> for multisource cross-lingual transfer. In particular, <cite>Naseem et al.</cite> showed that by selectively sharing parameters based on typological features of each language, substantial improvements can be achieved, compared to using a single set of parameters for all languages. However, these methods all employ generative models with strong independence assumptions and weak feature representations, which upper bounds their accuracy far below that of feature-rich discriminative parsers (McDonald et al., 2005; Nivre, 2008) . In this paper, we improve upon the state of the art in cross-lingual transfer of dependency parsers from multiple source languages by adapting feature-rich discriminatively trained parsers to a specific target language. First, in \u00a74 we show how selective sharing of model parameters based on typological traits can be incorporated into a delexicalized discriminative graph-based parsing model. This requires a careful decomposition of features into language-generic and language-specific sets in order to tie specific target language parameters to their relevant source language counterparts.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_1",
  "x": "**INTRODUCTION** Many languages still lack access to core NLP tools, such as part-of-speech taggers and syntactic parsers. This is largely due to the reliance on fully supervised learning methods, which require large quantities of manually annotated training data. Recently, methods for cross-lingual transfer have appeared as a promising avenue for overcoming this hurdle for both part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011) and syntactic dependency parsing (Hwa et al., 2005; Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; , * Work primarily carried out while at Google, NY. 2012). While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods (Klein and Manning, 2004 ) and weakly supervised methods (Naseem et al., 2010; Berg-Kirkpatrick and Klein, 2010) . A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related languages. This idea was first explored for weakly supervised learning (Cohen and Smith, 2009; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010) and recently by <cite>Naseem et al. (2012)</cite> for multisource cross-lingual transfer. In particular, <cite>Naseem et al.</cite> showed that by selectively sharing parameters based on typological features of each language, substantial improvements can be achieved, compared to using a single set of parameters for all languages. However, these methods all employ generative models with strong independence assumptions and weak feature representations, which upper bounds their accuracy far below that of feature-rich discriminative parsers (McDonald et al., 2005; Nivre, 2008) .",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_2",
  "x": "This requires a careful decomposition of features into language-generic and language-specific sets in order to tie specific target language parameters to their relevant source language counterparts. The resulting parser outperforms the method of <cite>Naseem et al. (2012)</cite> on 12 out of 16 evaluated languages. Second, in \u00a75 we introduce a train-ing method that can incorporate diverse knowledge sources through ambiguously predicted labelings of unlabeled target language data. This permits effective relexicalization and target language adaptation of the transfer parser. Here, we experiment with two different knowledge sources: arc sets, which are filtered by marginal probabilities from the cross-lingual transfer parser, are used in an ambiguity-aware self-training algorithm ( \u00a75.2); these arc sets are then combined with the predictions of a different transfer parser in an ambiguity-aware ensemble-training algorithm ( \u00a75.3). The resulting parser provides significant improvements over a strong baseline parser and achieves a 13% relative error reduction on average with respect to the best model of <cite>Naseem et al. (2012)</cite> , outperforming it on 15 out of the 16 evaluated languages. ---------------------------------- **MULTI-SOURCE DELEXICALIZED TRANSFER** The methods proposed in this paper fall into the delexicalized transfer approach to multilingual syntactic parsing (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; S\u00f8gaard, 2011) .",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_3",
  "x": "This permits effective relexicalization and target language adaptation of the transfer parser. Here, we experiment with two different knowledge sources: arc sets, which are filtered by marginal probabilities from the cross-lingual transfer parser, are used in an ambiguity-aware self-training algorithm ( \u00a75.2); these arc sets are then combined with the predictions of a different transfer parser in an ambiguity-aware ensemble-training algorithm ( \u00a75.3). The resulting parser provides significant improvements over a strong baseline parser and achieves a 13% relative error reduction on average with respect to the best model of <cite>Naseem et al. (2012)</cite> , outperforming it on 15 out of the 16 evaluated languages. ---------------------------------- **MULTI-SOURCE DELEXICALIZED TRANSFER** The methods proposed in this paper fall into the delexicalized transfer approach to multilingual syntactic parsing (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; S\u00f8gaard, 2011) . In contrast to annotation projection approaches (Yarowsky et al., 2001; Hwa et al., 2005; Ganchev et al., 2009; Spreyer and Kuhn, 2009) , delexicalized transfer methods do not rely on any bitext. Instead, a parser is trained on annotations in a source language, relying solely on features that are available in both the source and the target language, such as \"universal\" part-ofspeech tags (Zeman and Resnik, 2008; Naseem et al., 2010; Petrov et al., 2012) , cross-lingual word clusters or type-level features derived from bilingual dictionaries (Durrett et al., 2012) . 1 This parser is then directly used to parse the target language.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_4",
  "x": "This idea was explored by McDonald et al. (2011) , who showed that target language accuracy can be improved by simply concatenating delexicalized treebanks in multiple languages. In similar work, Cohen et al. (2011) proposed a mixture model in which the parameters of a generative target language parser is expressed as a linear interpolation of source language parameters, whereas S\u00f8gaard (2011) showed that target side language models can be used to selectively subsample training sentences to improve accuracy. Recently, inspired by the phylogenetic prior of Berg-Kirkpatrick and Klein (2010) , S\u00f8gaard and Wulff (2012) proposed -among other ideas -a typologically informed weighting heuristic for linearly interpolating source language parameters. However, this weighting did not provide significant improvements over uniform weighting. The aforementioned approaches work well for transfer between similar languages. However, their assumptions cease to hold for typologically divergent languages; a target language can rarely be described as a linear combination of data or model parameters from a set of source languages, as languages tend to share varied typological traits; this critical insight is discussed further in \u00a74. To account for this issue, <cite>Naseem et al. (2012)</cite> recently introduced a novel generative model of dependency parsing, in which the generative process is factored into separate steps for the selection of dependents and their ordering. The parameters used in the selection step are all language independent, capturing only head-dependent attachment preferences. In the ordering step, however, parameters are selectively shared between subsets of <cite>Naseem et al. (2012)</cite> restricts its potential performance.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_5",
  "x": "Recently, inspired by the phylogenetic prior of Berg-Kirkpatrick and Klein (2010) , S\u00f8gaard and Wulff (2012) proposed -among other ideas -a typologically informed weighting heuristic for linearly interpolating source language parameters. However, this weighting did not provide significant improvements over uniform weighting. The aforementioned approaches work well for transfer between similar languages. However, their assumptions cease to hold for typologically divergent languages; a target language can rarely be described as a linear combination of data or model parameters from a set of source languages, as languages tend to share varied typological traits; this critical insight is discussed further in \u00a74. To account for this issue, <cite>Naseem et al. (2012)</cite> recently introduced a novel generative model of dependency parsing, in which the generative process is factored into separate steps for the selection of dependents and their ordering. The parameters used in the selection step are all language independent, capturing only head-dependent attachment preferences. In the ordering step, however, parameters are selectively shared between subsets of <cite>Naseem et al. (2012)</cite> restricts its potential performance. ---------------------------------- **BASIC MODELS AND EXPERIMENTAL SETUP**",
  "y": "motivation"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_6",
  "x": "However, this weighting did not provide significant improvements over uniform weighting. The aforementioned approaches work well for transfer between similar languages. However, their assumptions cease to hold for typologically divergent languages; a target language can rarely be described as a linear combination of data or model parameters from a set of source languages, as languages tend to share varied typological traits; this critical insight is discussed further in \u00a74. To account for this issue, <cite>Naseem et al. (2012)</cite> recently introduced a novel generative model of dependency parsing, in which the generative process is factored into separate steps for the selection of dependents and their ordering. The parameters used in the selection step are all language independent, capturing only head-dependent attachment preferences. In the ordering step, however, parameters are selectively shared between subsets of <cite>Naseem et al. (2012)</cite> restricts its potential performance. ---------------------------------- **BASIC MODELS AND EXPERIMENTAL SETUP** Inspired by the superiority of discriminative graphbased parsing in the supervised scenario, we investigate whether the insights of <cite>Naseem et al. (2012)</cite> on selective parameter sharing can be incorporated into such models in the transfer scenario.",
  "y": "motivation"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_7",
  "x": "However, their assumptions cease to hold for typologically divergent languages; a target language can rarely be described as a linear combination of data or model parameters from a set of source languages, as languages tend to share varied typological traits; this critical insight is discussed further in \u00a74. To account for this issue, <cite>Naseem et al. (2012)</cite> recently introduced a novel generative model of dependency parsing, in which the generative process is factored into separate steps for the selection of dependents and their ordering. The parameters used in the selection step are all language independent, capturing only head-dependent attachment preferences. In the ordering step, however, parameters are selectively shared between subsets of <cite>Naseem et al. (2012)</cite> restricts its potential performance. ---------------------------------- **BASIC MODELS AND EXPERIMENTAL SETUP** Inspired by the superiority of discriminative graphbased parsing in the supervised scenario, we investigate whether the insights of <cite>Naseem et al. (2012)</cite> on selective parameter sharing can be incorporated into such models in the transfer scenario. We first review the basic graph-based parser framework and the experimental setup that we will use throughout. We then delve into details on how to incorporate selective sharing in this model in \u00a74.",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_8",
  "x": "Let x denote an input sentence and let y \u2208 Y(x) denote a dependency tree, where Y(x) is the set of well-formed dependency trees spanning x. Henceforth, we restrict Y(x) to projective dependency trees, but all our methods are equally applicable in the nonprojective case. Provided a vector of model parameters \u03b8, the probability of a dependency tree y \u2208 Y(x), conditioned on a sentence x, has the following form: Without loss of generality, we restrict ourselves to first-order models, where the feature function \u03a6(x, y) factors over individual arcs (h, m) in y, such that We use the standard gradient-based L-BFGS algorithm (Liu and Nocedal, 1989) to maximize the loglikelihood. Eisner's algorithm (Eisner, 1996) is used for inference of the Viterbi parse and arc-marginals. ---------------------------------- **DATA SETS AND EXPERIMENTAL SETUP** To facilitate comparison with the state of the art, we use the same treebanks and experimental setup as <cite>Naseem et al. (2012)</cite> . Notably, we use the mapping proposed by Naseem et al. (2010) to map from fine-grained treebank specific part-of-speech tags to coarse-grained \"universal\" tags, rather than the more recent mapping proposed by Petrov et al. (2012) .",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_9",
  "x": "Figure 2: Arc-factored feature templates for graph-based parsing. Direction: d \u2208 {LEFT, RIGHT}; dependency length: l \u2208 {1, 2, 3, 4, 5+}; part of speech of head / dependent / words between head and dependent: h.p / m.p / between.p \u2208 {NOUN, VERB, ADJ, ADV, PRON, DET, ADP, NUM, CONJ, PRT, PUNC, X}; token to the left / right of z: z \u22121 / z +1 ; WALS features: w. X for X = 81A, 85A, 86A, 87A (see Table 1 ). [\u00b7] denotes an optional template, e.g., p, so that the template also falls back on its undirectional variant. each target language evaluated, the treebanks of the remaining languages are used as labeled training data, while the target language treebank is used for testing only (in \u00a75 a different portion of the target language treebank is additionally used as unlabeled training data). We refer the reader to <cite>Naseem et al. (2012)</cite> for detailed information on the different treebanks. Due to divergent treebank annotation guidelines, which makes fine-grained evaluation difficult, all results are evaluated in terms of unlabeled attachment score (UAS). In line with <cite>Naseem et al. (2012)</cite>, we use gold part-of-speech tags and evaluate only on sentences of length 50 or less excluding punctuation.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_10",
  "x": "In line with <cite>Naseem et al. (2012)</cite>, we use gold part-of-speech tags and evaluate only on sentences of length 50 or less excluding punctuation. ---------------------------------- **BASELINE MODELS** We compare our models to two multi-source baseline models. The first baseline, <cite>NBG</cite>, is the generative model with selective parameter sharing from <cite>Naseem et al. (2012)</cite> . 3 <cite>This model</cite> is trained without target language data, but we investigate the use of such data in \u00a75.4. The second baseline, Delex, is a delexicalized projective version of the well-known graph-based MSTParser (McDonald et al., 2005) . The feature templates used by this model are shown to the left in Figure 2 . Note that there is no selective sharing in this model.",
  "y": "similarities uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_11",
  "x": "The first baseline, <cite>NBG</cite>, is the generative model with selective parameter sharing from <cite>Naseem et al. (2012)</cite> . 3 <cite>This model</cite> is trained without target language data, but we investigate the use of such data in \u00a75.4. The second baseline, Delex, is a delexicalized projective version of the well-known graph-based MSTParser (McDonald et al., 2005) . The feature templates used by this model are shown to the left in Figure 2 . Note that there is no selective sharing in this model. The second and third columns of Table 2 show the unlabeled attachment scores of the baseline models for each target language. We see that Delex performs well on target languages that are related to the majority of the source languages. However, for languages 3 Model \"D-,To\" in Table 2 from <cite>Naseem et al. (2012)</cite> . that diverge from the Indo-European majority family, the selective sharing model, <cite>NBG</cite>, achieves substantially higher accuracies.",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_12",
  "x": "---------------------------------- **BASELINE MODELS** We compare our models to two multi-source baseline models. The first baseline, <cite>NBG</cite>, is the generative model with selective parameter sharing from <cite>Naseem et al. (2012)</cite> . 3 <cite>This model</cite> is trained without target language data, but we investigate the use of such data in \u00a75.4. The second baseline, Delex, is a delexicalized projective version of the well-known graph-based MSTParser (McDonald et al., 2005) . The feature templates used by this model are shown to the left in Figure 2 . Note that there is no selective sharing in this model. The second and third columns of Table 2 show the unlabeled attachment scores of the baseline models for each target language.",
  "y": "extends differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_13",
  "x": "3 <cite>This model</cite> is trained without target language data, but we investigate the use of such data in \u00a75.4. The second baseline, Delex, is a delexicalized projective version of the well-known graph-based MSTParser (McDonald et al., 2005) . The feature templates used by this model are shown to the left in Figure 2 . Note that there is no selective sharing in this model. The second and third columns of Table 2 show the unlabeled attachment scores of the baseline models for each target language. We see that Delex performs well on target languages that are related to the majority of the source languages. However, for languages 3 Model \"D-,To\" in Table 2 from <cite>Naseem et al. (2012)</cite> . that diverge from the Indo-European majority family, the selective sharing model, <cite>NBG</cite>, achieves substantially higher accuracies. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_14",
  "x": "First, all but one template include the arc direction. Second, some features are sensitive to local word order; e.g., p, which models direction as well as word order in the local contexts of the head and the dependent. Such features do not transfer well across typologically different languages. In order to verify that these issues are the cause of the poor performance of the Delex model, we remove all directional features and all features that model local word order from Delex. The feature templates of the resulting Bare model are shown in the center of Figure 2 . These features only model selectional preferences and dependency length, analogously to the selection component of <cite>NBG</cite>. The performance of Bare is shown in the fourth column of Table 2 . The removal of most of the features results in a performance drop on average.",
  "y": "similarities"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_15",
  "x": "Inspired by <cite>Naseem et al.</cite> Table 2 from <cite>Naseem et al. (2012)</cite> . (2012), we make use of the typological features from WALS (Dryer and Haspelmath, 2011), listed in Table 1, to selectively share directional parameters between languages. As a natural first attempt at sharing parameters, one might consider forming the crossproduct of all features of Delex with all WALS properties, similarly to a common domain adaptation technique (Daum\u00e9 III, 2007; Finkel and Manning, 2009 ). However, this approach has two issues. First, it results in a huge number of features, making the model prone to overfitting. Second, and more critically, it ties together languages via features for which they are not typologically similar. Consider English and French, which are both prepositional and thus have the same value for WALS property 85A. These languages will end up sharing a parameter for the feature 85A; yet they have the exact opposite direction of attachment preference when it comes to nouns and adjectives.",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_16",
  "x": "First, it results in a huge number of features, making the model prone to overfitting. Second, and more critically, it ties together languages via features for which they are not typologically similar. Consider English and French, which are both prepositional and thus have the same value for WALS property 85A. These languages will end up sharing a parameter for the feature 85A; yet they have the exact opposite direction of attachment preference when it comes to nouns and adjectives. This problem applies to any method for parameter mixing that treats all the parameters as equal. Like <cite>Naseem et al. (2012)</cite> , we instead share parameters more selectively. Our strategy is to use the relevant part-of-speech tags of the head and dependent to select which parameters to share, based on very basic linguistic knowledge. The resulting features are shown to the right in Figure 2 .",
  "y": "similarities uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_17",
  "x": "85A; yet they have the exact opposite direction of attachment preference when it comes to nouns and adjectives. This problem applies to any method for parameter mixing that treats all the parameters as equal. Like <cite>Naseem et al. (2012)</cite> , we instead share parameters more selectively. Our strategy is to use the relevant part-of-speech tags of the head and dependent to select which parameters to share, based on very basic linguistic knowledge. The resulting features are shown to the right in Figure 2 . For example, there is a shared directional feature that models the order of Subject, Object and Verb by conjoining WALS feature 81A with the arc direction and an indicator feature that fires only if the head is a verb and the dependent is a noun. These features would not be very useful by themselves, so we combine them with the Bare features. The accuracy of the resulting Share model is shown in column five of Table 2 . Although this model still performs worse than <cite>NBG</cite>, it is an improvement over the Delex baseline and actually outperforms the former on 5 out of the 16 languages.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_18",
  "x": "The results for these models are given in the last two columns of Table 2 . We see that by adding these rich features back into the fold, but having them fire only for languages in the same group, we can significantly increase the performance -from 57.4% to 62.0% on average when considering Family. If we consider our original Delex baseline, we see an absolute improvement of 6.9% on average and a relative error reduction of 15%. Particular gains are seen for non-Indo-European languages; e.g., Japanese increases from 38.9% to 65.9%. Furthermore, Family achieves a 7% relative error reduction over the <cite>NBG</cite> baseline and outperforms it on 12 of the 16 languages. This shows that a discriminative graph-based parser can achieve higher accuracies compared to generative models when the features are carefully constructed. ---------------------------------- **TARGET LANGUAGE ADAPTATION** While some higher-level linguistic properties of the target language have been incorporated through selective sharing, so far no features specific to the target language have been employed. Cohen et al. (2011) and <cite>Naseem et al. (2012)</cite> have shown that using expectation-maximization (EM) to this end can in some cases bring substantial accuracy gains.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_19",
  "x": "Cohen et al. (2011) and <cite>Naseem et al. (2012)</cite> have shown that using expectation-maximization (EM) to this end can in some cases bring substantial accuracy gains. For discriminative models, self-training has been shown to be quite effective for adapting monolingual parsers to new domains (McClosky et al., 2006) , as well as for relexicalizing delexicalized parsers using unlabeled target language data (Zeman and Resnik, 2008) . Similarly T\u00e4ckstr\u00f6m (2012) used self-training to adapt a multi-source direct transfer named-entity recognizer to different target languages, \"relexicalizing\" the model with word cluster features. However, as discussed in \u00a75.2, standard self-training is not optimal for target language adaptation. ---------------------------------- **AMBIGUITY-AWARE TRAINING** In this section, we propose a related training method: ambiguity-aware training. In this setting a discriminative probabilistic model is induced from automatically inferred ambiguous labelings over unlabeled target language data, in place of gold-standard dependency trees. The ambiguous labelings can combine multiple sources of evidence to guide the estimation or simply encode the underlying uncertainty from the base parser.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_20",
  "x": "While some higher-level linguistic properties of the target language have been incorporated through selective sharing, so far no features specific to the target language have been employed. Cohen et al. (2011) and <cite>Naseem et al. (2012)</cite> have shown that using expectation-maximization (EM) to this end can in some cases bring substantial accuracy gains. For discriminative models, self-training has been shown to be quite effective for adapting monolingual parsers to new domains (McClosky et al., 2006) , as well as for relexicalizing delexicalized parsers using unlabeled target language data (Zeman and Resnik, 2008) . Similarly T\u00e4ckstr\u00f6m (2012) used self-training to adapt a multi-source direct transfer named-entity recognizer to different target languages, \"relexicalizing\" the model with word cluster features. However, as discussed in \u00a75.2, standard self-training is not optimal for target language adaptation. ---------------------------------- **AMBIGUITY-AWARE TRAINING** In this section, we propose a related training method: ambiguity-aware training. In this setting a discriminative probabilistic model is induced from automatically inferred ambiguous labelings over unlabeled target language data, in place of gold-standard dependency trees.",
  "y": "motivation"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_21",
  "x": "Let A k (x, m) be the set of arcs for the mth token in x according to the kth parser in the ensemble. When arc-marginals are used to construct the ambiguity set, |A k (x, m)| \u2265 1, but when the Viterbiparse is used, A k (x, m) is a singleton. We next form , m) as the ensemble arc ambiguity set from which\u1ef9(x) is assembled. In this study, we combine the arc sets of two base parsers: first, the arc-marginal ambiguity set of the base parser ( \u00a75.2); and second, the Viterbi arc set from the <cite>NBG</cite> parser of <cite>Naseem et al. (2012)</cite> in Table 2 . 4 Thus, the latter will have singleton arc ambiguity sets, but when combined with the arc-marginal ambiguity sets of our base parser, the result will encode uncertainty derived from both parsers. ---------------------------------- **ADAPTATION EXPERIMENTS** We now study the different approaches to target language adaptation empirically. As in <cite>Naseem et al. (2012)</cite> , we use the CoNLL training sets, stripped of all dependency information, as the unlabeled target language data in our experiments.",
  "y": "uses background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_22",
  "x": "---------------------------------- **ADAPTATION EXPERIMENTS** We now study the different approaches to target language adaptation empirically. As in <cite>Naseem et al. (2012)</cite> , we use the CoNLL training sets, stripped of all dependency information, as the unlabeled target language data in our experiments. We use the Family model as the base parser, which is used to label the unlabeled target data with the Viterbi parses as well as with the ambiguous labelings. The final model is then trained on this data using standard lexicalized features (McDonald et al., 2005) . Since labeled training data is unavailable in the target language, we cannot tune any hyper-parameters and simply set \u03bb = 1 and \u03c3 = 0.95 throughout. Although the latter may suggest that\u1ef9(x) contains a high degree of ambiguity, in reality, the marginal distributions of the base model have low entropy and after filtering with \u03c3 = 0.95, the average number of potential heads per dependent ranges from 1.4 to 3.2, depending on the target language. The ambiguity-aware training methods, that is ambiguity-aware self-training (AAST) and ambiguityaware ensemble-training (AAET), are compared to three baseline systems.",
  "y": "uses similarities"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_23",
  "x": "As in <cite>Naseem et al. (2012)</cite> , we use the CoNLL training sets, stripped of all dependency information, as the unlabeled target language data in our experiments. We use the Family model as the base parser, which is used to label the unlabeled target data with the Viterbi parses as well as with the ambiguous labelings. The final model is then trained on this data using standard lexicalized features (McDonald et al., 2005) . Since labeled training data is unavailable in the target language, we cannot tune any hyper-parameters and simply set \u03bb = 1 and \u03c3 = 0.95 throughout. Although the latter may suggest that\u1ef9(x) contains a high degree of ambiguity, in reality, the marginal distributions of the base model have low entropy and after filtering with \u03c3 = 0.95, the average number of potential heads per dependent ranges from 1.4 to 3.2, depending on the target language. The ambiguity-aware training methods, that is ambiguity-aware self-training (AAST) and ambiguityaware ensemble-training (AAET), are compared to three baseline systems. First, <cite>NBG+EM</cite> is the generative model of <cite>Naseem et al. (2012)</cite> trained with expectation-maximization on additional unlabeled target language text. Second, Family is the best discriminative model from the previous section. Third, Viterbi is the basic Viterbi self-training model.",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_24",
  "x": "The final model is then trained on this data using standard lexicalized features (McDonald et al., 2005) . Since labeled training data is unavailable in the target language, we cannot tune any hyper-parameters and simply set \u03bb = 1 and \u03c3 = 0.95 throughout. Although the latter may suggest that\u1ef9(x) contains a high degree of ambiguity, in reality, the marginal distributions of the base model have low entropy and after filtering with \u03c3 = 0.95, the average number of potential heads per dependent ranges from 1.4 to 3.2, depending on the target language. The ambiguity-aware training methods, that is ambiguity-aware self-training (AAST) and ambiguityaware ensemble-training (AAET), are compared to three baseline systems. First, <cite>NBG+EM</cite> is the generative model of <cite>Naseem et al. (2012)</cite> trained with expectation-maximization on additional unlabeled target language text. Second, Family is the best discriminative model from the previous section. Third, Viterbi is the basic Viterbi self-training model. The results of each of these models are shown in Table 3 . There are a number of things that can be observed.",
  "y": "extends"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_25",
  "x": "Second, AAST outperforms the Viterbi variant on all languages and nearly always improves on the base parser, although it sees a slight drop for Italian. AAST improves the accuracy over the base model by 2% absolute on average and by as much as 5% absolute for Turkish. Comparing this model to the <cite>NBG+EM</cite> baseline, we observe an improvement by 3.6% absolute, outperforming it on 14 of the 16 languages. Furthermore, ambiguity-aware self-training appears to help more than expectation-maximization for generative (unlexicalized) models. <cite>Naseem et al.</cite> observed an increase from 59.3% to 60.4% on average by adding unlabeled target language data and the gains were not consistent across languages. AAST, on the other hand, achieves consistent gains, rising from 62.0% to 64.0% on average. Third, as shown in the rightmost column of Table 3 , ambiguity-aware ensemble-training is indeed a successful strategy; AAET outperforms the previous best self-trained model on 13 and NB&G+EM on 15 out of 16 languages. The relative error reduction with respect to the base Family model is 9% on average, while the average reduction with respect to <cite>NBG+EM</cite> is 13%. Before concluding, two additional points are worth making.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_26",
  "x": "Furthermore, ambiguity-aware self-training appears to help more than expectation-maximization for generative (unlexicalized) models. <cite>Naseem et al.</cite> observed an increase from 59.3% to 60.4% on average by adding unlabeled target language data and the gains were not consistent across languages. AAST, on the other hand, achieves consistent gains, rising from 62.0% to 64.0% on average. Third, as shown in the rightmost column of Table 3 , ambiguity-aware ensemble-training is indeed a successful strategy; AAET outperforms the previous best self-trained model on 13 and NB&G+EM on 15 out of 16 languages. The relative error reduction with respect to the base Family model is 9% on average, while the average reduction with respect to <cite>NBG+EM</cite> is 13%. Before concluding, two additional points are worth making. First, further gains may potentially be achievable with feature-rich discriminative models. While the best generative transfer model of <cite>Naseem et al. (2012)</cite> approaches its upper-bounding supervised accuracy (60.4% vs. 67.1%), our relaxed selftraining model is still far below its supervised counterpart (64.0% vs. 84.1%). One promising statistic along these lines is that the oracle accuracy for the ambiguous labelings of AAST is 75.7%, averaged across languages, which suggests that other training algorithms, priors or constraints could improve the accuracy substantially.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_27",
  "x": "There are a number of things that can be observed. First, Viterbi self-training helps slightly on average, but the gains are not consistent and there are even drops in accuracy for some languages. Second, AAST outperforms the Viterbi variant on all languages and nearly always improves on the base parser, although it sees a slight drop for Italian. AAST improves the accuracy over the base model by 2% absolute on average and by as much as 5% absolute for Turkish. Comparing this model to the <cite>NBG+EM</cite> baseline, we observe an improvement by 3.6% absolute, outperforming it on 14 of the 16 languages. Furthermore, ambiguity-aware self-training appears to help more than expectation-maximization for generative (unlexicalized) models. <cite>Naseem et al.</cite> observed an increase from 59.3% to 60.4% on average by adding unlabeled target language data and the gains were not consistent across languages. AAST, on the other hand, achieves consistent gains, rising from 62.0% to 64.0% on average. Third, as shown in the rightmost column of Table 3 , ambiguity-aware ensemble-training is indeed a successful strategy; AAET outperforms the previous best self-trained model on 13 and NB&G+EM on 15 out of 16 languages.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_28",
  "x": "First, Viterbi self-training helps slightly on average, but the gains are not consistent and there are even drops in accuracy for some languages. Second, AAST outperforms the Viterbi variant on all languages and nearly always improves on the base parser, although it sees a slight drop for Italian. AAST improves the accuracy over the base model by 2% absolute on average and by as much as 5% absolute for Turkish. Comparing this model to the <cite>NBG+EM</cite> baseline, we observe an improvement by 3.6% absolute, outperforming it on 14 of the 16 languages. Furthermore, ambiguity-aware self-training appears to help more than expectation-maximization for generative (unlexicalized) models. <cite>Naseem et al.</cite> observed an increase from 59.3% to 60.4% on average by adding unlabeled target language data and the gains were not consistent across languages. AAST, on the other hand, achieves consistent gains, rising from 62.0% to 64.0% on average. Third, as shown in the rightmost column of Table 3 , ambiguity-aware ensemble-training is indeed a successful strategy; AAET outperforms the previous best self-trained model on 13 and NB&G+EM on 15 out of 16 languages. The relative error reduction with respect to the base Family model is 9% on average, while the average reduction with respect to <cite>NBG+EM</cite> is 13%.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_29",
  "x": "Third, as shown in the rightmost column of Table 3 , ambiguity-aware ensemble-training is indeed a successful strategy; AAET outperforms the previous best self-trained model on 13 and NB&G+EM on 15 out of 16 languages. The relative error reduction with respect to the base Family model is 9% on average, while the average reduction with respect to <cite>NBG+EM</cite> is 13%. Before concluding, two additional points are worth making. First, further gains may potentially be achievable with feature-rich discriminative models. While the best generative transfer model of <cite>Naseem et al. (2012)</cite> approaches its upper-bounding supervised accuracy (60.4% vs. 67.1%), our relaxed selftraining model is still far below its supervised counterpart (64.0% vs. 84.1%). One promising statistic along these lines is that the oracle accuracy for the ambiguous labelings of AAST is 75.7%, averaged across languages, which suggests that other training algorithms, priors or constraints could improve the accuracy substantially. Second, relexicalization is a key component of self-training. If we use delexicalized features during self-training, we only observe a small average improvement from 62.0% to 62.1%. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_30",
  "x": [
   "If we use delexicalized features during self-training, we only observe a small average improvement from 62.0% to 62.1%. ---------------------------------- **CONCLUSIONS** We contributed to the understanding of multi-source syntactic transfer in several complementary ways. First, we showed how selective parameter sharing, based on typological features and language family membership, can be incorporated in a discriminative graph-based model of dependency parsing. We then showed how ambiguous labelings can be used to integrate heterogenous knowledge sources in parser training. Two instantiations of this framework were explored. First, an ambiguity-aware self-training method that can be used to effectively relexicalize and adapt a delexicalized transfer parser using unlabeled target language data. Second, an ambiguityaware ensemble-training method, in which predictions from different parsers can be incorporated and further adapted."
  ],
  "y": "differences"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_0",
  "x": "We investigate applying repurposed generic QA data and models to a recently proposed relation extraction task. We find that training on SQuAD produces better zero-shot performance and more robust generalisation compared to the task specific training set. We also show that standard QA architectures (e.g. FastQA or BiDAF) can be applied to the slot filling queries without the need for model modification. ---------------------------------- **INTRODUCTION** Knowledge Base Population (KBP, e.g.: Riedel et al., 2013; Sterckx et al., 2016) attempts to identify facts within raw text and convert them into triples consisting of a subject, object and the relation between them. One common form of this task is slot filling (Surdeanu and Heng, 2014) , in which a knowledge base (KB) query, such as place of birth(Obama, ?) is applied to a set of documents and a set of slot fillers is returned. By converting such KB queries to natural language questions, <cite>Levy et al. (2017)</cite> showed that a question answering (QA) system could be effectively applied to this task. However, <cite>their approach</cite> relied on a modified QA model architecture and a dedicated slot-filling training corpus.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_1",
  "x": "By converting such KB queries to natural language questions, <cite>Levy et al. (2017)</cite> showed that a question answering (QA) system could be effectively applied to this task. However, <cite>their approach</cite> relied on a modified QA model architecture and a dedicated slot-filling training corpus. Here, we investigate the utility of standard QA data and models for this task. Our results show that this approach is effective in the zero-shot and low-resource cases, and is more robust on a set of test instances that challenge the models' ability to identify relations between subject and object. Figure 1 gives an overview of using QA on the slot-filling task. Starting at the top right, a KB query is translated into a natural language question, which can then be fed into a QA model that has been trained on an appropriate resource. When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_2",
  "x": "However, <cite>their approach</cite> relied on a modified QA model architecture and a dedicated slot-filling training corpus. Here, we investigate the utility of standard QA data and models for this task. Our results show that this approach is effective in the zero-shot and low-resource cases, and is more robust on a set of test instances that challenge the models' ability to identify relations between subject and object. Figure 1 gives an overview of using QA on the slot-filling task. Starting at the top right, a KB query is translated into a natural language question, which can then be fed into a QA model that has been trained on an appropriate resource. When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK**",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_3",
  "x": "When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zeroshot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) both positive examples, containing answers, and negative examples, without answers. <cite>These examples</cite> were derived from a pre-existing relation extraction resource, as <cite>their</cite> intention was to show the utility of the QA model.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_4",
  "x": "In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zeroshot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) both positive examples, containing answers, and negative examples, without answers. <cite>These examples</cite> were derived from a pre-existing relation extraction resource, as <cite>their</cite> intention was to show the utility of the QA model. In this section, we evaluate whether the same model trained on QA data, specifically SQuAD (Rajpurkar et al., 2016) , can be applied to the relation extraction task. We first investigate the zeroshot case, where no examples of the relations are available, and then evaluate how performance improves as more data becomes available. Data We compare two sources of training data: The <cite>University of Washington relation extraction</cite> (<cite>UWRE</cite>) dataset created by <cite>Levy et al. (2017</cite>) and the Stanford Question Answering Dataset (SQuAD) created by Rajpurkar et al. (2016) . The <cite>UWRE</cite> data is derived from WikiReading (Hewlett et al., 2016) , which is itself derived from WikiData (Vrande\u010di\u0107, 2012) , and consists of a set of positive and negative examples for relation extraction from Wikipedia sentences.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_5",
  "x": "When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zeroshot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) both positive examples, containing answers, and negative examples, without answers. <cite>These examples</cite> were derived from a pre-existing relation extraction resource, as <cite>their</cite> intention was to show the utility of the QA model.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_6",
  "x": "In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zeroshot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) both positive examples, containing answers, and negative examples, without answers. <cite>These examples</cite> were derived from a pre-existing relation extraction resource, as <cite>their</cite> intention was to show the utility of the QA model. In this section, we evaluate whether the same model trained on QA data, specifically SQuAD (Rajpurkar et al., 2016) , can be applied to the relation extraction task. We first investigate the zeroshot case, where no examples of the relations are available, and then evaluate how performance improves as more data becomes available. Data We compare two sources of training data: The <cite>University of Washington relation extraction</cite> (<cite>UWRE</cite>) dataset created by <cite>Levy et al. (2017</cite>) and the Stanford Question Answering Dataset (SQuAD) created by Rajpurkar et al. (2016) . The <cite>UWRE</cite> data is derived from WikiReading (Hewlett et al., 2016) , which is itself derived from WikiData (Vrande\u010di\u0107, 2012) , and consists of a set of positive and negative examples for relation extraction from Wikipedia sentences.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_7",
  "x": "We first investigate the zeroshot case, where no examples of the relations are available, and then evaluate how performance improves as more data becomes available. Data We compare two sources of training data: The <cite>University of Washington relation extraction</cite> (<cite>UWRE</cite>) dataset created by <cite>Levy et al. (2017</cite>) and the Stanford Question Answering Dataset (SQuAD) created by Rajpurkar et al. (2016) . The <cite>UWRE</cite> data is derived from WikiReading (Hewlett et al., 2016) , which is itself derived from WikiData (Vrande\u010di\u0107, 2012) , and consists of a set of positive and negative examples for relation extraction from Wikipedia sentences. Each instance consists of an entity, a relation, a question template for the relation and a sentence drawn from the wikipedia article for that entity which may or may not answer the question. Under the assumption that each relation triple found in a Wikipedia info-box is also expressed in the text of its article, the positive examples contain the first sentence from the article that contains both the subject and object of the triple. The negative examples also contain the subject entity of the relation, but express a different relation. <cite>Levy et al. (2017)</cite> provide a number of train/dev/test splits, to allow <cite>them</cite> to evaluate a variety of modes of generalisation. Here we use the relation and entity splits. The former tests the ability to generalise from one set of relations to another, i.e. to do zero-shot learning for the unseen relations in the test set.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_8",
  "x": "The <cite>UWRE</cite> data is derived from WikiReading (Hewlett et al., 2016) , which is itself derived from WikiData (Vrande\u010di\u0107, 2012) , and consists of a set of positive and negative examples for relation extraction from Wikipedia sentences. Each instance consists of an entity, a relation, a question template for the relation and a sentence drawn from the wikipedia article for that entity which may or may not answer the question. Under the assumption that each relation triple found in a Wikipedia info-box is also expressed in the text of its article, the positive examples contain the first sentence from the article that contains both the subject and object of the triple. The negative examples also contain the subject entity of the relation, but express a different relation. <cite>Levy et al. (2017)</cite> provide a number of train/dev/test splits, to allow <cite>them</cite> to evaluate a variety of modes of generalisation. Here we use the relation and entity splits. The former tests the ability to generalise from one set of relations to another, i.e. to do zero-shot learning for the unseen relations in the test set. In contrast, the latter tests on the easier task of generalising from one set of entities to another for the same set of relations. We use this dataset to investigate how having access to various quantities of data about the test set relations changes performance.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_9",
  "x": "<cite>Levy et al. (2017)</cite> provide a number of train/dev/test splits, to allow <cite>them</cite> to evaluate a variety of modes of generalisation. Here we use the relation and entity splits. The former tests the ability to generalise from one set of relations to another, i.e. to do zero-shot learning for the unseen relations in the test set. In contrast, the latter tests on the easier task of generalising from one set of entities to another for the same set of relations. We use this dataset to investigate how having access to various quantities of data about the test set relations changes performance. To build a dataset using SQuAD (Rajpurkar et al., 2016) , we construct negative examples by removing sentences that contain the answer, based on the spans provided by the annotators. In other words, we are left with the original question and a paragraph relevant to the topic of that question, but which typically no longer contains sentences answering it. Alongside these negative examples, we also retain the original SQuAD instances as positive examples. This process is applied to both the train and dev sets, allowing us to evaluate a model that uses only question answering data at training time.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_10",
  "x": "This process is applied to both the train and dev sets, allowing us to evaluate a model that uses only question answering data at training time. We also construct a series of datasets that combine increasing quantities of the <cite>UWRE</cite> entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 <cite>UWRE</cite> instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction on the relation split test set, using models trained on the original <cite>UWRE</cite> and SQuAD datasets. As can be seen, BIDAF is actually more effective at answering the questions for the unseen relation types in the <cite>UWRE</cite> test set when it is trained on a standard QA dataset, rather than a dedicated relation extraction dataset.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_11",
  "x": "We use this dataset to investigate how having access to various quantities of data about the test set relations changes performance. To build a dataset using SQuAD (Rajpurkar et al., 2016) , we construct negative examples by removing sentences that contain the answer, based on the spans provided by the annotators. In other words, we are left with the original question and a paragraph relevant to the topic of that question, but which typically no longer contains sentences answering it. Alongside these negative examples, we also retain the original SQuAD instances as positive examples. This process is applied to both the train and dev sets, allowing us to evaluate a model that uses only question answering data at training time. We also construct a series of datasets that combine increasing quantities of the <cite>UWRE</cite> entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 <cite>UWRE</cite> instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_12",
  "x": "We use this dataset to investigate how having access to various quantities of data about the test set relations changes performance. To build a dataset using SQuAD (Rajpurkar et al., 2016) , we construct negative examples by removing sentences that contain the answer, based on the spans provided by the annotators. In other words, we are left with the original question and a paragraph relevant to the topic of that question, but which typically no longer contains sentences answering it. Alongside these negative examples, we also retain the original SQuAD instances as positive examples. This process is applied to both the train and dev sets, allowing us to evaluate a model that uses only question answering data at training time. We also construct a series of datasets that combine increasing quantities of the <cite>UWRE</cite> entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 <cite>UWRE</cite> instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_13",
  "x": "In other words, we are left with the original question and a paragraph relevant to the topic of that question, but which typically no longer contains sentences answering it. Alongside these negative examples, we also retain the original SQuAD instances as positive examples. This process is applied to both the train and dev sets, allowing us to evaluate a model that uses only question answering data at training time. We also construct a series of datasets that combine increasing quantities of the <cite>UWRE</cite> entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 <cite>UWRE</cite> instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_14",
  "x": "However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction on the relation split test set, using models trained on the original <cite>UWRE</cite> and SQuAD datasets. As can be seen, BIDAF is actually more effective at answering the questions for the unseen relation types in the <cite>UWRE</cite> test set when it is trained on a standard QA dataset, rather than a dedicated relation extraction dataset. Figure 2 plots how performance improves as more data becomes available about the relations in the entity split test set. We compare training purely on <cite>UWRE</cite> instances to those same instances combined with the whole SQuAD dataset. As can be seen, when only small amounts of relation extraction data is available, combining this with the QA data gives a substantial boost to performance. Discussion The SQuAD trained model appears to be effective in the limited data and zero-shot cases, but contributes little when large numbers of examples of the relations of interest are available. In this case, the dedicated relation extraction model is able to achieve an F1 of around 90%, with or without augmentation with SQuAD. This level of performance suggests that such a model would be accurate enough for practical applications.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_15",
  "x": "This process is applied to both the train and dev sets, allowing us to evaluate a model that uses only question answering data at training time. We also construct a series of datasets that combine increasing quantities of the <cite>UWRE</cite> entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 <cite>UWRE</cite> instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction on the relation split test set, using models trained on the original <cite>UWRE</cite> and SQuAD datasets. As can be seen, BIDAF is actually more effective at answering the questions for the unseen relation types in the <cite>UWRE</cite> test set when it is trained on a standard QA dataset, rather than a dedicated relation extraction dataset.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_16",
  "x": "However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction on the relation split test set, using models trained on the original <cite>UWRE</cite> and SQuAD datasets. As can be seen, BIDAF is actually more effective at answering the questions for the unseen relation types in the <cite>UWRE</cite> test set when it is trained on a standard QA dataset, rather than a dedicated relation extraction dataset. Figure 2 plots how performance improves as more data becomes available about the relations in the entity split test set. We compare training purely on <cite>UWRE</cite> instances to those same instances combined with the whole SQuAD dataset. As can be seen, when only small amounts of relation extraction data is available, combining this with the QA data gives a substantial boost to performance. Discussion The SQuAD trained model appears to be effective in the limited data and zero-shot cases, but contributes little when large numbers of examples of the relations of interest are available. In this case, the dedicated relation extraction model is able to achieve an F1 of around 90%, with or without augmentation with SQuAD. This level of performance suggests that such a model would be accurate enough for practical applications.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_17",
  "x": "As can be seen, when only small amounts of relation extraction data is available, combining this with the QA data gives a substantial boost to performance. Discussion The SQuAD trained model appears to be effective in the limited data and zero-shot cases, but contributes little when large numbers of examples of the relations of interest are available. In this case, the dedicated relation extraction model is able to achieve an F1 of around 90%, with or without augmentation with SQuAD. This level of performance suggests that such a model would be accurate enough for practical applications. However, test set performance may not be a reliable indicator of the model's ability to generalise to more challenging examples (Jia and Liang, 2017) . ---------------------------------- **GENERALISATION TO A CHALLENGE TEST SET** In this second experiment, we want to test the ability of the models decribed above to generalise to data beyond the <cite>UWRE</cite> test set. In particular, we want to verify that the BiDAF model is able to recognise the assertion of a relation between the entity and the answer, rather than just recognising an answer phrase of the right type.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_18",
  "x": "In this second experiment, we want to test the ability of the models decribed above to generalise to data beyond the <cite>UWRE</cite> test set. In particular, we want to verify that the BiDAF model is able to recognise the assertion of a relation between the entity and the answer, rather than just recognising an answer phrase of the right type. Data We construct a challenge test set of negative examples based on sentences which are about the wrong entity but which do contain potential answers that are valid for the question and relation type. Thus, each positive example from the original <cite>UWRE</cite> entity split test set is turned into a negative example by pairing the sentence with an equivalent question about another entity. A model that has merely learnt to identify answer spans of the right form, irrespective of their relation to the rest of the sentence, is likely to return the original span rather than recognise that the sentence no longer contains an answer. We then build new train, dev and test sets (UWRE+) from the original entity split datasets in which half the original negative instances have been replaced with these more challenging instances. As before, a series of datasets combining SQuAD with increasing amounts of this new data is also constructed. Models We re-use the <cite>UWRE</cite> and SQuAD trained models in addition to training on the UWRE+ datasets described in the previous section. Evaluation Here, F1 is not an appropriate measure, as there are no positive instances in the challenge data.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_19",
  "x": "A model that has merely learnt to identify answer spans of the right form, irrespective of their relation to the rest of the sentence, is likely to return the original span rather than recognise that the sentence no longer contains an answer. We then build new train, dev and test sets (UWRE+) from the original entity split datasets in which half the original negative instances have been replaced with these more challenging instances. As before, a series of datasets combining SQuAD with increasing amounts of this new data is also constructed. Models We re-use the <cite>UWRE</cite> and SQuAD trained models in addition to training on the UWRE+ datasets described in the previous section. Evaluation Here, F1 is not an appropriate measure, as there are no positive instances in the challenge data. Instead, we use accuracy of the predictions, which in this case is just the number of 'no answer' predictions divided by the total number of instances. Table 3 : Zero-shot Precision, Recall and F1 on the <cite>UWRE</cite> relation split test set. Results Table 2 reports the accuracy of predictions on the challenge test set of negative examples. Although the original <cite>UWRE</cite> model achieved an F1 of around 90% on the unmodified entity split test set, here it only manages to get 2% of its predictions correct.",
  "y": "uses extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_20",
  "x": "Table 3 : Zero-shot Precision, Recall and F1 on the <cite>UWRE</cite> relation split test set. Results Table 2 reports the accuracy of predictions on the challenge test set of negative examples. Although the original <cite>UWRE</cite> model achieved an F1 of around 90% on the unmodified entity split test set, here it only manages to get 2% of its predictions correct. In contrast, the modified UWRE+ training data results in a model that is much more accurate, predicting over 70% of the negative examples correctly. Nonetheless, the performance of the SQuAD trained model is stronger still, even without modification to address this problem. Figure 3 shows the accuracy on the challenge test set as increasing quantities of relation extraction instances are added to SQuAD. Looking first at the effect of adding the original <cite>UWRE</cite> training instances, performance drops dramatically as the size of this expansion increases. In contrast, as the quantity of UWRE+ data grows, performance improves, peaking at around 100,000 instances, which is around the same size as SQuAD. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_21",
  "x": "Evaluation Here, F1 is not an appropriate measure, as there are no positive instances in the challenge data. Instead, we use accuracy of the predictions, which in this case is just the number of 'no answer' predictions divided by the total number of instances. Table 3 : Zero-shot Precision, Recall and F1 on the <cite>UWRE</cite> relation split test set. Results Table 2 reports the accuracy of predictions on the challenge test set of negative examples. Although the original <cite>UWRE</cite> model achieved an F1 of around 90% on the unmodified entity split test set, here it only manages to get 2% of its predictions correct. In contrast, the modified UWRE+ training data results in a model that is much more accurate, predicting over 70% of the negative examples correctly. Nonetheless, the performance of the SQuAD trained model is stronger still, even without modification to address this problem. Figure 3 shows the accuracy on the challenge test set as increasing quantities of relation extraction instances are added to SQuAD. Looking first at the effect of adding the original <cite>UWRE</cite> training instances, performance drops dramatically as the size of this expansion increases.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_22",
  "x": "Table 3 : Zero-shot Precision, Recall and F1 on the <cite>UWRE</cite> relation split test set. Results Table 2 reports the accuracy of predictions on the challenge test set of negative examples. Although the original <cite>UWRE</cite> model achieved an F1 of around 90% on the unmodified entity split test set, here it only manages to get 2% of its predictions correct. In contrast, the modified UWRE+ training data results in a model that is much more accurate, predicting over 70% of the negative examples correctly. Nonetheless, the performance of the SQuAD trained model is stronger still, even without modification to address this problem. Figure 3 shows the accuracy on the challenge test set as increasing quantities of relation extraction instances are added to SQuAD. Looking first at the effect of adding the original <cite>UWRE</cite> training instances, performance drops dramatically as the size of this expansion increases. In contrast, as the quantity of UWRE+ data grows, performance improves, peaking at around 100,000 instances, which is around the same size as SQuAD. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_23",
  "x": "Other models may show different patterns of strength and weakness, but to be able to investigate and exploit further QA systems quickly would require a means of producing 'no answer' predictions without the need to modify the model implementation. 4 Using an unmodified QA model for slot filling <cite>Levy et al. (2017)</cite> modify the BiDAF architecture to produce an additional output representing the probability that no answer is present in the text. In this experiment, we investigate whether it is possible to adapt a QA model to the slot filling task without having to understand and modify its internal structure and implementation. Our approach merely requires prefixing all texts with a dummy token that stands in for the answer when no real answer is present. Data We train our models on a modified version of SQuAD, which has been augmented with negative examples by removing answer spans, as described in Section 2, and then had the token NoAnswerFound inserted into every text and as the answer for the negative examples, as described above. Models We train both BiDAF (Seo et al., 2016) and FastQA (Weissenborn et al., 2017 ) models on the modified SQuAD training data, using their standard architectures and hyperparameters. Evaluation We evaluate F1 on the same zeroshot evaluation considered in Section 2 and also accuracy on the challenge test set from Section 3. Results Table 3 reveals that the unmodified BiDAF model is almost as effective as the <cite>Levy et al. (2017)</cite> model in terms of zero-shot F1 on the original <cite>UWRE</cite> test set. In contrast, FastQA's performance is substantially worse.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_24",
  "x": "In the case of SQuAD, the multi-sentence paragraph structure around the answer provides enough potential distractors to overcome this issue. Other models may show different patterns of strength and weakness, but to be able to investigate and exploit further QA systems quickly would require a means of producing 'no answer' predictions without the need to modify the model implementation. 4 Using an unmodified QA model for slot filling <cite>Levy et al. (2017)</cite> modify the BiDAF architecture to produce an additional output representing the probability that no answer is present in the text. In this experiment, we investigate whether it is possible to adapt a QA model to the slot filling task without having to understand and modify its internal structure and implementation. Our approach merely requires prefixing all texts with a dummy token that stands in for the answer when no real answer is present. Data We train our models on a modified version of SQuAD, which has been augmented with negative examples by removing answer spans, as described in Section 2, and then had the token NoAnswerFound inserted into every text and as the answer for the negative examples, as described above. Models We train both BiDAF (Seo et al., 2016) and FastQA (Weissenborn et al., 2017 ) models on the modified SQuAD training data, using their standard architectures and hyperparameters. Evaluation We evaluate F1 on the same zeroshot evaluation considered in Section 2 and also accuracy on the challenge test set from Section 3. Results Table 3 reveals that the unmodified BiDAF model is almost as effective as the <cite>Levy et al. (2017)</cite> model in terms of zero-shot F1 on the original <cite>UWRE</cite> test set.",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_0",
  "x": "When programmers translate Natural Language (NL) specifications into executable source code, they typically start with a high-level plan of the major structures required, such as nested loops, conditionals, etc. and then proceed to fill in specific details into these components. We refer to these high-level structures (Figure 1 (b) ) as code idioms (Allamanis and Sutton, 2014) . In this paper, we demonstrate how learning to use code idioms leads to an improvement in model accuracy and training time for the task of semantic parsing, i.e., mapping intents in NL into general purpose source code (Iyer et al., 2017; Ling et al., 2016) . State-of-the-art semantic parsers are neural encoder-decoder models, where decoding is guided by the grammar of the target programming language (Yin and Neubig, 2017; Rabinovich et al., 2017; <cite>Iyer et al., 2018</cite>) to ensure syntactically valid programs. For general purpose programming languages with large formal grammars, this can easily lead to long decoding paths even for short snippets of code. For example, Figure 1 shows an intermediate parse tree for a generic if-then-else code snippet, for which the decoder requires as many as eleven decoding steps before ultimately filling in the slots for the if condition, the then expression and the else expression. However, the if-then-else block can be seen as a higher level structure such as shown in Figure 1 (b) that can be applied in one decoding step and reused in many different programs. In this paper, we refer to frequently recurring subtrees of programmatic parse trees as code idioms, and we equip semantic parsers with the ability to learn and directly generate idiomatic structures as in Figure 1 (b). We introduce a simple iterative method to extract idioms from a dataset of programs by repeatedly collapsing the most frequent depth-2 subtrees of syntax parse trees.",
  "y": "background"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_1",
  "x": "In this paper, we refer to frequently recurring subtrees of programmatic parse trees as code idioms, and we equip semantic parsers with the ability to learn and directly generate idiomatic structures as in Figure 1 (b). We introduce a simple iterative method to extract idioms from a dataset of programs by repeatedly collapsing the most frequent depth-2 subtrees of syntax parse trees. Analogous to the byte pair encoding (BPE) method (Gage, 1994; Sennrich et al., 2016 ) that creates new subtokens of words by repeatedly combining frequently occurring adjacent pairs of subtokens, our method takes a depth-2 syntax subtree and replaces it with a tree of depth-1 by removing all the internal nodes. This method is in contrast with the approach using probabilistic tree substitution grammars (pTSG) taken by Allamanis and Sutton (2014) , who use the explanation quality of an idiom to prioritize idioms that are more interesting, with an end goal of suggesting useful idioms to programmers using IDEs. Once idioms are extracted, we greedily apply them to semantic parsing training sets to provide supervision for learning to apply idioms. We evaluate our approach on a context dependent semantic parsing task (<cite>Iyer et al., 2018</cite>) using the CONCODE dataset, where we improve the state of the art by 2.2% of BLEU score. Furthermore, generating source code using idioms results in a more than 50% reduction in the number of decoding steps, which cuts down training time to less than half, from 27 to 13 hours. Taking advantage of this reduced training time, we further push the state of the art on CONCODE to an EM of 13.4 and a BLEU score of 28.9 by training on an extended version of the training set (with 5x the amount of training examples). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_2",
  "x": "Once idioms are extracted, we greedily apply them to semantic parsing training sets to provide supervision for learning to apply idioms. We evaluate our approach on a context dependent semantic parsing task (<cite>Iyer et al., 2018</cite>) using the CONCODE dataset, where we improve the state of the art by 2.2% of BLEU score. Furthermore, generating source code using idioms results in a more than 50% reduction in the number of decoding steps, which cuts down training time to less than half, from 27 to 13 hours. Taking advantage of this reduced training time, we further push the state of the art on CONCODE to an EM of 13.4 and a BLEU score of 28.9 by training on an extended version of the training set (with 5x the amount of training examples). ---------------------------------- **RELATED WORK** Neural encoder-decoder models have proved effective in mapping NL to logical forms (Dong and Lapata, 2016) and also for directly producing general purpose programs (Iyer et al., 2017 (<cite>Iyer et al., , 2018</cite> . Ling et al. (2016) use a sequence-tosequence model with attention and a copy mechanism to generate source code. Instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamically determined modular structure paralleling the structure of the abstract syntax tree (AST) of the code (Rabinovich et al., 2017; Yin and Neubig, 2017) .",
  "y": "background"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_3",
  "x": "**RELATED WORK** Neural encoder-decoder models have proved effective in mapping NL to logical forms (Dong and Lapata, 2016) and also for directly producing general purpose programs (Iyer et al., 2017 (<cite>Iyer et al., , 2018</cite> . Ling et al. (2016) use a sequence-tosequence model with attention and a copy mechanism to generate source code. Instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamically determined modular structure paralleling the structure of the abstract syntax tree (AST) of the code (Rabinovich et al., 2017; Yin and Neubig, 2017) . Iy<cite>er et al. (2018</cite>) use a similar decoding approach but use a specialized context encoder for the task of context-dependent code generation. We augment these neural encoder-decoder models with the ability to decode in terms of frequently occurring higher level idiomatic structures to achieve gains in accuracy and training time. Another different but related method to produce source code is with the help of sketches, which are code snippets containing slots in the place of low-level information such as variable names and arguments. Dong and Lapata (2018) generate sketches as intermediate representations to convert NL to logical forms; Hayati et al. (2018) retrieve sketches from a large training corpus and later modify them for the current input; Murali et al. (2018) use a combination of neural learning and type-guided combinatorial search to convert existing sketches into executable programs, whereas Nye et al. (2019) additionally also generate the sketches before synthesising programs. While we don't explicitly generate sketches, we find that our idiom-based decoder learns to generate commonly used programming sketches with slots, and fills them in during subsequent decoding timesteps.",
  "y": "background"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_4",
  "x": "This process is illustrated in Figure 3 where we perform two applications of the first idiom from Figure 2 (b), followed by one application of the second idiom from Figure 2 (d) , after which, the tree cannot be further compressed using those two idioms. The final tree can be represented using |r i | = 2 rules instead of the original |p i | = 5 rules. The decoder is then trained similar to previous approaches (Yin and Neubig, 2017; <cite>Iyer et al., 2018</cite>) using the compressed set of rules. In later experiments, we find that this results in a rule set compression of more than 50% (see Section 7). ---------------------------------- **EXPERIMENTAL SETUP** We apply our approach to the context dependent encoder-decoder model of <cite>Iyer et al. (2018</cite>) on the CONCODE dataset, and compare performance to a better tuned instance of their best model. ---------------------------------- **TASK**",
  "y": "similarities"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_5",
  "x": "We apply our approach to the context dependent encoder-decoder model of <cite>Iyer et al. (2018</cite>) on the CONCODE dataset, and compare performance to a better tuned instance of their best model. ---------------------------------- **TASK** The CONCODE task involves mapping an NL query together with a class environment comprising a list of variables (with types) and methods (with return types), into the source code of a class member function. Figure 4 (a) shows an example where the context comprises variables and methods (with types) that would normally exist in a class that implements a vector, such as vecElements and dotProduct(). Conditioned on Source code: AST Derivation: this context, the task involves mapping the NL query Adds a scalar to this vector in place into a sequence of parsing rules to generate the source code in Figure 4 (b). Formally, their task is: Given a NL utterance q, a set of context variables {v i } with types {t i }, and a set of context methods {m i } with return types {r i }, predict a set of parsing rules {a i } of the target program. Their best performing model is an encoder-decoder model with a context aware encoder and a decoder that produces production rules from the grammar of the target programming language.",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_6",
  "x": "The CONCODE task involves mapping an NL query together with a class environment comprising a list of variables (with types) and methods (with return types), into the source code of a class member function. Figure 4 (a) shows an example where the context comprises variables and methods (with types) that would normally exist in a class that implements a vector, such as vecElements and dotProduct(). Conditioned on Source code: AST Derivation: this context, the task involves mapping the NL query Adds a scalar to this vector in place into a sequence of parsing rules to generate the source code in Figure 4 (b). Formally, their task is: Given a NL utterance q, a set of context variables {v i } with types {t i }, and a set of context methods {m i } with return types {r i }, predict a set of parsing rules {a i } of the target program. Their best performing model is an encoder-decoder model with a context aware encoder and a decoder that produces production rules from the grammar of the target programming language. ---------------------------------- **BASELINE MODEL** We follow the approach of <cite>Iyer et al. (2018</cite>) with three major modifications in their encoder, which yields improvements in speed and accuracy (IyerSimp) .",
  "y": "extends"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_7",
  "x": "We follow the approach of <cite>Iyer et al. (2018</cite>) with three major modifications in their encoder, which yields improvements in speed and accuracy (IyerSimp) . First, in addition to camel-case splitting of identifier tokens, we further use byte-pair encoding (BPE) (Sennrich et al., 2016) on all NL tokens, identifier names and types and embed all these BPE tokens using a single embedding matrix. Next, we replace their RNN that contextualizes the subtokens of identifiers and types with an average of the subtoken embeddings instead. Finally, we consolidate their three separate RNNs for contextualizing NL, variable names with types, and method names with types, into a single shared RNN, which greatly reduces the number of model parameters. Formally, let {q i } represent the set of BPE tokens of the NL, and {t ij }, {v ij }, {r ij } and {m ij } represent the jth BPE token of the ith variable type, variable name, method return type, and method name respectively. First, all these elements are embedded using a BPE token embedding matrix B to give us q i , t ij , v ij , r ij and m ij . Using Bi-LSTM f , the encoder then computes: Then, h 1 , . . . , h z , andt i ,v i ,r i ,m i are passed on to the attention mechanism in the decoder, exactly as in <cite>Iyer et al. (2018</cite>) . The decoder of <cite>Iyer et al. (2018)</cite> is left unchanged.",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_8",
  "x": "First, all these elements are embedded using a BPE token embedding matrix B to give us q i , t ij , v ij , r ij and m ij . Using Bi-LSTM f , the encoder then computes: Then, h 1 , . . . , h z , andt i ,v i ,r i ,m i are passed on to the attention mechanism in the decoder, exactly as in <cite>Iyer et al. (2018</cite>) . The decoder of <cite>Iyer et al. (2018)</cite> is left unchanged. This forms our baseline model (Iyer-Simp). ---------------------------------- **HYPERPARAMETERS** To create models that use idioms, we augment this decoder by first retrieving the top-K most frequent idioms from the training set (Algorithm 1), followed by post-processing the training set by greedily applying these idioms (Algorithm 2; we denote this model as Iyer-Simp-K). We evaluate all our models on the CONCODE dataset which was created using Java class files from github.com.",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_9",
  "x": "The decoder of <cite>Iyer et al. (2018)</cite> is left unchanged. This forms our baseline model (Iyer-Simp). ---------------------------------- **HYPERPARAMETERS** To create models that use idioms, we augment this decoder by first retrieving the top-K most frequent idioms from the training set (Algorithm 1), followed by post-processing the training set by greedily applying these idioms (Algorithm 2; we denote this model as Iyer-Simp-K). We evaluate all our models on the CONCODE dataset which was created using Java class files from github.com. It contains 100K tuples of (NL, code, context) for training, 2,000 tuples for development, and an additional 2,000 tuples for testing. We use a BPE vocabulary of 10K tokens for embedding matrix B and get the best validation set results using the original hyperparameters used by <cite>Iyer et al. (2018)</cite> . Since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400K training examples that <cite>Iyer et al. (2018)</cite> released as part of CONCODE.",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_10",
  "x": "**HYPERPARAMETERS** To create models that use idioms, we augment this decoder by first retrieving the top-K most frequent idioms from the training set (Algorithm 1), followed by post-processing the training set by greedily applying these idioms (Algorithm 2; we denote this model as Iyer-Simp-K). We evaluate all our models on the CONCODE dataset which was created using Java class files from github.com. It contains 100K tuples of (NL, code, context) for training, 2,000 tuples for development, and an additional 2,000 tuples for testing. We use a BPE vocabulary of 10K tokens for embedding matrix B and get the best validation set results using the original hyperparameters used by <cite>Iyer et al. (2018)</cite> . Since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400K training examples that <cite>Iyer et al. (2018)</cite> released as part of CONCODE. We report exact match accuracy, corpus level BLEU score (which serves as a measure of partial credit) (Papineni et al., 2002) , and training time for all these configurations. <cite>Iyer et al. (2018)</cite> . Significant improvements in training speed after incorporating idioms makes training on large amounts of data possible.",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_12",
  "x": "It contains 100K tuples of (NL, code, context) for training, 2,000 tuples for development, and an additional 2,000 tuples for testing. We use a BPE vocabulary of 10K tokens for embedding matrix B and get the best validation set results using the original hyperparameters used by <cite>Iyer et al. (2018)</cite> . Since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400K training examples that <cite>Iyer et al. (2018)</cite> released as part of CONCODE. We report exact match accuracy, corpus level BLEU score (which serves as a measure of partial credit) (Papineni et al., 2002) , and training time for all these configurations. <cite>Iyer et al. (2018)</cite> . Significant improvements in training speed after incorporating idioms makes training on large amounts of data possible. ---------------------------------- **RESULTS AND DISCUSSION** taining comparable EM accuracy.",
  "y": "similarities"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_13",
  "x": "Compared to the model of <cite>Iyer et al. (2018)</cite> , our significantly reduced training time enables us to train on their extended training set. We run IyerSimp using 400 idioms (taking advantage of even lower training time) on up to 5 times the amount of data, making sure that we do not include in training any NL from the validation or the test sets. Since the original set of idioms learned from the original training set are quite general, we directly use them rather than relearn the idioms from scratch. We report EM and BLEU scores for different amounts of training data on the same validation and test sets as CONCODE in Table 3 . In general, accuracies increase with the amount of data with the best model achieving a BLEU score of 28.9 and EM of 13.4. Figure 5 shows some of the idioms that were extracted from CONCODE. (a) is an idiom to construct a new object with arguments, (b) represents a try-catch block, and, (c) is an integerbased for loop. In (e), we show how small idioms are combined to form larger ones; it combines an if-then idiom with a throw-exception idiom, which throws an object instantiated using idiom (a). The decoder also learns idioms to directly generate common library methods such as System.out.println( StringLiteral ) in one decoding step (d).",
  "y": "differences"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_0",
  "x": "Therefore, social media contribute to the misinformation and polarization of society, as we have recently witnessed in the last presidential elections in USA or the Brexit referendum. Clearly, the polarization of society and its underlying discourses are not limited to social media, but rather reflected also in political dynamics (e.g., like those found in the US Congress [1] ): even in this domain, however, social media can provide a useful signal to estimate partisanship [4] . Closely related to the concept of controversy and the \"filter bubble effect\" is the concept of bias [2] , which refers to the presentation of information according to the standpoints or interests of the journalists and the news agencies. Detecting bias is very important to help users to acquire balanced information. Moreover, how a piece of information is reported has the capacity to evoke different sentiments in the audience, which may have large social implications (especially in very controversial topics such as terror attacks and religion issues). In this paper, we approach this very broad topic by focusing on the problem of detecting hyperpartisan news, namely news written with an extreme manipulation of the reality on the basis of an underlying, typically extreme, ideology. This problem has received little attention in the context of the automatic detection of fake news, despite the potential correlation between them. Seminal work from <cite>[5]</cite> presents a comparative style analysis of hyperpartisan news, evaluating features such as characters n-grams, stop words, part-of-speech, readability scores, and ratios of quoted words and external links. <cite>The results</cite> indicate that a topic-based model outperforms a style-based one to separate the left, right and mainstream orientations.",
  "y": "background"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_2",
  "x": "Seminal work from <cite>[5]</cite> presents a comparative style analysis of hyperpartisan news, evaluating features such as characters n-grams, stop words, part-of-speech, readability scores, and ratios of quoted words and external links. <cite>The results</cite> indicate that a topic-based model outperforms a style-based one to separate the left, right and mainstream orientations. We build upon <cite>previous work</cite> and use the dataset from <cite>[5]</cite> : this way we can investigate hyperpartisan-biased news (i.e., extremely one-sided) that have been manually fact-checked by professional journalists from BuzzFeed. The articles originated from 9 well-known political publishers, three each from the mainstream, the hyperpartisan left-wing, and the hyperpartisan right-wing. To detect hyperpartisanship, we apply a masking technique that transforms the original texts in a form where the textual structure is maintained, while letting the learning algorithm focus on the writing style or the topic-related information. This technique makes it possible for us to corroborate previous results that content matters more than style. However, perhaps surprisingly, we are able to achieve the overall best performance by simply using higher-length n-grams than those used in the original work from <cite>[5]</cite> : this seems to indicate a strong lexical overlap between different sources with the same orientation, which, in turn, calls for more challenging datasets and task formulations to encourage the development of models covering more subtle, i.e., implicit, forms of bias. The rest of the paper is structured as follows. In Section 2 we describe our method to hyperpartisan news detection based on masking.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_3",
  "x": "This technique makes it possible for us to corroborate previous results that content matters more than style. However, perhaps surprisingly, we are able to achieve the overall best performance by simply using higher-length n-grams than those used in the original work from <cite>[5]</cite> : this seems to indicate a strong lexical overlap between different sources with the same orientation, which, in turn, calls for more challenging datasets and task formulations to encourage the development of models covering more subtle, i.e., implicit, forms of bias. The rest of the paper is structured as follows. In Section 2 we describe our method to hyperpartisan news detection based on masking. Section 3 presents details on the dataset, experimental results and a discussion of our results. Finally, Section 4 concludes with some directions for future work. ---------------------------------- **INVESTIGATING MASKING FOR HYPERPARTISANSHIP DETECTION** The masking technique that we propose here for the hyperpartisan news detection task has been applied to text clustering [3] , authorship attribution [7] , and recently to deception detection [6] with encouraging results.",
  "y": "extends"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_4",
  "x": "Whatever the case, the way in which we mask the terms in this work is called Distorted View with Single Asterisks and consists in replacing w with a single asterisk or a single # symbol if the term is a word or a number, respectively. For further masking methods, refer to [7] . Table 1 shows a fragment of an original text and the result of masking style-related information or topic-related information. With the former we obtain distorted texts that allow for learning a topic-based model; on the other hand, with the latter, it is possible to learn a style-based model. One of the options to choose the terms to be masked or maintained without masking is to take the most frequent words of the target language [7] . In the original text from the table, we highlight some of the more frequent words in English. ---------------------------------- **EXPERIMENTS** We used the <cite>BuzzedFeed-Webis Fake News Corpus 2016</cite> collected by <cite>[5]</cite> whose articles were labeled with respect to three political orientations: mainstream, left-wing, and right-wing (see Table 2 ).",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_5",
  "x": "We used the <cite>BuzzedFeed-Webis Fake News Corpus 2016</cite> collected by <cite>[5]</cite> whose articles were labeled with respect to three political orientations: mainstream, left-wing, and right-wing (see Table 2 ). Each article was taken from one of 9 publishers known as hyperpartisan left/right or mainstream in a period close to the US presidential elections of 2016. Therefore, the content of all the articles is related to the same topic. During initial data analysis and prototyping we identified a variety of issues with the <cite>original dataset:</cite> we cleaned the data excluding articles with empty or bogus texts, e.g. 'The document has moved here' (23 and 14 articles respectively). Additionally, we removed duplicates (33) and files with the same text but inconsistent labels (2) . As a result, we obtained a new dataset with 1555 articles out of 1627. 4 Following the settings of <cite>[5]</cite> , we balance the training set using random duplicate oversampling. 4 <cite>The dataset</cite> is available at <cite>https://github.com/jjsjunquera/ UnmaskingBiasInNews</cite>. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_6",
  "x": "As a result, we obtained a new dataset with 1555 articles out of 1627. 4 Following the settings of <cite>[5]</cite> , we balance the training set using random duplicate oversampling. 4 <cite>The dataset</cite> is available at <cite>https://github.com/jjsjunquera/ UnmaskingBiasInNews</cite>. ---------------------------------- **MASKING CONTENT VS. STYLE IN HYPERPARTISAN NEWS** In this section, we reported the results of the masking technique from two different perspectives. In one setting, we masked topic-related information in order to maintain the predominant writing style used in each orientation. We call this approach a stylebased model. With that intention we selected the k most frequent words from the target language, and then we transformed the texts by masking the occurrences of the rest of the words.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_7",
  "x": "Therefore, the content of all the articles is related to the same topic. During initial data analysis and prototyping we identified a variety of issues with the <cite>original dataset:</cite> we cleaned the data excluding articles with empty or bogus texts, e.g. 'The document has moved here' (23 and 14 articles respectively). Additionally, we removed duplicates (33) and files with the same text but inconsistent labels (2) . As a result, we obtained a new dataset with 1555 articles out of 1627. 4 Following the settings of <cite>[5]</cite> , we balance the training set using random duplicate oversampling. 4 <cite>The dataset</cite> is available at <cite>https://github.com/jjsjunquera/ UnmaskingBiasInNews</cite>. ---------------------------------- **MASKING CONTENT VS. STYLE IN HYPERPARTISAN NEWS** In this section, we reported the results of the masking technique from two different perspectives.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_8",
  "x": "For this, we masked the k most frequent words and maintained intact the rest. After the text transformation by the masking process in both the training and test sets, we represented the documents with character n-grams and compared the results obtained with the style-based and the topic-related models. Machine (SVM) and Random Forest (RF); for the three classifiers we used the versions implemented in sklearn with the parameters set by default. Evaluation: We performed 3-fold cross-validation with the same configuration used in <cite>[5]</cite> . Therefore, each fold comprised one publisher from each orientation (the classifiers did not learn a publisher's style). We used macro F 1 as the evaluation measure since the test set is unbalanced with respect to the three classes. In order to compare our results with those reported in <cite>[5]</cite> , we also used accuracy, precision, and recall. Baseline: Our baseline method is based on the same text representation with the character n-grams features, but without masking any word. Table 3 shows the results of the proposed method.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_9",
  "x": "We call this a topic-based model. For this, we masked the k most frequent words and maintained intact the rest. After the text transformation by the masking process in both the training and test sets, we represented the documents with character n-grams and compared the results obtained with the style-based and the topic-related models. Machine (SVM) and Random Forest (RF); for the three classifiers we used the versions implemented in sklearn with the parameters set by default. Evaluation: We performed 3-fold cross-validation with the same configuration used in <cite>[5]</cite> . Therefore, each fold comprised one publisher from each orientation (the classifiers did not learn a publisher's style). We used macro F 1 as the evaluation measure since the test set is unbalanced with respect to the three classes. In order to compare our results with those reported in <cite>[5]</cite> , we also used accuracy, precision, and recall. Baseline: Our baseline method is based on the same text representation with the character n-grams features, but without masking any word.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_10",
  "x": "We used macro F 1 as the evaluation measure since the test set is unbalanced with respect to the three classes. In order to compare our results with those reported in <cite>[5]</cite> , we also used accuracy, precision, and recall. Baseline: Our baseline method is based on the same text representation with the character n-grams features, but without masking any word. Table 3 shows the results of the proposed method. We compare with <cite>[5]</cite> against their topic and style-based methods. In order to compare our results with those reported in <cite>[5]</cite> , we report the same measures the <cite>authors</cite> used. We also include the macro F 1 score because of the unbalance test set. For these experiments we extract the character 5-grams from the transformed texts, taking into account that as more narrow is the domain more sense has the use of longer n-grams. We follow the steps of [8] and set k = 500 for this comparison results.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_11",
  "x": "In order to compare our results with those reported in <cite>[5]</cite> , we also used accuracy, precision, and recall. Baseline: Our baseline method is based on the same text representation with the character n-grams features, but without masking any word. Table 3 shows the results of the proposed method. We compare with <cite>[5]</cite> against their topic and style-based methods. In order to compare our results with those reported in <cite>[5]</cite> , we report the same measures the <cite>authors</cite> used. We also include the macro F 1 score because of the unbalance test set. For these experiments we extract the character 5-grams from the transformed texts, taking into account that as more narrow is the domain more sense has the use of longer n-grams. We follow the steps of [8] and set k = 500 for this comparison results. The last two rows show the results obtained by applying the system from <cite>[5]</cite> 6 to our cleaned dataset (Section 3).",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_12",
  "x": "Baseline: Our baseline method is based on the same text representation with the character n-grams features, but without masking any word. Table 3 shows the results of the proposed method. We compare with <cite>[5]</cite> against their topic and style-based methods. In order to compare our results with those reported in <cite>[5]</cite> , we report the same measures the <cite>authors</cite> used. We also include the macro F 1 score because of the unbalance test set. For these experiments we extract the character 5-grams from the transformed texts, taking into account that as more narrow is the domain more sense has the use of longer n-grams. We follow the steps of [8] and set k = 500 for this comparison results. The last two rows show the results obtained by applying the system from <cite>[5]</cite> 6 to our cleaned dataset (Section 3). Similar to <cite>[5]</cite> , the topic-based model achieves better results than the style-related model.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_13",
  "x": "Similar to <cite>[5]</cite> , the topic-based model achieves better results than the style-related model. However, the differences between the results of the two evaluated approaches are much higher (0.66 vs. 0.57 according to Macro F 1 ) than those shown in <cite>[5]</cite> . The highest scores were consistently achieved using the SVM classifier and masking the style-related information (i.e., the topic-related model). This could be due to the fact that all the articles are about the same political event in a very limited period of time. ---------------------------------- **RESULTS AND DISCUSSION** In line with what was already pointed out in <cite>[5]</cite> , the left-wing orientation is harder to predict, possibly because this class is represented with fewer examples in the dataset. Another reason why our masking approach achieves better results could be that we use a higher length of character n-grams. In fact, comparing the results of <cite>[5]</cite> against our baseline model, it is possible to note that even without masking any word, the classifier obtains better results.",
  "y": "similarities"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_14",
  "x": "Similar to <cite>[5]</cite> , the topic-based model achieves better results than the style-related model. However, the differences between the results of the two evaluated approaches are much higher (0.66 vs. 0.57 according to Macro F 1 ) than those shown in <cite>[5]</cite> . The highest scores were consistently achieved using the SVM classifier and masking the style-related information (i.e., the topic-related model). This could be due to the fact that all the articles are about the same political event in a very limited period of time. ---------------------------------- **RESULTS AND DISCUSSION** In line with what was already pointed out in <cite>[5]</cite> , the left-wing orientation is harder to predict, possibly because this class is represented with fewer examples in the dataset. Another reason why our masking approach achieves better results could be that we use a higher length of character n-grams. In fact, comparing the results of <cite>[5]</cite> against our baseline model, it is possible to note that even without masking any word, the classifier obtains better results.",
  "y": "differences"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_15",
  "x": "---------------------------------- **RESULTS AND DISCUSSION** In line with what was already pointed out in <cite>[5]</cite> , the left-wing orientation is harder to predict, possibly because this class is represented with fewer examples in the dataset. Another reason why our masking approach achieves better results could be that we use a higher length of character n-grams. In fact, comparing the results of <cite>[5]</cite> against our baseline model, it is possible to note that even without masking any word, the classifier obtains better results. This suggests that the good results are due to the length of the character n-grams rather than the use of the masking technique. Robustness of the approach to different values of k and n. With the goals of: (i) understanding the robustness of the approach to different parameter values; and to see if (ii) it is possible to overcome the F 1 = 0.70 from the baseline model, we vary the values of k and n and evaluate the macro F 1 using SVM. Figures 1 shows the results of the variation of k \u2208 {100, 200, ..., 5000}. When k > 5000, we clearly can see that the topic-related model, in which the k most frequent terms are masked, is decreasing the performance. This could be explained by the fact that relevant topic-related terms start to be masked too.",
  "y": "similarities"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_16",
  "x": "In fact, comparing the results of <cite>[5]</cite> against our baseline model, it is possible to note that even without masking any word, the classifier obtains better results. This suggests that the good results are due to the length of the character n-grams rather than the use of the masking technique. Robustness of the approach to different values of k and n. With the goals of: (i) understanding the robustness of the approach to different parameter values; and to see if (ii) it is possible to overcome the F 1 = 0.70 from the baseline model, we vary the values of k and n and evaluate the macro F 1 using SVM. Figures 1 shows the results of the variation of k \u2208 {100, 200, ..., 5000}. When k > 5000, we clearly can see that the topic-related model, in which the k most frequent terms are masked, is decreasing the performance. This could be explained by the fact that relevant topic-related terms start to be masked too. However, a different behavior is seen in the style-related model, in which we tried to maintain only the style-related words without masking them. In this model, the higher is k the better is the performance. This confirms that for the used dataset, taking into account only style-related information is not good, and observing also topic-related information benefits the classification. When k tends to the vocabulary size, the style-related model tends to behave like the baseline model, which we already saw in Table 3 that achieves the best results.",
  "y": "differences"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_17",
  "x": "This confirms that for the used dataset, taking into account only style-related information is not good, and observing also topic-related information benefits the classification. When k tends to the vocabulary size, the style-related model tends to behave like the baseline model, which we already saw in Table 3 that achieves the best results. From this experiment, we conclude that: (i) the topic-related model is less sensitive than the style-related model when k < 500, i.e. the k most frequent terms are stylerelated ones; and (ii) when we vary the value of k, both models achieve worse results than our baseline. On the other hand, the results of extracting character 5-grams are higher than extracting smaller n-grams, as can be seen in Figures 2. These results confirm that perhaps the performance of our approach overcomes the models proposed in <cite>[5]</cite> because of the length of the n-grams 7 . Relevant features. Table 4 shows the features with the highest weights from the SVM (we use scikit-learn's method to collect feature weights). It is possible to note that the mention of cnn was learned as a discriminative feature when the news from that publisher were used in the training (in the topic-based model). However, this feature is infrequent in the test set where no news from CNN publisher was included.",
  "y": "differences"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_0",
  "x": "Japanese-ordered English: English: Figure 1: An English sentence re-ordered into Japanese order using the rule-based method of<cite> Isozaki et al. (2010b)</cite> , and its reference Japanese translation. Semi-supervised approaches for NMT are often based on automatically creating pseudoparallel sentences through methods such as backtranslation (Irvine and Callison-Burch, 2013; Sennrich et al., 2016) or adding an auxiliary autoencoding task on monolingual data (Cheng et al., 2016; Currey et al., 2017) . However, both methods have problems with lowresource and syntactically divergent language pairs. Back translation assumes enough data to create a functional NMT system, an unrealistic requirement in low-resource scenarios, while autoencoding target sentences by definition will not be able to learn source-target word reordering. This paper proposes a method to create pseudoparallel sentences for NMT for language pairs with divergent syntactic structures. Prior to NMT, word reordering was a major challenge for statistical machine translation (SMT), and many techniques have emerged over the years to address this challenge (Xia and McCord, 2004; . Importantly, even simple heuristic reordering methods with a few handcreated rules have been shown to be highly effective in closing syntactic gaps (Collins et al. (2005) ;<cite> Isozaki et al. (2010b)</cite> ; Fig. 1 ).",
  "y": "uses"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_1",
  "x": "Prior to NMT, word reordering was a major challenge for statistical machine translation (SMT), and many techniques have emerged over the years to address this challenge (Xia and McCord, 2004; . Importantly, even simple heuristic reordering methods with a few handcreated rules have been shown to be highly effective in closing syntactic gaps (Collins et al. (2005) ;<cite> Isozaki et al. (2010b)</cite> ; Fig. 1 ). Because these rules usually function solely in high-resourced languages such as English with high-quality synguages, but limited success on real low-resource settings and syntactically divergent language pairs (Neubig and Hu, 2018; Guzm\u00e1n et al., 2019) . Hence we focus on semi-supervised methods in this paper. tactic analysis tools, a linguist with rudimentary knowledge of the structure of the target language can create them in short order using these tools. However, similar pre-ordering methods have not proven useful in NMT (Du and Way, 2017) , largely because high-resource scenarios NMT is much more effective at learning reordering than previous SMT methods were (Bentivogli et al., 2016) . However, in low-resource scenarios it is less realistic to expect that NMT could learn this reordering from scratch on its own. Here we ask \"how can we efficiently leverage the monolingual target data to improve the performance of the NMT system in low-resource, syntactically divergent language pairs?\" We tackle this problem via a simple two-step data augmentation method: (1) we first reorder monolingual target sentences to create source-ordered target sentences as shown in Fig. 1 , (2) we then replace the words in the reordered sentences with source words using a bilingual dictionary, and add them as the source side of a pseudo-parallel corpus. Experiments demonstrate the effectiveness of our approach on translation from Japanese and Uyghur to English, with a simple, linguistically motivated method of head finalization (HF;<cite> Isozaki et al. (2010b)</cite> ) as our reordering method.",
  "y": "background"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_2",
  "x": "Because these rules usually function solely in high-resourced languages such as English with high-quality synguages, but limited success on real low-resource settings and syntactically divergent language pairs (Neubig and Hu, 2018; Guzm\u00e1n et al., 2019) . Hence we focus on semi-supervised methods in this paper. tactic analysis tools, a linguist with rudimentary knowledge of the structure of the target language can create them in short order using these tools. However, similar pre-ordering methods have not proven useful in NMT (Du and Way, 2017) , largely because high-resource scenarios NMT is much more effective at learning reordering than previous SMT methods were (Bentivogli et al., 2016) . However, in low-resource scenarios it is less realistic to expect that NMT could learn this reordering from scratch on its own. Here we ask \"how can we efficiently leverage the monolingual target data to improve the performance of the NMT system in low-resource, syntactically divergent language pairs?\" We tackle this problem via a simple two-step data augmentation method: (1) we first reorder monolingual target sentences to create source-ordered target sentences as shown in Fig. 1 , (2) we then replace the words in the reordered sentences with source words using a bilingual dictionary, and add them as the source side of a pseudo-parallel corpus. Experiments demonstrate the effectiveness of our approach on translation from Japanese and Uyghur to English, with a simple, linguistically motivated method of head finalization (HF;<cite> Isozaki et al. (2010b)</cite> ) as our reordering method. ---------------------------------- **THE PROPOSED METHOD**",
  "y": "uses"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_3",
  "x": "Experiments demonstrate the effectiveness of our approach on translation from Japanese and Uyghur to English, with a simple, linguistically motivated method of head finalization (HF;<cite> Isozaki et al. (2010b)</cite> ) as our reordering method. ---------------------------------- **THE PROPOSED METHOD** Training Framework We assume that there are two types of available resources: a small parallel corpus P = {(s, t)} and a large monolingual target corpus Q. The goal of our method is to create a pseudo-parallel corpusQ = {(\u015d, t)}, where\u015d is a pseudo-parallel sentence automatically created in two steps of (1) word reordering, and (2) word-byword translation. Word Reordering The first step reorders monolingual target sentences t \u2208 Q into the source order t s . Instead of devising an entirely new wordordering method, we can simply rely on methods that have already been widely studied and proven useful in SMT . Reordering can be done either using rules based on linguistic knowledge<cite> (Isozaki et al., 2010b</cite>; Collins et al., 2005) or learning from aligned parallel data (Xia and McCord, 2004; Habash, 2007) , and in principle our pseudo-corpus creation paradigm is compatible with any of these methods. Specifically, in this work we utilize rule-based methods, as our goal is to improve translation of low-resource languages, where large quantities of high-quality parallel data do not exist and we posit that current data-driven reordering methods are unlikely to function well. Examples of rule-based methods include those to reorder English into German (Navratil et al., 2012) , Arabic (Badr et al., 2009 ), or Japanese<cite> (Isozaki et al., 2010b)</cite> .",
  "y": "uses"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_4",
  "x": "---------------------------------- **THE PROPOSED METHOD** Training Framework We assume that there are two types of available resources: a small parallel corpus P = {(s, t)} and a large monolingual target corpus Q. The goal of our method is to create a pseudo-parallel corpusQ = {(\u015d, t)}, where\u015d is a pseudo-parallel sentence automatically created in two steps of (1) word reordering, and (2) word-byword translation. Word Reordering The first step reorders monolingual target sentences t \u2208 Q into the source order t s . Instead of devising an entirely new wordordering method, we can simply rely on methods that have already been widely studied and proven useful in SMT . Reordering can be done either using rules based on linguistic knowledge<cite> (Isozaki et al., 2010b</cite>; Collins et al., 2005) or learning from aligned parallel data (Xia and McCord, 2004; Habash, 2007) , and in principle our pseudo-corpus creation paradigm is compatible with any of these methods. Specifically, in this work we utilize rule-based methods, as our goal is to improve translation of low-resource languages, where large quantities of high-quality parallel data do not exist and we posit that current data-driven reordering methods are unlikely to function well. Examples of rule-based methods include those to reorder English into German (Navratil et al., 2012) , Arabic (Badr et al., 2009 ), or Japanese<cite> (Isozaki et al., 2010b)</cite> . In experiments we use<cite> Isozaki et al. (2010b)</cite> 's method of reordering SVO languages (e.g. English) into the order of SOV languages (e.g. Japanese) by simply (1) applying a syntactic parser to English (Tsuruoka et al., 2004) , (2) identifying the head constituent of each phrase and moving it to the end of the phrase, and (3) inserting special tokens after subjects and objects of predicates to mimic Japanese case markers.",
  "y": "background"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_5",
  "x": "Instead of devising an entirely new wordordering method, we can simply rely on methods that have already been widely studied and proven useful in SMT . Reordering can be done either using rules based on linguistic knowledge<cite> (Isozaki et al., 2010b</cite>; Collins et al., 2005) or learning from aligned parallel data (Xia and McCord, 2004; Habash, 2007) , and in principle our pseudo-corpus creation paradigm is compatible with any of these methods. Specifically, in this work we utilize rule-based methods, as our goal is to improve translation of low-resource languages, where large quantities of high-quality parallel data do not exist and we posit that current data-driven reordering methods are unlikely to function well. Examples of rule-based methods include those to reorder English into German (Navratil et al., 2012) , Arabic (Badr et al., 2009 ), or Japanese<cite> (Isozaki et al., 2010b)</cite> . In experiments we use<cite> Isozaki et al. (2010b)</cite> 's method of reordering SVO languages (e.g. English) into the order of SOV languages (e.g. Japanese) by simply (1) applying a syntactic parser to English (Tsuruoka et al., 2004) , (2) identifying the head constituent of each phrase and moving it to the end of the phrase, and (3) inserting special tokens after subjects and objects of predicates to mimic Japanese case markers. Word-by-word Translation To generate data for training MT models, we next perform wordby-word translation of t s into pseudo-source sentence\u015d using a bilingual dictionary (Xie et al., 2018) . 3 There are many ways we can obtain this dictionary: even for many low-resource languages with a paucity of bilingual text, we can obtain manually-curated lexicons with reasonable coverage, or run unsupervised word alignment on whatever parallel data we have available. In addition, we can induce word translations for more words in target language using methods for bilingual lexicon induction over pre-trained word embeddings (e.g. Grave et al. (2018) ). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_6",
  "x": "Japanese and Uyghur are phylogenetically distant languages, but they share similar SOV syntactic structure, which is greatly divergent from English SVO structure. ---------------------------------- **EXPERIMENTAL SETUP** For both language pairs, we use an attentionbased encoder-decoder NMT model with a onelayer bidirectional LSTM as the encoder and onelayer uni-directional LSTM as the decoder. 4 Embeddings and LSTM states were set to 300 and 256 dimensions respectively. Target word embeddings are shared with the softmax weight matrix in the decoder. As noted above, we use HF<cite> (Isozaki et al., 2010b)</cite> as our re-ordering rule. HF was designed for transforming English into Japanese order, but we use it as-is for the Uyghur-English pair as well to demonstrate that simple, linguistically motivated rules can generalize across pairs with similar syntax with little or no modification. Further details regarding the experimental settings are in the supplementary material.",
  "y": "uses"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_0",
  "x": "**ABSTRACT** Machine comprehension of text is the overarching goal of a great deal of research in natural language processing. The Machine Comprehension Test (Richardson et al., 2013) was recently proposed to assess methods on an open-domain, extensible, and easy-to-evaluate task consisting of two datasets. In this paper we develop a lexical matching method that takes into account multiple context windows, question types and coreference resolution. We show that the proposed method outperforms the baseline of <cite>Richardson et al. (2013)</cite> , and despite its relative simplicity, is comparable to recent work using machine learning. We hope that our approach will inform future work on this task. Furthermore, we argue that MC500 is harder than MC160 due to the way question answer pairs were created. ---------------------------------- **INTRODUCTION**",
  "y": "differences"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_1",
  "x": "Machine comprehension of text is the central goal in NLP. The academic community has proposed a variety of tasks, such as information extraction (Sarawagi, 2008) , semantic parsing (Mooney, 2007) and textual entailment (Androutsopoulos and Malakasiotis, 2010) . However, these tasks assess performance on each task individually, rather than on overall progress towards machine comprehension of text. To this end, <cite>Richardson et al. (2013)</cite> proposed the Machine Comprehension Test (MCTest), a new challenge that aims at evaluating machine comprehension. It does so through an opendomain multiple-choice question answering task on fictional stories requiring the common sense reasoning typical of a 7-year-old child. It is easy to evaluate as it consists of multiple choice questions. <cite>Richardson et al. (2013)</cite> also showed how the creation of stories and questions can be crowdsourced efficiently, constructing two datasets for the task, namely MC160 and MC500. In addition, the authors presented a lexical matching baseline which is combined with the textual entailment recognition system BIUTEE (Stern and Dagan, 2011) . In this paper we develop an approach based on lexical matching which we extend by taking into account the type of the question and coreference resolution.",
  "y": "background"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_2",
  "x": "To this end, <cite>Richardson et al. (2013)</cite> proposed the Machine Comprehension Test (MCTest), a new challenge that aims at evaluating machine comprehension. It does so through an opendomain multiple-choice question answering task on fictional stories requiring the common sense reasoning typical of a 7-year-old child. It is easy to evaluate as it consists of multiple choice questions. <cite>Richardson et al. (2013)</cite> also showed how the creation of stories and questions can be crowdsourced efficiently, constructing two datasets for the task, namely MC160 and MC500. In addition, the authors presented a lexical matching baseline which is combined with the textual entailment recognition system BIUTEE (Stern and Dagan, 2011) . In this paper we develop an approach based on lexical matching which we extend by taking into account the type of the question and coreference resolution. These components improve the performance on questions that are difficult to handle with pure lexical matching. When combined with BIUTEE, we achieved 74.27% accuracy on MC160 and 65.96% on MC500, which are significantly better than those reported by <cite>Richardson et al. (2013)</cite> . Despite the simplicity of our approach, these results are comparable with the recent machine learning-based approaches proposed by Narasimhan and Barzilay (2015) , Wang et al. (2015) and Sachan et al. (2015) .",
  "y": "background"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_3",
  "x": "These components improve the performance on questions that are difficult to handle with pure lexical matching. When combined with BIUTEE, we achieved 74.27% accuracy on MC160 and 65.96% on MC500, which are significantly better than those reported by <cite>Richardson et al. (2013)</cite> . Despite the simplicity of our approach, these results are comparable with the recent machine learning-based approaches proposed by Narasimhan and Barzilay (2015) , Wang et al. (2015) and Sachan et al. (2015) . Furthermore, we examine the types of questions and answers in the two datasets. We argue that some types are relatively simple to answer, partly due to the limited vocabulary used, which explains why simple lexical matching methods can perform well. On the other hand, some questions require understanding of higher level concepts such as those of the story and its characters, and/or require inference. This is still beyond the scope of current NLP systems. However, we believe our analysis will be useful in developing new methods and datasets for the task. To that extent, we will make our code and analysis publicly available.",
  "y": "differences"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_4",
  "x": "The zoo worker saw how hairy he was and thought he was a monkey that had escaped from his cage, so they put him in a cage. two datasets contain 160 and 500 stories respectively, with 4 questions per story, and 4 candidate answers per question (Figure 1 ). All stories and questions were crowd-sourced using Amazon Mechanical Turk. 2 MC160 was manually curated by Richardson et al., while MC500 was curated by crowdworkers. Both datasets are divided into training, development, and test sets. All development was conducted on the training and development sets; the test sets were used only to report the final results. 3 Scoring function <cite>Richardson et al. (2013)</cite> proposed a sliding window algorithm that ranks the answers by forming the bag-of-words vector of each answer paired with the question text and then scoring them according to their overlap with the story text. We propose a modified version of this algorithm, which combines the scores across a range of window sizes. More concretely, the algorithm of <cite>Richardson et al. (2013)</cite> passes a sliding window over the story, size of which is equal to the number of words in the question-answer pair.",
  "y": "background"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_5",
  "x": "All development was conducted on the training and development sets; the test sets were used only to report the final results. 3 Scoring function <cite>Richardson et al. (2013)</cite> proposed a sliding window algorithm that ranks the answers by forming the bag-of-words vector of each answer paired with the question text and then scoring them according to their overlap with the story text. We propose a modified version of this algorithm, which combines the scores across a range of window sizes. More concretely, the algorithm of <cite>Richardson et al. (2013)</cite> passes a sliding window over the story, size of which is equal to the number of words in the question-answer pair. The highest overlap score between a story text window and the question-answer pair is taken as the score for the answer. Therefore, their algorithm makes a single pass over the story text per answer. In comparison, our system scores each answer by making multiple passes and summing the obtained scores. Concretely, on the first pass, we set the sliding window size to 2 tokens, and increment this size on each subsequent pass, up to a length of 30 tokens. We then combine this score with the overall number of matches of the question-answer pair across the story as a whole.",
  "y": "background"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_6",
  "x": "More concretely, the algorithm of <cite>Richardson et al. (2013)</cite> passes a sliding window over the story, size of which is equal to the number of words in the question-answer pair. The highest overlap score between a story text window and the question-answer pair is taken as the score for the answer. Therefore, their algorithm makes a single pass over the story text per answer. In comparison, our system scores each answer by making multiple passes and summing the obtained scores. Concretely, on the first pass, we set the sliding window size to 2 tokens, and increment this size on each subsequent pass, up to a length of 30 tokens. We then combine this score with the overall number of matches of the question-answer pair across the story as a whole. This enables our algorithm to catch long-distance relations in the story. Similar to <cite>Richardson et al. (2013)</cite> , we use a linear combination of this score with their distancebased scoring function, and we weigh tokens with their inverse document frequencies in each individual story. By itself, this simple enhancement gives substantial improvements over the MSR baseline as shown in Table 1 (Enhanced SW+D), as it measures the overlap of the question-answer pair with multiple portions of the story text.",
  "y": "similarities uses"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_7",
  "x": "---------------------------------- **RESULTS** We evaluated our system on MC160 and MC500 test sets and the results are shown in Table 2 . Our proposed baseline outperforms the baseline of <cite>Richardson et al. (2013)</cite> by 4 and 3 points in accuracy on MC160 and MC500 respectively. 3 Our system is comparable to the MSR baseline with the RTE system BIUTEE (Stern and Dagan, 2011) . If we linearly combine the RTE scores used in the MSR baseline with our method, we achieve 5 and 2.5 accuracy points higher than the best results achieved by <cite>Richardson et al. (2013)</cite> . Concurrently with ours, three other approaches to solving MCTest were developed and subsequently published a few months before our method. Narasimhan and Barzilay (2015) presented a discourse-level approach, which chooses an answer by utilising relations between sentences chosen as important. Despite is simplicity, our method is comparable in performance, suggesting that better lexical matching could help improve their model.",
  "y": "differences"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_8",
  "x": "---------------------------------- **RESULTS** We evaluated our system on MC160 and MC500 test sets and the results are shown in Table 2 . Our proposed baseline outperforms the baseline of <cite>Richardson et al. (2013)</cite> by 4 and 3 points in accuracy on MC160 and MC500 respectively. 3 Our system is comparable to the MSR baseline with the RTE system BIUTEE (Stern and Dagan, 2011) . If we linearly combine the RTE scores used in the MSR baseline with our method, we achieve 5 and 2.5 accuracy points higher than the best results achieved by <cite>Richardson et al. (2013)</cite> . Concurrently with ours, three other approaches to solving MCTest were developed and subsequently published a few months before our method. Narasimhan and Barzilay (2015) presented a discourse-level approach, which chooses an answer by utilising relations between sentences chosen as important. Despite is simplicity, our method is comparable in performance, suggesting that better lexical matching could help improve their model.",
  "y": "differences"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_9",
  "x": "This is a consequence of the design and curation process of the MC500 dataset, which stipulated that answers must not be contained directly within the story text, or if they are, that two or more misleading choices included. <cite>Richardson et al. (2013)</cite> demonstrate that the MC160 and MC500 have similar ratings for clarity and grammar, and that humans perform equally well on both. However, in many cases MC500 appears to be designed in such a way to confuse lexical algorithms and encourage the use of more sophisticated techniques necessary to deal with phenomena such as elimination questions, negation, and common knowledge not explicitly written in the story. ---------------------------------- **RELATED WORK** The use of shallow methods for machine comprehension has been explored in previous work, for example Hirschman et al. (1999) used a bag-ofwords to match question-answer pairs to sentences in the text, and choose the best pair with the best matching sentence. As discussed in our analysis, such systems cannot handle well questions involving negation and quantification. Numerical questions, which we found to be particularly challenging, have been the focus of recent work on algebra word problems (Kushman et al., 2014) for which dedicated systems have been developed. MacCartney et al. (2006) demonstrated that a large set of rules can be used to recognize valid textual entailments.",
  "y": "background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_0",
  "x": "****EXTRACTING HIERARCHICAL RULES FROM A WEIGHTED ALIGNMENT MATRIX**** **ABSTRACT** Word alignment is a fundamental step in machine translation. Current statistical machine translation systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. To alleviate this problem, we extract hierarchical rules from weighted alignment matrix<cite> (Liu et al., 2009)</cite> . Since the sub-phrase pairs would change the inside and outside areas in the weighted alignment matrix of the hierarchical rules, we propose a new algorithm to calculate the relative frequencies and lexical weights of hierarchical rules. To achieve a balance between rule table size and performance, we construct a scoring measure that incorporates both frequency and lexical weight to select the best target phrase for each source phrase. Experiments show that our approach improves BLEU score by ranging from 1.4 to 1.9 points over baseline for hierarchical phrase-based, and 1.4 to 1.5 points for tree-to-string model. ----------------------------------",
  "y": "motivation background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_1",
  "x": "Figure 1 (a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X 1 jingji, China X 1 economy).To alleviate this problem, a natural solution is to extract rules from nbest alignments (Venugopal et al., 2008) . However, using n-best alignments still face two major challenges. First, n-best alignments have to be processed individually although they share many links, see (zhongguo, China) and (jingji, economy) in Figure 1 . Second, regardless of probabilities of links in each alignment, numerous wrong rule would be extracted from n-best alignments. For example, a wrong rule (X 1 de jingji, of X 1 's economy) would be extracted from the alignment in Figure 1 (a). Since<cite> Liu et al. (2009)</cite> show that weighted alignment matrix provides an elegant solution to these two drawbacks, we apply it to the hierarchical phrase-based model (Chiang, 2005) and the tree-to-string model Huang et al., 2006) . While such an idea seems intuitive, it is non-trivial to extract hierarchical rules from weighted alignment matrices. Our work faces two major challenges.",
  "y": "uses"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_2",
  "x": "The sub-phrase pairs that are replaced with NTs in a rule, would change the inside and outside areas in the weighted alignment matrix of the rule. In addition, the sub-phrase pairs have their own probabilities and we should incorporate them to better estimate the probabilities of the hierarchical rules. Therefore, the calculations of relative frequencies and lexical weights for hierarchical rules are more complicated. Another challenge is how to achieve a balance between performance and rule table size. Note that given a source phrase, there would be plenty of \"potential\" candidate target phrases in weighted matrices<cite> (Liu et al., 2009</cite> ). If we retain all of them, these phrase pairs would produce even more hierarchical rules. For computational tractability, we need to design a measure to score the phrase pairs and wipe out the low-quality ones. We propose a new algorithm to calculate the relative frequencies of rules, and construct a measure that incorporates both frequency and lexical weight to score target phrases. Experiments (Section 4) show that our approach improves BLEU score by ranging from 1.4 to 1.9 points over baseline for hierarchical phrase-based, and 1.4 to 1.8 points for tree-to-string model.",
  "y": "motivation differences background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_3",
  "x": "We propose a new algorithm to calculate the relative frequencies of rules, and construct a measure that incorporates both frequency and lexical weight to score target phrases. Experiments (Section 4) show that our approach improves BLEU score by ranging from 1.4 to 1.9 points over baseline for hierarchical phrase-based, and 1.4 to 1.8 points for tree-to-string model. ---------------------------------- **WEIGHTED ALIGNMENT MATRIX** A weighted alignment matrix<cite> (Liu et al., 2009)</cite> m is a J \u00d7 I matrix to encode the probabilities of n-best alignments of the same sentence pair. Each element in the matrix stores a link probability p m (j, i), which is estimated from an n-best list by calculating relative frequencies: where Here N is an n-best list, p(a) is the probability of an alignment a in the n-best list. The numbers in the cells in Figure 2 (c) are the corresponding p m .",
  "y": "background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_4",
  "x": "The key point to calculate the relative frequency of the phrase pair is to obtain its fractional count. Liu et al. (2009) use the product of inside and outside probabilities as the fractional count of a phrase pair. Liu et al. (2009) define that inside probability indicates the probability that at least one word in source phrase is aligned to a word in target phrase, and outside probability indicates the chance that no words in one phrase are aligned to a word outside the other phrase. The fractional count is calculated: where \u03b1(\u00b7) and \u03b2(\u00b7) denote the inside and outside probabilities respectively, which can be calculated as Here in(\u00b7) denotes the inside area, which includes elements that fall inside the phrase pair, while out(\u00b7) denotes the outside area including elements that fall outside the phrase pair while fall in the same row or the same column. Figure 3 shows an example. The light shading area is the outside area of phrase pair and the area inside the pane with bold lines is the inside area. To calculate the lexical weights,<cite> Liu et al. (2009)</cite> adapt p m (j, i) as the fractional count count(f j , e i ).",
  "y": "background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_5",
  "x": "where \u03b1(\u00b7) and \u03b2(\u00b7) denote the inside and outside probabilities respectively, which can be calculated as Here in(\u00b7) denotes the inside area, which includes elements that fall inside the phrase pair, while out(\u00b7) denotes the outside area including elements that fall outside the phrase pair while fall in the same row or the same column. Figure 3 shows an example. The light shading area is the outside area of phrase pair and the area inside the pane with bold lines is the inside area. To calculate the lexical weights,<cite> Liu et al. (2009)</cite> adapt p m (j, i) as the fractional count count(f j , e i ). The fractional counts of NULL words can be calculated as: Then the lexical weight can be calculated as: where We apply weighted alignment matrix to the hierarchical phrase-based model (Chiang, 2007) and the tree-to-string model Huang et al., 2006) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_6",
  "x": "In hierarchical rules, both source and target sides are strings with NTs. In tree-to-string rules, the source side is a tree with NTs, while the target side is a string with NTs. Since the tree structure of source side has no effect on the calculations of relative frequencies and lexical weights, we can represent both tree-to-string and hierarchical rules as below: where X is a nonterminal, \u03b3 and \u03b1 are source and target strings (consist of terminals and NTs), and \u223c represents word alignments between NTs in \u03b3 and \u03b1. The bulk of syntax grammars consists of two parts: phrase pairs and variable rules. The difference between them is containing NTs or not. Since we can calculate relative frequencies and lexical weights of phrase pairs as in<cite> Liu et al. (2009)</cite> , we only focus on the calculation of variable rules. ---------------------------------- **EXTRACTION ALGORITHM**",
  "y": "uses background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_7",
  "x": "Figure 5 shows an example of the matrix of a hierarchical rule, which is generated from the phrase pair in Figure 3 . Due to the existence of subphrase pairs, the inside and outside areas changes (see the difference between Figure 3 and Figure 5 ). Therefore, we can not simply calculate the outside probability of the hierarchical rule using the product of outside probabilities of phrase pair and subphrase pairs. ---------------------------------- **CALCULATING RELATIVE FREQUENCIES** We follow<cite> Liu et al. (2009)</cite> to calculate relative frequencies using the product of inside and outside probabilities. We now extend the definitions of inside and outside probabilities to hierarchical rules that contain NTs. Table 2 : Some hierarchical rules generated from the phrase pair (zhongguo de jingji, China's economy) in Figure 3 (suppose the structure of zhongguo de jingji is a complete sub-tree). Here \u03b1 is inside probability, \u03b2 is outside probability, and count is fractional count.",
  "y": "uses"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_8",
  "x": "Accordingly, the outside probability is calculated as: where For example, the inside probability of (X 1 de jingji, X 1 's economy) in Figure 5 is 1.0, and its outside probability is 0.4. We also use Equation 5 to calculate the fractional counts of hierarchical rules. We follow<cite> Liu et al. (2009)</cite> to prune rule table using a threshold of frequency. Table 2 lists some hierarchical rules generated from the phrase pair (zhongguo de jingji, China's economy) in Figure 3 . If the threshold is 0.2, we retain all the rules in Table 2 . ---------------------------------- **CALCULATING LEXICAL WEIGHTS**",
  "y": "uses"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_9",
  "x": "---------------------------------- **EXPERIMENTS** ---------------------------------- **DATA PREPARATION** Our experiments are on Chinese-English translation based on replications of hierarchical phrasebased system (Chiang, 2007) and tree-to-string system . We train a 4-gram language model on the Xinhua portion of GIGA-WORD corpus using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995 To obtain weighted alignment matrices, we follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 20-best lists in two translation directions, then used \"grow-diag-finaland\" (Koehn et al., 2003) to all 20 \u00d7 20 bidirectional alignment pairs. We follow<cite> Liu et al. (2009)</cite> to use p s2t \u00d7 p t2s as the probabilities of an alignment pair. Analogously, we abandon duplicate alignments that are produced from different alignment pairs.",
  "y": "uses"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_0",
  "x": "It is the fundamental step to many natural language processing applications, like Information Extraction (IE), Information Retrieval (IR) and Question Answering (QA). Most empirical approaches currently employed in NER task make decision only on local context for extract inference, which is based on the data independent assumption<cite> (Krishnan and Manning, 2006)</cite> . But often this assumption does not hold because non-local dependencies are prevalent in natural language (including the NER task). How to utilize the non-local dependencies effectively is a key issue in NER task. Unfortunately, few researches have been devoted to this issue, existing works mainly focus on using the non-local information for further improving NER label consistency. There are two methods to use non-local information. One is to add additional edges to graphical model structure to represent the distant dependencies and the other is to encode the non-locality with non-local features. However, in the first approach, heuristic rules are used to find the dependencies (Bunescu and Mooney, 2004; Sutton and McCallum, 2004) or penalties for label inconsistency are required to handset ad-hoc (Finkel et al., 2005) . Furthermore, high computational cost is spent for approximate inference.",
  "y": "motivation background"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_1",
  "x": "Furthermore, high computational cost is spent for approximate inference. In order to establish the long dependencies easily and overcome the disadvantage of the approximate inference,<cite> Krishnan and Manning (2006)</cite> propose a two-stage approach using Conditional Random Fields (CRFs) with extract inference. They represent the non-locality with non-local features, and extract the nonlocal features from the output of the first stage CRF using local context alone; then they incorporate the non-local features into the second CRF. But the features in this approach are only used to improve label consistency. To our best knowledge, up to now, non-local information has not been explored to improve NER recall in previous researches; on the other hand, NER is always impaired by its lower recall due to the imbalanced distribution where the NONE class dominates the entity classes. Classifiers built on such data typically have a higher precision and a lower recall and tend to overproduce the NONE class (Kambhatla, 2006) . In this paper, we employ non-local information to recall the missed entities. Similar to<cite> Krishnan and Manning (2006)</cite> , we also encode non-local information with features and apply the simple two-stage architecture. Different from their work for improve label consistency, their features are activated on the recognized entities coming from the first CRF, the non-local features we design are used to recall more missed entities which are seen in the training data or unseen entities but some of their occurrences being recognized correctly in the first stage, our features are fired on the raw token sequence directly with forward maximum match. Compared to their non-local information extracted from training data with 10-fold cross-validation, our non-local information is extracted from the training date directly; our approach obtaining the non-local features is simpler.",
  "y": "motivation background"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_2",
  "x": "In this paper, we employ non-local information to recall the missed entities. Similar to<cite> Krishnan and Manning (2006)</cite> , we also encode non-local information with features and apply the simple two-stage architecture. Different from their work for improve label consistency, their features are activated on the recognized entities coming from the first CRF, the non-local features we design are used to recall more missed entities which are seen in the training data or unseen entities but some of their occurrences being recognized correctly in the first stage, our features are fired on the raw token sequence directly with forward maximum match. Compared to their non-local information extracted from training data with 10-fold cross-validation, our non-local information is extracted from the training date directly; our approach obtaining the non-local features is simpler. Moreover, we design different non-local features encoding different useful information for NER two subtasks: entity boundary detection and entity semantic classification. Our features are also inspired by Wong and Ng (2007) . They extract entity majority type features from unlabelled data with an initial maximum entropy classifier. Our approach is validated on the third International Chinese language processing bakeoff (SIGHAN 2006) MSRA and CityU NER closed track, the experimental results show that non-local features can significantly improve the recall of the state-of-the-art NER system using local context alone. The remainder of the paper is structured as follows.",
  "y": "similarities differences"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_3",
  "x": "They extract entity majority type features from unlabelled data with an initial maximum entropy classifier. Our approach is validated on the third International Chinese language processing bakeoff (SIGHAN 2006) MSRA and CityU NER closed track, the experimental results show that non-local features can significantly improve the recall of the state-of-the-art NER system using local context alone. The remainder of the paper is structured as follows. In Section 2, we introduce the first stage CRF with local features alone; then we describe the second stage CRF using non-local features we design in Section 3. We demonstrate the experiments in Section 4 and we conclude the paper in Section 5. ---------------------------------- **OUR BASELINE NER SYSTEM** To validate the effectiveness of our approach of exploiting non-local features, we need to establish a baseline with state-of-the-art performance using local context alone. Similar to<cite> (Krishnan and Manning, 2006)</cite> , we employ two-stage architecture under conditional random fields (CRFs) framework.",
  "y": "similarities"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_4",
  "x": "These features enable us to capture the dependencies between the identical entities and their classes, so that the same candidate entities of different occurrences can be recalled favorably, and their label consistencies can be considered too. Token-position & entity-majority features (F4): These features capture non-local information from F2 and F3 simultaneously. They take into account the entity boundary and semantic class information at the same time. These non-local features are applied in English NER in one-step approach<cite> (Krishnan and Manning, 2006</cite>; Wong and Ng, 2007) , they employ these features to improve entity consistence among their different occurrences. These features are assigned to token sequences that are matched exactly with the (entity, majority-type) list in forward maximum matching (FMM) way. During training or testing, when the CRFs tagger encounters a token sequence C 1 ...C n such that (C k ...C s ) (k\u22651, s \u2264n) is the longest token sequence existing in the entity list; the correspondent features will be turned on to each token in C k ....C s . For example, considering the following ---------------------------------- **SENTENCE: \u6211(WO)\u7231 (AI)\u5317(BEI)\u4eac(JING)\u5929(TIAN)\u5b89(AN)\u95e8 (MEN)(I LOVE BEIJING TIANANMEN). IF (\u5317 \u4eac, MAJ-LOC), (\u4eac, MAJ-LOC), AND (\u5929\u5b89\u95e8, MAJ-LOC) ARE PRESENTED IN THE (ENTITY, MAJORITY** type) list, the features below will be turned on as table 1 shows.",
  "y": "background"
 },
 {
  "id": "5ed24e18f892d7092c183acab4b175_0",
  "x": "Semantic vector space models of language were developed in the 90s to predict joint probabilities of words that appear together in a sequence. A particular upturn was proposed by Bengio et al. in [1] , replacing sparse n-gram models with word embeddings which are more compact representations obtained using feed-forward or more advanced neural networks. Recently, high quality and easy to train Skip-gram shallow architectures were presented in <cite>[10]</cite> and considerably improved in [11] with the introduction of negative sampling and subsampling of frequent words. The \"magical\" ability of word embeddings to capture syntactic and semantic regularities on text words is applicable in various applications like machine translations, error correcting systems, sentiment analyzers etc. This ability has been tested in [12] and other studies with analogy question tests of the form \"A is to B as C is to \" or male/female relations. A recent improved method for generating word embeddings is Glove [15] which makes efficient use of global statistics of text words and preserves the linear substructure of Skip-gram word2vec, the other popular method. Authors report that Glove outperforms other methods such as Skip-gram in several tasks like word similarity, word analogy etc. In this paper we examine the quality of word embeddings on 2 sentiment analysis tasks: Lyrics mood recognition and movie review polarity analysis. We compare various models pretrained with Glove and Skip-gram, together with corpora we train ourself.",
  "y": "background"
 },
 {
  "id": "5ed24e18f892d7092c183acab4b175_1",
  "x": "In this section we present the different word embedding models that we compare. Most of them are pretrained and publicly available. Two of them (Text8Corpus and Moody-Corpus) were trained by us. The full list with some basic characteristics is presented in Table 1 . Wikipedia Gigaword is a combination of Wikipedia 2014 dump and Gigaword 5 with about 6 billion tokens in total. It was created by authors of [15] to evaluate Glove performance. Wikipedia Dependency corpus is a collection of 1 billion tokens from Wikipedia. The method used for training it is a modified version of Skip-gram word2vec described in [7] . Google News is one of the biggest and richest text sets with 100 billion tokens and a vocabulary of 3 million words and phrases <cite>[10]</cite> .",
  "y": "background"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_0",
  "x": "****SEMANTIC ENTITY RETRIEVAL TOOLKIT**** **ABSTRACT** Unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a ention. In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of <cite>our previously published entity representation models</cite>. e toolkit provides a uni ed interface to di erent representation learning algorithms, ne-grained parsing con guration and can be used transparently with GPUs. In addition, users can easily modify existing models or implement their own models in the framework. A er model training, SERT can be used to rank entities according to a textual query and extract the learned entity/word representation for use in downstream algorithms, such as clustering or recommendation. ---------------------------------- **INTRODUCTION**",
  "y": "motivation extends"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_1",
  "x": "---------------------------------- **INTRODUCTION** e unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a ention for the entity-oriented tasks of expert nding [9] and product search [<cite>8</cite>] . Representations are learned from a document collection and domain-speci c associations between documents and entities. Expert nding is the task of nding the right person with the appropriate skills or knowledge [1] and an association indicates document authorship (e.g., academic papers) or involvement in a project (e.g., annual progress reports). In the case of product search, an associated document is a product description or review [<cite>8</cite>] . In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models [<cite>8</cite>, 9] . Beyond a uni ed interface that combines di erent models, the toolkit allows for ne-grained parsing con guration and GPU-based training through integration with eano [3, 6] . Users can easily extend existing models or implement their own models within the uni ed framework.",
  "y": "background motivation"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_2",
  "x": "Expert nding is the task of nding the right person with the appropriate skills or knowledge [1] and an association indicates document authorship (e.g., academic papers) or involvement in a project (e.g., annual progress reports). In the case of product search, an associated document is a product description or review [<cite>8</cite>] . In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models [<cite>8</cite>, 9] . Beyond a uni ed interface that combines di erent models, the toolkit allows for ne-grained parsing con guration and GPU-based training through integration with eano [3, 6] . Users can easily extend existing models or implement their own models within the uni ed framework. A er model training, SERT can compute matching scores between an entity and a piece of text (e.g., a query). is matching score can then be used for ranking entities, or as a feature in a downstream machine learning system, such as the learning to rank component * e toolkit is licensed under the permissive MIT open-source license and can be found at h ps://github.com/cvangysel/SERT. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored.",
  "y": "background"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_3",
  "x": "In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models [<cite>8</cite>, 9] . Beyond a uni ed interface that combines di erent models, the toolkit allows for ne-grained parsing con guration and GPU-based training through integration with eano [3, 6] . Users can easily extend existing models or implement their own models within the uni ed framework. A er model training, SERT can compute matching scores between an entity and a piece of text (e.g., a query). is matching score can then be used for ranking entities, or as a feature in a downstream machine learning system, such as the learning to rank component * e toolkit is licensed under the permissive MIT open-source license and can be found at h ps://github.com/cvangysel/SERT. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). e collection is parsed, processed and packaged in a numerical format using the prepare ( \u00a72.1) utility.",
  "y": "extends"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_4",
  "x": "An alternative option that exists within the toolkit is to resample word sequence/entity pairs such that every entity is associated with the same number of word sequences, as used for product search [<cite>8</cite>] . Code snippet 1: Illustrative example of the SERT model interface. e full interface supports more functionality omitted here for brevity. Users can de ne a symbolic graph of computation using the eano library [6] in combination with Lasagne [3] . ---------------------------------- **REPRESENTATION LEARNING** A er the collection has been processed and packaged in a machinefriendly format, representations of words and entities can be learned. e toolkit includes implementations of state-of-the-art representation learning models that were applied to expert nding [9] and product search [<cite>8</cite>] . Users of the toolkit can use these implementations to learn representations out-of-the-box or adapt the algorithms to their needs.",
  "y": "similarities extends"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_5",
  "x": "Code snippet 1: Illustrative example of the SERT model interface. e full interface supports more functionality omitted here for brevity. Users can de ne a symbolic graph of computation using the eano library [6] in combination with Lasagne [3] . ---------------------------------- **REPRESENTATION LEARNING** A er the collection has been processed and packaged in a machinefriendly format, representations of words and entities can be learned. e toolkit includes implementations of state-of-the-art representation learning models that were applied to expert nding [9] and product search [<cite>8</cite>] . Users of the toolkit can use these implementations to learn representations out-of-the-box or adapt the algorithms to their needs. In addition, users can implement their own models by extending an interface provided by the framework.",
  "y": "extends background"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_7",
  "x": "**ENTITY RANKING & OTHER USES OF THE REPRESENTATIONS** Once a model has been trained, SERT can be used to rank entities w.r.t. a textual query. e concrete implementation used to rank entities depends on the model that was trained. In the most generic case, a matching score is computed for every entity and entities are ranked in decreasing order of his score. However, in the special case when the model is interpreted as a metric vector space [2, <cite>8</cite>] , SERT casts entity ranking as a k-nearest neighbor problem and uses specialized data structures for retrieval [5] . A er ranking, SERT outputs the entity rankings as a TREC-compatible le that can be used as input to the trec eval 1 evaluation utility. In this paper we described the Semantic Entity Retrieval Toolkit, a toolkit that learns latent representations of words and entities. e toolkit contains implementations of state-of-the-art entity representations algorithms [<cite>8</cite>, 9] and consists of three components: text processing, representation learning and inference.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_0",
  "x": "Abstract. Generating descriptions for videos has many applications including assisting blind people and human-robot interaction. The recent advances in image captioning as well as the release of large-scale movie description datasets such as <cite>MPII-MD</cite> <cite>[28]</cite> allow to study this task in more depth. Many of the proposed methods for image captioning rely on pre-trained object classifier CNNs and Long-Short Term Memory recurrent networks (LSTMs) for generating descriptions. While image description focuses on objects, we argue that it is important to distinguish verbs, objects, and places in the challenging setting of movie description. In this work we show how to learn robust visual classifiers from the weak annotations of the sentence descriptions. Based on these visual classifiers we learn how to generate a description using an LSTM. We explore different design choices to build and train the LSTM and achieve the best performance to date on the challenging <cite>MPII-MD dataset</cite>. We compare and analyze our approach and prior work along various dimensions to better understand the key challenges of the movie description task.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_2",
  "x": "Multiple works have successfully addressed the image captioning problem [6, 16, 17, 35] . Many of the proposed methods rely on Long-Short Term Memory networks (LSTMs) [13] . In the meanwhile, two large-scale <cite>movie description datasets</cite> have been proposed, namely <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . <cite>Both</cite> are based on movies with associated textual descriptions and allow studying the problem how to generate movie description for visually disabled people. Works addressing these datasets <cite>[28,</cite> 33, 39] show that <cite>they</cite> are indeed challenging in terms of visual recognition and automatic description. This results in a significantly lower performance then on simpler video datasets (e.g. MSVD [2] ), but a detailed analysis of the difficulties is missing. In this work we address this by taking a closer look at the performance of existing methods on the movie description task. This work contributes a) an approach to build robust visual classifiers which distinguish verbs, objects, and places extracted from weak sentence annotations; b) based on the visual classifiers we evaluate different design choices to train an LSTM for generating descriptions. This outperforms <cite>related work</cite> on the <cite>MPII-MD dataset</cite>, both using automatic and human evaluation; c) we perform a detailed analysis of prior work and our approach to understand the challenges of the movie description task.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_3",
  "x": "Multiple works have successfully addressed the image captioning problem [6, 16, 17, 35] . Many of the proposed methods rely on Long-Short Term Memory networks (LSTMs) [13] . In the meanwhile, two large-scale <cite>movie description datasets</cite> have been proposed, namely <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . <cite>Both</cite> are based on movies with associated textual descriptions and allow studying the problem how to generate movie description for visually disabled people. Works addressing these datasets <cite>[28,</cite> 33, 39] show that <cite>they</cite> are indeed challenging in terms of visual recognition and automatic description. This results in a significantly lower performance then on simpler video datasets (e.g. MSVD [2] ), but a detailed analysis of the difficulties is missing. In this work we address this by taking a closer look at the performance of existing methods on the movie description task. This work contributes a) an approach to build robust visual classifiers which distinguish verbs, objects, and places extracted from weak sentence annotations; b) based on the visual classifiers we evaluate different design choices to train an LSTM for generating descriptions. This outperforms <cite>related work</cite> on the <cite>MPII-MD dataset</cite>, both using automatic and human evaluation; c) we perform a detailed analysis of prior work and our approach to understand the challenges of the movie description task.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_4",
  "x": "Many of the proposed methods rely on Long-Short Term Memory networks (LSTMs) [13] . In the meanwhile, two large-scale <cite>movie description datasets</cite> have been proposed, namely <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . <cite>Both</cite> are based on movies with associated textual descriptions and allow studying the problem how to generate movie description for visually disabled people. Works addressing these datasets <cite>[28,</cite> 33, 39] show that <cite>they</cite> are indeed challenging in terms of visual recognition and automatic description. This results in a significantly lower performance then on simpler video datasets (e.g. MSVD [2] ), but a detailed analysis of the difficulties is missing. In this work we address this by taking a closer look at the performance of existing methods on the movie description task. This work contributes a) an approach to build robust visual classifiers which distinguish verbs, objects, and places extracted from weak sentence annotations; b) based on the visual classifiers we evaluate different design choices to train an LSTM for generating descriptions. This outperforms <cite>related work</cite> on the <cite>MPII-MD dataset</cite>, both using automatic and human evaluation; c) we perform a detailed analysis of prior work and our approach to understand the challenges of the movie description task. ----------------------------------",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_5",
  "x": "<cite>Both</cite> are based on movies with associated textual descriptions and allow studying the problem how to generate movie description for visually disabled people. Works addressing these datasets <cite>[28,</cite> 33, 39] show that <cite>they</cite> are indeed challenging in terms of visual recognition and automatic description. This results in a significantly lower performance then on simpler video datasets (e.g. MSVD [2] ), but a detailed analysis of the difficulties is missing. In this work we address this by taking a closer look at the performance of existing methods on the movie description task. This work contributes a) an approach to build robust visual classifiers which distinguish verbs, objects, and places extracted from weak sentence annotations; b) based on the visual classifiers we evaluate different design choices to train an LSTM for generating descriptions. This outperforms <cite>related work</cite> on the <cite>MPII-MD dataset</cite>, both using automatic and human evaluation; c) we perform a detailed analysis of prior work and our approach to understand the challenges of the movie description task. ---------------------------------- **RELATED WORK** Image captioning.",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_6",
  "x": "[38] jointly addresses the language generation and video/language retrieval tasks by learning a joint embedding model for a deep video model and compositional semantic language model. ---------------------------------- **MOVIE DESCRIPTION.** Recently two large-scale movie description datasets have been proposed, <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . Given that <cite>they</cite> are based on movies, <cite>they</cite> cover a much broader domain then previous video description datasets. Consequently <cite>they</cite> are much more varied and challenging with respect to the visual content and the associated description. <cite>They</cite> also do not have any additional annotations, as e.g. TACoS Multi-Level [27] , thus one has to rely on the weak annotations of the sentence descriptions. To handle this challenging scenario [39] proposes an attention based model which selects the most relevant temporal segments in a video and incorporates 3-D CNN and generates a sentence using an LSTM. [33] proposes an encoder-decoder framework, where a single LSTM encodes the input video frame by frame and decodes it into a sentence, outperforming [39] .",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_7",
  "x": "**MOVIE DESCRIPTION.** Recently two large-scale movie description datasets have been proposed, <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . Given that <cite>they</cite> are based on movies, <cite>they</cite> cover a much broader domain then previous video description datasets. Consequently <cite>they</cite> are much more varied and challenging with respect to the visual content and the associated description. <cite>They</cite> also do not have any additional annotations, as e.g. TACoS Multi-Level [27] , thus one has to rely on the weak annotations of the sentence descriptions. To handle this challenging scenario [39] proposes an attention based model which selects the most relevant temporal segments in a video and incorporates 3-D CNN and generates a sentence using an LSTM. [33] proposes an encoder-decoder framework, where a single LSTM encodes the input video frame by frame and decodes it into a sentence, outperforming [39] . Our approach for sentence generation is most similar to [6] and we also rely on their LSTM implementation based on Caffe [15] . However, we analyze different aspects and variants of this architecture for movie description.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_8",
  "x": "**MOVIE DESCRIPTION.** Recently two large-scale movie description datasets have been proposed, <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . Given that <cite>they</cite> are based on movies, <cite>they</cite> cover a much broader domain then previous video description datasets. Consequently <cite>they</cite> are much more varied and challenging with respect to the visual content and the associated description. <cite>They</cite> also do not have any additional annotations, as e.g. TACoS Multi-Level [27] , thus one has to rely on the weak annotations of the sentence descriptions. To handle this challenging scenario [39] proposes an attention based model which selects the most relevant temporal segments in a video and incorporates 3-D CNN and generates a sentence using an LSTM. [33] proposes an encoder-decoder framework, where a single LSTM encodes the input video frame by frame and decodes it into a sentence, outperforming [39] . Our approach for sentence generation is most similar to [6] and we also rely on their LSTM implementation based on Caffe [15] . However, we analyze different aspects and variants of this architecture for movie description.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_9",
  "x": "They show the benefit of pre-training the LSTM network for image captioning and fine-tuning it to video description. [25] proposes a framework that consists of a 2-D and/or 3-D CNN and the LSTM is trained jointly with a visual-semantic embedding to ensure better coherence between video and text. [38] jointly addresses the language generation and video/language retrieval tasks by learning a joint embedding model for a deep video model and compositional semantic language model. ---------------------------------- **MOVIE DESCRIPTION.** Recently two large-scale movie description datasets have been proposed, <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . Given that <cite>they</cite> are based on movies, <cite>they</cite> cover a much broader domain then previous video description datasets. Consequently <cite>they</cite> are much more varied and challenging with respect to the visual content and the associated description. <cite>They</cite> also do not have any additional annotations, as e.g. TACoS Multi-Level [27] , thus one has to rely on the weak annotations of the sentence descriptions.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_10",
  "x": "To extract labels from sentences we rely on the semantic parser of <cite>[28]</cite> , however we treat the labels differently to handle the weak supervision (see Section 3.1). We show that this improves over <cite>[28]</cite> and [33] . Fig. 1 : Overview of our approach. We first train the visual classifiers for verbs, objects and places, using different visual features: DT (dense trajectories [36] ), LSDA (large scale object detector [14] ) and PLACES (Places-CNN [41] ). Next, we concatenate the scores from a subset of selected robust classifiers and use them as input to our LSTM. ---------------------------------- **APPROACH** In this section we present our two-step approach to video description. The first step performs visual recognition, while the second step generates textual descriptions.",
  "y": "uses differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_11",
  "x": "Given that <cite>they</cite> are based on movies, <cite>they</cite> cover a much broader domain then previous video description datasets. Consequently <cite>they</cite> are much more varied and challenging with respect to the visual content and the associated description. <cite>They</cite> also do not have any additional annotations, as e.g. TACoS Multi-Level [27] , thus one has to rely on the weak annotations of the sentence descriptions. To handle this challenging scenario [39] proposes an attention based model which selects the most relevant temporal segments in a video and incorporates 3-D CNN and generates a sentence using an LSTM. [33] proposes an encoder-decoder framework, where a single LSTM encodes the input video frame by frame and decodes it into a sentence, outperforming [39] . Our approach for sentence generation is most similar to [6] and we also rely on their LSTM implementation based on Caffe [15] . However, we analyze different aspects and variants of this architecture for movie description. To extract labels from sentences we rely on the semantic parser of <cite>[28]</cite> , however we treat the labels differently to handle the weak supervision (see Section 3.1). We show that this improves over <cite>[28]</cite> and [33] .",
  "y": "uses differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_12",
  "x": "The first step performs visual recognition, while the second step generates textual descriptions. For the visual recognition we propose to use the visual classifiers trained according to the labels' semantics and \"visuality\". For the language generation we rely on a LSTM network which has been successfully used for image and video description [6, 33] . We discuss various design choices for building and training the LSTM. An overview of our approach is given in Figure 1 . ---------------------------------- **VISUAL LABELS FOR ROBUST VISUAL CLASSIFIERS** For training we rely on a parallel corpus of videos and weak sentence annotations. As in <cite>[28]</cite> we parse the sentences to obtain a set of labels (single words or short phrases, e.g. look up) to train our visual classifiers.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_13",
  "x": "As in <cite>[28]</cite> we parse the sentences to obtain a set of labels (single words or short phrases, e.g. look up) to train our visual classifiers. However, in contrast to <cite>[28]</cite> we do not want to keep all of these initial labels as they are noisy, but select only visual ones which actually can be robustly recognized. Avoiding parser failure. Not all sentences can be parsed successfully, as e.g. some sentences are incomplete or grammatically incorrect. To avoid loosing the potential labels in these sentences, we match our set of initial labels to the sentences which the parser failed to process. Semantic groups. Our labels correspond to different semantic groups. In this work we consider three most important groups: verbs (actions), objects and places, as they are typically visual. One could also consider e.g. groups like mood or emotions, which are naturally harder for visual recognition.",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_14",
  "x": "Second, we train one-vs-all SVM classifiers for each group separately. The intuition behind this is to discard \"wrong negatives\" (e.g. using object \"bed\" as negative for place \"bedroom\"). Visual labels. Now, how do we select visual labels for our semantic groups? In order to find the verbs among the labels we rely on the semantic parser of <cite>[28]</cite> . Next, we look up the list of \"places\" used in [41] and search for corresponding words among our labels. We look up the object classes used in [14] and search for these \"objects\", as well as their base forms (e.g. \"domestic cat\" and \"cat\"). We discard all the labels that do not belong to any of our three groups of interest as we assume that they are likely not visual and thus are difficult to recognize. Finally, we discard labels which the classifiers could not learn, as these are likely to be noisy or not visual.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_15",
  "x": "**EVALUATION** In this section we first analyze our approach on the <cite>MPII-MD</cite> <cite>[28]</cite> dataset and explore different design choices. Then, we compare our best system to <cite>prior work</cite>. ---------------------------------- **ANALYSIS OF OUR APPROACH** Experimental setup. We build on the labels discovered by our semantic parser <cite>[28]</cite> and additionally match these labels to sentences which <cite>the parser</cite> failed to process. To be able to learn classifiers we select the labels that appear at least 30 times, resulting in 1,263 labels. <cite>The parser</cite> additionally tells us whether the label is a verb.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_17",
  "x": "Then, we compare our best system to <cite>prior work</cite>. ---------------------------------- **ANALYSIS OF OUR APPROACH** Experimental setup. We build on the labels discovered by our semantic parser <cite>[28]</cite> and additionally match these labels to sentences which <cite>the parser</cite> failed to process. To be able to learn classifiers we select the labels that appear at least 30 times, resulting in 1,263 labels. <cite>The parser</cite> additionally tells us whether the label is a verb. We use the visual features (DT, LSDA, PLACES) provided with the <cite>MPII-MD dataset</cite> <cite>[28]</cite> . The LSTM output/hidden unit as well as memory cell have each 500 dimensions.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_18",
  "x": "We build on the labels discovered by our semantic parser <cite>[28]</cite> and additionally match these labels to sentences which <cite>the parser</cite> failed to process. To be able to learn classifiers we select the labels that appear at least 30 times, resulting in 1,263 labels. <cite>The parser</cite> additionally tells us whether the label is a verb. We use the visual features (DT, LSDA, PLACES) provided with the <cite>MPII-MD dataset</cite> <cite>[28]</cite> . The LSTM output/hidden unit as well as memory cell have each 500 dimensions. We train the SVM classifiers on the Training set (56,861 clips). We evaluate our method on the validation set (4,930 clips) using the METEOR [21] score, which, according to [7, 32] , supersedes other popular measures, such as BLEU [26] , ROUGE [22] , in terms of agreement with human judgments. The authors of CIDEr [32] showed that METEOR also outperforms CIDEr when the number of references is small and in the case of <cite>MPII-MD</cite> we have typically only a single reference. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_19",
  "x": "We build on the labels discovered by our semantic parser <cite>[28]</cite> and additionally match these labels to sentences which <cite>the parser</cite> failed to process. To be able to learn classifiers we select the labels that appear at least 30 times, resulting in 1,263 labels. <cite>The parser</cite> additionally tells us whether the label is a verb. We use the visual features (DT, LSDA, PLACES) provided with the <cite>MPII-MD dataset</cite> <cite>[28]</cite> . The LSTM output/hidden unit as well as memory cell have each 500 dimensions. We train the SVM classifiers on the Training set (56,861 clips). We evaluate our method on the validation set (4,930 clips) using the METEOR [21] score, which, according to [7, 32] , supersedes other popular measures, such as BLEU [26] , ROUGE [22] , in terms of agreement with human judgments. The authors of CIDEr [32] showed that METEOR also outperforms CIDEr when the number of references is small and in the case of <cite>MPII-MD</cite> we have typically only a single reference. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_21",
  "x": "Our results show that the step-based learning is superior to the polynomial learning. In most of experiments we trained our networks for 25,000 iterations. After looking at the METEOR performance for intermediate iterations we found that for the step size 4000 at iteration 15,000 we achieve best performance overall. Additionally we train multiple LSTMs with different random orderings of the training data. In our experiments we combine three in an ensemble, averaging the resulting word predictions. In most cases the ensemble improves over the single networks in terms of METEOR score (see Table 4 ). To summarize, the most important aspects that decrease over-fitting and lead to a better sentence generation are: (a) a correct learning rate and step size, (b) dropout after the LSTM layer, (c) choosing the training iteration based on METEOR score as opposed to only looking at the LSTM accuracy/loss which can be misleading, and (d) building ensembles of multiple networks with different random initializations. In the following section we evaluate our best ensemble (last line of Table 4 ) on the test set of <cite>MPII-MD</cite>. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_22",
  "x": "To summarize, the most important aspects that decrease over-fitting and lead to a better sentence generation are: (a) a correct learning rate and step size, (b) dropout after the LSTM layer, (c) choosing the training iteration based on METEOR score as opposed to only looking at the LSTM accuracy/loss which can be misleading, and (d) building ensembles of multiple networks with different random initializations. In the following section we evaluate our best ensemble (last line of Table 4 ) on the test set of <cite>MPII-MD</cite>. ---------------------------------- **COMPARISON TO RELATED WORK** Experimental setup. We compare the best method of <cite>[28]</cite> , the recently proposed method S2VT [33] and our proposed \"Visual Labels\"-LSTM on the test set of the <cite>MPII-MD dataset</cite> (6,578 clips). We report all popular automatic evaluation measures, CIDEr [32] , BLEU [26] , ROUGE [22] and METEOR [21] , computed using the evaluation code of [3] . We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank <cite>three systems</cite> (the best SMT of <cite>[28]</cite> , S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to <cite>[28]</cite> . Results.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_23",
  "x": "---------------------------------- **COMPARISON TO RELATED WORK** Experimental setup. We compare the best method of <cite>[28]</cite> , the recently proposed method S2VT [33] and our proposed \"Visual Labels\"-LSTM on the test set of the <cite>MPII-MD dataset</cite> (6,578 clips). We report all popular automatic evaluation measures, CIDEr [32] , BLEU [26] , ROUGE [22] and METEOR [21] , computed using the evaluation code of [3] . We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank <cite>three systems</cite> (the best SMT of <cite>[28]</cite> , S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to <cite>[28]</cite> . Results. Moreover, we improve over the recent approach of [33] , which also uses LSTM to generate video descriptions. Exploring different strategies to label selection and classifier training, as well as various LSTM configurations allows to obtain best result to date on the <cite>MPII-MD dataset</cite>.",
  "y": "similarities"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_24",
  "x": "Results. Moreover, we improve over the recent approach of [33] , which also uses LSTM to generate video descriptions. Exploring different strategies to label selection and classifier training, as well as various LSTM configurations allows to obtain best result to date on the <cite>MPII-MD dataset</cite>. Human evaluation mainly agrees with the automatic measures. We outperform <cite>both prior works</cite> in terms of Correctness and Relevance, however we lose to S2VT in terms of Grammar. This is due to the fact that S2VT produces overall shorter (7.4 versus 8.7 words per sentence) and simpler sentences, while our system generates longer sentences and therefore has higher chances to make mistakes. We also propose a retrieval upperbound (last line in Table 5 ). For every test sentence we retrieve the closest training sentence according to the METEOR. The rather low METEOR score of 19.43 reflects the difficulty of the dataset.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_25",
  "x": "We report all popular automatic evaluation measures, CIDEr [32] , BLEU [26] , ROUGE [22] and METEOR [21] , computed using the evaluation code of [3] . We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank <cite>three systems</cite> (the best SMT of <cite>[28]</cite> , S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to <cite>[28]</cite> . Results. Moreover, we improve over the recent approach of [33] , which also uses LSTM to generate video descriptions. Exploring different strategies to label selection and classifier training, as well as various LSTM configurations allows to obtain best result to date on the <cite>MPII-MD dataset</cite>. Human evaluation mainly agrees with the automatic measures. We outperform <cite>both prior works</cite> in terms of Correctness and Relevance, however we lose to S2VT in terms of Grammar. This is due to the fact that S2VT produces overall shorter (7.4 versus 8.7 words per sentence) and simpler sentences, while our system generates longer sentences and therefore has higher chances to make mistakes. We also propose a retrieval upperbound (last line in Table 5 ).",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_26",
  "x": "The rather low METEOR score of 19.43 reflects the difficulty of the dataset. A closer look at the sentences produced by <cite>all three methods</cite> gives us additional insights. An interesting characteristic is the output vocabulary size, which is 94 for <cite>[28] ,</cite> 86 for [33] and 605 for our method, while the test set contains 6422 unique words. This clearly shows a higher diversity of our output. Among the words generated by our system and absent in the outputs of others are such verbs as grab, drive, sip, climb, follow, objects as suit, chair, cigarette, mirror, bottle and places as kitchen, corridor, restaurant. We showcase some qualitative results in Figure 3 . Here, e.g. the verb pour, object drink and place courtyard only appear in our output. We attribute this, on one hand, to our diverse and robust visual classifiers. On the other hand, the architecture and parameter choices of our LSTM allow us to learn better correspondance between words and visual classifiers' scores.",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_27",
  "x": "We also propose a retrieval upperbound (last line in Table 5 ). For every test sentence we retrieve the closest training sentence according to the METEOR. The rather low METEOR score of 19.43 reflects the difficulty of the dataset. A closer look at the sentences produced by <cite>all three methods</cite> gives us additional insights. An interesting characteristic is the output vocabulary size, which is 94 for <cite>[28] ,</cite> 86 for [33] and 605 for our method, while the test set contains 6422 unique words. This clearly shows a higher diversity of our output. Among the words generated by our system and absent in the outputs of others are such verbs as grab, drive, sip, climb, follow, objects as suit, chair, cigarette, mirror, bottle and places as kitchen, corridor, restaurant. We showcase some qualitative results in Figure 3 . Here, e.g. the verb pour, object drink and place courtyard only appear in our output.",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_28",
  "x": "**ANALYSIS** Despite the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets (<cite>MPII-MD</cite> <cite>[28]</cite> and M-VAD [31] ) remains relatively low. In this section we want to take a closer look at <cite>three methods</cite>, best <cite>SMT</cite> of <cite>[28]</cite> , S2VT [33] and ours, in order to understand where these methods succeed and where they fail. In the following we evaluate <cite>all three methods</cite> on the <cite>MPII-MD</cite> test set. ---------------------------------- **APPROACH SENTENCE** <cite>SMT</cite> <cite>[28]</cite> Someone is a man, someone is a man. S2VT [33] Someone looks at him, someone turns to someone. Our Someone is standing in the crowd, a little man with a little smile.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_30",
  "x": "**ANALYSIS** Despite the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets (<cite>MPII-MD</cite> <cite>[28]</cite> and M-VAD [31] ) remains relatively low. In this section we want to take a closer look at <cite>three methods</cite>, best <cite>SMT</cite> of <cite>[28]</cite> , S2VT [33] and ours, in order to understand where these methods succeed and where they fail. In the following we evaluate <cite>all three methods</cite> on the <cite>MPII-MD</cite> test set. ---------------------------------- **APPROACH SENTENCE** <cite>SMT</cite> <cite>[28]</cite> Someone is a man, someone is a man. S2VT [33] Someone looks at him, someone turns to someone. Our Someone is standing in the crowd, a little man with a little smile.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_35",
  "x": "---------------------------------- **SEMANTIC ANALYSIS** WordNet Verb Topics. We closer analyze the test sentences with respect to different verbs. For this we rely on WordNet topics (high level entries in the WordNet ontology, e.g. \"motion\", \"perception\", \"competition\", \"emotion\"), defined for most synsets in WordNet [10] . We obtain the sense information from the semantic parser of <cite>[28]</cite> , thus senses might be noisy. We showcase the 5 most frequent verbs for each topic in Table 6 . We select sentences with a single verb, group them according to the verb topic and compute an average METEOR score for each topic, see Figure 6 . We find that our method is best for all topics except \"communication\", where <cite>[28]</cite> wins.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_36",
  "x": "**SEMANTIC ANALYSIS** WordNet Verb Topics. We closer analyze the test sentences with respect to different verbs. For this we rely on WordNet topics (high level entries in the WordNet ontology, e.g. \"motion\", \"perception\", \"competition\", \"emotion\"), defined for most synsets in WordNet [10] . We obtain the sense information from the semantic parser of <cite>[28]</cite> , thus senses might be noisy. We showcase the 5 most frequent verbs for each topic in Table 6 . We select sentences with a single verb, group them according to the verb topic and compute an average METEOR score for each topic, see Figure 6 . We find that our method is best for all topics except \"communication\", where <cite>[28]</cite> wins. The most frequent verbs in this topic are \"look up\" and \"nod\", which are also frequent in the dataset and in the sentences produced by <cite>[28]</cite> .",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_37",
  "x": "WordNet Verb Topics. We closer analyze the test sentences with respect to different verbs. For this we rely on WordNet topics (high level entries in the WordNet ontology, e.g. \"motion\", \"perception\", \"competition\", \"emotion\"), defined for most synsets in WordNet [10] . We obtain the sense information from the semantic parser of <cite>[28]</cite> , thus senses might be noisy. We showcase the 5 most frequent verbs for each topic in Table 6 . We select sentences with a single verb, group them according to the verb topic and compute an average METEOR score for each topic, see Figure 6 . We find that our method is best for all topics except \"communication\", where <cite>[28]</cite> wins. The most frequent verbs in this topic are \"look up\" and \"nod\", which are also frequent in the dataset and in the sentences produced by <cite>[28]</cite> . The best performing topic, \"cognition\", is highly biased to \"look at\" verb.",
  "y": "similarities"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_38",
  "x": "**CONCLUSION** We propose an approach to automatic movie description which trains visual classifiers and uses the classifier scores as input to LSTM. To handle the weak sentence annotations we rely on three main ingredients. First, we distinguish three semantic groups of labels (verbs, objects and places), second we train them discriminatively, removing potentially noisy negatives, and third, we select only a small number of the most reliable classifiers. For sentence generation we show the benefits of exploring different LSTM architectures and learning configurations. As the result we obtain the highest performance on the <cite>MPII-MD</cite> dataset as shown by all automatic evaluation measures and extensive human evaluation. We analyze the challenges in the movie description task using our and <cite>two prior works</cite>. We find that the factors which contribute to higher performance include: presence of frequent words, sentence length and simplicity as well as presence of \"visual\" verbs (e.g. \"nod\", \"walk\", \"sit\", \"smile\"). Textual and visual difficulties of sentences/clips strongly correlate with the performance of all methods.",
  "y": "uses"
 },
 {
  "id": "600317fc3ce88ea730993d3cc94f19_0",
  "x": "In this paper, we propose a new probabilistic GLR parsing method that can solve the problems of conventional methods. Our proposed Conditional Action Model uses Surface Phrasal Types (SPTs) encoding the functional word sequences of the sub-trees for describing structural characteristics of the partial parse. And, the proposed GLR model outperforms the previous methods by about 6~8%. ---------------------------------- **INTRODUCTION** Since the first approach [Wright and Wrigley 1991] of combining a probabilistic method into the GLR technique was published, Some probabilistic GLR parsers also have been implemented in which probabilities are assigned to actions of LR parsing tables by using lookaheads or LR states as simple context information of <cite>[Briscoe and Carroll 1993]</cite> , [Kentaro et al. 1998 ], and [Ruland, 2000] which does not use the stack information of the GLR parser effectively, because of highly complex internal GLR stack. As a result, they have used relatively limited contextual information for disambiguation. [Kwak et al., 2001] have proposed a conditional action model that uses the partially constructed parse represented by the graph-structured stack as the additional context. However, this method inappropriately defined sub-tree structure. Our proposed model uses Surface Phrasal Types representing the structural characteristics of the sub-trees for its additional contextual information.",
  "y": "background"
 },
 {
  "id": "600317fc3ce88ea730993d3cc94f19_1",
  "x": "In this paper, we propose a new probabilistic GLR parsing method that can solve the problems of conventional methods. Our proposed Conditional Action Model uses Surface Phrasal Types (SPTs) encoding the functional word sequences of the sub-trees for describing structural characteristics of the partial parse. And, the proposed GLR model outperforms the previous methods by about 6~8%. ---------------------------------- **INTRODUCTION** Since the first approach [Wright and Wrigley 1991] of combining a probabilistic method into the GLR technique was published, Some probabilistic GLR parsers also have been implemented in which probabilities are assigned to actions of LR parsing tables by using lookaheads or LR states as simple context information of <cite>[Briscoe and Carroll 1993]</cite> , [Kentaro et al. 1998 ], and [Ruland, 2000] which does not use the stack information of the GLR parser effectively, because of highly complex internal GLR stack. As a result, they have used relatively limited contextual information for disambiguation. [Kwak et al., 2001] have proposed a conditional action model that uses the partially constructed parse represented by the graph-structured stack as the additional context. However, this method inappropriately defined sub-tree structure. Our proposed model uses Surface Phrasal Types representing the structural characteristics of the sub-trees for its additional contextual information.",
  "y": "motivation"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_0",
  "x": "**INTRODUCTION** Recent work has shown evidence of substantial bias in machine learning systems, which is typically a result of bias in the training data. This includes both supervised (Blodgett and O'Connor, 2017; Tatman, 2017; Kiritchenko and Mohammad, 2018; De-Arteaga et al., 2019) and unsupervised natural language processing systems (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018) . Machine learning models are currently being deployed in the field to detect hate speech and abusive language on social media platforms including Facebook, Instagram, and Youtube. The aim of these models is to identify abusive language that directly targets certain individuals or groups, particularly people belonging to protected categories (Waseem et al., 2017) . Bias may reduce the accuracy of these models, and at worst, will mean that the models actively discriminate against the same groups they are designed to protect. Our study focuses on racial bias in hate speech and abusive language detection datasets (Waseem, 2016;<cite> Waseem and Hovy, 2016</cite>; Golbeck et al., 2017; Founta et al., 2018) , all of which use data collected from Twitter. We train classifiers using each of the datasets and use a corpus of tweets with demographic information to compare how each classifier performs on tweets written in African-American English (AAE) versus Standard American English (SAE) (Blodgett et al., 2016) . We use bootstrap sampling (Efron and Tibshirani, 1986) to estimate the proportion of tweets in each group that each classifier assigns to each class.",
  "y": "background motivation"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_1",
  "x": "---------------------------------- **RESEARCH DESIGN** ---------------------------------- **HATE SPEECH AND ABUSIVE LANGUAGE DATASETS** We focus on Twitter, the most widely used data source in abusive language research. We use all available datasets where tweets are labeled as various types of abuse and are written in English. We now briefly describe each of these datasets in chronological order. <cite>Waseem and Hovy (2016)</cite> collected 130k tweets containing one of seventeen different terms or phrases they considered to be hateful. They then annotated a sample of these tweets themselves, using guidelines inspired by critical race theory.",
  "y": "background"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_2",
  "x": "The term \"b*tch\" was used in 1.7% of black-aligned and 0.5% of whitealigned tweets. The substantial differences in the distributions for these two terms alone are consistent with our intuition that some of the results in Experiment 1 may be driven by differences in the frequencies of words associated with negative classes in the training datasets. Since we are using a subsample of the available data, we use smaller bootstrap samples, drawing k = 100 tweets each time. ---------------------------------- **RESULTS** The results of Experiment 1 are shown in Table 2. We observe substantial racial disparities in the performance of all classifiers. In all but one of the comparisons, there are statistically significant (p < 0.001) differences and in all but one of these we see that tweets in the black-aligned corpus are assigned negative labels more frequently than those by whites. The only case where blackaligned tweets are classified into a negative class less frequently than white-aligned tweets is the racism class in the <cite>Waseem and Hovy (2016)</cite> classifier.",
  "y": "differences"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_3",
  "x": "Table 3 shows that for tweets containing the word \"n*gga\", classifiers trained on <cite>Waseem and Hovy (2016)</cite> and Waseem (2016) are both predict black-aligned tweets to be instances of sexism approximately 1.5 times as often as white-aligned tweets. The classifier trained on the data is significantly less likely to classify black-aligned tweets as hate speech, although it is more likely to classify them as offensive. Golbeck et al. (2017) classifies black-aligned tweets as harassment at a higher rate for both groups than in the previous experiment, although the disparity is narrower. For the Founta et al. (2018) classifier we see that black-aligned tweets are slightly less frequently considered to be hate speech but are much more frequently classified as abusive. The results for the second variation of Experiment 2 where we conditioned on the word \"b*tch\" are shown in Table 4 . We see similar results for <cite>Waseem and Hovy (2016)</cite> and Waseem (2016) . In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism. The <cite>Waseem and Hovy (2016)</cite> classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class. For almost all of these tweets are classified as offensive, however those in the blackaligned corpus are 1.15 times as frequently classified as hate speech.",
  "y": "similarities uses"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_4",
  "x": "We see similar results for <cite>Waseem and Hovy (2016)</cite> and Waseem (2016) . In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism. The <cite>Waseem and Hovy (2016)</cite> classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class. For almost all of these tweets are classified as offensive, however those in the blackaligned corpus are 1.15 times as frequently classified as hate speech. We see a very similar result for Golbeck et al. (2017) compared to the previous experiment, with black-aligned tweets flagged as harassment at 1.1 times the rate of those in the white-aligned corpus. Finally, for the Founta et al. (2018) classifier we see a substantial racial disparity, with black-aligned tweets classified as hate speech at 2.7 times the rate of white aligned ones, a higher rate than in Experiment 1. ---------------------------------- **DISCUSSION** Our results demonstrate consistent, systematic and substantial racial biases in classifiers trained on all five datasets.",
  "y": "similarities"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_5",
  "x": "Golbeck et al. (2017) classifies black-aligned tweets as harassment at a higher rate for both groups than in the previous experiment, although the disparity is narrower. For the Founta et al. (2018) classifier we see that black-aligned tweets are slightly less frequently considered to be hate speech but are much more frequently classified as abusive. The results for the second variation of Experiment 2 where we conditioned on the word \"b*tch\" are shown in Table 4 . We see similar results for <cite>Waseem and Hovy (2016)</cite> and Waseem (2016) . In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism. The <cite>Waseem and Hovy (2016)</cite> classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class. For almost all of these tweets are classified as offensive, however those in the blackaligned corpus are 1.15 times as frequently classified as hate speech. We see a very similar result for Golbeck et al. (2017) compared to the previous experiment, with black-aligned tweets flagged as harassment at 1.1 times the rate of those in the white-aligned corpus. Finally, for the Founta et al. (2018) classifier we see a substantial racial disparity, with black-aligned tweets classified as hate speech at 2.7 times the rate of white aligned ones, a higher rate than in Experiment 1.",
  "y": "background"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_6",
  "x": "We now discuss the results as they pertain to each of the datasets used. Classifiers trained on data from <cite>Waseem and Hovy (2016)</cite> and Waseem (2016) only predicted a small fraction of the tweets to be racism. We suspect that this is due to the composition of their dataset, since the majority of the racist training examples consist of anti-Muslim rather than anti- Table 4 : Experiment 2, t = \"b*tch\" black language. Across both datasets the words \"n*gger\" and \"n*gga\" appear in 4 and 10 tweets respectively. Looking at the sexism class on the other hand, we see that both models were consistently classifying tweets in the black-aligned corpus as sexism at a substantially higher rate than those in the white-aligned corpus. Given this result, and the gender biases identified in these data by Park et al. (2018), it not apparent that the purportedly expert annotators were any less biased than amateur annotators (Waseem, 2016) . The classifier trained on shows the largest disparities in Experiment 1, with tweets in the black-aligned corpus classified as hate speech and offensive language at substantially higher rates than white-aligned tweets. We expect that this result occurred for two reasons. First, the dataset contains a large number of cases where AAE is used (Waseem et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_7",
  "x": "We find evidence of substantial racial bias in all of the datasets tested. This bias tends to persist even when comparing tweets containing certain relevant keywords. While these datasets are still valuable for academic research, we caution against using them in the field to detect and particularly to take enforcement action against different types of abusive language. If they are used in this way we expect that they will systematically penalize African-Americans more than whites, resulting in racial discrimination. We have not evaluated these datasets for bias related to other ethnic and racial groups, nor other protected categories like gender and sexuality, but expect that such bias is also likely to exist. We recommend that efforts to measure and mitigate bias should start by focusing on how bias enters into datasets as they are collected and labeled. In particular, future work should focus on the following three areas. First, we expect that some biases emerge at the point of data collection. Some studies sampled tweets using small, ad hoc sets of keywords created by the authors<cite> (Waseem and Hovy, 2016</cite>; Waseem, 2016; Golbeck et al., 2017) , an approach demonstrated to produce poor results (King et al., 2017) .",
  "y": "background"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_8",
  "x": "In particular, we need to consider whether the linguistic markers we use to identify potentially abusive language may be associated with language used by members of protected categories. For example, although started with thousands of terms from the Hatebase lexicon, AAE is over-represented in the dataset (Waseem et al., 2018) because some keywords associated with this speech community were used more frequently on Twitter than other keywords in the lexicon and were consequentially over-sampled. Second, we expect that the people who annotate data have their own biases. Since individual biases in reflect societal prejudices, they aggregate into systematic biases in training data. The datasets considered here relied upon a range of different annotators, from the authors (Golbeck et al., 2017;<cite> Waseem and Hovy, 2016)</cite> and crowdworkers Founta et al., 2018) to activists (Waseem, 2016) . Even the classifier trained on expert-labeled data (Waseem, 2016) flags black-aligned tweets as sexist at almost twice the rate of white-aligned tweets. While we agree that there is value in working with domain-experts to annotate data, these results suggest that activists may be prone to similar biases as academics and crowdworkers. Further work is therefore necessary to better understand how to integrate expertise into the process and how training can be used to help to mitigate bias. We also need to consider how sociocultural context influences annotators' decisions.",
  "y": "background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_0",
  "x": "While this is considered as a challenge for sentiment analysis, we adopt a different perspective, and discover benefits of the presence of subjunctive mood in opinionated text. Apart from the expression of criticism and satisfaction in customer reviews, reviews might include suggestions for improvements. Suggestions can either be expressed explicitly (Brun, 2013) , or by expressing wishes regarding new features and improvements<cite> (Ramanand et al., 2010)</cite> (Table 1) . Extraction of suggestions goes beyond the scope of sentiment analysis, and also complements it by providing another valuable information that is worth analyzing. Table 1 presents some examples of occurrence of subjunctive mood collected from different forums on English grammar 1 . There seems to be a high probability of the occurrence of subjunctive mood in wish and suggestion expressing sentences. This observation can be exploited for the tasks of wish detection<cite> (Ramanand et al., 2010)</cite> , and suggestion extraction (Brun, 2013) . To the best of our knowledge, subjunctive mood has never been analysed in the context of wish and suggestion detection. We collect a sample dataset comprising of example sentences of subjunctive mood, and identify features of subjunctive mood. We then employ a state of the art statistical classifier, and use subjunctive features in order to perform two kind of tasks on a given set of sentences: 1. Detect wish expressing sentences, and 2.",
  "y": "background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_1",
  "x": "Extraction of suggestions goes beyond the scope of sentiment analysis, and also complements it by providing another valuable information that is worth analyzing. Table 1 presents some examples of occurrence of subjunctive mood collected from different forums on English grammar 1 . There seems to be a high probability of the occurrence of subjunctive mood in wish and suggestion expressing sentences. This observation can be exploited for the tasks of wish detection<cite> (Ramanand et al., 2010)</cite> , and suggestion extraction (Brun, 2013) . To the best of our knowledge, subjunctive mood has never been analysed in the context of wish and suggestion detection. We collect a sample dataset comprising of example sentences of subjunctive mood, and identify features of subjunctive mood. We then employ a state of the art statistical classifier, and use subjunctive features in order to perform two kind of tasks on a given set of sentences: 1. Detect wish expressing sentences, and 2. Detect suggestion expressing sentences. Description Examples Suggestion bearing wishes in product reviews I wanted a dvd player that had basic features and would be able to play dvd or format discs that I had made myself. I wish canon would work out some way for that issue.",
  "y": "motivation background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_2",
  "x": "Benamara et al. (2012) studied modality and negation for French language, with an objective to examine its effect on sentiment polarity. Narayanan et al. (2009) performed sentiment analysis on conditional sentences. Our objective however is inclined towards wish and suggestion detection, rather than sentiment analysis. Wish Detection: Goldberg et al. (2009) performed wish detection on datasets obtained from political discussion forums and product reviews. They automatically extracted sentence templates from a corpus of new year wishes, and used them as features with a statistical classifier. Suggestion Detection: <cite>Ramanand et al. (2010)</cite> pointed out that wish is a broader category, which might not bear suggestions every time. They performed suggestion detection, where they focussed only on suggestion bearing wishes, and used manually formulated syntactic patterns for their detection. Brun (2013) also extracted suggestions from product reviews and used syntactico-semantic patterns for suggestion detection. None of these works on suggestion detection used a statistical classifier.",
  "y": "background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_3",
  "x": "Goldberg et al. (2009) follow this definition of wish and provide manually annotated datasets, where each sentence is labelled as wish or non-wish. Following two datasets are made available: a. Political Discussions: 6379 sentences, out of which 34% are annotated wishes. b. Product Reviews: 1235 sentences, out of which 12% are annotated as wishes. Table 1 presents some examples from these datasets. <cite>Ramanand et al. (2010)</cite> worked on product review dataset of the wish corpus, with an objective to extract suggestions for improvements. They considered suggestions as a subset of wishes, and thus retained the labels of only suggestion bearing wishes. They also annotated additional product reviews, but their data is not available for open research. \u2022 Suggestion Detection Product reviews (new): We re-annotated the product review dataset from Goldberg et al. (2009) , for suggestions. This also includes wishes for improvements and new features.",
  "y": "background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_4",
  "x": "They also annotated additional product reviews, but their data is not available for open research. \u2022 Suggestion Detection Product reviews (new): We re-annotated the product review dataset from Goldberg et al. (2009) , for suggestions. This also includes wishes for improvements and new features. Out of 1235 sentences, 6% are annotated as suggestions. Table 1 presents some examples from this dataset. Annotation Details: We had 2 annotators annotate each sentence with a suggestion or non-suggestion tag. We support the observation of <cite>Ramanand et al. (2010)</cite> that wishes for improvements and new features are implicit expression of suggestions. Therefore, annotators were also asked to annotate suggestions which were expressed as wishes. For inter-annotator agreement, a kappa value of 0.874 was obtained.",
  "y": "similarities motivation"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_5",
  "x": "VerbNet is a wide coverage verb lexicon, which places verbs into classes whose members have common syntactic and semantic properties. We collect all members of the VerbNet verb classes advice, wish, want, urge, require; 28 different verbs were obtained. <cite>Ramanand et al. (2010)</cite> also used a similar but much smaller subset {love, like, prefer and suggest} in their rules. ---------------------------------- **SYNTACTIC FEATURES:** \u2022 Frequent POS sequences: This is a set of 3,4 length sequences of Part Of Speech (POS) tags, which are automatically extracted from the subjunctive mood dataset. Words in the sentences are replaced by their corresponding POS tag, and top 200 sequences are extracted based on their weight. The weight of each sequence is a product of Term Frequency (TF) and Inverse Document Frequency (IDF). In order to apply the concept of TF and IDF to POS tag sequences, every 3 and 4 length tag sequence occurring in the corpus is treated as a term.",
  "y": "similarities"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_0",
  "x": "Rather, each of these models directly specifies the effects of these variables on exogenous word processing functions, and the eye movements the models produce are sensitive to these functions' output. Thus, this approach cannot answer the question of why these linguistic variables have the effects they do on eye movement behavior. Recently, <cite>Bicknell and Levy (2010)</cite> presented a model of eye movement control in reading that directly models the process of identifying the text from visual input, and makes eye movements to maximize the efficiency of the identification process. Bicknell and Levy (2012) demonstrated that this rational model produces effects of word frequency and predictability that qualitatively match those of humans: words that are less frequent and less predictable receive more and longer fixations. Because this model makes eye movements to maximize the efficiency of the identification process, this result gives an answer for the reason why these variables should have the effects that they do on eye movement behavior: a model that works to efficiently identify the text makes more and longer fixations on 21 words of lower frequency and predictability because it needs more visual information to identify them. Bicknell (2011) showed, however, that the effects of word length produced by the rational model look quite different from those of human readers. Because<cite> Bicknell and Levy's (2010)</cite> model implements the main proposal for why word length effects should arise, i.e., visual acuity limitations, the fact that the model does not reproduce humanlike word length effects suggests that our understanding of the causes of word length effects may be incomplete. In this paper, we argue that this result arose because of a simplifying assumption made in the rational model, namely, the assumption that the reader has veridical knowledge about the number of characters in a word being identified. We present an extension of<cite> Bicknell and Levy's (2010)</cite> model which does not make this simplifying assumption, and show in two sets of simulations that effects of word length produced by the extended model look more like those of humans.",
  "y": "background"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_1",
  "x": "Recently, <cite>Bicknell and Levy (2010)</cite> presented a model of eye movement control in reading that directly models the process of identifying the text from visual input, and makes eye movements to maximize the efficiency of the identification process. Bicknell and Levy (2012) demonstrated that this rational model produces effects of word frequency and predictability that qualitatively match those of humans: words that are less frequent and less predictable receive more and longer fixations. Because this model makes eye movements to maximize the efficiency of the identification process, this result gives an answer for the reason why these variables should have the effects that they do on eye movement behavior: a model that works to efficiently identify the text makes more and longer fixations on 21 words of lower frequency and predictability because it needs more visual information to identify them. Bicknell (2011) showed, however, that the effects of word length produced by the rational model look quite different from those of human readers. Because<cite> Bicknell and Levy's (2010)</cite> model implements the main proposal for why word length effects should arise, i.e., visual acuity limitations, the fact that the model does not reproduce humanlike word length effects suggests that our understanding of the causes of word length effects may be incomplete. In this paper, we argue that this result arose because of a simplifying assumption made in the rational model, namely, the assumption that the reader has veridical knowledge about the number of characters in a word being identified. We present an extension of<cite> Bicknell and Levy's (2010)</cite> model which does not make this simplifying assumption, and show in two sets of simulations that effects of word length produced by the extended model look more like those of humans. We argue from these results that uncertainty about word length is a necessary component of a full understanding of word length effects in reading. ----------------------------------",
  "y": "background"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_2",
  "x": "Because<cite> Bicknell and Levy's (2010)</cite> model implements the main proposal for why word length effects should arise, i.e., visual acuity limitations, the fact that the model does not reproduce humanlike word length effects suggests that our understanding of the causes of word length effects may be incomplete. In this paper, we argue that this result arose because of a simplifying assumption made in the rational model, namely, the assumption that the reader has veridical knowledge about the number of characters in a word being identified. We present an extension of<cite> Bicknell and Levy's (2010)</cite> model which does not make this simplifying assumption, and show in two sets of simulations that effects of word length produced by the extended model look more like those of humans. We argue from these results that uncertainty about word length is a necessary component of a full understanding of word length effects in reading. ---------------------------------- **REASONS FOR WORD LENGTH EFFECTS** The empirical effects of word length displayed by human readers are simple to describe: longer words receive more and longer fixations. The major reason proposed in the literature on eye movements in reading for this effect is that when fixating longer words, the average visual acuity of all the letters in the word will be lower than for shorter words, and this poorer average acuity is taken to lead to longer and more fixations. This intuition is built into the exogenous word processing functions in E-Z Reader and SWIFT.",
  "y": "differences extends"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_3",
  "x": "There are, however, reasons to believe that this account may be incomplete. First, while it is the case that the average visual acuity of all letters in a fixated word must be lower for longer words, this is just because there are additional letters in the longer word. While these additional letters pull down the average visual acuity of letters within the word, each additional letter should still provide additional visual information about the word's identity, an argument suggesting that longer words might require less -not more -time to be identified. In fact, in SWIFT, the exogenous word processing rate function slows as both the average and the sum of the visual acuities of the letters within the word decrease, but E-Z Reader does not implement this idea in any way. Additionally, a factor absent from both E-Z Reader and SWIFT, is that the visual neighborhoods of longer words (at least in English) appear to be sparser, when considering the number of words formed by a single letter substitution (Balota, Cortese, SergentMarshall, Spieler, & Yap, 2004) , or the average orthographic Levenshtein distance of the most similar 20 words (Yarkoni, Balota, & Yap, 2008) . Because reading words with more visual neighbors is generally slower (Pollatsek, Perea, & Binder, 1999) , this argument gives another reason to expect longer words to require less -not more -time to be read. So while E-Z Reader and SWIFT produce reasonable effects of word length on eye movement measures (in which longer words receive more and longer fixations) by assuming a particular effect of visual acuity, it is less clear whether a visual acuity account can yield reasonable word length effects in a model that also includes the two opposing effects mentioned above. Determining how these different factors should interact to produce word length effects requires a model of eye movements in reading that models the process of word identification from disambiguating visual input (Bicknell & Levy, in press ). The model presented by <cite>Bicknell and Levy (2010)</cite> fits this description, and includes visual acuity limitations (in fact, identical to the visual acuity function in SWIFT).",
  "y": "similarities"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_4",
  "x": "There are theoretical and empirical reasons to believe that this simplifying assumption is incorrect, that early in the word identification process human readers do have substantial uncertainty about the number of letters in a word, and further, that this may be especially so for long words. For example, results with masked priming have shown that recognition of a target word is facilitated by a prime that is a proper subset of the target's letters (e.g., blcn-balcon; Peressotti & Grainger, 1999; Grainger, Granier, Farioli, Van Assche, & van Heuven, 2006) , providing evidence that words of different length have substantial similarity in early processing. For these reasons, some recent models of isolated word recognition (Gomez, Ratcliff, & Perea, 2008; Norris, Kinoshita, & van Casteren, 2010) have suggested that readers have some uncertainty about the number of letters in a word early in processing. If readers have uncertainty about the length of words, we may expect that the amount of uncertainty would grow proportionally to length, as uncertainty is proportional to set size in other tasks of number estimation (Dehaene, 1997) . This would agree with the intuition that an 8-character word should be more easily confused with a 9-character word than a 3-character word with a 4-character word. Including uncertainty about word length that is larger for longer words would have the effect of increasing the number of visual neighbors for longer words more than for shorter words, providing another reason (in addition to visual acuity limitations) that longer words may require more and longer fixations. In the remainder of this paper, we describe an extension of<cite> Bicknell and Levy's (2010)</cite> model in which visual input provides stochastic -rather than veridical -information about the length of words, yielding uncertainty about word length, and in which the amount of uncertainty grows with length. We then present two sets of simulations with this extended model demonstrating that it produces more humanlike effects of word length, suggesting that uncertainty about word length may be an important component of a full understanding of the effects of word length in reading. ----------------------------------",
  "y": "differences extends"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_5",
  "x": "In this section, we describe our extension of<cite> Bicknell and Levy's (2010)</cite> rational model of eye movement control in reading. Except for the visual input system, and a small change to the behavior policy to allow for uncertainty about word length, the model is identical to that described by Bicknell and Levy. The reader is referred to that paper for further computational details beyond what is described here. In this model, the goal of reading is taken to be efficient text identification. While it is clear that this is not all that readers do -inferring the underlying structural relationships among words in a sentence and discourse relationships between sentences that determine text meaning is a fundamental part of most reading -all reader goals necessarily involve identification of at least part of the text, so text identification is taken to be a reasonable first approximation. There are two sources of information relevant to this goal: visual input and language knowledge, which the model combines via Bayesian inference. Specifically, it begins with a prior distribution over possible identities of the text given by its language model, and combines this with noisy visual input about the text at the eyes' position, giving the likelihood term, to form a posterior distribution over the identity of the text taking into account both the language model and the visual input obtained thus far. On the basis of the posterior distribution, the model decides whether or not to move its eyes (and if so where to move them to) and the cycle repeats. 23",
  "y": "extends"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_6",
  "x": "Because of motor error, the actual landing position of the eyes is normally distributed around the intended target with the standard deviation in characters given by a linear function of the intended distance d (.87 + .084d; Engbert et al., 2005) . 1 ---------------------------------- **LANGUAGE KNOWLEDGE** Following <cite>Bicknell and Levy (2010)</cite>, we use very simple probabilistic models of language knowledge: word n-gram models (Jurafsky & Martin, 2009) , which encode the probability of each word conditional on the n \u2212 1 previous words. ---------------------------------- **FORMAL MODEL OF VISUAL INPUT** Visual input in the model consists of noisy information about the positions and identities of the characters in the text. Crucially, in this extended version of the model, this includes noisy information about the length of words.",
  "y": "similarities uses"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_7",
  "x": "Following Bicknell (2011), we use the model to simulate reading of a modified version of the Schilling, Rayner, and Chumbley (1998) corpus of typical sentences used in reading experiments. We compare three levels of length uncertainty: \u03b4 \u2208 {0, .05, .1}. The first of these (\u03b4 = 0) corresponds to<cite> Bicknell and Levy's (2010)</cite> model, which has no uncertainty about word length. We predict that increasing the amount of length uncertainty will make effects of word length more like those of humans, and we compare the model's length effects to those of human readers of the Schilling corpus. ---------------------------------- **METHODS** ---------------------------------- **MODEL PARAMETERS AND LANGUAGE MODEL** Following Bicknell (2011) , the model's language knowledge was an unsmoothed bigram model using a vocabulary set consisting of the 500 most frequent words in the British National Corpus (BNC) as well as all the words in the test corpus. Every bigram in the BNC was counted for which both words were in vocabulary, and -due to the intense computation required for exact inference -this set was trimmed by removing rare bigrams that occur less than 200 times (except for bigrams that occur in the test corpus), resulting in a set of about 19,000 bigrams, from which the bigram model was constructed.",
  "y": "similarities"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_8",
  "x": "To find optimal values of the policy parameters \u03b1 and \u03b2 for each model, we use the PEGASUS method (Ng & Jordan, 2000) to transform this stochastic optimization problem into a deterministic one amenable to standard optimization algorithms, and then use coordinate ascent. ---------------------------------- **TEST CORPUS** We test the model on a corpus of 33 sentences from the Schilling corpus slightly modified by <cite>Bicknell and Levy (2010)</cite> so that every bigram occurred in the BNC, ensuring that the results do not depend on smoothing. ---------------------------------- **ANALYSIS** With each model, we performed 50 stochastic simulations of the reading of the corpus. For each run, we calculated the four standard eye movement measures mentioned above for each word in the corpus: first fixation duration, gaze duration, skipping probability, and refixation probability. We then averaged each of these four measures across runs for each word token in the corpus, yielding a single mean value for each measure for each word.",
  "y": "differences extends"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_9",
  "x": "In addition, comparing the results of Simulation 2 with Simulation 1 reveals the importance to this account of words having realistic visual neighborhoods. When the visual neighborhoods of (especially longer) words were trimmed to be artificially sparse, adding length uncertainty did not allow the model to recover the human pattern. ---------------------------------- **RESULTS** ---------------------------------- **CONCLUSION** In this paper, we argued that the success of major models of eye movements in reading to reproduce the (positive) human effect of word length via acuity limitations may be a result of not including opposing factors such as the negative correlation between visual neighborhood size and word length. We described the failure of the rational model presented in <cite>Bicknell and Levy (2010)</cite> to obtain humanlike effects of word length, despite including all of these factors, suggesting that our understanding of word length effects in reading is incomplete. We proposed a new reason for word length effects -uncertainty about word length that is larger for longer wordsand noted that this reason was not implemented in Bicknell and Levy's model because of a simplifying assumption.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_0",
  "x": "**ABSTRACT** We investigate the utility of pre-existing question answering models and data for a recently proposed relation extraction task. We find that in the low-resource and zeroshot cases, such resources are surprisingly useful. Moreover, the resulting models show robust performance on a new test set we create from the task's original datasets. ---------------------------------- **INTRODUCTION** Knowledge Base Population (KBP, e.g.: Riedel et al., 2013; Sterckx et al., 2016) attempts to identify facts within raw text and convert them into triples consisting of a subject, object and the relation between them. One common form of this task is slot filling (Surdeanu and Heng, 2014) , in which a knowledge base (KB) query, such as place of birth(Obama, ?) is applied to a set of documents and a set of slot fillers is returned. By converting such KB queries to natural language questions, <cite>Levy et al. (2017)</cite> showed that a question answering (QA) system could be effectively applied to this task.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_1",
  "x": "We find that in the low-resource and zeroshot cases, such resources are surprisingly useful. Moreover, the resulting models show robust performance on a new test set we create from the task's original datasets. ---------------------------------- **INTRODUCTION** Knowledge Base Population (KBP, e.g.: Riedel et al., 2013; Sterckx et al., 2016) attempts to identify facts within raw text and convert them into triples consisting of a subject, object and the relation between them. One common form of this task is slot filling (Surdeanu and Heng, 2014) , in which a knowledge base (KB) query, such as place of birth(Obama, ?) is applied to a set of documents and a set of slot fillers is returned. By converting such KB queries to natural language questions, <cite>Levy et al. (2017)</cite> showed that a question answering (QA) system could be effectively applied to this task. However, <cite>their</cite> approach relied on a modified QA model architecture and a dedicated slot-filling training corpus. Here, we investigate the utility of standard QA data and models for this task.",
  "y": "motivation"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_2",
  "x": "Figure 1 gives an overview of using QA on the slot-filling task. Starting at the top right, a KB query is translated into a natural language question, which can then be fed into a QA model that has been trained on an appropriate resource. When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a QA dataset in relation to the slot-filling task of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zero-shot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity.",
  "y": "uses"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_3",
  "x": "Our results show that this approach is effective in the zero-shot and low-resource cases, and is more robust on a set of test instances designed to challenge the models' ability to identify relations between subject and object. Figure 1 gives an overview of using QA on the slot-filling task. Starting at the top right, a KB query is translated into a natural language question, which can then be fed into a QA model that has been trained on an appropriate resource. When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a QA dataset in relation to the slot-filling task of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zero-shot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_4",
  "x": "Figure 1 gives an overview of using QA on the slot-filling task. Starting at the top right, a KB query is translated into a natural language question, which can then be fed into a QA model that has been trained on an appropriate resource. When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a QA dataset in relation to the slot-filling task of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zero-shot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_5",
  "x": "---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a QA dataset in relation to the slot-filling task of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zero-shot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) (Rajpurkar et al., 2016) , can be applied to the relation extraction task. This is motivated both by curiosity about the generalisation abilities of such QA models and also a practical interest in relation extraction applications. We first investigate the zero-shot case, where no examples of the relations are available, and then evaluate how performance improves as more data becomes available. Data We compare two sources of training data: The University of Washington relation extraction (UWRE) dataset created by <cite>Levy et al. (2017)</cite> and the Stanford Question Answering Dataset (SQuAD) created by Rajpurkar et al. (2016) .",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_6",
  "x": "We first investigate the zero-shot case, where no examples of the relations are available, and then evaluate how performance improves as more data becomes available. Data We compare two sources of training data: The University of Washington relation extraction (UWRE) dataset created by <cite>Levy et al. (2017)</cite> and the Stanford Question Answering Dataset (SQuAD) created by Rajpurkar et al. (2016) . The UWRE data is derived from WikiReading (Hewlett et al., 2016) , which is itself derived from WikiData (Vrande\u010di\u0107, 2012) , and consists of a set of positive and negative examples for relation extraction from Wikipedia sentences. Each instance consists of an entity, a relation, a question template for the relation and a sentence drawn from the wikipedia article for that entity which may or may not answer the question. Under the assumption that each relation triple found in a Wikipedia info-box is also expressed in the text of its article, the positive examples contain the first sentence from the article that contains both the subject and object of the triple. The negative examples also contain the subject entity of the relation, but express a different relation. <cite>Levy et al. (2017)</cite> provide a number of train/dev/test splits, to allow <cite>them</cite> to evaluate a variety of modes of generalisation. Here we use the relation and entity splits. The former tests the ability to generalise from one set of relations to another, i.e. to do zero-shot learning for the unseen relations in the test set.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_7",
  "x": "We first investigate the zero-shot case, where no examples of the relations are available, and then evaluate how performance improves as more data becomes available. Data We compare two sources of training data: The University of Washington relation extraction (UWRE) dataset created by <cite>Levy et al. (2017)</cite> and the Stanford Question Answering Dataset (SQuAD) created by Rajpurkar et al. (2016) . The UWRE data is derived from WikiReading (Hewlett et al., 2016) , which is itself derived from WikiData (Vrande\u010di\u0107, 2012) , and consists of a set of positive and negative examples for relation extraction from Wikipedia sentences. Each instance consists of an entity, a relation, a question template for the relation and a sentence drawn from the wikipedia article for that entity which may or may not answer the question. Under the assumption that each relation triple found in a Wikipedia info-box is also expressed in the text of its article, the positive examples contain the first sentence from the article that contains both the subject and object of the triple. The negative examples also contain the subject entity of the relation, but express a different relation. <cite>Levy et al. (2017)</cite> provide a number of train/dev/test splits, to allow <cite>them</cite> to evaluate a variety of modes of generalisation. Here we use the relation and entity splits. The former tests the ability to generalise from one set of relations to another, i.e. to do zero-shot learning for the unseen relations in the test set.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_8",
  "x": "We also construct a series of datasets that combine increasing quantities of the UWRE entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 UWRE instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction, using models trained on the original UWRE and SQuAD datasets. As can be seen, BIDAF is actually more effective at answering the questions for the unseen relation types in the UWRE test set when it is trained on a standard QA dataset, rather than a dedicated relation extraction dataset. Figure 2 plots how performance improves as more data is available about the relations in the test set.",
  "y": "uses similarities"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_9",
  "x": "This process is applied to both the train and dev sets, allowing us to evaluate a model that uses only question answering data at training time. We also construct a series of datasets that combine increasing quantities of the UWRE entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 UWRE instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction, using models trained on the original UWRE and SQuAD datasets. As can be seen, BIDAF is actually more effective at answering the questions for the unseen relation types in the UWRE test set when it is trained on a standard QA dataset, rather than a dedicated relation extraction dataset.",
  "y": "uses"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_10",
  "x": "Looking first at the effect of adding the original UWRE training instances, performance drops dramatically as the size of this expansion increases. In contrast, as the quantity of UWRE+ data grows, performance improves, peaking at around 100,000 instances, which is around the same size as SQuAD. ---------------------------------- **DISCUSSION** The results on our challenge test set suggest that the model does not learn to examine the relation between the answer span and the relation subject unless the training data requires it. In the case of SQuAD, the fact that the answer has to be found within a multi-sentence paragraph provides enough potential distractors to overcome this issue. Other models may show different patterns of strength and weakness, but to be able to investigate and exploit further QA systems quickly would require a means of producing 'no answer' predictions without the need to modify the model implementation. 4 Using an unmodified QA model for slot filling <cite>Levy et al. (2017)</cite> modify the BiDAF architecture to produce an additional output representing the probability that no answer is present in the text. In this experiment, we investigate whether it is possible to adapt a QA model to the slot filling task without having to understand and modify its internal structure and implementation.",
  "y": "motivation"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_11",
  "x": "Data We train our models on a modified version of SQuAD, which has been augmented with negative examples by removing answer spans, as described in Section 2, and then had the token NoAnswerFound inserted into every text and as the answer for the negative examples, as described above. Models We train both BiDAF (Seo et al., 2016) and FastQA (Weissenborn et al., 2017 ) models on the modified SQuAD training data, using their standard architectures and hyperparameters. Evaluation We evaluate F1 on the same zeroshot evaluation considered in Section 2 and also accuracy on the challenge test set from Section 3. Results Table 3 reveals that the unmodified BiDAF model is almost as effective as the <cite>Levy et al. (2017)</cite> model in terms of zero-shot F1 on the original UWRE test set. In contrast, FastQA's performance is substantially worse. However, Table 4 reveals that FastQA is extremely accurate on the challenge test set, while BiDAF's performance is comparable to the modified model trained on SQuAD. The unmodified BiDAF and FastQA architectures have complementary strengths on the two evaluations. FastQA's strong performance on the challenge instances may be related to its use of binary features indicating whether a word was present in the question. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_0",
  "x": "****MEASURING SYNTACTIC DIFFERENCE IN BRITISH ENGLISH**** **ABSTRACT** Recent work by <cite>Nerbonne and Wiersma (2006)</cite> has provided a foundation for measuring syntactic differences between corpora. It uses part-of-speech trigrams as an approximation to syntactic structure, comparing the trigrams of two corpora for statistically significant differences. This paper extends the method and its application. It extends the method by using leafpath ancestors of Sampson (2000) instead of trigrams, which capture internal syntactic structure-every leaf in a parse tree records the path back to the root. The corpus used for testing is the International Corpus of English, Great Britain (Nelson et al., 2002) , which contains syntactically annotated speech of Great Britain. The speakers are grouped into geographical regions based on place of birth. This is different in both nature and number than previous experiments, which found differences between two groups of Norwegian L2 learners of English.",
  "y": "background"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_1",
  "x": "In the measurement of linguistic distance, older work such as S\u00e9guy (1973) was able to measure distance in most areas of linguistics, such as phonology, morphology, and syntax. The features used for comparison were hand-picked based on linguistic knowledge of the area being surveyed. These features, while probably lacking in completeness of coverage, certainly allowed a rough comparison of distance in all linguistic domains. In contrast, computational methods have focused on a single area of language. For example, a method for determining phonetic distance is given by Heeringa (2004) . Heeringa and others have also done related work on phonological distance in Nerbonne and Heeringa (1997) and Gooskens and Heeringa (2004) . A measure of syntactic distance is the obvious next step: <cite>Nerbonne and Wiersma (2006)</cite> provide one such method. This method approximates internal syntactic structure using vectors of part-of-speech trigrams. The trigram types can then be compared for statistically significant differences using a permutation test.",
  "y": "motivation background"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_2",
  "x": "Second, the experiments did not test data for geographical dialect variation, but compared two generations of Norwegian L2 learners of English, with differences between ages of initial acquisition. We address these areas by using the syntactically annotated speech section of the International Corpus of English, Great Britain (ICE-GB) (Nelson et al., 2002) , which provides a corpus with full syntactic annotations, one that can be divided into groups for comparison. The sentences of the corpus, being represented as parse trees rather than a vector of POS tags, are converted into a vector of leafancestor paths, which were developed by Sampson (2000) to aid in parser evaluation by providing a way to compare gold-standard trees with parser output trees. In this way, each sentence produces its own vec-tor of leaf-ancestor paths. Fortunately, the permutation test used by <cite>Nerbonne and Wiersma (2006)</cite> is already designed to normalize the effects of differing sentence length when combining POS trigrams into a single vector per region. The only change needed is the substitution of leaf-ancestor paths for trigrams. The speakers in the ICE-GB are divided by place of birth into geographical regions of England based on the nine Government Office Regions, plus Scotland and Wales. The average region contains a little over 4,000 sentences and 40,000 words. This is less than the size of the Norwegian corpora, and leaf-ancestor paths are more complex than trigrams, meaning that the amount of data required for obtaining significance should increase.",
  "y": "motivation"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_3",
  "x": "Comparisons to judgments of dialectologists have not yet been made. The comparison is difficult because of the difference in methodology and amount of detail in reporting. Dialectology tends to collect data from a few informants at each location and to provide a more complex account of relationship than the like/unlike judgments provided by permutation tests. ---------------------------------- **METHODS** The methods used to implement the syntactic difference test come from two sources. The primary source is the syntactic comparison of <cite>Nerbonne and Wiersma (2006)</cite> , which uses a permutation test, explained in Good (1995) and in particular for linguistic purposes in Kessler (2001) . Their permutation test collects POS trigrams from a random subcorpus of sentences sampled from the combined corpora. The trigram frequencies are normalized to neutralize the effects of sentence length, then compared to the trigram frequencies of the complete corpora.",
  "y": "similarities uses"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_4",
  "x": "The primary source is the syntactic comparison of <cite>Nerbonne and Wiersma (2006)</cite> , which uses a permutation test, explained in Good (1995) and in particular for linguistic purposes in Kessler (2001) . Their permutation test collects POS trigrams from a random subcorpus of sentences sampled from the combined corpora. The trigram frequencies are normalized to neutralize the effects of sentence length, then compared to the trigram frequencies of the complete corpora. The principal difference between the work of <cite>Nerbonne and Wiersma (2006)</cite> and ours is the use of leaf-ancestor paths. Leaf-ancestor paths were developed by Sampson (2000) for estimating parser performance by providing a measure of similarity of two trees, in particular a gold-standard tree and a machine-parsed tree. This distance is not used for our method, since for our purposes, it is enough that leaf-ancestor paths represent syntactic information, such as upper-level tree structure, more explicitly than trigrams. The permutation test used by <cite>Nerbonne and Wiersma (2006)</cite> is independent of the type of item whose frequency is measured, treating the items as atomic symbols. Therefore, leaf-ancestor paths should do just as well as trigrams as long as they do not introduce any additional constraints on how they are generated from the corpus. Fortunately, this is not the case; <cite>Nerbonne and Wiersma (2006)</cite> generate N \u2212 2 POS trigrams from each sentence of length N ; we generate N leaf-ancestor paths from each parsed sentence in the corpus.",
  "y": "differences"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_5",
  "x": "The primary source is the syntactic comparison of <cite>Nerbonne and Wiersma (2006)</cite> , which uses a permutation test, explained in Good (1995) and in particular for linguistic purposes in Kessler (2001) . Their permutation test collects POS trigrams from a random subcorpus of sentences sampled from the combined corpora. The trigram frequencies are normalized to neutralize the effects of sentence length, then compared to the trigram frequencies of the complete corpora. The principal difference between the work of <cite>Nerbonne and Wiersma (2006)</cite> and ours is the use of leaf-ancestor paths. Leaf-ancestor paths were developed by Sampson (2000) for estimating parser performance by providing a measure of similarity of two trees, in particular a gold-standard tree and a machine-parsed tree. This distance is not used for our method, since for our purposes, it is enough that leaf-ancestor paths represent syntactic information, such as upper-level tree structure, more explicitly than trigrams. The permutation test used by <cite>Nerbonne and Wiersma (2006)</cite> is independent of the type of item whose frequency is measured, treating the items as atomic symbols. Therefore, leaf-ancestor paths should do just as well as trigrams as long as they do not introduce any additional constraints on how they are generated from the corpus. Fortunately, this is not the case; <cite>Nerbonne and Wiersma (2006)</cite> generate N \u2212 2 POS trigrams from each sentence of length N ; we generate N leaf-ancestor paths from each parsed sentence in the corpus.",
  "y": "background"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_6",
  "x": "The primary source is the syntactic comparison of <cite>Nerbonne and Wiersma (2006)</cite> , which uses a permutation test, explained in Good (1995) and in particular for linguistic purposes in Kessler (2001) . Their permutation test collects POS trigrams from a random subcorpus of sentences sampled from the combined corpora. The trigram frequencies are normalized to neutralize the effects of sentence length, then compared to the trigram frequencies of the complete corpora. The principal difference between the work of <cite>Nerbonne and Wiersma (2006)</cite> and ours is the use of leaf-ancestor paths. Leaf-ancestor paths were developed by Sampson (2000) for estimating parser performance by providing a measure of similarity of two trees, in particular a gold-standard tree and a machine-parsed tree. This distance is not used for our method, since for our purposes, it is enough that leaf-ancestor paths represent syntactic information, such as upper-level tree structure, more explicitly than trigrams. The permutation test used by <cite>Nerbonne and Wiersma (2006)</cite> is independent of the type of item whose frequency is measured, treating the items as atomic symbols. Therefore, leaf-ancestor paths should do just as well as trigrams as long as they do not introduce any additional constraints on how they are generated from the corpus. Fortunately, this is not the case; <cite>Nerbonne and Wiersma (2006)</cite> generate N \u2212 2 POS trigrams from each sentence of length N ; we generate N leaf-ancestor paths from each parsed sentence in the corpus.",
  "y": "differences"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_7",
  "x": "However, to find out if the value of R is significant, we must use a permutation test with a Monte Carlo technique described by Good (1995) , following closely the same usage by <cite>Nerbonne and Wiersma (2006)</cite> . The intuition behind the technique is to compare the R of the two corpora with the R of two random subsets of the combined corpora. For comparison to the experiment conducted by <cite>Nerbonne and Wiersma (2006)</cite> , the experiment was also run with POS trigrams.",
  "y": "similarities uses"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_8",
  "x": "Therefore, the leaf-ancestor paths each started at the root of the sentence and ended with a part of speech. For comparison to the experiment conducted by <cite>Nerbonne and Wiersma (2006)</cite> , the experiment was also run with POS trigrams. Finally, a control experiment was conducted by comparing two permutations from the same corpus and ensuring that they were not significantly different. ICE-GB reports the place of birth of each speaker, which is the best available approximation to which dialect a speaker uses. As a simple, objective partitioning, the speakers were divided into 11 geographical regions based on the 9 Government Office Regions of England with Wales and Scotland added as single regions. Some speakers had to be thrown out at this point because they lacked brithplace information or were born outside the UK. Each region varied in size; however, the average number of sentences per corpus was 4682, with an average of 44,726 words per corpus (see table 1). Thus, the average sentence length was 9.55 words. The average corpus was smaller than the Norwegian L2 English corpora of <cite>Nerbonne and Wiersma (2006)</cite> , which had two groups, one with 221,000 words and the other with 84,000.",
  "y": "differences"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_9",
  "x": "The significant differences found are given in table 2 and 3. It seems that summed corpus size must reach a certain threshold before differences can be observed reliably: about 250,000 words for leaf-ancestor paths and 100,000 for trigrams. There are exceptions in both directions; the total size of London compared to Wales is larger than the size of London compared to the East Midlands, but the former is not statistically different. On the other hand, the total size of Southeast England compared to Scotland is only half of the other significantly different comparisons; this difference may be a result of more extreme syntactic differences than the other areas. Finally, it is interesting to note that the summed Norwegian corpus size is around 305,000 words, which is about three times the size needed for significance as estimated from the ICE-GB data. ---------------------------------- **DISCUSSION** Our work extends that of <cite>Nerbonne and Wiersma (2006)</cite> in a number of ways. We have shown that an alternate method of representing syntax still allows the permutation test to find significant differences between corpora.",
  "y": "extends"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_10",
  "x": "A comparison that divides the speakers into traditional British dialect areas is needed to see if the same differences can be detected. This is very likely, because corpus divisions that better reflect reality have a better chance of achieving a significant difference. In fact, even though leaf-ancestor paths should provide finer distinctions than trigrams and thus require more data for detectable significance, the regional corpora presented here were smaller than the Norwegian speakers' corpora in <cite>Nerbonne and Wiersma (2006)</cite> by up to a factor of 10. This raises the question of a lower limit on corpus size. Our experiment suggests that the two corpora must have at least 250,000 words, although we suspect that better divisions will allow smaller corpus sizes. While we are reducing corpus size, we might as well compare the increasing numbers of smaller and smaller corpora in an advantageous order. It should be possible to cluster corpora by the point at which they fail to achieve a significant difference when split from a larger corpus. In this way, regions could be grouped by their detectable boundaries, not a priori distinctions based on geography or existing knowledge of dialect boundaries. Of course this indirect method would not be needed if one had a direct method for clustering speakers, by distance or other measure.",
  "y": "differences"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_0",
  "x": "****NRC: INFUSED PHRASE VECTORS FOR NAMED ENTITY RECOGNITION IN TWITTER**** **ABSTRACT** Our submission to the W-NUT Named Entity Recognition in Twitter task closely follows the approach detailed by <cite>Cherry and Guo (2015)</cite> , who use a discriminative, semi-Markov tagger, augmented with multiple word representations. We enhance this approach with updated gazetteers, and with infused phrase embeddings that have been adapted to better predict the gazetteer membership of each phrase. Our system achieves a typed F1 of 44.7, resulting in a third-place finish, despite training only on the official training set. A post-competition analysis indicates that also training on the provided development data improves our performance to 54.2 F1. ---------------------------------- **INTRODUCTION** Named entity recognition (NER) is the task of finding rigid designators as they appear in free text and assigning them to coarse types such as person or geo-location (Nadeau and Sekine, 2007) .",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_1",
  "x": "Our system achieves a typed F1 of 44.7, resulting in a third-place finish, despite training only on the official training set. A post-competition analysis indicates that also training on the provided development data improves our performance to 54.2 F1. ---------------------------------- **INTRODUCTION** Named entity recognition (NER) is the task of finding rigid designators as they appear in free text and assigning them to coarse types such as person or geo-location (Nadeau and Sekine, 2007) . NER is the first step in many information extraction tasks, but in social media, this task is extremely challenging. The text to be analyzed is unedited and noisy, and covers a much more diverse set of topics than one might expect in newswire. As such, we are quite interested in the W-NUT Named Entity Recognition in Twitter task (Baldwin et al., 2015) as a platform to benchmark and drive forward work on NER in social media. Our submission to this competition closely follows <cite>Cherry and Guo (2015)</cite> , who advocate the use of a semi-Markov tagger trained online with standard discriminative tagging features, gazetteer matches, Brown clusters, and word embeddings.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_2",
  "x": "---------------------------------- **DATA RESOURCES** We make use of two external data resources: gazetteers and unlabeled tweets. For gazetteers, we begin with the word lists provided with the W-NUT baseline system, which appear to be mostly derived from Freebase. We treat each file in the lexicon directory as a distinct word list. We update and augment these lists with our own Freebase queries in Section 3.2. We use unannotated tweets to build various word representations (see Section 3.1). Our unannotated corpus collects 98M tweets (1,995M tokens) from between May 2011 and April 2012. The same corpus is used by <cite>Cherry and Guo (2015)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_3",
  "x": "As we had very little recent data, we made no attempt to bias the corpus to be entity-rich. This corpus of recent tweets has an average tweet length of only 13.8 tokens. Our attempts to use this data to build word representations did not improve NER performance on the 2015 development set, regardless of whether we used the data on its own or in combination with our larger corpus. ---------------------------------- **METHODS** ---------------------------------- **BASE TAGGER** We first summarize the approach of <cite>Cherry and Guo (2015)</cite> , which we build upon for our system. Tagger: We tag each tweet independently using a semi-Markov tagger (Sarawagi and Cohen, 2004) , which tags phrasal entities using a single operation, as opposed to traditional word-based entity tagging schemes.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_4",
  "x": "For word embeddings, we use an in-house Java re-implementation of word2vec (Mikolov et al., 2013a) to build 300-dimensional vector representations for all types that occur at least 10 times in our unannotated corpus. Each word then reports a real-valued feature (as opposed to an indicator) for each of the 300 dimensions in its vector representation. A single random vector is created to represent all out-ofvocabulary words. Our vectors and clusters cover 2.5 million types. Note that we do not include part-of-speech tags as features, as they were not found to be useful by <cite>Cherry and Guo (2015)</cite> . ---------------------------------- **UPDATED GAZETTEERS** During development, we found that features involving gazetteers were having a larger impact on dev than on dev 2015. Therefore, we enhanced the gazetteers provided with the W-NUT baseline system with our own Freebase queries, issued between May 6-7, 2015.",
  "y": "differences extends"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_5",
  "x": "Phrase embeddings enable more gazetteer matches for gazetteer-infused vectors, which we discuss next. ---------------------------------- **GAZETTEER-INFUSED PHRASE VECTORS** We employ an auto-encoder to leverage knowledge derived from domain-specific gazetteers to make the distributed phrase representations more relevant to our NER task. In recent years, two sources of information have been found to be valuable to boost the performance for NER: distributed representation learned from a large corpus and domain-specific lexicons (Turian et al., 2010;<cite> Cherry and Guo, 2015)</cite> . Research has also shown that merging these two forms of information can result in further predictive improvement for an NER system (Passos et al., 2014) . A similar strategy for enhancing word embeddings has also been demonstrated for sentiment analysis (Tang et al., 2014) . Following this line of research, we aim to tailor (post-process) the unsupervised phrase embeddings, created in Section 3.3, for our NER task, using an auto-encoder. The auto-encoder eliminates the need to have access to the original training data and the vector training model, requiring only the trained distributed vectors.",
  "y": "background"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_6",
  "x": "The Rnk column lists the retro-active rank of each system in the competition. ---------------------------------- **ABLATION** The systems above [A]+[U] are intended to demonstrate our development process. Our baseline is our attempt to re-implement the provided baseline in our code base, and includes all lexical features and the baseline gazetteers. C&G 2015 adds Brown clusters and word embeddings to create a complete re-implementation of <cite>Cherry and Guo (2015)</cite> . We can see that these representations have a huge impact on NER performance for all dev and test sets. We then performed a careful hyper-parameter sweep using the two provided development sets, resulting in the Inc. Regularization system. The hyper-parameters suggested by <cite>Cherry and Guo (2015)</cite> (E=10, C=0.01, P=10) were selected to work well with and without representations.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_7",
  "x": "**ABLATION** The systems above [A]+[U] are intended to demonstrate our development process. Our baseline is our attempt to re-implement the provided baseline in our code base, and includes all lexical features and the baseline gazetteers. C&G 2015 adds Brown clusters and word embeddings to create a complete re-implementation of <cite>Cherry and Guo (2015)</cite> . We can see that these representations have a huge impact on NER performance for all dev and test sets. We then performed a careful hyper-parameter sweep using the two provided development sets, resulting in the Inc. Regularization system. The hyper-parameters suggested by <cite>Cherry and Guo (2015)</cite> (E=10, C=0.01, P=10) were selected to work well with and without representations. We found that once we have committed to using representations, the tagger benefits from increased regularization, so long as we allow the model to converge (E=30, C=0.001, P=8). Although we revisited these settings periodically, these hyperparameters have proved to be quite stable, and we use them for all remaining experiments.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_8",
  "x": "If we remove these 23 tweets from the test set, our submitted system increases is performance to a Precision / Recall / F-measure of 53.5 / 40.0 / 45.8, while our best post-competition system decreases to 61.0 / 46.2 / 52.6, narrowing the gap in F-measure by 2.6 points. The ability to extract entities from formulaic bots such as this one could be useful, but the core purpose of NER technology is to enable the extraction of information from human-written text. ---------------------------------- **CONCLUSION** We have summarized our entry to the first W-NUT Named Entity Recognition in Twitter task. Our entry extends the work of <cite>Cherry and Guo (2015)</cite> with updated lexicons, phrase embeddings, and gazetteer-infused phrase embeddings. Our gazetteer infusion technique is novel in that it allows us to adapt existing vectors, regardless of their source. Taken together with improved hyper-parameters, these extensions improve the approach of <cite>Cherry and Guo (2015)</cite> by 2.6 Fmeasure on a completely blind test. Our final submission achieves a test F-measure of 44.7, placing third in the competition, and could have achieved an F-measure of 54.2 had we included all development data as training data.",
  "y": "differences extends"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_9",
  "x": "If we remove these 23 tweets from the test set, our submitted system increases is performance to a Precision / Recall / F-measure of 53.5 / 40.0 / 45.8, while our best post-competition system decreases to 61.0 / 46.2 / 52.6, narrowing the gap in F-measure by 2.6 points. The ability to extract entities from formulaic bots such as this one could be useful, but the core purpose of NER technology is to enable the extraction of information from human-written text. ---------------------------------- **CONCLUSION** We have summarized our entry to the first W-NUT Named Entity Recognition in Twitter task. Our entry extends the work of <cite>Cherry and Guo (2015)</cite> with updated lexicons, phrase embeddings, and gazetteer-infused phrase embeddings. Our gazetteer infusion technique is novel in that it allows us to adapt existing vectors, regardless of their source. Taken together with improved hyper-parameters, these extensions improve the approach of <cite>Cherry and Guo (2015)</cite> by 2.6 Fmeasure on a completely blind test. Our final submission achieves a test F-measure of 44.7, placing third in the competition, and could have achieved an F-measure of 54.2 had we included all development data as training data.",
  "y": "differences extends"
 },
 {
  "id": "65fbdd0397473763bca35376d581be_0",
  "x": "Text simplification has been found to be beneficial for language learners (Shirzadi, 2014) , children (Kajiwara et al., 2013) , and adults with low literacy skills (Arnaldo Candido Jr. and Erick Maziero and Caroline Gasperin and Thiago A. S. Pardo and Lucia Specia and Sandra M. Aluisio, 2009) or language disabilities (John Carroll and Guido Minnen and Darren Pearce and Yvonne Canning and Siobhan Devlin and John Tait, 1999; Luz Rello and Ricardo Baeza-Yates, 2014) . To cater to these target reader populations, language teachers, linguists and other editors are often called upon to manually adapt a text. To automate this time-consuming task, there has been much effort in developing systems for lexical simplification (Zhu et al., 2010; Biran et al., 2011) and syntactic simplification (Siddharthan, 2002; Siddharthan and Angrosh, 2014) . The performance of the state-of-the-art systems has improved significantly<cite> (Horn et al., 2014</cite>; Siddharthan and Angrosh, 2014) . Nonetheless, one cannot expect any single system, trained on a particular dataset, to simplify arbitrary texts in a way that would suit all readers -for example, the kinds of English words and structures suitable for a native speaker in Grade 6 are unlikely to be suitable for a non-native speaker in Grade 4. Hence, human effort is generally needed for modifying the system output. To support human post-editing, a number of researchers have developed specialized editors for text simplification. While the editor described in Max (2006) shares similar goals as ours, it requires human intervention in much of the simplification process. The Automatic Text Adaptation tool suggests synonyms (Burstein et al., 2007) , but does not perform syntactic simplification.",
  "y": "background"
 },
 {
  "id": "65fbdd0397473763bca35376d581be_1",
  "x": "For example, many governments have drawn up graded vocabulary lists to guide students of English as a foreign language; likewise, developers of machine translation systems have specified controlled languages with restricted vocabulary. In this context, lexical simplification can be defined as follows: to rewrite a sentence by replacing all words that are not in the given vocabulary list (and hence presumed to be difficult for the reader) with those from the list (and hence presumed to be simple). For example, Kajiwara et al. (2013) performed lexical simplification based on 5,404 words that elementary school children are expected to know. ---------------------------------- **ALGORITHM** By default, the editor uses a list of approximately 4,000 words that all students in Hong Kong are expected to know upon graduation from primary school (EDB, 2012). However, the user can also upload his or her own vocabulary list. Given an input sentence, we first identify the target words, namely those words that do not appear in the vocabulary list. Following <cite>Horn et al. (2014)</cite> , our system simplifies neither proper nouns, as identified by the Natural Language Toolkit (Bird et al., 2009) , nor words in our stoplist, which are already simple.",
  "y": "uses"
 },
 {
  "id": "65fbdd0397473763bca35376d581be_2",
  "x": "In the next step, substitution selection, we re-rank these 20 words with a language model. We trained a trigram model with the kenlm (Heafield, 2011) , again using all sentences from Wikipedia. We then place the 10 words with the highest probabilities in a drop-down list in our editor 2 ; for example, Figure 1 shows the ten candidates offered for the word \"municipal\". If none of the candidates are appropriate, the user can easily revert to the original word, which is also included in the drop-down list; alternatively, the user can click on the text to directly edit it. ---------------------------------- **EVALUATION** We evaluated the performance of our algorithm on the Mechanical Turk Lexical Simplification Data Set<cite> (Horn et al., 2014)</cite> . This dataset contains 500 manually annotated sentences; the target word in each sentence was annotated by 50 independent annotators. To simulate a teacher adapting an English text for Hong Kong pupils, we used the vocabulary list from the Hong Kong Education Bureau (EDB, 2012) .",
  "y": "uses"
 },
 {
  "id": "65fbdd0397473763bca35376d581be_3",
  "x": "Finally, in the regeneration step, the editor restores the subject (e.g., \"City of Faizabad\") to newly formed sentences. Often, this step also requires generation of referring expressions, determiners, conjunctions and sentence re-ordering. Since most of these tasks require real-world knowledge, the editor currently leaves it to the user for post-editing. ---------------------------------- **EVALUATION** We evaluated the quality of syntactic simplification on the first 300 sentences in the Mechanical Turk Lexical Simplification Data Set<cite> (Horn et al., 2014)</cite> . For each sentence, we asked a professor of linguistics to mark the types of syntactic simplification ( ---------------------------------- **CONCLUSIONS AND FUTURE WORK**",
  "y": "uses"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_0",
  "x": "The current version takes into account also the instructions given to the participants of TUNA trials regarding the use of location information, showing an overall improvement on string-edit distance values driven by the results on the Furniture domain. ---------------------------------- **INTRODUCTION** In previous work <cite>(Lucena & Paraboni, 2008)</cite> we presented a frequency-based greedy attribute selection strategy submitted to the TUNA Challenge 2008. Presently we further the issue by taking additional information into accountnamely, the trial condition information available from the TUNA data -and report improved results for string-edit distance as required for the 2009 competition. ---------------------------------- **BACKGROUND** In<cite> Lucena & Paraboni (2008)</cite> we presented a combined strategy based on attribute frequency and certain aspects of a greedy attribute selection strategy for referring expressions generation. A list P of attributes sorted by frequency is the centre piece of the following selection strategy:",
  "y": "background"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_1",
  "x": "The current version takes into account also the instructions given to the participants of TUNA trials regarding the use of location information, showing an overall improvement on string-edit distance values driven by the results on the Furniture domain. ---------------------------------- **INTRODUCTION** In previous work <cite>(Lucena & Paraboni, 2008)</cite> we presented a frequency-based greedy attribute selection strategy submitted to the TUNA Challenge 2008. Presently we further the issue by taking additional information into accountnamely, the trial condition information available from the TUNA data -and report improved results for string-edit distance as required for the 2009 competition. ---------------------------------- **BACKGROUND** In<cite> Lucena & Paraboni (2008)</cite> we presented a combined strategy based on attribute frequency and certain aspects of a greedy attribute selection strategy for referring expressions generation. A list P of attributes sorted by frequency is the centre piece of the following selection strategy:",
  "y": "extends background"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_2",
  "x": "**INTRODUCTION** In previous work <cite>(Lucena & Paraboni, 2008)</cite> we presented a frequency-based greedy attribute selection strategy submitted to the TUNA Challenge 2008. Presently we further the issue by taking additional information into accountnamely, the trial condition information available from the TUNA data -and report improved results for string-edit distance as required for the 2009 competition. ---------------------------------- **BACKGROUND** In<cite> Lucena & Paraboni (2008)</cite> we presented a combined strategy based on attribute frequency and certain aspects of a greedy attribute selection strategy for referring expressions generation. A list P of attributes sorted by frequency is the centre piece of the following selection strategy: \u2022 select all attributes whose relative frequency falls above a threshold value t (t was estimated to be 0.8 for both Furniture and People domains.) \u2022 if the resulting description uniquely describes the target object, then finalizes. \u2022 if not, starting from the most frequent attribute in P, search exhaustively for an attribute g such that g, if selected, would rule out all remaining distractors in the context.",
  "y": "background"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_3",
  "x": "\u2022 select all attributes whose relative frequency falls above a threshold value t (t was estimated to be 0.8 for both Furniture and People domains.) \u2022 if the resulting description uniquely describes the target object, then finalizes. \u2022 if not, starting from the most frequent attribute in P, search exhaustively for an attribute g such that g, if selected, would rule out all remaining distractors in the context. The overall effect obtained is twofold: on the one hand, in a complex situation of reference (in which many attributes may rule out many distractors, but more than one will be required to achieve uniqueness) the algorithm simply selects frequent attributes. This may be comparable to a human speaker who has to single out the target object but who does not have the means to come up with the 'right' attribute straight away. On the other hand, as the number of distractors decreases, a single attribute capable of ruling out all distractors will eventually emerge, forcing the algorithm to switch to a greedy strategy and finalize. Once again, this may be comparable to what a human speaker may do when an appropriate attribute becomes sufficiently salient and all distractors in the context can be ruled out at once. The above approach performed fairly well (at least considering its simplicity) as reported in<cite> Lucena & Paraboni (2008)</cite> . However, there is one major source of information available from the TUNA data that was not taken into account in the above strategy: the trial condition represented by the +/-LOC feature. Because this feature distinguishes the very kinds of instruction given to each participant to complete the TUNA task, the information provided by -/+ LOC is likely to have a significant impact on the overall results.",
  "y": "background"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_4",
  "x": "On the other hand, as the number of distractors decreases, a single attribute capable of ruling out all distractors will eventually emerge, forcing the algorithm to switch to a greedy strategy and finalize. Once again, this may be comparable to what a human speaker may do when an appropriate attribute becomes sufficiently salient and all distractors in the context can be ruled out at once. The above approach performed fairly well (at least considering its simplicity) as reported in<cite> Lucena & Paraboni (2008)</cite> . However, there is one major source of information available from the TUNA data that was not taken into account in the above strategy: the trial condition represented by the +/-LOC feature. Because this feature distinguishes the very kinds of instruction given to each participant to complete the TUNA task, the information provided by -/+ LOC is likely to have a significant impact on the overall results. This clear gap in our previous work represents an opportunity for improvement discussed in the next section. ---------------------------------- **ALGORITHM** The present work is a refined version of the original frequency-based greedy attribute selection strategy submitted to the TUNA Challenge 2008 <cite>(Lucena & Paraboni, 2008)</cite> , now taking also the trial condition (+/-LOC) into account.",
  "y": "motivation"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_5",
  "x": "On the other hand, as the number of distractors decreases, a single attribute capable of ruling out all distractors will eventually emerge, forcing the algorithm to switch to a greedy strategy and finalize. Once again, this may be comparable to what a human speaker may do when an appropriate attribute becomes sufficiently salient and all distractors in the context can be ruled out at once. The above approach performed fairly well (at least considering its simplicity) as reported in<cite> Lucena & Paraboni (2008)</cite> . However, there is one major source of information available from the TUNA data that was not taken into account in the above strategy: the trial condition represented by the +/-LOC feature. Because this feature distinguishes the very kinds of instruction given to each participant to complete the TUNA task, the information provided by -/+ LOC is likely to have a significant impact on the overall results. This clear gap in our previous work represents an opportunity for improvement discussed in the next section. ---------------------------------- **ALGORITHM** The present work is a refined version of the original frequency-based greedy attribute selection strategy submitted to the TUNA Challenge 2008 <cite>(Lucena & Paraboni, 2008)</cite> , now taking also the trial condition (+/-LOC) into account.",
  "y": "extends"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_6",
  "x": "Our modified algorithm simply consists of computing separated frequency lists for +LOC and -LOC trial conditions, and then using the original frequency-based greedy approach with each list accordingly. In practice, descriptions are now generated in two different ways, depending on the trial condition, which may promote the Xand Y-DIMENSION attributes to higher positions in the list P when +LOC applies. Using the TUNA Challenge 2009 development data set, the attribute selection task was performed as above. For the surface realisation task, we have reused the English language surface realisation module provided by Irene Langkilde-Geary for the TUNA Challenge 2008. ---------------------------------- **RESULTS** The following Figure 1 shows mean sting-edit distance and BLEU-3 scores computed using the evaluation tool provided by the TUNA Challenge team. For ease of comparison with our previous work, we also present Dice and MASI scores computed as in the previous TUNA Challenge, although these scores were not required for the current competition. The most relevant comparison with our previous work is observed in the overall string-edit distance values in Figure 1 : considering that in<cite> Lucena & Paraboni (2008)</cite> we reported 6.12 editdistance for Furniture and 7.38 for People, the overall improvement (driven by the descriptions in the Furniture domain) may be explained by the fact that the current version makes more accurate decisions as to when to use these attributes according to the instructions given to the participants of the TUNA trials (the trial condition +/-LOC. )",
  "y": "differences"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_0",
  "x": "It is therefore desirable for relation extraction models to have the capability to learn continuously without catastrophic forgetting of previously learned relations. This would enable them exploit newly available supervision to both identify novel relations and improve performance without substantial retraining. Recently, <cite>Wang et al. (2019)</cite> introduced an embedding alignment approach to enable continual learning for relation extraction models. <cite>They</cite> consider a setting with streaming tasks, where each task consists of a number of distinct relations, and proposed to align the representation of relation instances in the embedding space to enable continual learning of new relations without forgetting knowledge from past relations. While <cite>they</cite> obtained promising results, a key weakness of the approach is that the use of an alignment model introduces additional parameters to already overparameterized relation extraction models, which may in turn lead to an increase in the quantity of supervision required for training. In addition, the approach can only align embeddings between observed relations, and does not have any explicit objective that encourages the model to transfer and exploit knowledge gathered from previously observed relations to facilitate the efficient learning of yet to be observed relations. In this work, we extend the work of <cite>Wang et al. (2019)</cite> by exploiting ideas from both lifelong learning and meta-learning. We propose to consider lifelong relation extraction as a metalearning challenge, to which the machinery of current optimization-based meta-learning algorithms can be applied. Unlike the use of a separate alignment model as proposed in <cite>Wang et al. (2019)</cite> , the proposed approach does not introduce additional parameters.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_1",
  "x": "**INTRODUCTION** The majority of existing supervised relation extraction models can only extract a fixed set of relations which has been specified at training time. They are unable to detect an evolving set of novel relations observed after training without substantial retraining, which can be computationally expensive and may lead to catastrophic forgetting of previously learned relations. Zero-shot relation extraction approaches (Rockt\u00e4schel et al., 2015; Demeester et al., 2016; Levy et al., 2017; Obamuyide and Vlachos, 2018) can extract unseen relations, but at lower performance levels, and are unable to continually exploit newly available supervision to improve performance without considerable retraining. These limitations also extend to approaches to extracting relations in other limited supervision settings, for instance in the oneshot setting (Obamuyide and Vlachos, 2017) . It is therefore desirable for relation extraction models to have the capability to learn continuously without catastrophic forgetting of previously learned relations. This would enable them exploit newly available supervision to both identify novel relations and improve performance without substantial retraining. Recently, <cite>Wang et al. (2019)</cite> introduced an embedding alignment approach to enable continual learning for relation extraction models. <cite>They</cite> consider a setting with streaming tasks, where each task consists of a number of distinct relations, and proposed to align the representation of relation instances in the embedding space to enable continual learning of new relations without forgetting knowledge from past relations.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_2",
  "x": "It is therefore desirable for relation extraction models to have the capability to learn continuously without catastrophic forgetting of previously learned relations. This would enable them exploit newly available supervision to both identify novel relations and improve performance without substantial retraining. Recently, <cite>Wang et al. (2019)</cite> introduced an embedding alignment approach to enable continual learning for relation extraction models. <cite>They</cite> consider a setting with streaming tasks, where each task consists of a number of distinct relations, and proposed to align the representation of relation instances in the embedding space to enable continual learning of new relations without forgetting knowledge from past relations. While <cite>they</cite> obtained promising results, a key weakness of the approach is that the use of an alignment model introduces additional parameters to already overparameterized relation extraction models, which may in turn lead to an increase in the quantity of supervision required for training. In addition, the approach can only align embeddings between observed relations, and does not have any explicit objective that encourages the model to transfer and exploit knowledge gathered from previously observed relations to facilitate the efficient learning of yet to be observed relations. In this work, we extend the work of <cite>Wang et al. (2019)</cite> by exploiting ideas from both lifelong learning and meta-learning. We propose to consider lifelong relation extraction as a metalearning challenge, to which the machinery of current optimization-based meta-learning algorithms can be applied. Unlike the use of a separate alignment model as proposed in <cite>Wang et al. (2019)</cite> , the proposed approach does not introduce additional parameters.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_3",
  "x": "This would enable them exploit newly available supervision to both identify novel relations and improve performance without substantial retraining. Recently, <cite>Wang et al. (2019)</cite> introduced an embedding alignment approach to enable continual learning for relation extraction models. <cite>They</cite> consider a setting with streaming tasks, where each task consists of a number of distinct relations, and proposed to align the representation of relation instances in the embedding space to enable continual learning of new relations without forgetting knowledge from past relations. While <cite>they</cite> obtained promising results, a key weakness of the approach is that the use of an alignment model introduces additional parameters to already overparameterized relation extraction models, which may in turn lead to an increase in the quantity of supervision required for training. In addition, the approach can only align embeddings between observed relations, and does not have any explicit objective that encourages the model to transfer and exploit knowledge gathered from previously observed relations to facilitate the efficient learning of yet to be observed relations. In this work, we extend the work of <cite>Wang et al. (2019)</cite> by exploiting ideas from both lifelong learning and meta-learning. We propose to consider lifelong relation extraction as a metalearning challenge, to which the machinery of current optimization-based meta-learning algorithms can be applied. Unlike the use of a separate alignment model as proposed in <cite>Wang et al. (2019)</cite> , the proposed approach does not introduce additional parameters. In addition, the proposed approach is more data efficient since it explicitly optimizes for the transfer of knowledge from past relations, while avoiding the catastrophic forgetting of previously learned relations.",
  "y": "extends"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_4",
  "x": "In addition, the approach can only align embeddings between observed relations, and does not have any explicit objective that encourages the model to transfer and exploit knowledge gathered from previously observed relations to facilitate the efficient learning of yet to be observed relations. In this work, we extend the work of <cite>Wang et al. (2019)</cite> by exploiting ideas from both lifelong learning and meta-learning. We propose to consider lifelong relation extraction as a metalearning challenge, to which the machinery of current optimization-based meta-learning algorithms can be applied. Unlike the use of a separate alignment model as proposed in <cite>Wang et al. (2019)</cite> , the proposed approach does not introduce additional parameters. In addition, the proposed approach is more data efficient since it explicitly optimizes for the transfer of knowledge from past relations, while avoiding the catastrophic forgetting of previously learned relations. Empirically, we evaluate on lifelong versions of the datasets by Bordes et al. (2015) and Han et al. (2018) and demonstrate con-siderable performance improvements over prior state-of-the-art approaches. ---------------------------------- **BACKGROUND** Lifelong Learning In the lifelong learning setting, also referred to as continual learning (Ring, 1994; Thrun, 1996; Zhao and Schmidhuber, 1996) , a model f \u03b8 is presented with a sequence of tasks {T t } t=1,2,3..,T , one task per round, and the goal is to learn model parameters {\u03b8 t } t=1,2,3,..,T with the best performance on the observed tasks.",
  "y": "extends"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_5",
  "x": "Update \u03b8 \u2190 \u03b8t 20: end while ---------------------------------- **RELATION CLASSIFICATION MODEL** In principle the learner model f \u03b8 could be any gradient-optimized relation extraction model. In order to use the same number of parameters and ensure fair comparison to <cite>Wang et al. (2019)</cite> , we adopt as the relation extraction model f \u03b8 the Hier- arachical Residual BiLSTM (HR-BiLSTM) model of Yu et al. (2017) , which is the same model used by <cite>Wang et al. (2019)</cite> for their experiments. The HR-BILSTM is a relation classifier which accepts as input a sentence and a candidate relation, then utilizes two Bidirectional Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005) (BiLSTM) units with shared parameters to process the Glove (Pennington et al., 2014) embeddings of words in the sentence and relation names, then selects the relation with the maximum cosine similarity to the sentence as its response. Hyperparameters Apart from the hyperparameters specific to meta-learning (such as the step size ), all other hyperparameters we use for the learner model are the same as used by <cite>Wang et al. (2019)</cite> . We also use the same buffer memory size (50) for each task. Note that the meta-learning algorithm uses SGD as the update rule (U), and does not add any additional trainable parameters to the learner model.",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_6",
  "x": "In order to use the same number of parameters and ensure fair comparison to <cite>Wang et al. (2019)</cite> , we adopt as the relation extraction model f \u03b8 the Hier- arachical Residual BiLSTM (HR-BiLSTM) model of Yu et al. (2017) , which is the same model used by <cite>Wang et al. (2019)</cite> for their experiments. The HR-BILSTM is a relation classifier which accepts as input a sentence and a candidate relation, then utilizes two Bidirectional Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005) (BiLSTM) units with shared parameters to process the Glove (Pennington et al., 2014) embeddings of words in the sentence and relation names, then selects the relation with the maximum cosine similarity to the sentence as its response. Hyperparameters Apart from the hyperparameters specific to meta-learning (such as the step size ), all other hyperparameters we use for the learner model are the same as used by <cite>Wang et al. (2019)</cite> . We also use the same buffer memory size (50) for each task. Note that the meta-learning algorithm uses SGD as the update rule (U), and does not add any additional trainable parameters to the learner model. ---------------------------------- **EXPERIMENTS** ---------------------------------- **SETUP**",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_7",
  "x": "Each experiment is run five (5) times and we report the average result. ---------------------------------- **DATASETS** We conduct experiments on <cite>Lifelong FewRel</cite> and Lifelong <cite>SimpleQuestions</cite> datasets, both introduced in <cite>Wang et al. (2019)</cite> . <cite>Lifelong FewRel</cite> is derived from the FewRel (Han et al., 2018) dataset, by partitioning its 80 relations into 10 distinct clusters made up of 8 relations each, with each cluster serving as a task where a sentence must be labeled with the correct relation. The 8 relations in each cluster were obtained by clustering the averaged Glove word embeddings of the relation names in the FewRel dataset. Each instance of the dataset contains a sentence, the relation it expresses and a set of randomly sampled negative relations. <cite>Lifelong SimpleQuestions</cite> was similarly obtained from the SimpleQuestions (Bordes et al., 2015) dataset, and is made up of 20 clusters of relations, with each cluster serving as a task. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_8",
  "x": "**SETUP** We conduct experiments in two settings. In the full supervision setting, we provide all models with all supervision available in the training set of each task. In the second, we limit the amount of supervision for each task to measure how the models are able to cope with limited supervision. Each experiment is run five (5) times and we report the average result. ---------------------------------- **DATASETS** We conduct experiments on <cite>Lifelong FewRel</cite> and Lifelong <cite>SimpleQuestions</cite> datasets, both introduced in <cite>Wang et al. (2019)</cite> . <cite>Lifelong FewRel</cite> is derived from the FewRel (Han et al., 2018) dataset, by partitioning its 80 relations into 10 distinct clusters made up of 8 relations each, with each cluster serving as a task where a sentence must be labeled with the correct relation.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_10",
  "x": "<cite>Lifelong SimpleQuestions</cite> was similarly obtained from the SimpleQuestions (Bordes et al., 2015) dataset, and is made up of 20 clusters of relations, with each cluster serving as a task. ---------------------------------- **EVALUATION METRICS** We report two measures, <cite>ACC whole</cite> and <cite>ACC avg</cite> , both introduced in <cite>Wang et al. (2019)</cite> . <cite>ACC whole</cite> measures accuracy on the test set of all tasks and gives a balanced measure of model performance on both observed (seen) and unobserved (unseen) tasks, and is the primary metric we report for all experiments. We also report <cite>ACC avg</cite> , which measures the average accuracy on the test set of only observed (seen) tasks. ---------------------------------- **RESULTS AND DISCUSSION** Full Supervision Results Table 1 gives both the <cite>ACC whole</cite> and <cite>ACC avg</cite> results of our approach compared to other approaches including Episodic Memory Replay (EMR) and its various embedding-aligned variants EA-EMR as proposed in <cite>Wang et al. (2019)</cite> .",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_11",
  "x": "<cite>Lifelong SimpleQuestions</cite> was similarly obtained from the SimpleQuestions (Bordes et al., 2015) dataset, and is made up of 20 clusters of relations, with each cluster serving as a task. ---------------------------------- **EVALUATION METRICS** We report two measures, <cite>ACC whole</cite> and <cite>ACC avg</cite> , both introduced in <cite>Wang et al. (2019)</cite> . <cite>ACC whole</cite> measures accuracy on the test set of all tasks and gives a balanced measure of model performance on both observed (seen) and unobserved (unseen) tasks, and is the primary metric we report for all experiments. We also report <cite>ACC avg</cite> , which measures the average accuracy on the test set of only observed (seen) tasks. ---------------------------------- **RESULTS AND DISCUSSION** Full Supervision Results Table 1 gives both the <cite>ACC whole</cite> and <cite>ACC avg</cite> results of our approach compared to other approaches including Episodic Memory Replay (EMR) and its various embedding-aligned variants EA-EMR as proposed in <cite>Wang et al. (2019)</cite> .",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_12",
  "x": "The 8 relations in each cluster were obtained by clustering the averaged Glove word embeddings of the relation names in the FewRel dataset. Each instance of the dataset contains a sentence, the relation it expresses and a set of randomly sampled negative relations. <cite>Lifelong SimpleQuestions</cite> was similarly obtained from the SimpleQuestions (Bordes et al., 2015) dataset, and is made up of 20 clusters of relations, with each cluster serving as a task. ---------------------------------- **EVALUATION METRICS** We report two measures, <cite>ACC whole</cite> and <cite>ACC avg</cite> , both introduced in <cite>Wang et al. (2019)</cite> . <cite>ACC whole</cite> measures accuracy on the test set of all tasks and gives a balanced measure of model performance on both observed (seen) and unobserved (unseen) tasks, and is the primary metric we report for all experiments. We also report <cite>ACC avg</cite> , which measures the average accuracy on the test set of only observed (seen) tasks. ----------------------------------",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_13",
  "x": "We also report <cite>ACC avg</cite> , which measures the average accuracy on the test set of only observed (seen) tasks. ---------------------------------- **RESULTS AND DISCUSSION** Full Supervision Results Table 1 gives both the <cite>ACC whole</cite> and <cite>ACC avg</cite> results of our approach compared to other approaches including Episodic Memory Replay (EMR) and its various embedding-aligned variants EA-EMR as proposed in <cite>Wang et al. (2019)</cite> . Across all metrics, our approach outperforms the previous approaches, demonstrating its effectiveness in this setting. This result is likely because our approach is able to efficiently learn new relations by exploiting knowledge from previously observed relations. ---------------------------------- **LIMITED SUPERVISION RESULTS** The aim of our limited supervision experiments is to compare the use of an alignment module as proposed by <cite>Wang et al. (2019)</cite> to using our approach when only limited supervision is available for all tasks.",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_14",
  "x": "Across all metrics, our approach outperforms the previous approaches, demonstrating its effectiveness in this setting. This result is likely because our approach is able to efficiently learn new relations by exploiting knowledge from previously observed relations. ---------------------------------- **LIMITED SUPERVISION RESULTS** The aim of our limited supervision experiments is to compare the use of an alignment module as proposed by <cite>Wang et al. (2019)</cite> to using our approach when only limited supervision is available for all tasks. We compare three approaches, Full EA-EMR (which uses their alignment module), its variant without the alignment module (EA-EMR NoAlign) and our approach (MLLRE). Figures 1(a) and 1(b) show results obtained using 100 supervision instances for each task on <cite>Lifelong FewRel</cite> and <cite>Lifelong SimpleQuestions</cite>. Figures 2(a) and 2(b) show the corresponding plots using 200 supervision instances for each task. From the figures, we observe that the use of a separate alignment model results in only minor gains when supervision for the tasks is limited, whereas the use of our approach leads to wide gains on both datasets.",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_15",
  "x": "We conduct experiments on <cite>Lifelong FewRel</cite> and Lifelong <cite>SimpleQuestions</cite> datasets, both introduced in <cite>Wang et al. (2019)</cite> . Figures 1(a) and 1(b) show results obtained using 100 supervision instances for each task on <cite>Lifelong FewRel</cite> and <cite>Lifelong SimpleQuestions</cite>.",
  "y": "uses"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_0",
  "x": "**INTRODUCTION** How many literary characters appear in a novel? Despite the seeming simplicity of the question, precisely identifying which characters appear in a story remains an open question in literary and narrative analysis. Characters form the core of many computational analyses, from inferring prototypical character types (Bamman et al., 2014) to identifying the structure of social networks in literature <cite>(Elson et al., 2010</cite>; Lee and Yeung, 2012; Agarwal et al., 2013; Ardanuy and Sporleder, 2014; Jayannavar et al., 2015) . These current approaches have largely assumed that characters can be reliably identified in text using standard techniques such as Named Entity Recognition (NER) and that the variations in how a character is named can be found through coreference resolution. However, such treatment of character identity often overlooks minor characters that serve to enrich the social structure and serve as foils for the identities of major characters (Eder et al., 2010) . This work provides a comprehensive examination of literary character detection, with three key contributions. First, we formalize the task with evaluation criteria and offer two datasets, including a complete, manually-annotated list of all characters in 58 literary works. Second, we propose a new technique for character detection based on inducing character prototypes, and in comparisons with three state-of-the-art methods, demonstrate superior performance, achieving significant improvements in F1 over the next-best method.",
  "y": "background"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_1",
  "x": "This work provides a comprehensive examination of literary character detection, with three key contributions. First, we formalize the task with evaluation criteria and offer two datasets, including a complete, manually-annotated list of all characters in 58 literary works. Second, we propose a new technique for character detection based on inducing character prototypes, and in comparisons with three state-of-the-art methods, demonstrate superior performance, achieving significant improvements in F1 over the next-best method. Third, as practical applications, we analyze literary trends in character density over 20 decades and revisit the character-based literary hypothesis tested by<cite> Elson et al. (2010)</cite> . ---------------------------------- **RELATED WORK** Character detection has primarily been performed in the context of mining literary social networks. Elson et al. (2010) extract character mentions from conversational segments, using the Stanford CoreNLP NER system to discover character names (Manning et al., 2014) . To account for variability in character naming, alternate forms of a name are generated using the method of Davis et al. (2003) and merged together as a single character.",
  "y": "uses"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_2",
  "x": "Agarwal et al. (2013) also rely on the CoreNLP NER and coreference resolution systems for character detection; however for literary analysis, they use gold character mentions that have been marked and resolved by a team of trained annotators, highlighting the difficulty of the task. He et al. (2013) propose an alternate approach for identifying speaker references in novels, using a probabilistic model to identify which character is speaking. However, to account for the multiple aliases used to refer to a character, the authors first manually constructed a list of characters and their aliases, which is the task proposed in this work and underscores the need for automated methods. Two approaches mined social interaction net-works without relying on dialogue, unlike the methods of<cite> Elson et al. (2010)</cite> and He et al. (2013) . Lee and Yeung (2012) build social networks by recognizing characters from explicit markers (e.g., kinship) and implicit markers (e.g., physical collocation). Similarly, Agarwal and Rambow (2010) build character networks using tree kernels on parse trees to identify interacting agents. In the two most-related works, Bamman et al. (2014) and Ardanuy and Sporleder (2014) , character names are extracted and clustered under a set of constraints. In the BookNLP system developed by Bamman et al. (2014) , NER-identified names are retained and merged based on animacy, determined through dependencies with \"sentient\" lemmas from a small dictionary (including for example, say and smile), and gender, assigned through pronomial resolution and a dictionary of genderspecific honorifics. Ardanuy and Sporleder (2014) similarly use NER to identify character name mentions.",
  "y": "background"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_3",
  "x": "---------------------------------- **COMPARISON SYSTEMS** The task of character recognition has largely been subsumed into the task of extracting the social network of novels. Therefore, three state-of-the-art systems for social network extraction were selected: the method described in<cite> Elson et al. (2010)</cite> , BookNLP (Bamman et al., 2014) , and the method described in Ardanuy and Sporleder (2014) . For each method, we follow their procedures for identifying the characters in the social network, which produces sets of one or more aliases associated with each identified character. As a baseline, we use the output of Stanford NER, where every name is considered a separate character; this baseline represents the upper-bound in recall from any system using only NER to identify character names. Table 1 shows the results for the manually-annotated and SparkNotes corpora. The Sherlock Holmes corpus presents a notable challenge due to the presence of many minor characters, which are not detected by NER. An error analysis for our approach revealed that while many characters were extracted, the coreference resolution did not link a characters' different referents together and hence, each name was reported as a separate character, which caused a drop in performance.",
  "y": "uses"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_4",
  "x": "Table 2 shows our system's performance without stage 7, which involved the extraction of minor characters. Stage 7 overall improves recall with a slight hindrance to precision. For the Sherlock Holmes corpus, stage 7 is slightly detrimental to overall performance, which as we stipulated earlier is caused by missing co-referent links. Finally, returning to the initially-posed question of how many characters are present, we find that despite the detection error in our method, the overall predicted number of characters is quite close to the actual: for Sherlock Holmes stories, the number of characters was estimated within 2.4 on average, for Pride and Prejudice our method predicted 72 compared with 73 actual characters, and for The Moonstone our method predicted 87 compared with 78. Thus, we argue that our procedure can provide a reasonable estimate for the total number of characters. (For comparison, BookNLP, the next best system, extracted 69 and 72 characters for Pride and Prejudice and The Moonstone, respectively, and within 1.2, on average, on the Sherlock Holmes set.) Experiment 2: Literary Theories<cite> Elson et al. (2010)</cite> analyze 60 novels to computationally test literary theories for novels in urban and rural settings (Williams, 1975; Moretti, 1999) . Recently, Jayannavar et al. (2015) challenged this analysis, showing their improved method for social network extraction did not support the same conclusions. While our work focuses only on character detection, we are nevertheless able to test the related hypothesis of whether the number of characters in novels with urban settings is more than those in rural. Character detection was run on the same novels from<cite> Elson et al. (2010)</cite> and we found no statistically-significant difference in the mean number of characters in urban and rural settings, even when accounting for text size.",
  "y": "uses"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_5",
  "x": "Recently, Jayannavar et al. (2015) challenged this analysis, showing their improved method for social network extraction did not support the same conclusions. While our work focuses only on character detection, we are nevertheless able to test the related hypothesis of whether the number of characters in novels with urban settings is more than those in rural. Character detection was run on the same novels from<cite> Elson et al. (2010)</cite> and we found no statistically-significant difference in the mean number of characters in urban and rural settings, even when accounting for text size. Thus, our work raises questions about how these character interact and whether the setting influences the structure of the social network, despite similar numbers of characters. Experiment 3: Historical Trends As a second application of our technique, we examine historical trends in how many characters appear in a novel. All fiction novels listed on Project Gutenberg were compiled and publication dates were automatically extracted for 1066 and manually entered for an additional 637. This set was combined with a corpus of 6333 novels, including works such as To The Lighthouse by Virginia Woolf, not available on Project Gutenberg. Books were then partitioned into the decade in which they were au- 1 8 4 0 -1 8 4 9 1 8 5 0 -1 8 5 9 1 8 6 0 -1 8 6 9 1 8 7 0 -1 8 7 9 1 8 8 0 -1 8 8 9 1 8 9 0 -1 8 9 9 1 9 0 0 -1 9 0 9 1 9 1 0 -1 9 1 9 1 9 2 0 -1 9 2 9 1 9 3 0 -1 9 3 9 1 9 4 0 -1 9 4 9 1 9 5 0 -1 9 5 9 1 9 6 0 -1 9 6 9 1 9 7 0 -1 9 7 9 1 9 8 0 -1 9 8 9 1 9 9 0 -1 9 9 9 Ratio of number of characters to book tokens Figure 2 : Distributions of the size-normalized number of characters per novel per decade.",
  "y": "uses"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_0",
  "x": "Abstractive summarization, the task of rewriting and compressing a document into a short summary, has achieved considerable success with neural sequence-tosequence models. However, these models can still benefit from stronger natural language inference skills, since a correct summary is logically entailed by the input document, i.e., it should not contain any contradictory or unrelated information. We incorporate such knowledge into an abstractive summarization model via multi-task learning, where we share its decoder parameters with those of an entailment generation model. We achieve promising initial improvements based on multiple metrics and datasets (including a test-only setting). The domain mismatch between the entailment (captions) and summarization (news) datasets suggests that the model is learning some domain-agnostic inference skills. ---------------------------------- **INTRODUCTION** Abstractive summarization, the task of rewriting a document into a short summary is a significantly more challenging (and natural) task than extractive summarization, which only involves choosing which sentence from the original document to keep or discard in the output summary. Neural sequence-to-sequence models have led to substantial improvements on this task of abstractive summarization, via machine translation inspired encoder-aligner-decoder approaches, further enhanced via convolutional encoders, pointer-copy mechanisms, and hierarchical attention (Rush et al., 2015;<cite> Nallapati et al., 2016</cite>; See et al., 2017) .",
  "y": "background"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_1",
  "x": "Neural sequence-to-sequence models have led to substantial improvements on this task of abstractive summarization, via machine translation inspired encoder-aligner-decoder approaches, further enhanced via convolutional encoders, pointer-copy mechanisms, and hierarchical attention (Rush et al., 2015;<cite> Nallapati et al., 2016</cite>; See et al., 2017) . Despite these promising recent improvements, Input Document: may is a pivotal month for moving and storage companies . Ground-truth Summary: moving companies hit bumps in economic road Baseline Summary: a month to move storage companies Multi-task Summary: pivotal month for storage firms there is still scope in better teaching summarization models about the general natural language inference skill of logical entailment generation. This is because the task of abstractive summarization involves two subtasks: salient (important) event detection as well as logical compression, i.e., the summary should not contain any information that is contradictory or unrelated to the original document. Current methods have to learn both these skills from the same dataset and a single model. Therefore, there is benefit in learning the latter ability of logical compression via external knowledge from a separate entailment generation task, that will specifically teach the model how to rewrite and compress a sentence such that it logically follows from the original input. To achieve this, we employ the recent paradigm of sequence-to-sequence multi-task learning (Luong et al., 2016) . We share the decoder parameters of the summarization model with those of the entailment-generation model, so as to generate summaries that are good at both extracting important facts from as well as being logically entailed by the input document. Fig. 1 shows such an (actual) output example from our model, where it successfully learns both salient information extraction as well as entailment, unlike the strong baseline model.",
  "y": "motivation"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_2",
  "x": "Empirically, we report promising initial improvements over some solid baselines based on several metrics, and on multiple datasets: Gigaword and also a test-only setting of DUC. Impor-tantly, these improvements are achieved despite the fact that the domain of the entailment dataset (image captions) is substantially different from the domain of the summarization datasets (general news), which suggests that the model is learning certain domain-independent inference skills. Our next steps to this workshop paper include incorporating stronger pointer-based models and employing the new multi-domain entailment corpus (Williams et al., 2017) . ---------------------------------- **RELATED WORK** Earlier summarization work focused more towards extractive (and compression) based summarization, i.e., selecting which sentences to keep vs discard, and also compressing based on choosing grammatically correct sub-sentences having the most important pieces of information (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015) . Bigger datasets and neural models have allowed the addressing of the complex reasoning involved in abstractive summarization, i.e., rewriting and compressing the input document into a new summary. Several advances have been made in this direction using machine translation inspired encoder-aligner-decoder models, convolution-based encoders, switching pointer and copy mechanisms, and hierarchical attention models (Rush et al., 2015;<cite> Nallapati et al., 2016</cite>; See et al., 2017) . Recognizing textual entailment (RTE) is the classification task of predicting whether the relationship between a premise and hypothesis sentence is that of entailment (i.e., logically follows), contradiction, or independence (Dagan et al., 2006) .",
  "y": "background"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_3",
  "x": "**SNLI CORPUS** For the task of entailment generation, we use the Standford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) , where we only use the entailment-labeled pairs and regroup the splits to have a zero overlap traintest split and have a multi-reference test set, as suggested by Pasunuru and Bansal (2017) . Out of 190, 113 entailments pairs, we use 145, 822 unique premise pairs for training, and the rest of them are equally divided into dev and test sets. ---------------------------------- **EVALUATION** Following previous work<cite> (Nallapati et al., 2016</cite>; Chopra et al., 2016; Rush et al., 2015) , we use the full-length F1 variant of Rouge (Lin, 2004) for the Gigaword results, and the 75-bytes length limited Recall variant of Rouge for DUC. Additionally, we also report other standard language generation metrics (as motivated recently by See et al. (2017) ): METEOR (Denkowski and Lavie, 2014) , BLEU-4 (Papineni et al., 2002) , and CIDEr-D , based on the MS-COCO evaluation script (Chen et al., 2015) . ---------------------------------- **TRAINING DETAILS**",
  "y": "uses"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_5",
  "x": "This suggests that the entailment generation model is teaching the summarization model some skills about how to choose a logical subset of the events in the full input document. This is especially promising given that the domain of the entailment dataset (image captions) is very different from the domain of the summarization datasets (news), suggesting that the model might be learning some domain-agnostic inference skills. ---------------------------------- **SUMMARIZATION RESULTS: DUC** Here, we directly use the Gigaword-trained model to test on the DUC-2004 dataset (see tuning discussion in Sec. 4.1). In Table 2 , we again see that et al. (2015) 28.18 8.49 23.81 Chopra et al. (2016) 28.97 8.26 24.06<cite> Nallapati et al. (2016)</cite> our Luong et al. (2015) baseline model achieves competitive performance with previous work, esp. on Rouge-2 and Rouge-L. Next, we show promising multi-task improvements over this baseline of around 0.4% across all metrics, despite being a test-only setting and also with the mismatch between the summarization and entailment domains. Figure 3 shows some additional interesting output examples of our multi-task model and how it generates summaries that are better at being logically entailed by the input document, whereas the baseline model contains some crucial contradictory or unrelated information. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_6",
  "x": "In Table 2 , we again see that et al. (2015) 28.18 8.49 23.81 Chopra et al. (2016) 28.97 8.26 24.06<cite> Nallapati et al. (2016)</cite> our Luong et al. (2015) baseline model achieves competitive performance with previous work, esp. on Rouge-2 and Rouge-L. Next, we show promising multi-task improvements over this baseline of around 0.4% across all metrics, despite being a test-only setting and also with the mismatch between the summarization and entailment domains. Figure 3 shows some additional interesting output examples of our multi-task model and how it generates summaries that are better at being logically entailed by the input document, whereas the baseline model contains some crucial contradictory or unrelated information. ---------------------------------- **ANALYSIS EXAMPLES** ---------------------------------- **CONCLUSION AND NEXT STEPS** We presented a multi-task learning approach to incorporate entailment generation knowledge into summarization models. We demonstrated promising initial improvements based on multiple datasets and metrics, even when the entailment knowledge was extracted from a domain different from the summarization domain.",
  "y": "future_work"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_0",
  "x": "However, this could be alleviated by exploiting annotation projection across parallel corpora to create more linguistically annotated resources for new languages. More importantly, applying annotation projection using several source languages would support the creation of corpora less biased towards the peculiarities of a single source annotation scheme. In our study, we aim at exploring the usability of annotation projection for the transfer of automatically produced coreference chains. In particular, our idea is that using several source annotations produced by different systems could improve the performance of the projection method. Our approach to the annotation projection builds upon the approach recently introduced by <cite>(Grishina and Stede, 2017)</cite> , who experimented with projecting manually annotated coreference chains from two source languages to the target language. However, our goal is slightly different: We are interested in developing a fully automatic pipeline, which would support the automatic creation of parallel annotated corpora in new languages. Therefore, in contrast to <cite>(Grishina and Stede, 2017)</cite> , we use automatic source annotations produced by two state-of-the-art coreference systems, and we combine the output of our projection method for two source languages (English and German) to obtain target annotations for a third language (Russian). Through performing the error analysis of the projected annotations, we investigate the most common projection errors and assess the benefits and drawbacks of our method. The paper is organized as follows: Section 2 presents an overview of the related work and Section 3 describes the experimental setup.",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_1",
  "x": "However, this could be alleviated by exploiting annotation projection across parallel corpora to create more linguistically annotated resources for new languages. More importantly, applying annotation projection using several source languages would support the creation of corpora less biased towards the peculiarities of a single source annotation scheme. In our study, we aim at exploring the usability of annotation projection for the transfer of automatically produced coreference chains. In particular, our idea is that using several source annotations produced by different systems could improve the performance of the projection method. Our approach to the annotation projection builds upon the approach recently introduced by <cite>(Grishina and Stede, 2017)</cite> , who experimented with projecting manually annotated coreference chains from two source languages to the target language. However, our goal is slightly different: We are interested in developing a fully automatic pipeline, which would support the automatic creation of parallel annotated corpora in new languages. Therefore, in contrast to <cite>(Grishina and Stede, 2017)</cite> , we use automatic source annotations produced by two state-of-the-art coreference systems, and we combine the output of our projection method for two source languages (English and German) to obtain target annotations for a third language (Russian). Through performing the error analysis of the projected annotations, we investigate the most common projection errors and assess the benefits and drawbacks of our method. The paper is organized as follows: Section 2 presents an overview of the related work and Section 3 describes the experimental setup.",
  "y": "differences extends"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_2",
  "x": "**RELATED WORK** Annotation projection is a method that allows for automatically transferring annotations from a well-studied (source) language to a low-resource (target) language in a parallel corpus in order to automatically obtain annotated data. It was first introduced in the work of (Yarowsky et al., 2001) Rahman and Ng, 2012; Martins, 2015; Grishina and Stede, 2015) . Thereafter, <cite>(Grishina and Stede, 2017)</cite> proposed a multi-source method for annotation projection: They used a manually annotated trilingual coreference corpus and two source languages (English-German, English-Russian) to transfer annotations to the target language (Russian and German, respectively). Although their approach showed promising results, it was based on transferring manually produced annotations, which are typically not available for other languages and, more importantly, can not be acquired large-scale due to the complexity of the annotation task. ---------------------------------- **ANNOTATION PROJECTION EXPERIMENT** In our experiment, we propose a fully automatic projection setup: First, we perform coreference resolution on the source language data and then we implement the single-and multi-source approaches to transfer the automatically produced annotations. We use the English-German-Russian unannotated corpus of <cite>(Grishina and Stede, 2017)</cite> as the basis for our experiment, which contains texts in two genres -newswire texts (229 sentences per language) and short stories (184 sentences per language).",
  "y": "background"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_3",
  "x": "**ANNOTATION PROJECTION EXPERIMENT** In our experiment, we propose a fully automatic projection setup: First, we perform coreference resolution on the source language data and then we implement the single-and multi-source approaches to transfer the automatically produced annotations. We use the English-German-Russian unannotated corpus of <cite>(Grishina and Stede, 2017)</cite> as the basis for our experiment, which contains texts in two genres -newswire texts (229 sentences per language) and short stories (184 sentences per language). Furthermore, we use manual annotations present in the corpus as the gold standard for our evaluation. It should be noted that the manual annotations were performed according to the parallel coreference annotation guidelines of (Grishina and Stede, 2016) that are in general compatible with the annotation of the the OntoNotes corpus (Hovy et al., 2006) and are therefore suitable for our evaluation. ---------------------------------- **COREFERENCE RESOLUTION ON THE SOURCE LANGUAGE DATA** Since the main goal of this experiment is to assess the quality of the projection of automatic annotations, first we need to automatically label the source language data. For the English side of the corpus, we chose the Berkeley Entity Resolution system (Durrett and Klein, 2014) , which was trained on the English part of the OntoNotes corpus (Hovy et al., 2006) and achieves the average F1 of 61.71 on the OntoNotes dataset (Durrett and Klein, 2014).",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_4",
  "x": "Thereafter, we re-implement the multi-source approach as described in <cite>(Grishina and Stede, 2017)</cite> . In particular, they (a) looked at disjoint chains coming from different sources and (b) used the notion of chain overlap to measure the similarity between two coreference chains that contain some identical mentions 3 . In our experiment, we apply the following strategies from <cite>(Grishina and Stede, 2017)</cite>: 1. Setting 1 ('add'): disjoint chains from one source language are added to all the chains projected from the other source language; 2. Setting 2 ('unify-intersect'): the intersection of mentions for overlapping chains is selected. 3. Setting 3 ('unify-concatenate'): chains that overlap are treated as one chain starting from a certain percentage of overlap. For both single-and multi-source approaches, we deliberately rely solely on word alignment information to project the annotations, in order to keep our approach easily transferable to other languages. 2 Sentence alignment was performed using HunAlign (Varga et al., 2007) ; word alignments were computed with GIZA++ (Och and Ney, 2003) on a parallel newswire corpus (Grishina and Stede, 2015) . 3 Computed as Dice coefficient. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_5",
  "x": "To estimate the quality of the automatically produced annotations, we evaluate the resulting dataset against the manually annotated English and German parts of the corpus ( Table 3 : Projection results from English and German into Russian ---------------------------------- **ANNOTATION PROJECTION STRATEGIES** For our experiment, we implement a direct projection method for coreference as described in (Grishina and Stede, 2015) . Our method works as follows: For each markable on the source side, we automatically select all the corresponding tokens on the target side aligned to it, and we then take the span between the first and the last word as the new target markable, which has the same coreference chain number as the source one. Since the corpus was already sentence-and word-aligned 2 , we use the available alignments to transfer the annotations. Thereafter, we re-implement the multi-source approach as described in <cite>(Grishina and Stede, 2017)</cite> . In particular, they (a) looked at disjoint chains coming from different sources and (b) used the notion of chain overlap to measure the similarity between two coreference chains that contain some identical mentions 3 . In our experiment, we apply the following strategies from <cite>(Grishina and Stede, 2017)</cite>:",
  "y": "uses similarities"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_6",
  "x": "3 Computed as Dice coefficient. ---------------------------------- **RESULTS** To evaluate the projection results, we computed the standard coreference metrics -MUC (Vilain et al., 1995) , B-cubed (Bagga and Baldwin, 1998) and CEAF (Luo, 2005) -and their average for each of the approaches (Table 3) . As one can see from the table, the quality of projections from English to Russian outperforms the quality of projections from German to Russian by 6.5 points F1. Moreover, while Precision number are quite similar, projections from English exhibit higher Recall numbers. As for the multi-source settings, we were able to achieve the highest F1 of 36.2 by combining disjoint chains (Setting 1), which is 1.9 point higher than the best single-source projection scores and constitutes almost 62% of the quality of the projection of gold standard annotations reported in <cite>(Grishina and Stede, 2017)</cite> . We were able to achieve the highest Precision scores by intersecting the overlapping chains (Setting 2) and the highest Recall by concatenating them (Setting 3). Finally, we evaluate the annotations coming from English and German against each other, in order to estimate their comparability and the percentage of overlap.",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_7",
  "x": "---------------------------------- **ENGLISH** German # % # % Markables 757 82.7 596 57.6 Chains 182 100.0 227 84.7 Table 4 : Transferred chains and markables Since we do not have access to any gold alignment data, we estimate the quality of the word alignments by computing the number of unaligned tokens. Not surprisingly, we see a higher percentage of unaligned words for German-Russian than for English-Russian: 17.03% vs. 14.96% respectively, which supports our hypothesis regarding the difference in the alignment quality for the two pairs. Furthermore, we computed the distribution of unaligned words: The highest percentage of unaligned tokens disregarding punctuation marks are prepositions; pronouns constitute only 3% and 5% of all unaligned words for the alignments between English-Russian and German-Russian respectively. However, these numbers do not constitute more than 5% of the overall number of pronouns in the corpus. Following the work of <cite>(Grishina and Stede, 2017)</cite> , we analyse the projection accuracy for common nouns ('Nc'), named entities ('Np') and pronouns ('P') separately 4 : Table 5 shows the percentage of correctly projected markables of each type out of all the projected markables of this type. Our results conform to the results of <cite>(Grishina and Stede, 2017)</cite> : For both languages, pronouns exhibit the highest projection quality, while common and proper nouns are projected slightly less accurately, which is probably due to the fact that pronouns typically consist of single tokens and are better aligned than multi-token common and proper names.",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_8",
  "x": "---------------------------------- **ENGLISH** German # % # % Markables 757 82.7 596 57.6 Chains 182 100.0 227 84.7 Table 4 : Transferred chains and markables Since we do not have access to any gold alignment data, we estimate the quality of the word alignments by computing the number of unaligned tokens. Not surprisingly, we see a higher percentage of unaligned words for German-Russian than for English-Russian: 17.03% vs. 14.96% respectively, which supports our hypothesis regarding the difference in the alignment quality for the two pairs. Furthermore, we computed the distribution of unaligned words: The highest percentage of unaligned tokens disregarding punctuation marks are prepositions; pronouns constitute only 3% and 5% of all unaligned words for the alignments between English-Russian and German-Russian respectively. However, these numbers do not constitute more than 5% of the overall number of pronouns in the corpus. Following the work of <cite>(Grishina and Stede, 2017)</cite> , we analyse the projection accuracy for common nouns ('Nc'), named entities ('Np') and pronouns ('P') separately 4 : Table 5 shows the percentage of correctly projected markables of each type out of all the projected markables of this type. Our results conform to the results of <cite>(Grishina and Stede, 2017)</cite> : For both languages, pronouns exhibit the highest projection quality, while common and proper nouns are projected slightly less accurately, which is probably due to the fact that pronouns typically consist of single tokens and are better aligned than multi-token common and proper names.",
  "y": "differences"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_0",
  "x": "**INTRODUCTION** Raw speech waveforms are densely sampled in time, and thus require downsampling to make many analysis techniques computationally tractable. For speech recognition, this presents the challenge to reduce the number of timesteps in the signal without throwing away relevant information. Representations based on the Fourier transform have proven effective at this task as the transform forms a complete basis for signal reconstruction. Deep learning's recent success in speech recognition is based on learning feature hierarchies atop these representations [1,<cite> 2]</cite> . There has been increasing focus on extending this end-toend learning approach down to the level of the raw waveform. A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8] . While some studies find inferior performance for convolutional filters learned in this way, deeper networks have recently matched the performance of hand-engineered features on large vocabulary speech recognition tasks [4] . Features based on the Fourier transform are computationally efficient, but exhibit an intrinsic tradeoff of temporal and frequency resolution.",
  "y": "background"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_1",
  "x": "While much research has already been conducted on learn- ing directly from waveforms for speech recognition, the unique contributions of this paper are threefold: \u2022 We perform with an in-depth analysis of scaling to low strides and large numbers of filters and discover that a convolutional front end can significantly outperform Fourier features by independently tuning temporal and frequency resolution, at the cost of additional computation and memory. \u2022 We propose a new multiscale convolutional front end, composed of concatenated filters with different window sizes, that requires less computation and outperforms features learned on a single scale (20.7% relative to spectrogram baseline). \u2022 We find that multiscale features naturally learn the frequencies they can most efficiently represent, with large and small windows learning low and high frequencies respectively. This contrasts with the single scale features which try to cover the entire frequency spectrum regardless of window size. ---------------------------------- **EXPERIMENTAL SETUP** The experimental design of this study is modelled after our previous work on end-to-end speech recognition [12,<cite> 2]</cite> . However, to decrease the experimental latency, we train on a reduced version of the model and a subset of the training data.",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_2",
  "x": "**EXPERIMENTAL SETUP** The experimental design of this study is modelled after our previous work on end-to-end speech recognition [12,<cite> 2]</cite> . However, to decrease the experimental latency, we train on a reduced version of the model and a subset of the training data. The basic architecture is shown in Table 1 . While we vary the front end processing, the backend remains the same: a convolutional (through time) layer, followed by three bidirectional simple recurrent layers, and a fully connected layer. Batch normalization [13] , is employed between each layer, but not between individual timesteps <cite>[2]</cite> . Rectified linear unit (ReLU) activation functions are used for all layers, including between timesteps. We use the Connectionist Temporal Classification (CTC) cost function to integrate over all possible alignments between the network outputs and characters of the English alphabet [14] . Training is conducted on 2,400 hours of audio randomly sampled from 12,000 hours of data.",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_3",
  "x": "While we vary the front end processing, the backend remains the same: a convolutional (through time) layer, followed by three bidirectional simple recurrent layers, and a fully connected layer. Batch normalization [13] , is employed between each layer, but not between individual timesteps <cite>[2]</cite> . Rectified linear unit (ReLU) activation functions are used for all layers, including between timesteps. We use the Connectionist Temporal Classification (CTC) cost function to integrate over all possible alignments between the network outputs and characters of the English alphabet [14] . Training is conducted on 2,400 hours of audio randomly sampled from 12,000 hours of data. The training data is drawn from a diverse collection of sources including read, conversational, accented, and noisy speech <cite>[2]</cite> . At each epoch, 40% of the utterances are randomly selected to have background noise Table 2 : Single scale waveform convolution outperforms the spectrogram baseline at low strides. The trend is visualized in Figure 2 . (superpositions of YouTube clips) added at signal-to-noise ratios ranging from 0dB to 15dB [12] .",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_4",
  "x": "Hyperparameters are tuned for each model by optimizing a hold-out set. Typical values are a learning rate of 3e-4 and momentum of 0.99, and training converges after 20 epochs. Following <cite>[2]</cite> , we sort the first epoch by utterance length (SortaGrad), to promote stability of training on long utterances. While the CTC-trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement it with a Kneser-Ney smoothed 5-gram model that is trained using the KenLM toolkit [15] on cleaned text from the Common Crawl Repository. Decoding is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function. Test set word error rates (WER) are reported on a difficult in-house test set of 2048 utterances, diversely composed of noisy, conversational, voice-command, and accented speech. The test set is collected internally and from industry partners and is not represented in the training data. As previously observed <cite>[2]</cite> , deep neural networks trained on sufficient data perform better as the model size grows. In order to make fair comparisons, the number of parameters of the models used in our experiments is held constant at 35M.",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_5",
  "x": "Hyperparameters are tuned for each model by optimizing a hold-out set. Typical values are a learning rate of 3e-4 and momentum of 0.99, and training converges after 20 epochs. Following <cite>[2]</cite> , we sort the first epoch by utterance length (SortaGrad), to promote stability of training on long utterances. While the CTC-trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement it with a Kneser-Ney smoothed 5-gram model that is trained using the KenLM toolkit [15] on cleaned text from the Common Crawl Repository. Decoding is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function. Test set word error rates (WER) are reported on a difficult in-house test set of 2048 utterances, diversely composed of noisy, conversational, voice-command, and accented speech. The test set is collected internally and from industry partners and is not represented in the training data. As previously observed <cite>[2]</cite> , deep neural networks trained on sufficient data perform better as the model size grows. In order to make fair comparisons, the number of parameters of the models used in our experiments is held constant at 35M.",
  "y": "uses similarities"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_6",
  "x": "Typical values are a learning rate of 3e-4 and momentum of 0.99, and training converges after 20 epochs. Following <cite>[2]</cite> , we sort the first epoch by utterance length (SortaGrad), to promote stability of training on long utterances. While the CTC-trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement it with a Kneser-Ney smoothed 5-gram model that is trained using the KenLM toolkit [15] on cleaned text from the Common Crawl Repository. Decoding is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function. Test set word error rates (WER) are reported on a difficult in-house test set of 2048 utterances, diversely composed of noisy, conversational, voice-command, and accented speech. The test set is collected internally and from industry partners and is not represented in the training data. As previously observed <cite>[2]</cite> , deep neural networks trained on sufficient data perform better as the model size grows. In order to make fair comparisons, the number of parameters of the models used in our experiments is held constant at 35M. We are aware that the results are not directly comparable to literature due to the use of proprietary datasets.",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_7",
  "x": "One such approach could be to learn basis functions in the real and imaginary domain by performing backpropagation through the Hilbert transformation, enabling the use of larger strides. Alternatively, learned features can be fixed and used to augment other fixed features at train time. Sainath et al. [4] found noticeable improvements from supplementing log-mel filterbanks in such a manner. While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs<cite> [2,</cite> 16] . Our learned features underperform in this context, which is understandable as they are not spectrally ordered, and lack spatial structure. Regularization techniques such as [17] could perhaps be key to learning ordered filter maps with useful structure. In our experiments, we made sure to downsample each scale equally with appropriate stride such that the signals can be concatenated for the later recurrent layers. This temporal pooling only takes into account local structure and has no explicit knowledge of what information to preserve based on longrange dependencies. Recently proposed architectures that operate simultaneously at different timescales, such as the Clockwork RNN [18] , could provide a more elegant way of combining multiscale signals.",
  "y": "similarities"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_8",
  "x": "Many modern state of the art systems train on clusters of GPUs, where memory is precious, as requiring memory transfer between GPU and CPU can be prohibitively slow for training. This is especially problematic for training on long utterances, where the amount of memory required to save all the activations increases both with the number of filters and the reduction of stride. It remains to be seen whether the power of learned input features can be combined with the efficiency of analytic signal transformations such as the Fourier transform. One such approach could be to learn basis functions in the real and imaginary domain by performing backpropagation through the Hilbert transformation, enabling the use of larger strides. Alternatively, learned features can be fixed and used to augment other fixed features at train time. Sainath et al. [4] found noticeable improvements from supplementing log-mel filterbanks in such a manner. While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs<cite> [2,</cite> 16] . Our learned features underperform in this context, which is understandable as they are not spectrally ordered, and lack spatial structure. Regularization techniques such as [17] could perhaps be key to learning ordered filter maps with useful structure.",
  "y": "differences"
 },
 {
  "id": "692f7edc151a9a833c7dd7943bb608_0",
  "x": "Sub Task B aims to categorize the offensive type as targeted text (TIN) or untargeted text (UNT). Sub Task C focuses on identification of target as individual (IND), group (GRP) or others (OTH). Our team SSN NLP participated in all the three subtasks. ---------------------------------- **RELATED WORK** Several research work have been reported since 2010 in this research field of hate speech detection (Kwok and Wang, 2013; Burnap and Williams, 2015; Djuric et al., 2015; <cite>Davidson et al., 2017</cite>; Malmasi and Zampieri, 2018; Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018; ElSherief et al., 2018; Gamb\u00e4ck and Sikdar, 2017; Zhang et al., 2018; Mathur et al., 2018) . Schmidt and Wiegand (2017) & Fortuna and Nunes (2018) reviewed the approaches used for hate speech detection. Kwok and Wang (2013) used bag of words and bi-gram features with machine learning approach to classify the tweets as \"racist\" or \"nonracist\". Burnap and Williams (2015) developed a supervised algorithm for hateful and antagonistic content in Twitter using voted ensemble meta-classifier.",
  "y": "background"
 },
 {
  "id": "692f7edc151a9a833c7dd7943bb608_1",
  "x": [
   "---------------------------------- **CONCLUSION** We have implemented both traditional machine learning and deep learning approaches for identifying offensive languages from social media. The approaches are evaluated on OffensEval@SemEval2019 dataset. The given instances are preprocessed and vectorized using word embeddings in deep learning models. We have employed 2 layered bi-directional LSTM with Scaled Luong and Normed Bahdanau attention mechanisms to build the model for all the three sub tasks. The instances are vectorized using TF-IDF score for traditional machine learning models with minimum count two. The classifiers namely Multinomial Naive Bayes and Support Vector Machine with Stochastic Gradient Descent optimizer were employed to build the models for sub tasks B and C. Deep learning with Scaled Luong attention, deep learning with Normed Bahdanau attention, traditional machine learning with SVM give better results for Task A, Task B and Task C respectively. Our models outperform the base line for all the three tasks."
  ],
  "y": "future_work"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_0",
  "x": "**COPYRIGHT 2009 BY SRIVATSAN RAMANUJAM AND JASON BALDRIDGE** FHMMs to supertagging for the categories defined in CCGbank for English. Fully supervised maximum entropy Markov models have been used for cascaded prediction of POS tags followed by supertags (Clark and Curran, 2007) . Here, we learn supertaggers given only a POS tag dictionary and supertag dictionary or a small amount of material labeled with both types of information. Previous work has used Bayesian HMMs to learn taggers for both POS tagging and supertagging<cite> (Baldridge, 2008)</cite> separately. Modeling them jointly has the potential to produce more robust and accurate supertaggers trained with less supervision and thereby potentially help in the creation of useful models for new languages and domains. Our results show that joint inference improves supervised supertag prediction (compared to HMMs), especially when labeled training data is scarce. Secondly, when training data is limited, the generative FHMMs and a maximum entropy Markov model (a discriminative model like C&C) can bootstrap each other, in a single round co-training setup, to complement each other. Finally, FHMMs trained on tag dictionaries also outperform standard HMMs, thereby providing a stronger basis for learning accurate supertaggers with less supervision.",
  "y": "background"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_1",
  "x": "We consider two supervised training scenarios here. (1) ---------------------------------- **SUPERTAGGING WITH VARYING AMOUNTS OF TRAINING DATA** In this experiment, we use the training and test sets used by <cite>Baldridge (2008)</cite> from CCGbank. We vary the amount of training material by using 100, 1000, 10,000 and all 38015 training set sentences. We also vary the transition prior \u03b1 choosing \u03b1 = 1.0 and \u03b1 = 0.05 on the CCG tags. The emission prior \u03b2 was held constant at 1.0. The results of these experiments for \u03b1 = 0.05 are tabulated in Table 3 (a).",
  "y": "uses similarities"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_2",
  "x": "For comparison, we also show the results of the C&C supertagger of Clark and Curran (2007) in Table 3 (b). The parameter \u03b1, which determines the sparsity of the transition matrix, has been reported to have a greater influence on the performance of the tagger in Goldwater and Griffiths (2007) in weakly supervised POS tagging. We also observed this in supervised supertagging, in the models HMM and FHMMB. The HMM model and FHMMB showed a slight dip in their performance for \u03b1 = 1.0 while FHMMA did slightly better. What stands out in these results is the performance of the FHMM models with minimal amount of training data (for 100 sentences, FHMMB is quite close to the discriminatively trained C&C supertagger). The FHMMA model achieves a 22% absolute accuracy improvement for CCG tags (ambiguous types alone) when compared to the HMM model and the FHMMB model achieves a 41% improvement compared to the HMM model. State-of-the-art POS taggers report accuracies in the range of 96\u221297%; our model FHMMB was comparable (95.35% for \u03b1 = 0.05 and 94.41 for \u03b1 = 1.0). The FHMMA model and the HMM model achieved 91% and 92.5% accuracy on POS tags, respectively. The accuracy of our HMM is lower than the performance of <cite>Baldridge (2008)</cite> for supertags.",
  "y": "differences"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_3",
  "x": "We draw the initial sample of CCG tag sequences corresponding to the observation sequence, using probabilities based on grammar informed initialization<cite> (Baldridge, 2008)</cite> . We consider the prior probability of occurrence of categories based on their complexity: given a lexicon L, the probability of a category c i is inversely proportional to its complexity: where complexity(c i ) is defined as the number of sub-categories contained in category c i . The POS tag corresponding to an observed word w i is drawn uniformly at random from the set of all tags corresponding to w i in the dictionary. For the FHHMs, we first draw a POS tag t i corresponding to a word w i uniformly at random from the tag dictionary of w i and then from the set of all CCG tags that have occurred with t i and w i in the dictionary, we randomly sample a CCG tag c i based on its complexity, as defined above. ---------------------------------- **EFFECT OF FREQUENCY CUT-OFF ON SUPERTAGS** Any category c, that occurs less than k% of the times with a word type w, is removed from the tag dictionary of that word, when the lexicon is constructed. This is in fact a form of supervision, which we use here as an oracle to explore the effect of reducing lexical ambiguity.",
  "y": "uses"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_4",
  "x": "**EFFECT OF FREQUENCY CUT-OFF ON SUPERTAGS** Any category c, that occurs less than k% of the times with a word type w, is removed from the tag dictionary of that word, when the lexicon is constructed. This is in fact a form of supervision, which we use here as an oracle to explore the effect of reducing lexical ambiguity. Results of this experiment for \u03b1 = 1.0, on ambiguous CCG categories, are tabulated in Table  5 (a). The results for \u03b1 = 0.05 is shown in Table 6 (a). We also report the CCG accuracy values inclusive of unambiguous types in Table 5 (b) for \u03b1 = 1.0 and Table 6 (b) respectively. The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of <cite>Baldridge (2008)</cite> that uses variational Bayes EM (33%). Our complexity based initialization is not directly comparable to the results in <cite>Baldridge (2008)</cite> because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism. However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of <cite>Baldridge (2008)</cite> .",
  "y": "similarities"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_5",
  "x": "The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of <cite>Baldridge (2008)</cite> that uses variational Bayes EM (33%). Our complexity based initialization is not directly comparable to the results in <cite>Baldridge (2008)</cite> because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism. However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of <cite>Baldridge (2008)</cite> . It is however, quite short of the 56.1% accuracy achieved by the model of <cite>Baldridge (2008)</cite> that uses grammar informed initialization (combination of category based initialization along with category transition rules). Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM. The HMM performs much better when there is a high level of frequency based filtering of the categories. However, recall that frequency based filtering of categories is a strong form of supervision that we use here only as an oracle and which one could not expect to have in real world tag dictionaries. The POS accuracies in these experiments were 83.5-85%, 84.5-86.2% and 78.3-78.4% for models FHMMB, FHMMA and HMM respectively (without any frequency cut-off). In the weakly supervised setting, the choice of the transition prior \u03b1 of 0.05 lead to severe degradation in the prediction accuracy of CCG tags.",
  "y": "differences"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_6",
  "x": "The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of <cite>Baldridge (2008)</cite> that uses variational Bayes EM (33%). Our complexity based initialization is not directly comparable to the results in <cite>Baldridge (2008)</cite> because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism. However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of <cite>Baldridge (2008)</cite> . It is however, quite short of the 56.1% accuracy achieved by the model of <cite>Baldridge (2008)</cite> that uses grammar informed initialization (combination of category based initialization along with category transition rules). Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM. The HMM performs much better when there is a high level of frequency based filtering of the categories. However, recall that frequency based filtering of categories is a strong form of supervision that we use here only as an oracle and which one could not expect to have in real world tag dictionaries. The POS accuracies in these experiments were 83.5-85%, 84.5-86.2% and 78.3-78.4% for models FHMMB, FHMMA and HMM respectively (without any frequency cut-off). In the weakly supervised setting, the choice of the transition prior \u03b1 of 0.05 lead to severe degradation in the prediction accuracy of CCG tags.",
  "y": "differences"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_7",
  "x": "It is however, quite short of the 56.1% accuracy achieved by the model of <cite>Baldridge (2008)</cite> that uses grammar informed initialization (combination of category based initialization along with category transition rules). Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM. The HMM performs much better when there is a high level of frequency based filtering of the categories. However, recall that frequency based filtering of categories is a strong form of supervision that we use here only as an oracle and which one could not expect to have in real world tag dictionaries. The POS accuracies in these experiments were 83.5-85%, 84.5-86.2% and 78.3-78.4% for models FHMMB, FHMMA and HMM respectively (without any frequency cut-off). In the weakly supervised setting, the choice of the transition prior \u03b1 of 0.05 lead to severe degradation in the prediction accuracy of CCG tags. Unlike POS tagging, where a symmetric transition prior of \u03b1 = 0.05 captured the sparsity of the tag transition distribution , in supertagging the transition priors are asymmetric. We expect that CCG transition rules<cite> (Baldridge, 2008)</cite> when encoded as category specific transition priors, will lead to better performance with the FHMMs. ----------------------------------",
  "y": "background"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_8",
  "x": "The POS accuracies in these experiments were 83.5-85%, 84.5-86.2% and 78.3-78.4% for models FHMMB, FHMMA and HMM respectively (without any frequency cut-off). In the weakly supervised setting, the choice of the transition prior \u03b1 of 0.05 lead to severe degradation in the prediction accuracy of CCG tags. Unlike POS tagging, where a symmetric transition prior of \u03b1 = 0.05 captured the sparsity of the tag transition distribution , in supertagging the transition priors are asymmetric. We expect that CCG transition rules<cite> (Baldridge, 2008)</cite> when encoded as category specific transition priors, will lead to better performance with the FHMMs. ---------------------------------- **RELATED WORK** This paper follows the work of Duh (2005) , <cite>Baldridge (2008)</cite> and Goldwater and Griffiths (2007) . Duh (2005) uses FHMMs for jointly labeling the POS and NP chunk tags for the CoNLL2000 dataset (Sang et al., 2000) . His is a fully supervised model for a simpler task.",
  "y": "future_work"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_9",
  "x": "Unlike POS tagging, where a symmetric transition prior of \u03b1 = 0.05 captured the sparsity of the tag transition distribution , in supertagging the transition priors are asymmetric. We expect that CCG transition rules<cite> (Baldridge, 2008)</cite> when encoded as category specific transition priors, will lead to better performance with the FHMMs. ---------------------------------- **RELATED WORK** This paper follows the work of Duh (2005) , <cite>Baldridge (2008)</cite> and Goldwater and Griffiths (2007) . Duh (2005) uses FHMMs for jointly labeling the POS and NP chunk tags for the CoNLL2000 dataset (Sang et al., 2000) . His is a fully supervised model for a simpler task. We address the harder problem of supertagging in this paper and especially in the weakly supervised setting, with FHMMs. Goldwater and Griffiths (2007) uses a Bayesian tritag HMM (BHMM) for POS tagging and considers three different scenarios: (1) a weakly supervised setting with fixed hyperparameters \u03b1 and \u03b2, (2) hyper parameter inference (learning the optimal values for \u03b1 and \u03b2) and (3) hyper parameter inference with varying corpus size and dictionary knowledge.",
  "y": "extends"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_10",
  "x": "We demonstrated that joint inference in supertagging, boosts the prediction accuracy of both POS and CCG tags by a considerable margin. The improvement is more significant when training data is scarce. The results from the single round co-training experiments were encouraging. The generative FHMM model is able to rival a discriminative model like the C&C supertagger, when more labeled sentences are made available by a bootstrapped supertagger. To the best of our knowledge, this is the first work on joint inference in the Bayesian framework for supertagging. There is plenty of scope for further improvements. Overall, the discriminative C&C supertagger outperforms the FHMMs in all supervised settings. Despite this, the FHMMs are suited for estimating models with less supervision, such as from tag dictionaries alone and incorporating more informative prior distributions such as those in <cite>Baldridge (2008)</cite> . This may make them more appropriate for developing CCGbanks for other languages and domains.",
  "y": "similarities"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_0",
  "x": "Past research has investigated a wide range of aspects pertaining to the ordering of sentences in text. The most prominent approaches include: (1) temporal ordering in terms of publication date (Barzilay, 2003) , (2) temporal ordering in terms of textual cues in sentences (Bollegala et al., 2006) , (3) the topic of the sentences (Barzilay, 2003) , (4) coherence theories (<cite>Barzilay and Lapata, 2008</cite>) , e.g., Centering Theory, (5) content models (Barzilay and Lee, 2004) , and (6) ordering(s) in the underlying documents in the case of summarisation (Bollegala et al., 2006; Barzilay, 2003) . ---------------------------------- **THE MODEL** We view coherence assessment, which we recast as a sentence ordering problem, as a machine learning problem using the feature representation discussed in Section 2.1. It can be viewed as a ranking task because a text can only be more or less coherent than some other text. The sentence ordering task used in this paper can easily be transformed into a ranking problem. Hence, paralleling <cite>Barzilay and Lapata (2008)</cite> , our model has the following structure. The data consists of alternative orderings (x ij , x ik ) of the sentences of the same document d i .",
  "y": "background"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_1",
  "x": "It can be viewed as a ranking task because a text can only be more or less coherent than some other text. The sentence ordering task used in this paper can easily be transformed into a ranking problem. Hence, paralleling <cite>Barzilay and Lapata (2008)</cite> , our model has the following structure. The data consists of alternative orderings (x ij , x ik ) of the sentences of the same document d i . In the training data, the preference ranking of the alternative orderings is known. As a result, training consists of determining a parameter vector w that minimizes the number of violations of pairwise rankings in the training set, a problem which can be solved using SVM constraint optimization (Joachims, 2002) . The following section explores the features available for this optimization. ---------------------------------- **FEATURES**",
  "y": "similarities"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_2",
  "x": "The use of VerbOcean is meant to reveal the degree to which common sense orderings of events affect the ordering of sentences, or whether the order is reversed. With this background, the sentence ordering features used in this paper can be grouped into three categories: ---------------------------------- **GROUP SIMILARITY** The features in this category are inspired by discourse entity-based accounts of local coherence. Yet, in contrast to <cite>Barzilay and Lapata (2008)</cite> , <cite>who</cite> employ the syntactic properties of the respective occurrences, we reduce the accounts to whether or not the entities occur in subsequent sentences (similar to Karamanis (2004) 's NOCB metric). We also investigate whether using only the information from the head of the noun group (cf. <cite>Barzilay and Lapata (2008)</cite> ) suffices, or whether performance is gained when allowing the whole noun group in order to determine similarity. Moreover, as indicated above, some of the noun group measures make use of WordNet synonym, hypernym, hyponym, antonym relationships.",
  "y": "differences"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_3",
  "x": "---------------------------------- **LONGER RANGE RELATIONS** The group similarity features only capture the relation between a sentence and its immediate successor. However, the coherence of a text is clearly not only defined by direct relations, but also requires longer range relations between sentences (e.g., <cite>Barzilay and Lapata (2008)</cite> ). The features in this section explore the impact of such relations on the coherence of the overall document as well as the appropriate way of modeling them. ---------------------------------- **EXPERIMENTS** This section introduces the datasets used for the experiments, describes the experiments, and discusses our main findings. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_4",
  "x": "**EXPERIMENTS** This section introduces the datasets used for the experiments, describes the experiments, and discusses our main findings. ---------------------------------- **EVALUATION DATASETS** The three datasets used for the automatic evaluation in this paper are based on human-generated texts (Table 1 ). The first two are the earthquake and accident datasets used by <cite>Barzilay and Lapata (2008)</cite> . Each of these sets consists of 100 datasets in the training and test sets, respectively, as well as 20 random permutations for each text. The third dataset is similar to the first two in that it contains original texts and random permutations. In contrast to the other two sources, however, this dataset is based on the human summaries from DUC 2005 (Dang, 2005) .",
  "y": "uses"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_5",
  "x": "The first two are the earthquake and accident datasets used by <cite>Barzilay and Lapata (2008)</cite> . Each of these sets consists of 100 datasets in the training and test sets, respectively, as well as 20 random permutations for each text. The third dataset is similar to the first two in that it contains original texts and random permutations. In contrast to the other two sources, however, this dataset is based on the human summaries from DUC 2005 (Dang, 2005) . It comprises 300 human summaries on 50 document sets, resulting in a total of 6,000 pairwise rankings split into training and test sets. The source furthermore differs from <cite>Barzilay and Lapata (2008)</cite> 's datasets in that the content of each text is not based on one individual event (an earthquake or accident), but on more complex topics followed over a period of time (e.g., the espionage case between GM and VW along with the various actions taken to resolve it). Since the different document sets cover completely different topics the third dataset will mainly be used to evaluate the topic-independent properties of our model. ---------------------------------- **EXPERIMENT 1**",
  "y": "differences"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_6",
  "x": "Having evaluated the potential contributions of the individual features and their modeling, we now use SVMs to combine the features into one comprehensive measure. Given the indications from the foregoing experiments, the results in Table 6 are disappointing. In particular, the performance on the earthquake dataset is below standard. However, it seems that sentence ordering in that set is primarily defined by topics, as only content models perform well. (<cite>Barzilay and Lapata (2008)</cite> only perform well when using <cite>their</cite> coreference module, which determines antecedents based on the identified coreferences in the original sentence ordering, thereby biasing <cite>their</cite> orderings towards the correct ordering.) Longer range and WordNet relations together (Chunk+Temp-WN+LongRange+) achieve the best performance. The corresponding configuration is also the only one that achieves reasonable performance when compared with other systems. ---------------------------------- **EXPERIMENT 2** As stated, the ultimate goal of the models presented in this paper is the application of sentence ordering to automatically generated summaries.",
  "y": "differences"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_7",
  "x": "**EXPERIMENT 2** As stated, the ultimate goal of the models presented in this paper is the application of sentence ordering to automatically generated summaries. It is, in this regard, important to distinguish coherence as studied in Experiment 1 and coherence in the context of automatic summarization. Namely, for newswire summarization systems, the topics of the documents are Table 7 : Cross-Training between Accident and Earthquake datasets. The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from <cite>Barzilay and Lapata (2008)</cite>. unknown at the time of training. As a result, model performance on out-of-domain texts is important for summarization. Experiment 2 seeks to evaluate how well our model performs in such cases. To this end, we carry out two sets of tests.",
  "y": "similarities"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_8",
  "x": "To this end, we carry out two sets of tests. First, we crosstrain the models between the accident and earthquake datasets to determine system performance in unseen domains. Second, we use the dataset based on the DUC 2005 model summaries to investigate whether our model's performance on unseen topics reaches a plateau after training on a particular number of different topics. Surprisingly, the results are rather good, when compared to the poor results in part of the previous experiment (Table 7) . In fact, model performance is nearly independent of the training topic. Nevertheless, the results on the earthquake test set indicate that our model is missing essential components for the correct prediction of sentence orderings on this set. When compared to the results obtained by <cite>Barzilay and Lapata (2008)</cite> and Barzilay and Lee (2004) , it would appear that direct sentenceto-sentence similarity (as suggested by the <cite>Barzilay and Lapata</cite> baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset. The final experimental setup applies the best Table 8 : Accuracy on 20 test topics (2,700 pairs) with respect to the number of topics used for training using the model Chunk+Temporal-WordNet+LongRange+ model (Chunk+Temporal-WordNet+LongRange+) to the summarization dataset and evaluates how well the model generalises as the number of topics in the training dataset increases. The results -provided in Table 8 -indicate that very little training data (both regarding the number of pairs and the number of different topics) is needed.",
  "y": "similarities"
 },
 {
  "id": "6bf17a793eaee0593596df0c2249b5_0",
  "x": "---------------------------------- **INTRODUCTION** Multi-hop QA requires finding multiple supporting evidence, and reasoning over them in order to answer a question (Welbl et al., 2018; Talmor and Berant, 2018; <cite>Yang et al., 2018)</cite> . For example, to answer the question shown in figure 1 , the QA system has to retrieve two different paragraphs and reason over them. Moreover, the paragraph containing the answer to the question has very little lexical overlap with the question, making it difficult for search engines to retrieve them from a large corpus. For instance, the accuracy of a BM25 retriever for finding all supporting evidence for a question decreases from 53.7% to 25.9% on the 'easy' and 'hard' subsets of the HOTPOTQA training dataset. 1 We hypothesize that an effective retriever for multi-hop QA should have the \"hopiness\" built into it, by design. That is, after retrieving an initial set of documents, the retriever should be able to \"hop\" onto other documents, if required. We note that, many supporting evidence often share common (bridge) entities between them (e.g. \"Rochester Hills\" in figure 1).",
  "y": "background"
 },
 {
  "id": "6bf17a793eaee0593596df0c2249b5_1",
  "x": "To summarize, this paper presents an entitycentric IR approach that jointly performs entity linking and effectively finds relevant evidence required for questions that need multi-hop reasoning from a large corpus containing millions of paragraphs. When the retrieved paragraphs are supplied to the baseline QA model introduced in<cite> Yang et al. (2018)</cite> , it improved the QA performance on the hidden test set by 10.59 F1 points. 2 ---------------------------------- **METHODOLOGY** Our approach is summarized in Figure 2 . The first component of our model is a standard IR system that takes in a natural language query 'Q' and returns an initial set of evidence. For our experiments, we use the popular BM25 retriever, but this component can be replaced by any IR model. We assume that all spans of entity mentions have been identified in the paragraph text by a one-time preprocessing, with an entity tagger.",
  "y": "uses"
 },
 {
  "id": "6bf17a793eaee0593596df0c2249b5_2",
  "x": "---------------------------------- **EXPERIMENTS** For all our experiment, unless specified otherwise, we use the open domain corpus 4 released by<cite> Yang et al. (2018)</cite> which contains over 5.23 million Wikipedia abstracts (introductory paragraphs). To identify spans of entities, we use the implementation of the state-of-the-art entity tagger presented in Peters et al. (2018) . 5 For the BERT encoder, we use the BERT-BASE-UNCASED model. 6 We use the implementation of widely-used BM25 retrieval available in Lucene. Table 1 : Retrieval performance of models on the HOT-POTQA benchmark. A successful retrieval is when all the relevant passages for a question are retrieved from more than 5 million paragraphs in the corpus. systems is via pseudo-relevance feedback (PRF).",
  "y": "uses"
 },
 {
  "id": "6bf17a793eaee0593596df0c2249b5_4",
  "x": "However, within that hard subset, we find the set of question, that has the answer span present in all the supporting passages (SINGLE-HOP (HARD)) and only in one of the supporting passages (MULTI-HOP (HARD)) 11 . The intuition is that if there are multiple evidence containing the answer spans then it might be a little easier for a downstream QA model to identify the answer span. Figure 3 shows that our model performs equally well on both type of queries and hence can be applied in a practical setting. Baseline Reader<cite> (Yang et al., 2018)</cite> Table 2 shows the performance on the QA task. We were able to achieve better scores than reported in the baseline reader model of<cite> Yang et al. (2018)</cite> by using Adam (Kingma and Ba, 2014) instead of standard SGD (our re-implementation). Next, we use the top-10 paragraphs retrieved by our system from the entire corpus and feed it to the reader model. We achieve a 10.59 absolute increase in F1 score than the baseline. It should be noted that we use the simple baseline reader model and we are confident that we can achieve better scores by using more sophisticated reader architectures, e.g. using BERT based architectures. Our results show that retrieval is an important component of an opendomain system and equal importance should be given to both the retriever and reader component.",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_0",
  "x": "State-of-the-art neural machine translation systems use autoregressive decoding where words are predicted one-byone conditioned on all previous words (Bahdanau et al., 2015; Vaswani et al., 2017) . Non-autoregressive machine translation (NAT, Gu et al. (2018) ), on the other hand, generates all words in one shot and speeds up decoding at the expense of performance drop. Parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations (Gu et al., 2018) . One way to remedy this fundamental problem is to refine model output iteratively (Lee et al., 2018;<cite> Ghazvininejad et al., 2019)</cite> . This work pursues this iterative approach to non-autoregressive translation. 1 In this work, we propose a transformer-based architecture with attention masking, which we call Disentangled Context (DisCo) transformer, and use it for non-autoregressive decoding. Specifically, our DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words. Unlike the masked language models (Devlin et al., 2019;<cite> Ghazvininejad et al., 2019)</cite> where the model only predicts the masked words, the DisCo transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large. We also introduce a new inference algorithm for iterative parallel decoding, parallel easy-first, where each word is predicted by attending to the words that the model is more confident about.",
  "y": "background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_1",
  "x": "We also develop the parallel easy-first inference algorithm, which iteratively refines every token in parallel and reduces the number of required iterations. Our extensive experiments on 7 directions with varying data sizes demonstrate that our model achieves competitive, if not better, performance compared to the state of the art in non-autoregressive machine translation while significantly reducing decoding time on average. ---------------------------------- **INTRODUCTION** State-of-the-art neural machine translation systems use autoregressive decoding where words are predicted one-byone conditioned on all previous words (Bahdanau et al., 2015; Vaswani et al., 2017) . Non-autoregressive machine translation (NAT, Gu et al. (2018) ), on the other hand, generates all words in one shot and speeds up decoding at the expense of performance drop. Parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations (Gu et al., 2018) . One way to remedy this fundamental problem is to refine model output iteratively (Lee et al., 2018;<cite> Ghazvininejad et al., 2019)</cite> . This work pursues this iterative approach to non-autoregressive translation.",
  "y": "background motivation"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_2",
  "x": "**INTRODUCTION** State-of-the-art neural machine translation systems use autoregressive decoding where words are predicted one-byone conditioned on all previous words (Bahdanau et al., 2015; Vaswani et al., 2017) . Non-autoregressive machine translation (NAT, Gu et al. (2018) ), on the other hand, generates all words in one shot and speeds up decoding at the expense of performance drop. Parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations (Gu et al., 2018) . One way to remedy this fundamental problem is to refine model output iteratively (Lee et al., 2018;<cite> Ghazvininejad et al., 2019)</cite> . This work pursues this iterative approach to non-autoregressive translation. 1 In this work, we propose a transformer-based architecture with attention masking, which we call Disentangled Context (DisCo) transformer, and use it for non-autoregressive decoding. Specifically, our DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words. Unlike the masked language models (Devlin et al., 2019;<cite> Ghazvininejad et al., 2019)</cite> where the model only predicts the masked words, the DisCo transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large.",
  "y": "differences background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_3",
  "x": "State-of-the-art neural machine translation systems use autoregressive decoding where words are predicted one-byone conditioned on all previous words (Bahdanau et al., 2015; Vaswani et al., 2017) . Non-autoregressive machine translation (NAT, Gu et al. (2018) ), on the other hand, generates all words in one shot and speeds up decoding at the expense of performance drop. Parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations (Gu et al., 2018) . One way to remedy this fundamental problem is to refine model output iteratively (Lee et al., 2018;<cite> Ghazvininejad et al., 2019)</cite> . This work pursues this iterative approach to non-autoregressive translation. 1 In this work, we propose a transformer-based architecture with attention masking, which we call Disentangled Context (DisCo) transformer, and use it for non-autoregressive decoding. Specifically, our DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words. Unlike the masked language models (Devlin et al., 2019;<cite> Ghazvininejad et al., 2019)</cite> where the model only predicts the masked words, the DisCo transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large. We also introduce a new inference algorithm for iterative parallel decoding, parallel easy-first, where each word is predicted by attending to the words that the model is more confident about.",
  "y": "differences"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_4",
  "x": "Parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations (Gu et al., 2018) . One way to remedy this fundamental problem is to refine model output iteratively (Lee et al., 2018;<cite> Ghazvininejad et al., 2019)</cite> . This work pursues this iterative approach to non-autoregressive translation. 1 In this work, we propose a transformer-based architecture with attention masking, which we call Disentangled Context (DisCo) transformer, and use it for non-autoregressive decoding. Specifically, our DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words. Unlike the masked language models (Devlin et al., 2019;<cite> Ghazvininejad et al., 2019)</cite> where the model only predicts the masked words, the DisCo transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large. We also introduce a new inference algorithm for iterative parallel decoding, parallel easy-first, where each word is predicted by attending to the words that the model is more confident about. This decoding algorithm allows for predicting all tokens with different context in each iteration and terminates when the output prediction converges, contrasting with the constant number of iterations<cite> (Ghazvininejad et al., 2019)</cite> . Indeed, we will show in a later section that this method substantially reduces the number of required iterations without loss in performance.",
  "y": "differences"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_5",
  "x": "Specifically, our DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words. Unlike the masked language models (Devlin et al., 2019;<cite> Ghazvininejad et al., 2019)</cite> where the model only predicts the masked words, the DisCo transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large. We also introduce a new inference algorithm for iterative parallel decoding, parallel easy-first, where each word is predicted by attending to the words that the model is more confident about. This decoding algorithm allows for predicting all tokens with different context in each iteration and terminates when the output prediction converges, contrasting with the constant number of iterations<cite> (Ghazvininejad et al., 2019)</cite> . Indeed, we will show in a later section that this method substantially reduces the number of required iterations without loss in performance. Our extensive empirical evaluations on 7 translation directions from standard WMT benchmarks show that our approach achieves competitive performance to state-of-the-art non-autoregressive and autoregressive machine translation while significantly reducing decoding time on average. ---------------------------------- **DISCO TRANSFORMER** In this section, we introduce our DisCo transformer for nonautoregressive translation (Fig. 1 ).",
  "y": "background differences"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_6",
  "x": "---------------------------------- **DISCO OBJECTIVE** Similar to masked language models for contextual word representations (Devlin et al., 2019; Liu et al., 2019 ), a con- ditional masked language model (CMLM, <cite>Ghazvininejad et al. (2019)</cite> ) predicts randomly masked target tokens Y mask given a source text X and the rest of the target tokens Y obs . Namely, for every sentence pair in bitext X and Y , where RS denotes random sampling of masked tokens. 2 CMLMs have proven successful in parallel decoding for machine translation<cite> (Ghazvininejad et al., 2019)</cite> , video captioning (Yang et al., 2019a) , and speech recognition (Nakayama et al., 2019) . However, the fundamental inefficiency with this masked language modeling objective is that the model can only be trained to predict a subset of the reference tokens (Y mask ) for each network pass unlike a normal autoregressive model where we predict all Y from left to right. To address this limitation, we propose a Disentangled Context (DisCo) objective. The objective involves prediction of every token given an arbitrary (thus disentangled) subset of the other tokens.",
  "y": "background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_7",
  "x": "We propose a DisCo objective as an efficient alternative to masked language modeling and design an architecture that can compute the objective in a single pass. ---------------------------------- **DISCO OBJECTIVE** Similar to masked language models for contextual word representations (Devlin et al., 2019; Liu et al., 2019 ), a con- ditional masked language model (CMLM, <cite>Ghazvininejad et al. (2019)</cite> ) predicts randomly masked target tokens Y mask given a source text X and the rest of the target tokens Y obs . Namely, for every sentence pair in bitext X and Y , where RS denotes random sampling of masked tokens. 2 CMLMs have proven successful in parallel decoding for machine translation<cite> (Ghazvininejad et al., 2019)</cite> , video captioning (Yang et al., 2019a) , and speech recognition (Nakayama et al., 2019) . However, the fundamental inefficiency with this masked language modeling objective is that the model can only be trained to predict a subset of the reference tokens (Y mask ) for each network pass unlike a normal autoregressive model where we predict all Y from left to right. To address this limitation, we propose a Disentangled Context (DisCo) objective.",
  "y": "background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_8",
  "x": "To address this limitation, we propose a Disentangled Context (DisCo) objective. The objective involves prediction of every token given an arbitrary (thus disentangled) subset of the other tokens. For every 1 \u2264 n \u2264 N where |Y | = N , we predict: ---------------------------------- **DISCO TRANSFORMER ARCHITECTURE** Simply computing conditional probabilities P (Y n |X, Y n obs ) with a vanilla transformer decoder will necessitate N separate transformer passes for each Y n obs . We introduce the 2 BERT (Devlin et al., 2019 ) masks a token with probability 0.15 while CMLMs<cite> (Ghazvininejad et al., 2019)</cite> sample the number of masked tokens uniformly from [1, N ]. DisCo transformer to compute these N contexts in one shot: In particular, our DisCo transformer makes crucial use of attention masking to achieve this computational efficiency.",
  "y": "differences"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_9",
  "x": "---------------------------------- **TRAINING LOSS** We use a standard transformer as an encoder and stacked DisCo layers as a decoder. For each Y n in Y where |Y | = N , we uniformly sample the number of visible tokens from [0, N \u2212 1], and then we randomly choose that number of tokens from Y \\ Y n as Y n obs , similarly to CMLMs<cite> (Ghazvininejad et al., 2019)</cite> . We optimize the negative log likelihood loss from P (Y n |X, Y n obs ) (1 \u2264 n \u2264 N ). Again following CMLMs, we append a special token to the encoder and project the vector to predict the target length for parallel decoding. We add the negative log likelihood loss from this length prediction to the loss from word predictions. ---------------------------------- **DISCO OBJECTIVE AS GENERALIZATION**",
  "y": "similarities"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_10",
  "x": "In this section, we discuss inference algorithms for our DisCo transformer. We first review mask-predict from prior work as a baseline and introduce a new parallelizable inference algorithm, parallel easy-first (Alg. 1). ---------------------------------- **MASK-PREDICT** Mask-predict is an iterative inference algorithm introduced in <cite>Ghazvininejad et al. (2019)</cite> to decode a conditional masked language model (CMLM). The target length N is first predicted, and then the algorithm iterates over two steps: mask where i t tokens with lowest probability are masked and predict where those masked tokens are updated given the other N \u2212 i t tokens. The number of masked tokens i t decays from N with a constant rate over a fixed number of iterations T . Specifically, at iteration t, This method is directly applicable to our DisCo transformer by fixing Y n,t obs regardless of the position n.",
  "y": "background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_11",
  "x": "**LENGTH BEAM** Following <cite>Ghazvininejad et al. (2019)</cite> , we apply length beam. In particular, we predict top K lengths from the distribution in length prediction and run parallel easy-first simultaneously. In order to speed up decoding, we terminate if the one with the highest average log score N n=1 log(p t n )/N converges. It should be noted that for parallel easy-first, obs for all positions n while mask-predict may keep updating tokens even after because Y t obs changes over iterations. See Alg. 1 for full pseudo-code. Notice that all for-loops are parallelizable except the one over iterations t. In the subsequent experiments, we use length beam size of 5<cite> (Ghazvininejad et al., 2019)</cite> unless otherwise noted.",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_12",
  "x": "In particular, we predict top K lengths from the distribution in length prediction and run parallel easy-first simultaneously. In order to speed up decoding, we terminate if the one with the highest average log score N n=1 log(p t n )/N converges. It should be noted that for parallel easy-first, obs for all positions n while mask-predict may keep updating tokens even after because Y t obs changes over iterations. See Alg. 1 for full pseudo-code. Notice that all for-loops are parallelizable except the one over iterations t. In the subsequent experiments, we use length beam size of 5<cite> (Ghazvininejad et al., 2019)</cite> unless otherwise noted. In Sec. 5.2, we Algorithm 1 Parallel Easy-First with Length Beam Source sentence: X Predicted lengths: N1, \u00b7 \u00b7 \u00b7 , NK Max number of iterations: T for k \u2208 {1, 2, ..., K} do for n \u2208 {1, 2, ..., N k } do Y 1,k n , p k n = (arg)max w P (yn = w|X) end for Get the easy-first order z k by sorting p k and let z k (i) be the rank of the ith position. end for",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_13",
  "x": "These datasets are all encoded into subword units by BPE (Sennrich et al., 2016) . 4 We use the same preprocessed data and train/dev/test splits as prior work for fair comparisons (EN-DE: Vaswani et al. 2017); 4 We run joint BPE on all language pairs except EN-ZH. Ott et al. (2018) ). We evaluate performance with BLEU scores (Papineni et al., 2002) for all directions except that we use SacreBLEU (Post, 2018) 5 in en\u2192zh again for fair comparison with prior work<cite> (Ghazvininejad et al., 2019)</cite> . For all autoregressive models, we use beam search with b = 5 (Vaswani et al., 2017; Ott et al., 2018) and tune length penalty of \u03b1 \u2208 [0.0, 0.2, \u00b7 \u00b7 \u00b7 , 2.0] in validation. For parallel easy-first, we set the max number of iterations T = 10 and use T = 4, 10 for constant-time mask-predict. ---------------------------------- **BASELINES AND COMPARISON** There has been a flurry of recent work on non-autoregressive machine translation (NAT) that finds a balance between parallelism and performance.",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_14",
  "x": "There has been a flurry of recent work on non-autoregressive machine translation (NAT) that finds a balance between parallelism and performance. Performance can be measured using automatic evaluation such as BLEU scores (Papineni et al., 2002) . Latency is, however, challenging to compare across different methods. For models that have an autoregressive component (e.g. Kaiser et al. (2018) ; Ran et al. (2019)), we can speed up sequential computation by caching states. Further, many of prior NAT approaches generate varying numbers of translation candidates and rescore them using an autoregressive model. The rescoring process typically costs overhead of one parallel pass of a transformer encoder followed by a decoder. Given this complexity in latency comparison, we highlight two state-of-the-art iteration-based NAT models whose latency is comparable to our DisCo transformer due to the similar model structure. See Sec. 6 for descriptions of more work on NAT. CMLM As discussed earlier, we can generate a translation with mask-predict from a CMLM<cite> (Ghazvininejad et al., 2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_15",
  "x": "Its latency is roughly comparable by the average number of sequential transformer runs. Each iteration consists of three transformer runs except that the first iteration skips the deletion step. See Gu et al. (2019) (Luong et al., 2015; Vaswani et al., 2017) . Unfortunately, we lack consensus in evaluation (Post, 2018) . Hyperparameters We generally follow the hyperparameters for a transformer base (Vaswani et al., 2017;<cite> Ghazvininejad et al., 2019)</cite> : 6 layers for both the encoder and decoder, 8 attention heads, 512 model dimensions, and 2048 hidden dimensions. We sample weights from N (0, 0.02), initialize biases to zero, and set layer normalization parameters to \u03b2 = 0, \u03b3 = 1 (Devlin et al., 2019) . For regularization, we tune the dropout rate from [0.1, 0.2, 0.3] based on dev performance in each direction, and use 0.01 L 2 weight decay and label smoothing with \u03b5 = 0.1. We train batches of 128K tokens using Adam (Kingma & Ba, 2015) with \u03b2 = (0.9, 0.999) and \u03b5 = 10 \u22126 . The learning rate warms up to 5 \u00b7 10 \u22124 in the first 10K steps, and then decays with the inverse square-root schedule.",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_16",
  "x": "We also present results obtained from training a standard autoregressive base transformer on the same distillation data for comparison. We assess the impact of distillation in Sec. 5.1 and demonstrate that distillation is still a key component in our non-autoregressive models. ---------------------------------- **RESULTS AND DISCUSSION** Seen in Table 1 are the results in the four directions from the WMT'14 EN-DE and WMT'16 EN-RO datasets. First, our re-implementations of CMLM + Mask-Predict outperform <cite>Ghazvininejad et al. (2019)</cite> (e.g. 31.24 vs. 30.53 in de\u2192en with 10 steps). This is probably due to our tuning on the dropout rate and weight averaging of the 5 best epochs based on the validation BLEU performance (Sec. 4.1). Our DisCo transformer with the parallel easy-first inference achieves at least comparable performance to the CMLM with 10 steps despite the significantly fewer steps on average (e.g. 4.82 steps in en\u2192de). The one exception is ro\u2192en (33.25 vs. 33.67), but DisCo + Easy-First requires only 3.10 steps, and CMLM + Mask-Predict with 4 steps achieves similar performance of 33.27.",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_17",
  "x": "Length Beam Fig. 4 shows performance of the CMLM and DisCo transformer with varying size of length beam. All cases benefit from multiple candidates with different lengths to a certain point, but DisCo + Easy-First improves most. This can be because parallel easy-first relies on the easyfirst order as well as the length, and length beam provides opportunity to try multiple orderings. ---------------------------------- **EXAMPLE TRANSLATION SEEN IN** ---------------------------------- **RELATED AND FUTURE WORK** Recent work on non-autoregressive translation developed ways to mitigate the trade-off between decoding parallelism and performance. As in this work, several prior work proposed methods to iteratively refine output predictions (Lee et al., 2018;<cite> Ghazvininejad et al., 2019</cite>; Gu et al., 2019; Mansimov et al., 2019) .",
  "y": "similarities"
 },
 {
  "id": "6e5ee9176bcc54c3c9c32965765990_0",
  "x": "Therefore, 2 translators were contracted as part of the project to create 30,000 segments of in-domain data, translating public administrations websites. They also cleaned United Nations material and post-edited general-domain data that was previously filtered as indomain following the \"invitation model\" (Hoang and Sima'an, 2014) . For the other language pairs, the input material was 30,000 post-edited segments. The main part of the training corpora (approximately 75%) was part of Pangeanic's own repository harvested through web crawling and also OpenSubtitles (Tiedemann, 2012) . The rest of the corpus was automatically validated synthetic material using general data from Leipzig (Goldhahn et al., 2012 Engine customization The data was cleaned using the Bicleaner tool (S\u00e1nchez-Cartagena et al., 2018) . The data was lowercased and extra embeddings were added in order to keep the case information. The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE (Sennrich et al., 2016) approach. The models were trained with multi-domain data and we improved performance following a domainmixing approach <cite>(Britz et al., 2017)</cite> . The domain information was prepended with special tokens for each target sequence.",
  "y": "uses"
 },
 {
  "id": "6e5ee9176bcc54c3c9c32965765990_1",
  "x": "For the other language pairs, the input material was 30,000 post-edited segments. The main part of the training corpora (approximately 75%) was part of Pangeanic's own repository harvested through web crawling and also OpenSubtitles (Tiedemann, 2012) . The rest of the corpus was automatically validated synthetic material using general data from Leipzig (Goldhahn et al., 2012) . Engine customization The data was cleaned using the Bicleaner tool (S\u00e1nchez-Cartagena et al., 2018) . The data was lowercased and extra embeddings were added in order to keep the case information. The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE (Sennrich et al., 2016) approach. The models were trained with multi-domain data and we improved performance following a domainmixing approach <cite>(Britz et al., 2017)</cite> . The domain information was prepended with special tokens for each target sequence. The domain prediction was based only on the source as the extra token was added at target-side and there was no need for apriori domain information.",
  "y": "uses"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_0",
  "x": "A breakthrough has come in the form of research by McClosky et al. (2006a;<cite> 2006b</cite> ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005) . Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al., 2003) . McClosky et al. (2006a;<cite> 2006b</cite> ) proceed as follows: sentences * Now affiliated to Lalic, Universit\u00e9 Paris 4 La Sorbonne. from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker. The highest ranked parse trees are added to the training set of the parser and the parser is retrained. This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Ku\u010dera, 1979 ) (an absolute fscore improvement of 2.6%). In the experiments of McClosky et al. (2006a;<cite> 2006b)</cite> , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees.",
  "y": "background"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_2",
  "x": "**INTRODUCTION** Given the success of statistical parsing models on the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (Charniak, 2000; Collins, 2003, for example) , there has been a change in focus in recent years towards the problem of replicating this success on genres other than American financial news stories. The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples. A breakthrough has come in the form of research by McClosky et al. (2006a;<cite> 2006b</cite> ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005) . Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al., 2003) . McClosky et al. (2006a;<cite> 2006b</cite> ) proceed as follows: sentences * Now affiliated to Lalic, Universit\u00e9 Paris 4 La Sorbonne. from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker. The highest ranked parse trees are added to the training set of the parser and the parser is retrained. This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Ku\u010dera, 1979 ) (an absolute fscore improvement of 2.6%).",
  "y": "background"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_3",
  "x": "This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Ku\u010dera, 1979 ) (an absolute fscore improvement of 2.6%). In the experiments of McClosky et al. (2006a;<cite> 2006b)</cite> , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees. However, McCloskey et al. (<cite>2006b)</cite> report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training. In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a;<cite> 2006b)</cite> . We parse the BNC (Burnard, 2000) in its entirety using the reranking parser of Charniak and Johnson (2005) . 1,000 BNC sentences are manually annotated for constituent structure, resulting in the first gold standard set for this corpus. The gold standard set is split into a development set of 500 parse trees and a test set of 500 parse trees and used in a series of self-training experiments: Charniak and Johnson's parser is retrained on combinations of WSJ treebank data and its own parses of BNC sentences.",
  "y": "background"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_4",
  "x": "Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees. However, McCloskey et al. (<cite>2006b)</cite> report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training. In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a;<cite> 2006b)</cite> . We parse the BNC (Burnard, 2000) in its entirety using the reranking parser of Charniak and Johnson (2005) . 1,000 BNC sentences are manually annotated for constituent structure, resulting in the first gold standard set for this corpus. The gold standard set is split into a development set of 500 parse trees and a test set of 500 parse trees and used in a series of self-training experiments: Charniak and Johnson's parser is retrained on combinations of WSJ treebank data and its own parses of BNC sentences. These combinations are tested on the BNC development set and Section 00 of the WSJ. An optimal combination is chosen which achieves a Parseval labelled bracketing f-score of 91.7% on Section 23 and 85.6% on the BNC gold standard test set.",
  "y": "background"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_5",
  "x": "In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a;<cite> 2006b)</cite> . We parse the BNC (Burnard, 2000) in its entirety using the reranking parser of Charniak and Johnson (2005) . 1,000 BNC sentences are manually annotated for constituent structure, resulting in the first gold standard set for this corpus. The gold standard set is split into a development set of 500 parse trees and a test set of 500 parse trees and used in a series of self-training experiments: Charniak and Johnson's parser is retrained on combinations of WSJ treebank data and its own parses of BNC sentences. These combinations are tested on the BNC development set and Section 00 of the WSJ. An optimal combination is chosen which achieves a Parseval labelled bracketing f-score of 91.7% on Section 23 and 85.6% on the BNC gold standard test set. For Section 23 this is an absolute improvement of 0.4% on the baseline results of this parser, and for the BNC data this is a statistically significant improvement of 1.7%. ---------------------------------- **THE BNC DATA**",
  "y": "uses"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_6",
  "x": "This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Ku\u010dera, 1979 ) (an absolute fscore improvement of 2.6%). In the experiments of McClosky et al. (2006a;<cite> 2006b)</cite> , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees. However, McCloskey et al. (<cite>2006b)</cite> report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training. In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a;<cite> 2006b)</cite> . We parse the BNC (Burnard, 2000) in its entirety using the reranking parser of Charniak and Johnson (2005) . 1,000 BNC sentences are manually annotated for constituent structure, resulting in the first gold standard set for this corpus. The gold standard set is split into a development set of 500 parse trees and a test set of 500 parse trees and used in a series of self-training experiments: Charniak and Johnson's parser is retrained on combinations of WSJ treebank data and its own parses of BNC sentences.",
  "y": "uses differences"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_7",
  "x": "The annotator noticed that the PTB parse trees sometimes violate the PTB bracketing guidelines, and in these cases, the annotator chose the analysis set out in the guidelines. It took approximately 60 hours to build the gold standard set. ---------------------------------- **SELF-TRAINING EXPERIMENTS** Charniak and Johnson's reranking parser (June 2006 version) is evaluated against the BNC gold standard development set. Labelled precision (LP), recall (LR) and f-score measures 2 for this parser are shown in the first row of Table 1 . The f-score of 83.7% is lower than the f-score of 85.2% reported by<cite> McClosky et al. (2006b)</cite> for the same parser on Brown corpus data. This difference is reasonable since there is greater domain variation between the WSJ and the BNC than between the WSJ and the Brown corpus, and all BNC gold standard sentences contain verbs not attested in WSJ Sections 2-21. We retrain the first-stage generative statistical parser of Charniak and Johnson using combinations of BNC trees (parsed using the reranking parser) and WSJ treebank trees.",
  "y": "differences"
 },
 {
  "id": "6fbfc9f887e736472510bce30c9228_0",
  "x": "Commercial software is available to partially automated online support by suggesting responses to a human agent, which may then be accepted or overwritten. The research presented in this paper aims to provide a degree of natural language understanding to assist in automating task-oriented dialogue, such as support services, by suggesting utterances during the dialogue. We apply various probabilistic methods to improve discourse modelling in the support services domain. In previous work, we collected a small corpus of task-oriented dialogues between customers and support representatives from the MSN Shopping online support service <cite>(Ivanovic, 2005b)</cite> . The service is designed to assist potential customers with finding various items for sale on the MSN Shopping web site. A sample from one of the dialogues in this corpus is shown in Table 1 . The research presented here advances previous work which examined various models and tech-niques to predict dialogue acts in task-oriented instant messaging. In <cite>Ivanovic (2005b)</cite> , the MSN Shopping corpus was collected and a gold standard produced by labelling the utterances with dialogue acts. Probabilistic models were then trained to predict dialogue acts given a sequence of utterances.",
  "y": "uses background"
 },
 {
  "id": "6fbfc9f887e736472510bce30c9228_1",
  "x": "The research presented in this paper aims to provide a degree of natural language understanding to assist in automating task-oriented dialogue, such as support services, by suggesting utterances during the dialogue. We apply various probabilistic methods to improve discourse modelling in the support services domain. In previous work, we collected a small corpus of task-oriented dialogues between customers and support representatives from the MSN Shopping online support service <cite>(Ivanovic, 2005b)</cite> . The service is designed to assist potential customers with finding various items for sale on the MSN Shopping web site. A sample from one of the dialogues in this corpus is shown in Table 1 . The research presented here advances previous work which examined various models and tech-niques to predict dialogue acts in task-oriented instant messaging. In <cite>Ivanovic (2005b)</cite> , the MSN Shopping corpus was collected and a gold standard produced by labelling the utterances with dialogue acts. Probabilistic models were then trained to predict dialogue acts given a sequence of utterances. Ivanovic (2005a) examined probabilistic and linguistic methods to automatically segment messages from the same corpus into utterances.",
  "y": "background"
 },
 {
  "id": "6fbfc9f887e736472510bce30c9228_2",
  "x": "In <cite>Ivanovic (2005b)</cite> , the MSN Shopping corpus was collected and a gold standard produced by labelling the utterances with dialogue acts. Probabilistic models were then trained to predict dialogue acts given a sequence of utterances. Ivanovic (2005a) examined probabilistic and linguistic methods to automatically segment messages from the same corpus into utterances. The present paper concludes this work by applying the models to a dialogue simulation program which suggests utterance responses during a dialogue. The performance of the suggested utterances is then evaluated. ---------------------------------- **BACKGROUND** Our dialogue act tag set contains 12 dialogue acts, which are intended to represent the illocutionary force of an utterance. The tags were derived in <cite>Ivanovic (2005b)</cite> by manually labelling the MSN Shopping corpus using the tags that seemed appropriate from a list of 42 tags in Stolcke et al. (2000) .",
  "y": "uses"
 },
 {
  "id": "6fbfc9f887e736472510bce30c9228_3",
  "x": "Our dialogue act tag set contains 12 dialogue acts, which are intended to represent the illocutionary force of an utterance. The tags were derived in <cite>Ivanovic (2005b)</cite> by manually labelling the MSN Shopping corpus using the tags that seemed appropriate from a list of 42 tags in Stolcke et al. (2000) . The MSN Shopping corpus we use comprises approximately 550 utterances and 6,500 words. <cite>Ivanovic (2005b)</cite> describes the manual process of segmenting the messages into utterances and labelling the utterances with dialogue act tags to produce a gold standard version of the data. Kappa analysis on both the labelling and segmentation tasks was conducted with results showing high interannotator agreement (Ivanovic, 2005a ). ---------------------------------- **EVALUATION AND RESULTS** As part of a high-level, end-to-end evaluation of dialogue act prediction and their usefulness in semiautomated dialogue systems, we developed a program that simulates a live conversation while suggesting responses. The suggested utterances are ranked by their respective probabilities given the dialogue history.",
  "y": "background"
 },
 {
  "id": "6fd0c2fbbe0c7fb669f2618f4d01f7_0",
  "x": "About 74% of Wikipedia articles fall under the category of named entity classes <cite>[4]</cite> , therefore, we consider Wikipedia links among articles to construct the Hi-En-WP NER annotated corpus. Wikipedia includes content pages which contain concepts and facts about the article, category pages provides a list of content pages into several classes based on specific criteria and disambiguation pages help to locate different content pages with same title. Wikipedia is an abundant resource for generation of different types of multilingual, cross lingual, cross script and language independent corpus, etc., its markup has been used extensively to automatically generate NER annotated corpus for training machine learning models <cite>[4]</cite> [5] [6] [11] [12] [13] [14] 19] . The latest involvement using Wikipedia is the portable cross lingual NER for low resource languages using translation of an annotated NER corpus from English [12, 19] . Another approach to cross lingual and language independent corpora is to learn a model on language independent features of a source language and test the model on other languages using same features [13] . Nothman, et al. (2008) [5] constructed a massive English NER corpus by the classification of Wikipedia articles to its category types by mapping them to CoNLL-2003 NER tagset. A similar approach to massive multilingual NER corpus is found in [14] . A hybrid approach to generate parallel sentences with NE notations reveal strong results on Wikipedia dataset [19] . Kazama and Torisawa (2007) [15] use the external knowledge along with the analysis of first sentence in Wikipedia articles to infer the entity category.",
  "y": "uses background"
 },
 {
  "id": "6fd0c2fbbe0c7fb669f2618f4d01f7_1",
  "x": "Constantly, the articles are updated and new articles are added by its collaborators. About 74% of Wikipedia articles fall under the category of named entity classes <cite>[4]</cite> , therefore, we consider Wikipedia links among articles to construct the Hi-En-WP NER annotated corpus. Wikipedia includes content pages which contain concepts and facts about the article, category pages provides a list of content pages into several classes based on specific criteria and disambiguation pages help to locate different content pages with same title. Wikipedia is an abundant resource for generation of different types of multilingual, cross lingual, cross script and language independent corpus, etc., its markup has been used extensively to automatically generate NER annotated corpus for training machine learning models <cite>[4]</cite> [5] [6] [11] [12] [13] [14] 19] . The latest involvement using Wikipedia is the portable cross lingual NER for low resource languages using translation of an annotated NER corpus from English [12, 19] . Another approach to cross lingual and language independent corpora is to learn a model on language independent features of a source language and test the model on other languages using same features [13] . Nothman, et al. (2008) [5] constructed a massive English NER corpus by the classification of Wikipedia articles to its category types by mapping them to CoNLL-2003 NER tagset. A similar approach to massive multilingual NER corpus is found in [14] . A hybrid approach to generate parallel sentences with NE notations reveal strong results on Wikipedia dataset [19] .",
  "y": "background"
 },
 {
  "id": "6fd0c2fbbe0c7fb669f2618f4d01f7_2",
  "x": "We strongly take into account the assumption that the entities present on these pages are prominently Hindi NEs. This assumption is based on human assessment that the information on such pages is based on Indian background especially from Hindi linguistic majority sections of India. ---------------------------------- **CORPUS ACQUISITION** Wikipedia being a huge source of information, its articles comprise of: topic and its comprehensive summary in paragraphs and images; reference to reliable resources; and hyperlinks, also called wikilinks to other articles. Our method takes the advantage of wikilinks between the articles from which linktext is extracted. Since wikilinks are links to articles, it may be considered as a named entity. This approach is similar to Nothman et al (2008) <cite>[4]</cite> to generate the NER data from wikilinks. A total number of 7285 tokens and multi-tokens expressions were extracted from the links by parsing the 13 identified Wikipedia webpages.",
  "y": "similarities"
 },
 {
  "id": "6fd0c2fbbe0c7fb669f2618f4d01f7_3",
  "x": "Whereas, low precision value for LOC tag suggests that other entities are wrongly classified as location. The MISC F-score is expectedly low, in agreement with the results of Nothman et al (2008) <cite>[4]</cite> . The variation reflected in F-score among all may be the effect of diversity in linguistic attributes. An increase in accuracy from 89% to 92% is observed when the model is trained without MISC tag which reflects that the confusion is created in data by the inclusion of training examples that belong to MISC tag. The Fig.1 illustrates the effect of varying size of the Hi-En-WP training data. The improving trend of accuracy score on increasing training data sufficiently produces scope to scale the size of corpus in future. ---------------------------------- **5** ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_0",
  "x": "The method takes inspiration from the NRC method, which gives the best results in SemEval13 by leveraging emoticons in large tweets, using the PMI between words and tweet sentiments to define the sentiment attributes of words. We show that better lexicons can be learned by using them to predict the tweet sentiment labels. By using a very simple neural network, our method is fast and can take advantage of the same data volume as the NRC method. Experiments show that our lexicons give significantly better accuracies on multiple languages compared to the current best methods. ---------------------------------- **INTRODUCTION** Sentiment lexicons contain the sentiment polarity and/or the strength of words or phrases (Baccianella et al., 2010; Taboada et al., 2011; Tang et al., 2014a; Ren et al., 2016a) . They have been used for both rule-based (Taboada et al., 2011) and unsupervised (Turney, 2002; Hu and Liu, 2004; or supervised<cite> (Mohammad et al., 2013</cite>; Tang et al., 2014b; Vo and Zhang, 2015) machine-learning-based sentiment analysis. As a result, constructing sentiment lexicons is one important research task in sentiment analysis.",
  "y": "background"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_1",
  "x": "On the other hand, the methods are timeconsuming, requiring language and domain expertise. Recently, statistical methods have been exploited to learn sentiment lexicons automatically (Esuli and Sebastiani, 2006; Baccianella et al., 2010;<cite> Mohammad et al., 2013)</cite> . Such methods leverage knowledge resources (Bravo-Marquez et al., 2015) or labeled sentiment data (Tang et al., 2014a) , giving significantly better coverage compared to manual lexicons. Among the automatic methods,<cite> Mohammad et al. (2013)</cite> proposed to use tweets with emoticons or hashtags as training data. The main advantage is that such training data are abundant, and manual annotation can be avoided. Despite that emoticons or hashtags can be noisy in indicating the sentiment of a tweet, existing research (Go et al., 2009; Pak and Paroubek, 2010; Agarwal et al., 2011; Kalchbrenner et al., 2014; Ren et al., 2016b) has shown that effectiveness of such data when used to supervise sentiment classifiers. Mohammad et al. (2013) collect sentiment lexicons by calculating pointwise mutual information (PMI) between words and emoticons. The resulting lexicons give the best results in a SemEval13 benchmark (Nakov et al., 2013) . In this paper, we show that a better lexicon can be learned by directly optimizing the prediction accuracy, taking the lexicon as input and emoticon as the output.",
  "y": "background"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_2",
  "x": "Traditional methods manually label the sentiment attributes of words (Hu and Liu, 2004; Wilson et al., 2005; Taboada et al., 2011) . One benefit of such lexicons is high quality. On the other hand, the methods are timeconsuming, requiring language and domain expertise. Recently, statistical methods have been exploited to learn sentiment lexicons automatically (Esuli and Sebastiani, 2006; Baccianella et al., 2010;<cite> Mohammad et al., 2013)</cite> . Such methods leverage knowledge resources (Bravo-Marquez et al., 2015) or labeled sentiment data (Tang et al., 2014a) , giving significantly better coverage compared to manual lexicons. Among the automatic methods,<cite> Mohammad et al. (2013)</cite> proposed to use tweets with emoticons or hashtags as training data. The main advantage is that such training data are abundant, and manual annotation can be avoided. Despite that emoticons or hashtags can be noisy in indicating the sentiment of a tweet, existing research (Go et al., 2009; Pak and Paroubek, 2010; Agarwal et al., 2011; Kalchbrenner et al., 2014; Ren et al., 2016b) has shown that effectiveness of such data when used to supervise sentiment classifiers. Mohammad et al. (2013) collect sentiment lexicons by calculating pointwise mutual information (PMI) between words and emoticons.",
  "y": "background"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_3",
  "x": "Mohammad et al. (2013) collect sentiment lexicons by calculating pointwise mutual information (PMI) between words and emoticons. The resulting lexicons give the best results in a SemEval13 benchmark (Nakov et al., 2013) . In this paper, we show that a better lexicon can be learned by directly optimizing the prediction accuracy, taking the lexicon as input and emoticon as the output. The correlation between our method and the method of<cite> Mohammad et al. (2013)</cite> is analogous to the \"predicting\" vs \"counting\" correlation between distributional and distributed word representations (Baroni et al., 2014) . We follow Esuli and Sebastiani (2006) in using two simple attributes to represent each sentiment word, and take inspiration from Mikolov et al. (2013) in using a very simple neural network for sentiment prediction. The method can leverage the same data as<cite> Mohammad et al. (2013)</cite> and therefore benefits from both scale and annotation independence. Experiments show that the neural model gives the best results on standard benchmarks across multiple languages. Our code and lexicons are publicly available at https://github.com/duytinvo/acl2016. ----------------------------------",
  "y": "background"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_4",
  "x": "The correlation between our method and the method of<cite> Mohammad et al. (2013)</cite> is analogous to the \"predicting\" vs \"counting\" correlation between distributional and distributed word representations (Baroni et al., 2014) . We follow Esuli and Sebastiani (2006) in using two simple attributes to represent each sentiment word, and take inspiration from Mikolov et al. (2013) in using a very simple neural network for sentiment prediction. The method can leverage the same data as<cite> Mohammad et al. (2013)</cite> and therefore benefits from both scale and annotation independence. Experiments show that the neural model gives the best results on standard benchmarks across multiple languages. Our code and lexicons are publicly available at https://github.com/duytinvo/acl2016. ---------------------------------- **RELATED WORK** Existing methods for automatically learning sentiment lexicons can be classified into three main categories. The first category augments existing lexicons with sentiment information.",
  "y": "differences"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_5",
  "x": "Mohammad et al. (2013) collect sentiment lexicons by calculating pointwise mutual information (PMI) between words and emoticons. The resulting lexicons give the best results in a SemEval13 benchmark (Nakov et al., 2013) . In this paper, we show that a better lexicon can be learned by directly optimizing the prediction accuracy, taking the lexicon as input and emoticon as the output. The correlation between our method and the method of<cite> Mohammad et al. (2013)</cite> is analogous to the \"predicting\" vs \"counting\" correlation between distributional and distributed word representations (Baroni et al., 2014) . We follow Esuli and Sebastiani (2006) in using two simple attributes to represent each sentiment word, and take inspiration from Mikolov et al. (2013) in using a very simple neural network for sentiment prediction. The method can leverage the same data as<cite> Mohammad et al. (2013)</cite> and therefore benefits from both scale and annotation independence. Experiments show that the neural model gives the best results on standard benchmarks across multiple languages. Our code and lexicons are publicly available at https://github.com/duytinvo/acl2016. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_6",
  "x": "This approach can be used to automatically extract multilingual sentiment lexicons Mohammad et al., 2015) without using manual resources, which makes it more flexible compared to the first two methods. We consider it as our baseline. We use the same data source as<cite> Mohammad et al. (2013)</cite> to train lexicons. However, rather than relying on PMI, we take a machine-learning method in optimizing the prediction accuracy of emoticons using the lexicons. To leverage large data, we use a very simple neural network to train the lexicons. ---------------------------------- **BASELINE** Mohammad et al. (2013) employ emoticons and relevant hashtags contained in a tweet as the sentiment label of the tweet. Given a set of tweets with their labels, the sentiment score (SS) for a token w was computed as:",
  "y": "uses"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_7",
  "x": "Evaluation: We follow in employing precision (P), recall (R) and F1 score (F) to evaluate unsupervised classification. We follow Hsu et al. (2003) and use accuracy (acc), the tuning criterion, to evaluate supervised classification. Code and lexicons: We make the Python implementation of our models and the resulting sentiment lexicons available at https://github.com/duytinvo/acl2016 Table 4 : Standard splits of ASTD. ---------------------------------- **ENGLISH LEXICONS** The Twitter benchmark of SemEval13 (Nakov et al., 2013 ) is used as the English test set. In order to evaluate both unsupervised and supervised methods, we follow Tang et al. (2014b) and , removing neutral tweets. The statistics is shown in Table 2 . We compare our lexicon with the lexicons of NRC 4<cite> (Mohammad et al., 2013)</cite> , HIT 5 (Tang et al., 2014a) and WEKA 6 (Bravo-Marquez et al., 2015) .",
  "y": "uses"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_8",
  "x": "We compare our lexicon with only the lexicons of NRC 7 , because the methods of Tang et al. (2014a) Table 5 , our lexicon consistently gives the best performance on both the balanced and unbalanced datasets, showing the advantage of \"predicting\" over \"counting\". Table 6 shows examples of our predicting-based lexicon and the counting-based lexicon of<cite> Mohammad et al. (2013)</cite> . First, both lexicons can correctly reflect the strength of emotional words (e.g. bad, worse, worst), which demonstrates that our method can learn statistical relevance as effectively as PMI. Second, we find many cases where our lexicon gives the correct polarity (e.g. suitable, lazy) but the lexicon of<cite> Mohammad et al. (2013)</cite> does not. To quantitatively compare the lexicons, we calculated the accuracies of their polarities (i.e. sign) by using the manually-annotated lexicon of Hu and Liu (2004) as the gold standard. We take the intersection between the automatic lexicons and the lexicon of Hu and Liu (2004) as the test set, which contains 3270 words. The polarity accuracy of our lexicon is 78.2%, in contrast to 76.9% by the lexicon of<cite> Mohammad et al. (2013)</cite> , demonstrating the relative strength of our method. Third, by having two attributes (n, p) instead of one, our lexicon is better in compositionality (e.g. SS(strong memory) > 0, SS(strong snowstorm) < 0). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_9",
  "x": "Table 6 shows examples of our predicting-based lexicon and the counting-based lexicon of<cite> Mohammad et al. (2013)</cite> . First, both lexicons can correctly reflect the strength of emotional words (e.g. bad, worse, worst), which demonstrates that our method can learn statistical relevance as effectively as PMI. Second, we find many cases where our lexicon gives the correct polarity (e.g. suitable, lazy) but the lexicon of<cite> Mohammad et al. (2013)</cite> does not. To quantitatively compare the lexicons, we calculated the accuracies of their polarities (i.e. sign) by using the manually-annotated lexicon of Hu and Liu (2004) as the gold standard. We take the intersection between the automatic lexicons and the lexicon of Hu and Liu (2004) as the test set, which contains 3270 words. The polarity accuracy of our lexicon is 78.2%, in contrast to 76.9% by the lexicon of<cite> Mohammad et al. (2013)</cite> , demonstrating the relative strength of our method. Third, by having two attributes (n, p) instead of one, our lexicon is better in compositionality (e.g. SS(strong memory) > 0, SS(strong snowstorm) < 0). ---------------------------------- **ANALYSIS**",
  "y": "differences"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_10",
  "x": "The standard splits of ASTD are shown in Table 4 . We follow Nabil et al. (2015) by merging training and validating data for learning model. We compare our lexicon with only the lexicons of NRC 7 , because the methods of Tang et al. (2014a) Table 5 , our lexicon consistently gives the best performance on both the balanced and unbalanced datasets, showing the advantage of \"predicting\" over \"counting\". Table 6 shows examples of our predicting-based lexicon and the counting-based lexicon of<cite> Mohammad et al. (2013)</cite> . First, both lexicons can correctly reflect the strength of emotional words (e.g. bad, worse, worst), which demonstrates that our method can learn statistical relevance as effectively as PMI. Second, we find many cases where our lexicon gives the correct polarity (e.g. suitable, lazy) but the lexicon of<cite> Mohammad et al. (2013)</cite> does not. To quantitatively compare the lexicons, we calculated the accuracies of their polarities (i.e. sign) by using the manually-annotated lexicon of Hu and Liu (2004) as the gold standard. We take the intersection between the automatic lexicons and the lexicon of Hu and Liu (2004) as the test set, which contains 3270 words. The polarity accuracy of our lexicon is 78.2%, in contrast to 76.9% by the lexicon of<cite> Mohammad et al. (2013)</cite> , demonstrating the relative strength of our method.",
  "y": "differences"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_11",
  "x": "Second, we find many cases where our lexicon gives the correct polarity (e.g. suitable, lazy) but the lexicon of<cite> Mohammad et al. (2013)</cite> does not. To quantitatively compare the lexicons, we calculated the accuracies of their polarities (i.e. sign) by using the manually-annotated lexicon of Hu and Liu (2004) as the gold standard. We take the intersection between the automatic lexicons and the lexicon of Hu and Liu (2004) as the test set, which contains 3270 words. The polarity accuracy of our lexicon is 78.2%, in contrast to 76.9% by the lexicon of<cite> Mohammad et al. (2013)</cite> , demonstrating the relative strength of our method. Third, by having two attributes (n, p) instead of one, our lexicon is better in compositionality (e.g. SS(strong memory) > 0, SS(strong snowstorm) < 0). ---------------------------------- **ANALYSIS** ---------------------------------- **CONCLUSION**",
  "y": "differences"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_0",
  "x": "We present an approach to improving the precision of an initial document ranking wherein we utilize cluster information within a graph-based framework. The main idea is to perform re-ranking based on centrality within bipartite graphs of documents (on one side) and clusters (on the other side), on the premise that these are mutually reinforcing entities. Links between entities are created via consideration of language models induced from them. We find that our cluster-document graphs give rise to much better retrieval performance than previously proposed document-only graphs do. For example, authority-based re-ranking of documents via a HITS-style cluster-based approach outperforms a previously-proposed PageRank-inspired algorithm applied to solely-document graphs. Moreover, we also show that computing authority scores for clusters constitutes an effective method for identifying clusters containing a large percentage of relevant documents. ---------------------------------- **INTRODUCTION** To improve the precision of retrieval output, especially within the very few (e.g, 5 or 10) highest-ranked documents that are returned, a number of researchers [36, 13, 16, 7, 22, 34, 25, 1, <cite>18,</cite> 9] have considered a structural re-ranking strategy.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_1",
  "x": "To improve the precision of retrieval output, especially within the very few (e.g, 5 or 10) highest-ranked documents that are returned, a number of researchers [36, 13, 16, 7, 22, 34, 25, 1, <cite>18,</cite> 9] have considered a structural re-ranking strategy. The idea is to re-rank the top N documents that some initial search engine produces, where the re-ordering utilizes information about inter-document relationships within that set. Promising results have been previously obtained by using document centrality within the initially retrieved list to perform structural re-ranking, on the premise that if the quality of this list is reasonable to begin with, then the documents that are most related to most of the docuPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'06, August 6-11, 2006 , Seattle, Washington, USA. Copyright 2006 ACM 1-59593-369-7/06/0008 ...$5.00. ments on the list are likely to be the most relevant ones. In particular, in our prior work<cite> [18]</cite> we adapted PageRank [3] -which, due to the success of Google, is surely the most well-established algorithm for defining and computing centrality within a directed graph -to the task of re-ranking non-hyperlinked document sets. The arguably most well-known alternative to PageRank is Kleinberg's HITS algorithm [16] . The major conceptual way in which HITS differs from PageRank is that it defines two different types of central items: each node is assigned both a hub and an authority score as opposed to a single PageRank score.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_2",
  "x": "The arguably most well-known alternative to PageRank is Kleinberg's HITS algorithm [16] . The major conceptual way in which HITS differs from PageRank is that it defines two different types of central items: each node is assigned both a hub and an authority score as opposed to a single PageRank score. In the Web setting, in which HITS was originally proposed, good hubs correspond roughly to highquality resource lists or collections of pointers, whereas good authorities correspond to the high-quality resources themselves; thus, distinguishing between two differing but interdependent types of Webpages is quite appropriate. Our previous study<cite> [18]</cite> applied HITS to non-Web documents. We found that its performance was comparable to or better than that of algorithms that do not involve structural re-ranking; however, HITS was not as effective as PageRank<cite> [18]</cite> . Do these results imply that PageRank is better than HITS for structural re-ranking of non-Web documents? Not necessarily, because there may exist graph-construction methods that are more suitable for HITS. Note that the only entities considered in our previous study were documents. If we could introduce entities distinct from documents but enjoying a mutually reinforcing relationship with them, then we might better satisfy the spirit of the hubs-versus-authorities distinction, and thus derive stronger results utilizing HITS.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_3",
  "x": "To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'06, August 6-11, 2006 , Seattle, Washington, USA. Copyright 2006 ACM 1-59593-369-7/06/0008 ...$5.00. ments on the list are likely to be the most relevant ones. In particular, in our prior work<cite> [18]</cite> we adapted PageRank [3] -which, due to the success of Google, is surely the most well-established algorithm for defining and computing centrality within a directed graph -to the task of re-ranking non-hyperlinked document sets. The arguably most well-known alternative to PageRank is Kleinberg's HITS algorithm [16] . The major conceptual way in which HITS differs from PageRank is that it defines two different types of central items: each node is assigned both a hub and an authority score as opposed to a single PageRank score. In the Web setting, in which HITS was originally proposed, good hubs correspond roughly to highquality resource lists or collections of pointers, whereas good authorities correspond to the high-quality resources themselves; thus, distinguishing between two differing but interdependent types of Webpages is quite appropriate. Our previous study<cite> [18]</cite> applied HITS to non-Web documents. We found that its performance was comparable to or better than that of algorithms that do not involve structural re-ranking; however, HITS was not as effective as PageRank<cite> [18]</cite> .",
  "y": "motivation background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_4",
  "x": "If we could introduce entities distinct from documents but enjoying a mutually reinforcing relationship with them, then we might better satisfy the spirit of the hubs-versus-authorities distinction, and thus derive stronger results utilizing HITS. A crucial insight of the present paper is that document clusters appear extremely well-suited to play this complementary role. The intuition is that: (a) given those clusters that are \"most representative\" of the user's information need, the documents within those clusters are likely to be relevant; and (b) the \"most representative\" clusters should be those that contain many relevant documents. This apparently circular reasoning is strongly reminiscent of the interrelated hubs and authorities concepts underlying HITS. Also, clusters have long been considered a promising source of information. The well-known cluster hypothesis [35] encapsulates the intuition that clusters can reveal groups of relevant documents; in practice, the potential utility of clustering for this purpose has been demonstrated for both the case wherein clusters were created in a query-independent fashion [14, 4] and the re-ranking setting [13, 22, 34] . In this paper, we show through an array of experiments that consideration of the mutual reinforcement of clusters and documents in determining centrality can lead to highly effective algorithms for re-ranking an initially retrieved list. Specifically, our experimental results show that the centralityinduction methods that we previously studied solely in the context of document-only graphs<cite> [18]</cite> result in much better re-ranking performance if implemented over bipartite graphs of documents (on one side) and clusters (on the other side). For example, ranking documents by their \"authoritativeness\" as computed by HITS upon these cluster-document graphs yields better performance than that of a previously proposed PageRank implementation applied to documentonly graphs.",
  "y": "extends"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_5",
  "x": "Contrariwise, 2 we can create document-as-hub graphs by setting V Left = Dinit and V Right = C l(Dinit). But the advantages of incorporating cluster-based information are not just formal. The well-known cluster hypothesis [35] encapsulates the intuition that clusters can reveal groups of relevant documents; in practice, the potential utility of clustering for this purpose has been demonstrated a number of times, whether the clusters were created in a query-independent fashion [14, 4] , or from the initially mosthighly-ranked documents for some query [13, 22, 34] (i.e., in the re-ranking setting). Since central clusters are, supposedly, those that accrue the most evidence for relevance, documents that are strongly identified with such clusters should themselves be judged highly relevant. 3 4 But identifying such clusters is facilitated by knowledge of which documents are most likely to be relevant -exactly the mutual reinforcement property that HITS was designed to leverage. ---------------------------------- **ALTERNATIVE SCORES: PAGERANK AND INFLUX** We will compare the results of using the HITS algorithm against those derived using PageRank instead. This is a natural comparison because PageRank is the most well-known centrality-induction algorithm utilized for ranking documents, and because in earlier work<cite> [18]</cite> , PageRank performed quite well as a tool for structural re-ranking of non-Web documents, at least when applied to document-to-document graphs.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_6",
  "x": "---------------------------------- **ALTERNATIVE SCORES: PAGERANK AND INFLUX** We will compare the results of using the HITS algorithm against those derived using PageRank instead. This is a natural comparison because PageRank is the most well-known centrality-induction algorithm utilized for ranking documents, and because in earlier work<cite> [18]</cite> , PageRank performed quite well as a tool for structural re-ranking of non-Web documents, at least when applied to document-to-document graphs. One can think of PageRank as a version of HITS in which the hub/authority distinction has been collapsed. Thus, writing \"PR\" for both auth and hub, we conceptually have the (single) equation However, in practice, we incorporate Brin and Page's smoothing scheme [3] together with a correction for nodes with no positive-weight edges emanating from them [27, 21] : where out(u) , and \u03bb \u2208 (0, 1) is the damping factor.",
  "y": "uses background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_7",
  "x": "Equation 4 is recursive, but there are iterative algorithms that provably converge to the unique positive solution PR * satisfying the sum-normalization constraint P v\u2208V PR(v) = 1 [21] . Moreover, a (non-trivial) closed-form -and quite easily computed -solution exists for one-way bipartite graphs: is an affine transformation (with respect to positive constants) of, and therefore equivalent for ranking purposes to, the unique positive sum-normalized solution to Equation 4. (Proof omitted due to space constraints.) Interestingly, this result shows that while one might have thought that clusters and documents would \"compete\" for PageRank score when placed within the same graph, in our document-as-authority and document-as-hub graphs this is not the case. Earlier work<cite> [18]</cite> also considered scoring a node v by its influx, P u\u2208V w t(u \u2192 v). This can be viewed as either a non-recursive version of Equation 3, or as an un-normalized analog of Equation 5. ---------------------------------- **ALGORITHMS BASED ON CENTRALITY SCORES** Clearly, we can rank documents by their scores as computed by any of the functions introduced above. But when we operate on document-as-authority or document-as-hub graphs, centrality scores for the clusters are also produced.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_8",
  "x": "**RELATED WORK** The potential merits of query-dependent clustering, that is, clustering the documents retrieved in response to a query, have long been recognized [30, 36, 23, 34, 25] , especially in interactive retrieval settings [13, 22, 32] . However, automatically detecting clusters that contain many relevant documents remains a very hard task [36] . Section 5.2 presents results for detecting such clusters using centrality-based cluster ranking. Recently, there has been a growing body of work on graphbased modeling for different language-processing tasks wherein links are induced by inter-entity textual similarities. Examples include document (re-)ranking [7, 24, 9, <cite>18,</cite> 39 ], text summarization [11, 26] , sentence retrieval [28] , and document representation [10] . In contrast to our methods, links connect entities of the same type, and clusters of entities are not modeled within the graphs. While ideas similar to ours by virtue of leveraging the mutual reinforcement of entities of different types, or using bipartite graphs of such entities for clustering (rather than using clusters), are abundant (e.g., [15, 8, 2] ), we focus here on exploiting mutual reinforcement in ad hoc retrieval. Random walks (with early stopping) over bipartite graphs of terms and documents were used for query expansion [20] , but in contrast to our work, no stationary solution was sought.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_9",
  "x": "Random walks (with early stopping) over bipartite graphs of terms and documents were used for query expansion [20] , but in contrast to our work, no stationary solution was sought. A similar \"short chain\" approach utilizing bipartite graphs of clusters and documents for ranking an entire corpus was recently proposed [19] , thereby constituting the work most resembling ours. However, again, a stationary distribution was not sought. Also, query drift prevention mechanisms were required to obtain good performance; in our re-ranking setting, we need not employ such mechanisms. ---------------------------------- **EVALUATION FRAMEWORK** Most aspects of the evaluation framework described below are adopted from our previous experiments with noncluster-based structural re-ranking<cite> [18]</cite> so as to facilitate direct comparison. Section 4.1 of<cite> [18]</cite> provides a more detailed justification of the experimental design. The main conceptual changes 6 here are: a slightly larger parameter search-space for the \"out-degree\" parameter \u03b4 (called the \"ancestry\" parameter \u03b1 in<cite> [18]</cite> ); and, of course, the incorporation of clusters.",
  "y": "uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_10",
  "x": "However, again, a stationary distribution was not sought. Also, query drift prevention mechanisms were required to obtain good performance; in our re-ranking setting, we need not employ such mechanisms. ---------------------------------- **EVALUATION FRAMEWORK** Most aspects of the evaluation framework described below are adopted from our previous experiments with noncluster-based structural re-ranking<cite> [18]</cite> so as to facilitate direct comparison. Section 4.1 of<cite> [18]</cite> provides a more detailed justification of the experimental design. The main conceptual changes 6 here are: a slightly larger parameter search-space for the \"out-degree\" parameter \u03b4 (called the \"ancestry\" parameter \u03b1 in<cite> [18]</cite> ); and, of course, the incorporation of clusters. ---------------------------------- **GRAPH CONSTRUCTION**",
  "y": "uses background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_11",
  "x": "Section 4.1 of<cite> [18]</cite> provides a more detailed justification of the experimental design. The main conceptual changes 6 here are: a slightly larger parameter search-space for the \"out-degree\" parameter \u03b4 (called the \"ancestry\" parameter \u03b1 in<cite> [18]</cite> ); and, of course, the incorporation of clusters. ---------------------------------- **GRAPH CONSTRUCTION** Relevance flow based on language models (LMs). To estimate the degree to which one item, if considered relevant, can vouch for the relevance of another, we follow our previous work on document-based graphs<cite> [18]</cite> and utilize p [\u00b5] d (\u00b7), the unigram Dirichlet-smoothed language model induced from a given document d (\u00b5 is the smoothing parameter) [38] . To adapt this estimation scheme to settings involving clusters, we derive the language model p c (\u00b7) for a cluster c by treating c as the (large) document formed by concatenating 7 its constituent (or most strongly associated) documents [17, 25, 19] . The relevance-flow measure we use is essentially a directed similarity in language-model space: where D is the Kullback-Leibler divergence.",
  "y": "differences"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_12",
  "x": "Relevance flow based on language models (LMs). To estimate the degree to which one item, if considered relevant, can vouch for the relevance of another, we follow our previous work on document-based graphs<cite> [18]</cite> and utilize p [\u00b5] d (\u00b7), the unigram Dirichlet-smoothed language model induced from a given document d (\u00b5 is the smoothing parameter) [38] . To adapt this estimation scheme to settings involving clusters, we derive the language model p c (\u00b7) for a cluster c by treating c as the (large) document formed by concatenating 7 its constituent (or most strongly associated) documents [17, 25, 19] . The relevance-flow measure we use is essentially a directed similarity in language-model space: where D is the Kullback-Leibler divergence. The asymmetry of this measure corresponds nicely to the intuition that relevance flow is not symmetric<cite> [18]</cite> . Moreover, this function 6 Some of the PageRank results appearing in our previous paper<cite> [18]</cite> accidentally reflect experiments utilizing a suboptimal choice of Dinit. For citation purposes, the numbers reported in the current paper should be used. 7 Concatenation order is irrelevant for unigram LMs.",
  "y": "uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_13",
  "x": "The main conceptual changes 6 here are: a slightly larger parameter search-space for the \"out-degree\" parameter \u03b4 (called the \"ancestry\" parameter \u03b1 in<cite> [18]</cite> ); and, of course, the incorporation of clusters. ---------------------------------- **GRAPH CONSTRUCTION** Relevance flow based on language models (LMs). To estimate the degree to which one item, if considered relevant, can vouch for the relevance of another, we follow our previous work on document-based graphs<cite> [18]</cite> and utilize p [\u00b5] d (\u00b7), the unigram Dirichlet-smoothed language model induced from a given document d (\u00b5 is the smoothing parameter) [38] . To adapt this estimation scheme to settings involving clusters, we derive the language model p c (\u00b7) for a cluster c by treating c as the (large) document formed by concatenating 7 its constituent (or most strongly associated) documents [17, 25, 19] . The relevance-flow measure we use is essentially a directed similarity in language-model space: where D is the Kullback-Leibler divergence. The asymmetry of this measure corresponds nicely to the intuition that relevance flow is not symmetric<cite> [18]</cite> .",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_14",
  "x": "The asymmetry of this measure corresponds nicely to the intuition that relevance flow is not symmetric<cite> [18]</cite> . Moreover, this function 6 Some of the PageRank results appearing in our previous paper<cite> [18]</cite> accidentally reflect experiments utilizing a suboptimal choice of Dinit. For citation purposes, the numbers reported in the current paper should be used. 7 Concatenation order is irrelevant for unigram LMs. is somewhat insensitive to large length differences between the items in question<cite> [18]</cite> , which is advantageous when both documents and clusters (which we treat as very long documents) are considered. Previous work<cite> [18,</cite> 33] makes heavy use of the idea of nearest neighbors in language-model space. It is therefore convenient to introduce the notation N bhd(x | m, R), pronounced \"neighborhood\", to denote the m items y within the \"restriction set\" R that have the highest values of rflow(x, y) (we break ties by item ID, assuming that these have been assigned to documents and clusters). Note that the neighborhood of x corresponds to what we previously termed the \"top generators\" of x<cite> [18]</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_15",
  "x": "is somewhat insensitive to large length differences between the items in question<cite> [18]</cite> , which is advantageous when both documents and clusters (which we treat as very long documents) are considered. Previous work<cite> [18,</cite> 33] makes heavy use of the idea of nearest neighbors in language-model space. It is therefore convenient to introduce the notation N bhd(x | m, R), pronounced \"neighborhood\", to denote the m items y within the \"restriction set\" R that have the highest values of rflow(x, y) (we break ties by item ID, assuming that these have been assigned to documents and clusters). Note that the neighborhood of x corresponds to what we previously termed the \"top generators\" of x<cite> [18]</cite> . ---------------------------------- **GRAPHS USED IN EXPERIMENTS.** For a given set Dinit of initially retrieved documents and positive integer \u03b4 (an \"outdegree\" parameter), we consider the following three graphs. Each connects nodes u to the \u03b4 other nodes, drawn from some specified set, that u has the highest relevance flow to. The document-to-document graph d\u2194d has vertex set Dinit and weight function",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_16",
  "x": "7 Concatenation order is irrelevant for unigram LMs. is somewhat insensitive to large length differences between the items in question<cite> [18]</cite> , which is advantageous when both documents and clusters (which we treat as very long documents) are considered. Previous work<cite> [18,</cite> 33] makes heavy use of the idea of nearest neighbors in language-model space. It is therefore convenient to introduce the notation N bhd(x | m, R), pronounced \"neighborhood\", to denote the m items y within the \"restriction set\" R that have the highest values of rflow(x, y) (we break ties by item ID, assuming that these have been assigned to documents and clusters). Note that the neighborhood of x corresponds to what we previously termed the \"top generators\" of x<cite> [18]</cite> . ---------------------------------- **GRAPHS USED IN EXPERIMENTS.** For a given set Dinit of initially retrieved documents and positive integer \u03b4 (an \"outdegree\" parameter), we consider the following three graphs. Each connects nodes u to the \u03b4 other nodes, drawn from some specified set, that u has the highest relevance flow to.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_17",
  "x": "Previous work<cite> [18,</cite> 33] makes heavy use of the idea of nearest neighbors in language-model space. It is therefore convenient to introduce the notation N bhd(x | m, R), pronounced \"neighborhood\", to denote the m items y within the \"restriction set\" R that have the highest values of rflow(x, y) (we break ties by item ID, assuming that these have been assigned to documents and clusters). Note that the neighborhood of x corresponds to what we previously termed the \"top generators\" of x<cite> [18]</cite> . ---------------------------------- **GRAPHS USED IN EXPERIMENTS.** For a given set Dinit of initially retrieved documents and positive integer \u03b4 (an \"outdegree\" parameter), we consider the following three graphs. Each connects nodes u to the \u03b4 other nodes, drawn from some specified set, that u has the highest relevance flow to. The document-to-document graph d\u2194d has vertex set Dinit and weight function The document-as-authority graph c\u2192d has vertex set Dinit\u222a C l(Dinit) and a weight function such that positive-weight edges go only from clusters to documents:",
  "y": "similarities background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_18",
  "x": "There are two motivations underlying our approach to choosing values for our algorithms' parameters<cite> [18]</cite> . First, we hope to show that structural re-ranking can provide better results than the optimized baselines even when initialized with a sub-optimal (yet reasonable) ranking. Hence, let the initial ranking be the document ordering induced on the entire corpus by p (q), where \u00b51000 is the smoothing-parameter value optimizing the average noninterpolated precision of the top 1000 documents. We set Dinit to the top 50 documents in the initial ranking. Second, we wish to show that good results can be achieved without a great deal of parameter tuning. Therefore, we did not tune the smoothing parameter for any of the language models used to determine graph edge-weights, but rather simply set \u00b5 = 2000 when smoothing was required, following a prior suggestion [38] . Also, the other free parameters' values were chosen so as to optimize prec@5, regardless of the evaluation metric under consideration. 8 As a consequence, our prec@10 and MRR results are presumably not as high as possible; but the advantage of our policy is that we can see whether optimization with respect to a fixed criterion yields good results no matter how \"goodness\" is measured.",
  "y": "motivation"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_19",
  "x": "---------------------------------- **RE-RANKING BY DOCUMENT CENTRALITY** Main result. We first consider our main question: can we substantially boost the effectiveness of HITS by applying it to cluster-to-document graphs, which we have argued are more suitable for it than the document-to-document graphs we constructed in our previous work<cite> [18]</cite> ? The answer, as shown in Table 1 , is clearly \"yes\": we see that moving to cluster-to-document graphs results in substantial improvement for HITS, and indeed boosts its results over those for PageRank on document-to-document graphs. Full suite of comparisons. We now turn to Figure 2 , which gives the results for the re-ranking algorithms docInflux, doc-PageRank and doc-Auth as applied to either the document-based graph d\u2194d (as in<cite> [18]</cite> ) or the clusterdocument graph c\u2192d. (Discussion of doc-Hub is deferred to Section 5.3.) To focus our discussion, it is useful to first point out that in almost all of our nine evaluation settings (3 corpora \u00d7 3 evaluation measures), all three of the re-ranking algorithms perform better when applied to c\u2192d graphs than to d\u2194d graphs, as the number of dark bars in Figure 2 indicates.",
  "y": "motivation"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_20",
  "x": "Full suite of comparisons. We now turn to Figure 2 , which gives the results for the re-ranking algorithms docInflux, doc-PageRank and doc-Auth as applied to either the document-based graph d\u2194d (as in<cite> [18]</cite> ) or the clusterdocument graph c\u2192d. (Discussion of doc-Hub is deferred to Section 5.3.) To focus our discussion, it is useful to first point out that in almost all of our nine evaluation settings (3 corpora \u00d7 3 evaluation measures), all three of the re-ranking algorithms perform better when applied to c\u2192d graphs than to d\u2194d graphs, as the number of dark bars in Figure 2 indicates. Since it is thus clearly useful to incorporate cluster-based information, we will now mainly concentrate on c\u2192d-based algorithms. The results for prec@5, the metric for which the re-ranking algorithms' parameters were optimized, show that all c\u2192d-based algorithms outperform the prec@5-optimized baseline -significantly so for the AP corpus -even though applied to a sub-optimally-ranked initial set. (We hasten to point out that while the initial ranking is always inferior to the corresponding optimized baseline, the differences are never significant.) In contrast, the use of d\u2194d graphs never leads to significantly superior prec@5 results. We also observe in Figure 2 that the doc-Auth[c\u2192d] algorithm is always either the best of the c\u2192d-based algorithms or clearly competitive with the best. We also experimented with a few alternate graph-construction methods, such as sum-normalizing the weights of edges out of nodes, and found that the doc-Auth[c\u2192d] algorithm remained superior to doc-Influx[c\u2192d] and doc-PageRank[c\u2192d].",
  "y": "uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_21",
  "x": "c (q) in detecting clusters with a high percentage of relevant documents -thereby neutralizing within-cluster ranking effects -we present in Table 3 the percent of documents in the highest ranked cluster that are relevant. (Cluster size (k) was fixed to either 5 or 10 and out-degree (\u03b4) was chosen to optimize the above percentage.) Indeed, these results clearly show that our best cluster-based algorithms are much better than clust-p We see that in many cases, hub-based re-ranking does yield better performance than the initial ranking. But authoritybased re-ranking appears to be an even better choice overall. ---------------------------------- **FURTHER ANALYSIS** HITS on PageRank-style graphs. Consider our comparison of doc-Auth[d\u2194d] against doc-PageRank[d\u2194d]. As the notation suggests, this corresponds to running HITS and PageRank on the same graph, d\u2194d. But an alternative interpretation<cite> [18]</cite> is that non-smoothed (or no-random-jump) PageRank, as expressed by Equation (3), is applied to a different version of d\u2194d wherein the original edge weights w t(u \u2192 v) have been smoothed as follows: (we ignore nodes with no positive-weight out-edges to simplify discussion, and omit the d\u2194d superscripts for clarity).",
  "y": "differences"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_22",
  "x": "Only in two of the nine evaluation settings did this change cause an increase in performance of docAuth[c\u2192d] over the results attained under the original edgeweighting scheme, despite the fact that the re-weighting involves an extra free parameter. Thus, while we have already demonstrated in previous sections of this paper that information about document-cluster similarity relationships is very valuable, the results just mentioned suggest that such information is more useful in \"raw\" form. Re-anchoring to the query. In previous work, we showed that PageRank centrality scores induced over documentbased graphs can be used as a multiplicative weight on document query-likelihood terms, the intent being to cope with cases in which centrality in Dinit and relevance are not strongly correlated<cite> [18]</cite> . Indeed, employing this technique on the AP, TREC8, and WSJ corpora, prec@5 increases from .519, .524 and .536, to .531, .56 and .572 respectively. The same modification could be applied to the c\u2192d-based algorithms, although it is not particularly well-motivated in the HITS case. While PageRank scores correspond to a stationary distribution that could be loosely interpreted as a prior<cite> [18]</cite> , in which case multiplicative combination with query likelihood is sensible, it is not usual to assign a probabilistic interpretation to hub or authority scores. Nonetheless, for the sake of comparison completeness, we applied this idea to the doc-Auth[c\u2192d] algorithm, yielding the following performance changes: from .541, .544, and .564 to .537, .572 and .572 respectively. These results are still as good as -and for two corpora better than -those for PageRank as a multiplicative weight on query likelihood.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_23",
  "x": "The same modification could be applied to the c\u2192d-based algorithms, although it is not particularly well-motivated in the HITS case. While PageRank scores correspond to a stationary distribution that could be loosely interpreted as a prior<cite> [18]</cite> , in which case multiplicative combination with query likelihood is sensible, it is not usual to assign a probabilistic interpretation to hub or authority scores. Nonetheless, for the sake of comparison completeness, we applied this idea to the doc-Auth[c\u2192d] algorithm, yielding the following performance changes: from .541, .544, and .564 to .537, .572 and .572 respectively. These results are still as good as -and for two corpora better than -those for PageRank as a multiplicative weight on query likelihood. Thus, it may be the case that centrality scores induced over a document-based graph are more effective as a multiplicative bias on query-likelihood than as direct representations of relevance in Dinit (see also<cite> [18]</cite> ); but, modulo the caveat above, it seems that when centrality is induced over cluster-based one-way bipartite graphs, the correlation with relevance is much stronger, and hence this kind of centrality serves as a better \"bias\" on query-likelihood. ---------------------------------- **CONCLUSION** We have shown that leveraging the mutually reinforcing relationship between clusters and documents to determine centrality is very beneficial not only for directly finding relevant documents in an initially retrieved list, but also for finding clusters of documents from this list that contain a high number of relevant documents. Specifically, we demonstrated the superiority of clusterdocument bipartite graphs to document-only graphs as the input to centrality-induction algorithms.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_24",
  "x": "The same modification could be applied to the c\u2192d-based algorithms, although it is not particularly well-motivated in the HITS case. While PageRank scores correspond to a stationary distribution that could be loosely interpreted as a prior<cite> [18]</cite> , in which case multiplicative combination with query likelihood is sensible, it is not usual to assign a probabilistic interpretation to hub or authority scores. Nonetheless, for the sake of comparison completeness, we applied this idea to the doc-Auth[c\u2192d] algorithm, yielding the following performance changes: from .541, .544, and .564 to .537, .572 and .572 respectively. These results are still as good as -and for two corpora better than -those for PageRank as a multiplicative weight on query likelihood. Thus, it may be the case that centrality scores induced over a document-based graph are more effective as a multiplicative bias on query-likelihood than as direct representations of relevance in Dinit (see also<cite> [18]</cite> ); but, modulo the caveat above, it seems that when centrality is induced over cluster-based one-way bipartite graphs, the correlation with relevance is much stronger, and hence this kind of centrality serves as a better \"bias\" on query-likelihood. ---------------------------------- **CONCLUSION** We have shown that leveraging the mutually reinforcing relationship between clusters and documents to determine centrality is very beneficial not only for directly finding relevant documents in an initially retrieved list, but also for finding clusters of documents from this list that contain a high number of relevant documents. Specifically, we demonstrated the superiority of clusterdocument bipartite graphs to document-only graphs as the input to centrality-induction algorithms.",
  "y": "uses background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_25",
  "x": "In previous work, we showed that PageRank centrality scores induced over documentbased graphs can be used as a multiplicative weight on document query-likelihood terms, the intent being to cope with cases in which centrality in Dinit and relevance are not strongly correlated<cite> [18]</cite> . Indeed, employing this technique on the AP, TREC8, and WSJ corpora, prec@5 increases from .519, .524 and .536, to .531, .56 and .572 respectively. The same modification could be applied to the c\u2192d-based algorithms, although it is not particularly well-motivated in the HITS case. While PageRank scores correspond to a stationary distribution that could be loosely interpreted as a prior<cite> [18]</cite> , in which case multiplicative combination with query likelihood is sensible, it is not usual to assign a probabilistic interpretation to hub or authority scores. Nonetheless, for the sake of comparison completeness, we applied this idea to the doc-Auth[c\u2192d] algorithm, yielding the following performance changes: from .541, .544, and .564 to .537, .572 and .572 respectively. These results are still as good as -and for two corpora better than -those for PageRank as a multiplicative weight on query likelihood. Thus, it may be the case that centrality scores induced over a document-based graph are more effective as a multiplicative bias on query-likelihood than as direct representations of relevance in Dinit (see also<cite> [18]</cite> ); but, modulo the caveat above, it seems that when centrality is induced over cluster-based one-way bipartite graphs, the correlation with relevance is much stronger, and hence this kind of centrality serves as a better \"bias\" on query-likelihood. ---------------------------------- **CONCLUSION**",
  "y": "background"
 },
 {
  "id": "74471d4e333ce76fd62b968045eba5_0",
  "x": "****DEEP BAYESIAN LEARNING AND UNDERSTANDING**** **ABSTRACT** Given the current growth in research and related emerging technologies in machine learning and deep learning, it is timely to introduce this tutorial to a large number of researchers and practitioners who are attending COLING 2018 and working on statistical models, deep neural networks, sequential learning and natural language understanding. To the best of our knowledge, there is no similar tutorial presented in previous ACL/COLING/EMNLP/NAACL. This three-hour tutorial will concentrate on a wide range of theories and applications and systematically present the recent advances in deep Bayesian and sequential learning which are impacting the communities of computational linguistics, human language technology and machine learning for natural language processing. ---------------------------------- **TUTORIAL DESCRIPTION** This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (Saon and Chien, 2012; Chan et al., 2016) to document summarization (Chang and Chien, 2009 ), text classification (Blei et al., 2003; <cite>Zhang et al., 2015</cite>) , text segmentation (Chien and Chueh, 2012) , information extraction (Narasimhan et al., 2016) , image caption generation (Vinyals et al., 2015; Xu et al., 2015) , sentence generation (Li et al., 2016b) , dialogue control (Zhao and Eskenazi, 2016; Li et al., 2016a) , sentiment classification, recommendation system, question answering (Sukhbaatar et al., 2015) and machine translation , to name a few. Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model.",
  "y": "uses background"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_0",
  "x": "And recently, Muis and Lu (2017) introduced a multigraph representation based on mention separators for this task. All of these models depend on manually crafted features. In addition, they cannot be directly applied to extend current state-of-the-art recurrent neural networkbased models -for flat named entity recognition (Lample et al., 2016) or the joint extraction of entities and relations (Katiyar and Cardie, 2016) to handle nested entities. In this paper, we propose a recurrent neural network-based model for nested named entity and nested entity mention recognition. We present a modification to the standard LSTM-based sequence labeling model that handles both problems and operates linearly in the number of tokens and the number of possible output labels at any token. The proposed neural network approach additionally jointly models entity mention head 2 information, a subtask found to be useful for many information extraction applications. Our model significantly outperforms the previously mentioned hypergraph model of<cite> Lu and Roth (2015)</cite> and Muis and Lu (2017) on entity mention recognition for the ACE2004 and ACE2005 corpora. It also outperforms their model on joint extraction of nested entity mentions and their heads. Finally, we evaluate our approach on nested named entity recognition using the GENIA dataset and show that our model outperforms the previous state-of-the-art parser-based approach of Finkel and Manning (2009) .",
  "y": "differences"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_1",
  "x": "However, the time complexity of their model is O(n 3 ), where n is the number of tokens in the sentence, making inference slow. As a result, we do not adopt their parse tree-based representation of nested entities and propose instead a linear time directed hypergraph-based model similar to that of<cite> Lu and Roth (2015)</cite> . Directed hypergraphs were also introduced for parsing by Klein and Manning (2001) . While most previous efforts for nested entity recognition were limited to named entities,<cite> Lu and Roth (2015)</cite> addressed the problem of nested entity mention detection where mentions can either be named, nominal or pronominal. Their hypergraph-based approach is able to represent the potentially exponentially many combinations of nested mentions of different types. They adopted a CRF-like log-linear approach to learn these mention hypergraphs and employed several hand-crafted features defined over the input sentence and the output hypergraph structure. Our approach also learns a similar hypergraph representation with differences in the types of nodes and edges in the hypergraph. It does not depend on any manually crafted features. Also, our model learns the hypergraph greedily and significantly outperforms their approach.",
  "y": "similarities"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_2",
  "x": "While most previous efforts for nested entity recognition were limited to named entities,<cite> Lu and Roth (2015)</cite> addressed the problem of nested entity mention detection where mentions can either be named, nominal or pronominal. Their hypergraph-based approach is able to represent the potentially exponentially many combinations of nested mentions of different types. They adopted a CRF-like log-linear approach to learn these mention hypergraphs and employed several hand-crafted features defined over the input sentence and the output hypergraph structure. Our approach also learns a similar hypergraph representation with differences in the types of nodes and edges in the hypergraph. It does not depend on any manually crafted features. Also, our model learns the hypergraph greedily and significantly outperforms their approach. Recently, Muis and Lu (2017) introduced the notion of mention separators for nested entity mention detection. In contrast to the hypergraph representation that we and<cite> Lu and Roth (2015)</cite> adopt, they learn a multigraph representation and are able to perform exact inference on their structure. It is an interesting orthogonal possible approach for nested entity mention detection.",
  "y": "background"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_3",
  "x": "They adopted a CRF-like log-linear approach to learn these mention hypergraphs and employed several hand-crafted features defined over the input sentence and the output hypergraph structure. Our approach also learns a similar hypergraph representation with differences in the types of nodes and edges in the hypergraph. It does not depend on any manually crafted features. Also, our model learns the hypergraph greedily and significantly outperforms their approach. Recently, Muis and Lu (2017) introduced the notion of mention separators for nested entity mention detection. In contrast to the hypergraph representation that we and<cite> Lu and Roth (2015)</cite> adopt, they learn a multigraph representation and are able to perform exact inference on their structure. It is an interesting orthogonal possible approach for nested entity mention detection. How-ever, we will show that our model also outperforms their approach on all tasks. Recently, recurrent neural networks (RNNs) have been widely applied to several sequence labeling tasks achieving state-of-the-art results.",
  "y": "uses"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_4",
  "x": "This is accomplished by collapsing the shared states (labels) in the output entity label sequences into a single state as shown in Figure 2 : e.g., the three \"O\" labels for \"that\" become a single \"O\"; the two \"B PER\" labels at \"his\" are collapsed into one \"B PER\" node that joins \"U PER\", the latter of which represents the entity mention \"his\". Thus at any time step, the representation size is bounded by the number of possible output states instead of the potentially exponential number of output sequences. We then also adjust the directed edges such that they have the same type of head node and the same type of tail node as before in Figure 1 . If we look closely at Figure 2 then we realise that there is an extra \"O\" node in the hypergraph corresponding to the token \"his\" which did not appear in any entity output sequence in Figure 1 : in our task-specific hypergraph construction we make sure that there is an \"O\" node at every timestep to model the possibility of beginning of a new entity. The need for this will become more clear in Section 4. Note that the hypergraph representation of our model is similar to<cite> Lu and Roth (2015)</cite> . Also, the expressiveness of our model is exactly the same as<cite> Lu and Roth (2015)</cite> ; Muis and Lu (2017) . The major difference in the two approaches is in learning. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_5",
  "x": "This is accomplished by collapsing the shared states (labels) in the output entity label sequences into a single state as shown in Figure 2 : e.g., the three \"O\" labels for \"that\" become a single \"O\"; the two \"B PER\" labels at \"his\" are collapsed into one \"B PER\" node that joins \"U PER\", the latter of which represents the entity mention \"his\". Thus at any time step, the representation size is bounded by the number of possible output states instead of the potentially exponential number of output sequences. We then also adjust the directed edges such that they have the same type of head node and the same type of tail node as before in Figure 1 . If we look closely at Figure 2 then we realise that there is an extra \"O\" node in the hypergraph corresponding to the token \"his\" which did not appear in any entity output sequence in Figure 1 : in our task-specific hypergraph construction we make sure that there is an \"O\" node at every timestep to model the possibility of beginning of a new entity. The need for this will become more clear in Section 4. Note that the hypergraph representation of our model is similar to<cite> Lu and Roth (2015)</cite> . Also, the expressiveness of our model is exactly the same as<cite> Lu and Roth (2015)</cite> ; Muis and Lu (2017) . The major difference in the two approaches is in learning. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_6",
  "x": "**DATA** We perform experiments on the English section of the ACE2004 and ACE2005 corpora. There are 7 main entity types -Person (PER), Organization (ORG), Geographical Entities (GPE), Location (LOC), Facility (FAC), Weapon (WEA) and Vehicle (VEH). For each entity type, there are annotations for the entity mention and mention heads. ---------------------------------- **EVALUATION METRICS** We use a strict evaluation metric similar to<cite> Lu and Roth (2015)</cite> : an entity mention is considered correct if both the mention span and the mention type are exactly correct. Similarly, for the task of joint extraction of entity mentions and mention heads, the mention span, head span and the entity type should all exactly match the gold label. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_9",
  "x": "We show the performance of our approaches in Table 1 compared to the previous state-of-the-art system<cite> (Lu and Roth, 2015</cite>; Muis and Lu, 2017) on both the ACE2004 and ACE2005 datasets. We find that our LSTM-flat baseline that ignores embedded entity mentions during training performs worse than<cite> Lu and Roth (2015)</cite> ; however, our other neural network-based approaches all outperform the previous feature-based approach. Among the neural network-based models, we find that our models that construct a hypergraph perform better than the LSTM-flat models. Also, our approach that models dependencies between the input and the output by passing the prediction from the pre- vious timestep as shown in Figure 3 performs better than the LSTM-output layer model which only models dependencies at the output layer. Also, as expected, the sparsemax method that produces a sparse probability distribution performs better than the softmax approach for modeling hyperedges. In summary, our sparsemax model is the best performing model. ---------------------------------- **JOINT MODELING OF HEADS** We report the performance of our best performing models on the joint modeling of entity mentions and its head in Table 2.",
  "y": "differences"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_10",
  "x": "---------------------------------- **DATA** We also evaluate our model on the GENIA dataset (Ohta et al., 2002) for nested named entity recognition. We follow the same dataset split as Finkel and Manning (2009);<cite> Lu and Roth (2015)</cite> ; Muis and Lu (2017) . Thus, the first 90% of the sentences were used in training and the remaining 10% were used for evaluation. We also consider five entity types -DNA, RNA, protein, cell line and cell type. ---------------------------------- **BASELINES AND PREVIOUS MODELS** We compare our model with Finkel and Manning (2009) based on a constituency CRF-based parser and the mention hypergraph model by<cite> Lu and Roth (2015)</cite> and a recent multigraph model by Muis and Lu (2017) .",
  "y": "similarities uses"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_12",
  "x": "We suspect that it is because we use pretrained word embeddings 5 trained on PubMed data (Pyysalo et al., 2013) whereas<cite> Lu and Roth (2015)</cite> did not have access to them. We again find that our neural network model outperforms the previous state-of-the-art (Finkel and Manning, 2009; Muis and Lu, 2017) system. However, we see that both softmax and sparsemax models perform comparably on this dataset. ---------------------------------- **ERROR ANALYSIS** Consistent with existing results on the joint modeling of related tasks in NLP, we find that joint modeling of heads and their entity mentions leads to an increase in F-score by 1pt (i.e., 71.4 for the sparsemax model on the ACE2005 dataset) on the performance of the entity mentions. The precision on extracting entity mentions is 72.1 (vs. 70.6 in Table 1) for our sparsemax model for the ACE2005 dataset. Example S1 below compares the output from a softmax vs. a sparsemax model on the joint modeling of an entity mention and its head on the ACE2005 dataset. Gold-standard annotations are shown in red.",
  "y": "differences"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_13",
  "x": "---------------------------------- **PRONOMINAL ENTITY MENTION (IT** ---------------------------------- **CONCLUSION AND FUTURE WORK** In this paper, we present a novel recurrent network-based model for nested named entity recognition and nested entity mention detection. We propose a hypergraph representation for this problem and learn the structure using an LSTM network in a greedy manner. We show that our model significantly outperforms a feature based mention hypergraph model<cite> (Lu and Roth, 2015)</cite> and a recent multigraph model (Muis and Lu, 2017) on the ACE dataset. Our model also outperforms the constituency parser-based approach of Finkel and Manning (2009) on the GENIA dataset. In future work, it would be interesting to learn global dependencies between the output labels for such a hypergraph structure and training the model globally.",
  "y": "differences"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_0",
  "x": "It is distinct from its similar supervised counterpart, word sense disambiguation (WSD) (Stevenson and Wilks 2003) , because WSI models should consider the following challenges due to its unsupervised nature: (C1) adaptability to new domains, (C2) ability to detect novel senses, and (C3) flexibility to different word sense granularities (Jurgens and Klapaftis 2013) . Another task similar to the WSI is the unsupervised author name disambiguation (UAND) task (Song et al. 2007) , where it aims to automatically find different authors, instead of words, with the same name. In this paper, we consider a latent variable modeling approach to WSI problem as it is proven to be more effective than other approaches (Chang, Pei, and Chen 2014; Komninos and Manandhar 2016) . Specifically, we look into methods based on Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) , a topic modeling method that automatically discovers the topics underlying a set of documents using Dirichlet priors to infer the multinomial distribution over words and topics. LDA naturally answers two of the three main problems mentioned above, i.e. (C1) and (C2), of the WSI task (Brody and Lapata 2009) . However, it is not flexible with regards to (C3), or the sense granularity problem, as it requires the users to specify the number of senses: Current systems (<cite>Wang et al. 2015</cite>; Chang, Pei, and Chen 2014) required to set the number of senses to a small number (set to 3 or 5 in the literature) to get a good accuracy, however many words may have a large number of senses, e.g. play in Figure 1 . ---------------------------------- **LDA: !(#|%)** AutoSense: !('|#, (% ) , %)) Figure 2: Example induced senses when the target word is cold from LDA and AutoSense.",
  "y": "motivation"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_1",
  "x": "---------------------------------- **RELATED WORK** Previous works on WSI used context vectors and attributes (Almuhareb, Poesio, and others 2006) , pretrained classification systems (Tsvetkov et al. 2014) , and alignment of parallel corpus (Yao, Van Durme, and Callison-Burch 2012) . In the most recent shared task on WSI (Jurgens and Klapaftis 2013) , top models used lexical substitution method (AI-KU) (Baskaya et al. 2013) and Hierarchical Dirichlet Process trained with additional instances (Unimelb) . Latent variable models such as LDA (Blei, Ng, and Jordan 2003) are used to induce the word sense of a target word after rigorous preprocessing and feature extraction (LDA, Spectral) (Goyal and Hovy 2014). More recent models introduced a latent variable for the sense of a word, with the assumption that a sense has multiple concepts (HC, HC+Zipf) (Chang, Pei, and Chen 2014) and that topics and senses should be inferred jointly (STM) (<cite>Wang et al. 2015</cite>) . In this paper, we also use a separate sense latent variable, however we show boost in performance by representing it with more versatility and by incorporating the use of targetneighbor pairs. HC was also extended to a nonparametric model (BNP-HC) (Teh et al. 2004 ) in order to automatically set the number of senses of a word, providing flexibility to the sense granularity (Yao and Van Durme 2011; Lau et al. 2012; . In our experiments, we show that the sense granularity induced from nonparametric models are incorrect making the models less effective.",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_2",
  "x": "HC was also extended to a nonparametric model (BNP-HC) (Teh et al. 2004 ) in order to automatically set the number of senses of a word, providing flexibility to the sense granularity (Yao and Van Durme 2011; Lau et al. 2012; . In our experiments, we show that the sense granularity induced from nonparametric models are incorrect making the models less effective. Recent inclusions to the WSI models are neural-based dense distributional representation models. STM also used word embeddings (Mikolov et al. 2013) to assign similarity weights during inference (STM+w2v) (<cite>Wang et al. 2015</cite>) . Existing sense embeddings are also used to perform word sense induction (CRP-PPMI, SE-WSI-fix, WG, DIVE) (Song 2016; Pelevina et al. 2016; Chang et al. 2018 ). These models, on their own, do not perform well on the WSI task until recently when embeddings of words and their dependencies are used to construct a probabilistic model (MCC) (Komninos and Manandhar 2016) . We show that neuralbased embeddings are still ineffective for this task and that our model performs better than these models as well. In the unsupervised author name disambiguation (UAND) domain, LDA-based models have also been used (Shu, Long, and Meng 2009) to employ text features for the task, while non-text features such as co-authors, publication venue, year, and citations are found to be stronger features (Tang et al. 2012) . In this paper, we study on how to improve the performance of text features for UAND using latent variable models, which can later be combined with non-text features in the future work.",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_3",
  "x": "---------------------------------- **PROPOSED MODEL** There are two reasons why Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) is not effective for WSI. First, LDA tries to give instance assignments to all senses even when it is unnecessary. For example, when the number of senses S is set to 10, the model tries to assign all the senses to all instances even when the original number of senses of a target word is 3. LDA extensions (<cite>Wang et al. 2015</cite>; Chang, Pei, and Chen 2014) mitigated this problem by setting S to a small number (e.g. 3 or 5). However, this is not a good solution because there are many words with more than five senses. Second, LDA and its extensions do not consider the existence of fine-grained senses. For example, the cold: absence of heat and the cold: sensation from low temperature senses are fine-grained senses because they are similarly related to temperature yet have different usage.",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_4",
  "x": "---------------------------------- **PROPOSED MODEL** There are two reasons why Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) is not effective for WSI. First, LDA tries to give instance assignments to all senses even when it is unnecessary. For example, when the number of senses S is set to 10, the model tries to assign all the senses to all instances even when the original number of senses of a target word is 3. LDA extensions (<cite>Wang et al. 2015</cite>; Chang, Pei, and Chen 2014) mitigated this problem by setting S to a small number (e.g. 3 or 5). However, this is not a good solution because there are many words with more than five senses. Second, LDA and its extensions do not consider the existence of fine-grained senses. For example, the cold: absence of heat and the cold: sensation from low temperature senses are fine-grained senses because they are similarly related to temperature yet have different usage.",
  "y": "motivation"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_5",
  "x": "Previous works also attempted to introduce a separate sense latent variable to generate all the words (Chang, Pei, and Chen 2014) , or to generate only the neighboring words within a local context, decided by a strict user-specified window (<cite>Wang et al. 2015</cite>) . We improve by softening the strict local context assumption by introducing a switch variable which decides whether a word not in a local context should be generated by conditioning also on the sense latent variable. Our experiments show that our sense representation provides superior improvements from previous models. Second, we force the model to generate target-neighbor pairs at once in the local context, instead of generating words one by one. A target-neighbor pair (w t , w) consists of the target word w t and a neighboring word w in the local context. For example, the target-neighbor pairs in \"cold snowy weather\", where w t is cold, are (cold, snowy) and (cold, weather). These pairs give explicit information on the lexical semantics of the target word given the neighboring words. In our running example (Figure 2 ), the cold: absence of heat and the cold: sensation from low temperature senses can be easily differentiated when we are given the target-neighbor pairs (cold, weather) and (cold, climate) for the former, and (cold, water) and (cold, f resh) for the latter sense, rather than the individual words. These extensions bring us to our proposed model called AutoSense.",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_6",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** Datasets and preprocessing We use two publicly available datasets: SemEval 2010 Task 14 (Manandhar et al. 2010) For preprocessing, we do tokenization, lemmatization, and removing of symbols to build the word lists using Stanford CoreNLP (Manning et al. 2014) . We divide the word lists into two contexts: the local and global context. Following (<cite>Wang et al. 2015</cite>), we set the local context window to 10, with a maximum number of words of 21 (i.e. 10 words before and 10 words after). Other words are put into the global context. Note however that AutoSense has a less strict global/local context assumption as it treats some words in the global context as local depending on the switch variable. Parameter setting We set the hyperparameters to \u03b1 = 0.1, \u03b2 = 0.01, \u03b3 = 0.3, following the conventional setup (Griffiths and Steyvers 2004; Chemudugunta, Smyth, and Steyvers 2006) . We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following (<cite>Wang et al. 2015</cite>) .",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_7",
  "x": "Note however that AutoSense has a less strict global/local context assumption as it treats some words in the global context as local depending on the switch variable. Parameter setting We set the hyperparameters to \u03b1 = 0.1, \u03b2 = 0.01, \u03b3 = 0.3, following the conventional setup (Griffiths and Steyvers 2004; Chemudugunta, Smyth, and Steyvers 2006) . We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following (<cite>Wang et al. 2015</cite>) . We also include four other versions of our model: AutoSense \u2212wp removes the target-neighbor pair constraint and transforms the local context to that of STM, AutoSense \u2212sw removes the switch variable and transforms the global context to that of LDA, AutoSense s=X is a tuned and best version of the model, where the number of senses is tuned over a separate development set provided by the shared tasks and X is the tuned number of sense, different for each dataset, and AutoSense s=100 is the overestimated and worst version of the model, where we set the number of senses to an arbitrary large number, i.e. 100. We set the number of iterations to 2000 and run the Gibbs sampler. Following the convention of previous works (Lau et al. 2012; Goyal and Hovy 2014; <cite>Wang et al. 2015</cite>) , we assume convergence when the number of iterations is high. However, due to the randomized nature of Gibbs sampling, we report the average scores over 5 runs of Gibbs sampling. We then use the distribution \u03b8 s|d as shown in Equation 1 as the solution of the WSI problem. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_8",
  "x": "Note however that AutoSense has a less strict global/local context assumption as it treats some words in the global context as local depending on the switch variable. Parameter setting We set the hyperparameters to \u03b1 = 0.1, \u03b2 = 0.01, \u03b3 = 0.3, following the conventional setup (Griffiths and Steyvers 2004; Chemudugunta, Smyth, and Steyvers 2006) . We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following (<cite>Wang et al. 2015</cite>) . We also include four other versions of our model: AutoSense \u2212wp removes the target-neighbor pair constraint and transforms the local context to that of STM, AutoSense \u2212sw removes the switch variable and transforms the global context to that of LDA, AutoSense s=X is a tuned and best version of the model, where the number of senses is tuned over a separate development set provided by the shared tasks and X is the tuned number of sense, different for each dataset, and AutoSense s=100 is the overestimated and worst version of the model, where we set the number of senses to an arbitrary large number, i.e. 100. We set the number of iterations to 2000 and run the Gibbs sampler. Following the convention of previous works (Lau et al. 2012; Goyal and Hovy 2014; <cite>Wang et al. 2015</cite>) , we assume convergence when the number of iterations is high. However, due to the randomized nature of Gibbs sampling, we report the average scores over 5 runs of Gibbs sampling. We then use the distribution \u03b8 s|d as shown in Equation 1 as the solution of the WSI problem. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_9",
  "x": "We then use the distribution \u03b8 s|d as shown in Equation 1 as the solution of the WSI problem. ---------------------------------- **MODEL F-S V-M AVG \u0394(#S)** ---------------------------------- **EXPERIMENTS WORD SENSE INDUCTION** SemEval 2010 For the SemEval 2010 dataset, we compare models using two unsupervised metrics: V-measure (V-M) and paired F-score (F-S). V-M favors a high number of senses (e.g. assigning one cluster per instance), while F-S favors a small number of senses (e.g. all instances in one cluster) (Manandhar et al. 2010) . In order to get a common ground for comparison, we do a geometric average AVG of both metrics, following (<cite>Wang et al. 2015</cite>) . Finally, we also report the absolute difference between the actual (3.85) and induced number of senses as \u03b4(#S).",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_10",
  "x": "SemEval 2013 Two metrics are used for the SemEval 2013 dataset: fuzzy B-cubed (F-BC) and fuzzy normalized mutual information (F-NMI). F-BC gives preference to labelling all instances with the same sense, while F-NMI gives preference to labelling all instances with distinct senses. Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in (<cite>Wang et al. 2015</cite>) . We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (Jurgens and Klapaftis 2013) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in (<cite>Wang et al. 2015</cite>) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (Chang et al. 2018) , and g) Multi Context Continuous model MCC as reported in (Komninos and Manandhar 2016) . Results are shown in Table 2b . Among the models, all versions of AutoSense perform better than other models on AVG. The untuned AutoSense and AutoSense s=7 especially garner noticeable increase of 6.1% on fuzzy B-cubed metric from MCC, the previous best model. We also notice a big 6.0% decrease on the fuzzy B-cubed of AutoSense when the target-neighbor pair context is removed. This means that introducing the target-neighbor pair is crucial to the improvement of the model.",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_11",
  "x": "On the \u03b4(#S) metric, the untuned AutoSense and AutoSense s=5 perform the best. The V-M metric needs to be interpreted carefully, because it can easily be maximized by separating all instances into different sense clusters and thus overestimating the actual number of senses #S and decreasing the F-S metric. The model BNP-HC is an example of such: Though its V-M metric is the highest, it scores the lowest on the F-S metric and greatly overestimates #S, thus having a very high \u03b4(#S). The goal is thus a good balance of V-M and F-S (i.e. highest AVG), and a close estimation of #S (i.e. lowest \u03b4(#S), which is successfully achieved by our models. SemEval 2013 Two metrics are used for the SemEval 2013 dataset: fuzzy B-cubed (F-BC) and fuzzy normalized mutual information (F-NMI). F-BC gives preference to labelling all instances with the same sense, while F-NMI gives preference to labelling all instances with distinct senses. Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in (<cite>Wang et al. 2015</cite>) . We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (Jurgens and Klapaftis 2013) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in (<cite>Wang et al. 2015</cite>) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (Chang et al. 2018) , and g) Multi Context Continuous model MCC as reported in (Komninos and Manandhar 2016) . Results are shown in Table 2b .",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_12",
  "x": "For completeness, we also report STM with additional contexts, STM+actual and STM+ukWac (<cite>Wang et al. 2015</cite>) , where they used the actual additional contexts from the original data and semantically similar contexts from ukWac, respectively, as additional global context. With the performance gain we achieved, AutoSense without additional context can perform comparably to models with additional contexts: Our model greatly outperforms these models on the Sense Word distribution   #Docs  1  hotel tour tourist summer flight  22  2  month ticket available performance  3  3 guest office stateroom class suite 3 * advance overseas line popular japan 0 * email day buy unable tour 0 * sort basic tour time 0 Table 3 : Six of the 15 senses of the target verb book using AutoSense with S = 15. The word lists shown are preprocessed to remove stopwords and the target word. The first three senses are senses which are assigned at least once to an instance document. The last three are garbage senses. F-BC metric by at least 2%. Also, considering that both AutoSense and STM are LDA-based models, the same data enhancements can straightforwardly be applied when the needs arise. We similarly apply the actual additional contexts to AutoSense and find that we achieve state-of-the-art performance on AVG. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_13",
  "x": "---------------------------------- **SENSE GRANULARITY PROBLEM** The main weakness of LDA when used on WSI task is the sense granularity problem. Recent models such as HC (Chang, Pei, and Chen 2014) and STM (<cite>Wang et al. 2015</cite>) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error. However, such tuning, often empirically set to a small number such as S = 3 (<cite>Wang et al. 2015</cite>) , fails to infer varying number of senses of words, especially for words with a higher number of senses. Nonparametric models such as HDP and BNP-HC Chang, Pei, and Chen 2014) claim to automatically induce different S for each word. However, as shown in the results in Table 2 , the estimated S is far from the actual number of senses and both models are ineffective. On the other hand, Table 2 also shows that AutoSense is effective even when S is overestimated. We explain why through an example result shown in Table 3 , where the target word is the verb book, the actual number of senses is three, and S is set to 15.",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_14",
  "x": "**SENSE GRANULARITY PROBLEM** The main weakness of LDA when used on WSI task is the sense granularity problem. Recent models such as HC (Chang, Pei, and Chen 2014) and STM (<cite>Wang et al. 2015</cite>) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error. However, such tuning, often empirically set to a small number such as S = 3 (<cite>Wang et al. 2015</cite>) , fails to infer varying number of senses of words, especially for words with a higher number of senses. Nonparametric models such as HDP and BNP-HC Chang, Pei, and Chen 2014) claim to automatically induce different S for each word. However, as shown in the results in Table 2 , the estimated S is far from the actual number of senses and both models are ineffective. On the other hand, Table 2 also shows that AutoSense is effective even when S is overestimated. We explain why through an example result shown in Table 3 , where the target word is the verb book, the actual number of senses is three, and S is set to 15. First, we see that there are senses which are not assigned to any instance document, signified by * , which we call garbage senses.",
  "y": "motivation"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_15",
  "x": "First, we see that there are senses which are not assigned to any instance document, signified by * , which we call garbage senses. We notice that effectively representing a new latent variable for sense as a distribution over topics forces the model to throw garbage senses. Second, while it is easy to distinguish the third sense (i.e., book: register in a booker) to the two other senses, the first and second senses both refer to planning or arranging for an event in advance. Incorporating the target-neighbor pairs helps the model differentiates both into fine-grained senses book: arrange for and reserve in advance and book: engage for a performance. We compare the competing models quantitatively on how they correctly detect the actual number of sense clusters using cluster error, which is the mean absolute error between the detected number and the actual number of sense clusters. We compare the cluster errors of LDA (Blei, Ng, and Jordan 2003) , STM (<cite>Wang et al. 2015</cite>) , HC (Chang, Pei, and Chen 2014) , and a nonparametric model HDP (Teh et al. 2004 ), with AutoSense. We report the results in Figure 4 . Results show that the cluster error of LDA increases sharply as the number of senses exceeds the actual mean number of senses. HC and STM also throw garbage senses since they also introduce in some way a new sense variable, however the cluster errors of both models still increase when S is set beyond the maximum number of senses.",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_16",
  "x": "It includes the PubMed ID of the papers authored by the given author name. We extract the title, author list, publication venue, and abstract of each PubMed ID from the PubMed website. We use LDA (Blei, Ng, and Jordan 2003) , HC (Chang, Pei, and Chen 2014) and STM (<cite>Wang et al. 2015</cite>) as baselines. We do not compare with non-text feature-based models (Tang et al. 2012; Cen et al. 2013 ) because our goal is to compare sense topic models on a task where the sense granularities are more varied. For STM and AutoSense, the title, publication venue and the author names are used as local contexts while the abstract is used as the global context. This decision is based on conclusions from previous works (Tang et al. 2012 ) that the title, publication venue, and the author names are more informative than the abstract when disambiguating author names. We use the same parameters as used above, and we set S to 5, 25, 50, and 100 to com-4 https://aminer.org/disambiguation 5 https://github.com/Yonsei-TSMM/author_ name_disambiguation Model S = 5 S = 25 S = 50 S = 100 LDA 31.5 13. pare the performances of the models as the number of senses increases. Results For evaluation, we use the pairwise F1 measure to compare the performance of competing models, following (Tang et al. 2012) .",
  "y": "uses"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_0",
  "x": "Instead of starting with a complex architecture, we proceed from the bottom up and examine the effectiveness of a simple, word-level Siamese architecture augmented with attention-based mechanisms for capturing semantic \"soft\" matches between query and post terms. Extensive experiments on datasets from the TREC Microblog Tracks show that our simple models not only demonstrate better effectiveness than existing approaches that are far more complex or exploit a more diverse set of relevance signals, but also achieve 4\u00d7 speedup in model training and inference. ---------------------------------- **INTRODUCTION** Despite a large body of work on neural ranking models for \"traditional\" ad hoc retrieval over web pages and newswire documents (Huang et al., 2013; Shen et al., 2014; Pang et al., 2016; Xiong et al., 2017; Mitra et al., 2017; Pang et al., 2017; Dai et al., 2018; McDonald et al., 2018) , there has been surprisingly little work on applying neural networks to searching short social media posts such as tweets on Twitter. <cite>Rao et al. (2018)</cite> identified short document length, informality of language, and heterogeneous relevance signals as main challenges in relevance modeling, and proposed a model specifically designed to handle these characteristics. Evaluation on a number of datasets from the TREC Microblog Tracks demonstrates state-of-the-art effectiveness as well as the necessity of different model components to capture a multitude of relevance signals. In this paper, we also examine the problem of modeling relevance for ranking short social media posts, but from a complementary perspective. As Weissenborn et al. (2017) argues, most Figure 1: Our model architecture: a general sentence encoder is applied on query and post embeddings to generate g q and g p ; an attention encoder is applied on post embeddings to generate variable-length queryaware features h i .",
  "y": "background"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_1",
  "x": "---------------------------------- **INTRODUCTION** Despite a large body of work on neural ranking models for \"traditional\" ad hoc retrieval over web pages and newswire documents (Huang et al., 2013; Shen et al., 2014; Pang et al., 2016; Xiong et al., 2017; Mitra et al., 2017; Pang et al., 2017; Dai et al., 2018; McDonald et al., 2018) , there has been surprisingly little work on applying neural networks to searching short social media posts such as tweets on Twitter. <cite>Rao et al. (2018)</cite> identified short document length, informality of language, and heterogeneous relevance signals as main challenges in relevance modeling, and proposed a model specifically designed to handle these characteristics. Evaluation on a number of datasets from the TREC Microblog Tracks demonstrates state-of-the-art effectiveness as well as the necessity of different model components to capture a multitude of relevance signals. In this paper, we also examine the problem of modeling relevance for ranking short social media posts, but from a complementary perspective. As Weissenborn et al. (2017) argues, most Figure 1: Our model architecture: a general sentence encoder is applied on query and post embeddings to generate g q and g p ; an attention encoder is applied on post embeddings to generate variable-length queryaware features h i . These features are further aggregated to yield v, which feeds into the final prediction. systems are built in a top-down process: authors proposing a complex architecture and validating design decisions with ablation experiments.",
  "y": "similarities background"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_2",
  "x": "**TRAINING** To obtain the final score, the feature vectors g q , g p and v are concatenated and fed into an MLP with ReLU activate function for dimension reduction and obtain o, followed by batch normalization and fully-connected layer and softmax to output the final prediction. The model is trained endto-end with Stochastic Gradient Decent optimizer, and negative log-likelihood loss function is used. 3 Experiment Experimental Setup. Our models are evaluated on four tweets test collections from the TREC 2011-2014 Microblog (MB) Tracks (Ounis et al., 2011; Soboroff et al., 2012; Lin and Efron, 2013; Lin et al., 2014) . Each dataset contains around 50 queries and the more detailed statistics are shown in Table 1 . Following <cite>Rao et al. (2018)</cite> , we evaluate our models in a reranking task, where the inputs are up to the top 1000 tweets retrieved from the classical query likelihood (QL) language model (Ponte and Croft, 1998) . We run four-fold cross-validation test split by year (i.e., train on three year's data, test on one year's data), and we randomly sample 10 queries from each year in the training sets (in total 30 queries) as our validation set. The mean average precision (MAP) and precision at top 30 (P30) are adopted as our evaluation metrics.",
  "y": "uses"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_3",
  "x": "Baselines: QL is a competitive language modeling baseline. RM3 (Lavrenko and Croft, 2001 ) is an interpolation model combining the QL score with a relevance model using pseudo-relevance feedback. MP-HCNN<cite> (Rao et al., 2018)</cite> is the first neural model that captures the characteristics of social media domain. Their method improves current neural IR methods, e.g., K-NRM (Xiong et al., 2017) , DUET (Mitra et al., 2017) , by a signifi- 4 BiCNN+PAtt+QL 0.4728 1-3 5-7 0.4293 1-3 5-7 0.4147 1,2 5,6 0.2621 1-3 5-7 0.5367 1-3 5,6 0.2990 1-3 5 0.6806 1,2 5-8 0.4563 1-3 ---------------------------------- **5,7** Existing Models 5 QL 0.4000 1 0.3576 1 0.3311 1 0.2091 1 0.4450 1 0.2532 1 0.6182 1 0.3924 1 6 RM3 0.4211 1 0.3824 1 0.3452 1 0.2342 1 0.4733 1 0.2766 1 0.6339 1 0.4480 1 7 MP-HCNN(+URL) 0.4306 1 0.3940 1,2 0.3757 1,5 0.2313 1,5 0.5211 1,5 0.2856 1,5 0.6279 1 0.4178 1 8 MP-HCNN(+URL)+QL 0.4435 1-2 5,6 0.4040 1,2 5,6 0.3915 1,5 6 0.2482 1,2 5 0.5250 1,5 6 0.2937 1,2 5 0.6455 1 0.4403 1,5 Table 2 : Results of non-neural and neural models on the TREC Microblog Tracks datasets. Results from 5 -8 are adopted from <cite>Rao et al. (2018)</cite> . Models denoted with (+URL) represents utilizing the URL information.",
  "y": "uses"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_4",
  "x": "Results from 5 -8 are adopted from <cite>Rao et al. (2018)</cite> . Models denoted with (+URL) represents utilizing the URL information. Models denoted with +QL are interpolated with QL baseline. Bi-CNN denotes general sentence encoder architecture. Both superscripts and subscripts indicate the row indexes for which a metric difference is statistically significant at p < 0.05. cant margin. To the best of our knowledge, <cite>Rao et al. (2018)</cite> is the best neural model to date, and there are no neural models from TREC evaluations for further comparison. We also compared to MP-HCNN+QL, which is a linear interpolation to combine the raw MP-HCNN and QL scores. Table 2 shows our experiment results of all settings and the results of existing models.",
  "y": "uses"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_5",
  "x": "Models denoted with (+URL) represents utilizing the URL information. Models denoted with +QL are interpolated with QL baseline. Bi-CNN denotes general sentence encoder architecture. Both superscripts and subscripts indicate the row indexes for which a metric difference is statistically significant at p < 0.05. cant margin. To the best of our knowledge, <cite>Rao et al. (2018)</cite> is the best neural model to date, and there are no neural models from TREC evaluations for further comparison. We also compared to MP-HCNN+QL, which is a linear interpolation to combine the raw MP-HCNN and QL scores. Table 2 shows our experiment results of all settings and the results of existing models. Model 1 is the effectiveness of BiCNN model with kernel window size 2.",
  "y": "background"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_0",
  "x": "In this paper, we propose to use language as a guide for representation learning, building few-shot classification models that learn visual representations while jointly predicting task-specific language during training. Crucially, our models can operate without language at test time: a more practical setting, since it is often unrealistic to assume that linguistic supervision is available for unseen classes encountered in the wild. Compared to meta-learning baselines and recent approaches which use language supervision as a more fundamental bottleneck in a model <cite>[1]</cite> , we find this simple auxiliary training objective results in learned representations that generalize better to new concepts. ---------------------------------- **RELATED WORK** Language has been shown to assist visual classification in various settings, including traditional visual classification with no transfer [16] and with language available at test time in the form of class labels or descriptions for zero- [10,<cite> 1</cite>1, 27] or few-shot [24, 33] learning. Unlike this work, we study a setting where we have no language at test time and test tasks are unseen, so language from training can no longer be used as additional class information [cf.<cite> 1</cite>6] or weak supervision for labeling additional in-domain data [cf.<cite> 1</cite>5] . Our work can thus be seen as an instance of the learning using privileged information (LUPI) [31] framework, where richer supervision augments a model during training only. In this framework, learning with attributes and other domain-specific rationales has been tackled extensively [8, 9, 29] , but language remains relatively unexplored.",
  "y": "differences"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_3",
  "x": "Language has been shown to assist visual classification in various settings, including traditional visual classification with no transfer [16] and with language available at test time in the form of class labels or descriptions for zero- [10,<cite> 1</cite>1, 27] or few-shot [24, 33] learning. Unlike this work, we study a setting where we have no language at test time and test tasks are unseen, so language from training can no longer be used as additional class information [cf.<cite> 1</cite>6] or weak supervision for labeling additional in-domain data [cf.<cite> 1</cite>5] . Our work can thus be seen as an instance of the learning using privileged information (LUPI) [31] framework, where richer supervision augments a model during training only. In this framework, learning with attributes and other domain-specific rationales has been tackled extensively [8, 9, 29] , but language remains relatively unexplored. [13] use METEOR scores between captions as a similarity metric for specializing embeddings for image retrieval, but do not directly Figure<cite> 1</cite> : Building on prototype networks [26] , we propose few-shot classification models whose learned representations are constrained to predict natural language descriptions of the task during training, in contrast to models <cite>[1]</cite> which explicitly use language as a bottleneck for classification. ground language explanations. [28] explore a supervision setting similar to ours, except in highly structured text and symbolic domains where descriptions can be easily converted to executable forms via semantic parsing. Another line of work studies models which generate natural language explanations of decisions for interpretability for both textual (e.g. natural language inference; [3] ) and visual [17,<cite> 1</cite>8] tasks, but here we examine whether this act of predicting language can actually improve downstream task performance; similar ideas have been explored in text [22] and reinforcement learning [2,<cite> 1</cite>4] domains. Our work is most similar to <cite>[1]</cite> , which we describe and compare to later.",
  "y": "differences"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_5",
  "x": "Language has been shown to assist visual classification in various settings, including traditional visual classification with no transfer [16] and with language available at test time in the form of class labels or descriptions for zero- [10,<cite> 1</cite>1, 27] or few-shot [24, 33] learning. Unlike this work, we study a setting where we have no language at test time and test tasks are unseen, so language from training can no longer be used as additional class information [cf.<cite> 1</cite>6] or weak supervision for labeling additional in-domain data [cf.<cite> 1</cite>5] . Our work can thus be seen as an instance of the learning using privileged information (LUPI) [31] framework, where richer supervision augments a model during training only. In this framework, learning with attributes and other domain-specific rationales has been tackled extensively [8, 9, 29] , but language remains relatively unexplored. [13] use METEOR scores between captions as a similarity metric for specializing embeddings for image retrieval, but do not directly Figure<cite> 1</cite> : Building on prototype networks [26] , we propose few-shot classification models whose learned representations are constrained to predict natural language descriptions of the task during training, in contrast to models <cite>[1]</cite> which explicitly use language as a bottleneck for classification. ground language explanations. [28] explore a supervision setting similar to ours, except in highly structured text and symbolic domains where descriptions can be easily converted to executable forms via semantic parsing. Another line of work studies models which generate natural language explanations of decisions for interpretability for both textual (e.g. natural language inference; [3] ) and visual [17,<cite> 1</cite>8] tasks, but here we examine whether this act of predicting language can actually improve downstream task performance; similar ideas have been explored in text [22] and reinforcement learning [2,<cite> 1</cite>4] domains. Our work is most similar to <cite>[1]</cite> , which we describe and compare to later.",
  "y": "similarities"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_9",
  "x": "At test, we simply discard g \u03c6 and use f \u03b8 to classify. With this component, we call our approach language-shaped learning (LSL) (Figure<cite> 1</cite> ). Relation to L3. LSL is similar to another recent model for this setting: Learning with Latent Language (L3) <cite>[1]</cite> , which proposes to use language not only as a supervision source, but as a bottleneck for classification ( Figure<cite> 1</cite> ). L3 has the same basic architecture of LSL, but the concepts c n are the language descriptions themselves, embedded with an additional recurrent neural network (RNN) encoder h \u03b7 : c n = h \u03b7 (w n ). During training, the ground-truth description is used for classification, while g \u03c6 is trained to produce the description; at test, L3 samples descriptions\u0175 n from g \u03c6 , keeping the description most similar to the support according to the similarity function s. While L3 has been shown to outperform meta-learning baselines, there are two potential sources of this benefit: is it the linguistic bottleneck itself, or the regularization imposed by training f \u03b8 to predict language? Our evaluation aims to disentangle these effects: LSL isolates the regularization component, and thus is simpler than L3 since it (1) does not require the additional embedding module h \u03b7 and (2) does not need the test-time language sampling procedure. 2",
  "y": "similarities"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_10",
  "x": "2 ---------------------------------- **EXPERIMENTS** Here we describe our two tasks and models. For full training details and code, see Appendix A. ShapeWorld. First, we use the ShapeWorld [20] dataset devised by <cite>[1]</cite> , which consists of 9000 training,<cite> 1</cite>000 validation, and 4000 test tasks ( Figure 2 ). 3 Each task contains a single support set of K = 4 images representing a visual concept with an associated (artificial) English language description, generated with a minimal recursion semantics representation of the concept [7] . Each concept is a spatial relation between two objects, optionally qualified by color and/or shape; 2-3 distractor shapes are also present in each image.",
  "y": "uses"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_11",
  "x": "Each concept is a spatial relation between two objects, optionally qualified by color and/or shape; 2-3 distractor shapes are also present in each image. The task is to predict whether a single query image x belongs to the concept. Model details are identical to <cite>[1]</cite> for easy comparison. f \u03b8 is the final convolutional layer of a fixed ImageNet-pretrained VGG-16 [25] fed through two fully-connected layers: Since this is a binary classification task with only<cite> 1</cite> (positive) support class S and prototype c, we define the similarity function s(a, b) = \u03c3(a \u00b7 b) and the prediction P (\u0177 =<cite> 1</cite> | x) = s (f \u03b8 (x), c). g \u03c6 is a gated recurrent unit (GRU) RNN [5] with hidden size h = 512, trained with teacher forcing. Using a grid search on the validation set, we set \u03bb NL = 20. Birds. To see if LSL can scale to more realistic scenarios, we use the Caltech-UCSD Birds dataset [32] , which contains 200 bird species, each with 40-60 images, split into<cite> 1</cite>00 train, 50 validation, The bird has a white underbelly, black feathers in the wings, a large wingspan, and a white beak.",
  "y": "uses"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_17",
  "x": "**A TASK/MODEL TRAINING DETAILS** Our code is publicly available at https://github.com/jayelm/lsl. A.1 ShapeWorld f \u03b8 . Like <cite>[1]</cite> , f \u03b8 starts with features extracted from the last convolutional layer of a fixed ImageNetpretrained VGG-19 network [25] . These 4608-d embeddings are then fed into two fully connected layers \u2208 R 4608\u00d7512 , R 512\u00d7512 with one ReLU nonlinearity in between. LSL. For LSL, the 512-d embedding from f \u03b8 directly initializes the 512-d hidden state of the GRU g \u03c6 . We use 300-d word embeddings initialized randomly (initializing with GloVe made no significant difference). L3. f \u03b8 and g \u03c6 are the same as in LSL and Meta.",
  "y": "similarities"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_19",
  "x": "We use 300-d word embeddings initialized randomly (initializing with GloVe made no significant difference). L3. f \u03b8 and g \u03c6 are the same as in LSL and Meta. h \u03b7 is a unidirectional<cite> 1</cite>-layer GRU with hidden size 512 sharing the same word embeddings as g \u03c6 . The output of the last hidden state is taken as the embedding of the description w (t) . Like <cite>[1]</cite> , a total of<cite> 1</cite>0 descriptions per task are sampled at test time. Training. We train for 50 epochs, each epoch consisting of<cite> 1</cite>00 batches with<cite> 1</cite>00 tasks in each batch, with the Adam optimizer [19] and a learning rate of 0.001. We selected the model with highest epoch validation accuracy during training.",
  "y": "similarities"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_21",
  "x": "We selected the model with highest epoch validation accuracy during training. This differs slightly from <cite>[1]</cite> , who use different numbers of epochs per model and did not specify how they were chosen; otherwise, the training and evaluation process is the same. Data. We recreated the ShapeWorld dataset using the same code as <cite>[1]</cite> , except generating 4x as many test tasks (4000 vs<cite> 1</cite>000) for more stable confidence intervals. Note that results for both L3 and Baseline (Meta) are 3-4 points lower than the scores of the corresponding implementations in <cite>[1]</cite> . This is likely due to (1) differences in model initialization due to our PyTorch reimplementation, (2) recreation of the dataset, and (3) our use of early stopping. A.2 Birds f \u03b8 . The 4-layer convolutional backbone f \u03b8 is the same as the one used in much of the few-shot literature [4, 26] . The model has 4 convolutional blocks, each consisting of a 64-filter 3x3 convolution, batch normalization, ReLU nonlinearity, and 2x2 max-pooling layer.",
  "y": "differences"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_22",
  "x": "h \u03b7 is a unidirectional<cite> 1</cite>-layer GRU with hidden size 512 sharing the same word embeddings as g \u03c6 . The output of the last hidden state is taken as the embedding of the description w (t) . Like <cite>[1]</cite> , a total of<cite> 1</cite>0 descriptions per task are sampled at test time. Training. We train for 50 epochs, each epoch consisting of<cite> 1</cite>00 batches with<cite> 1</cite>00 tasks in each batch, with the Adam optimizer [19] and a learning rate of 0.001. We selected the model with highest epoch validation accuracy during training. This differs slightly from <cite>[1]</cite> , who use different numbers of epochs per model and did not specify how they were chosen; otherwise, the training and evaluation process is the same. Data. We recreated the ShapeWorld dataset using the same code as <cite>[1]</cite> , except generating 4x as many test tasks (4000 vs<cite> 1</cite>000) for more stable confidence intervals.",
  "y": "uses similarities differences"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_23",
  "x": "The output of the last hidden state is taken as the embedding of the description w (t) . Like <cite>[1]</cite> , a total of<cite> 1</cite>0 descriptions per task are sampled at test time. Training. We train for 50 epochs, each epoch consisting of<cite> 1</cite>00 batches with<cite> 1</cite>00 tasks in each batch, with the Adam optimizer [19] and a learning rate of 0.001. We selected the model with highest epoch validation accuracy during training. This differs slightly from <cite>[1]</cite> , who use different numbers of epochs per model and did not specify how they were chosen; otherwise, the training and evaluation process is the same. Data. We recreated the ShapeWorld dataset using the same code as <cite>[1]</cite> , except generating 4x as many test tasks (4000 vs<cite> 1</cite>000) for more stable confidence intervals. Note that results for both L3 and Baseline (Meta) are 3-4 points lower than the scores of the corresponding implementations in <cite>[1]</cite> .",
  "y": "differences"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_0",
  "x": "Following <cite>Webber et al. (2003)</cite> , the paper argues that in contrast crucially involves discourse anaphora and, thus, resembles other discourse adverbials such as then, otherwise, and nevertheless. The compositional semantics proposed for other discourse connectives, however, does not straightforwardly generalize to in contrast, for which the notions of contrast pairs and contrast properties are essential. ---------------------------------- **INTRODUCTION** The semantics and pragmatics of discourse structure has been a central theme in linguistic research for quite some time. Recent research on large-scale annotation of discourse relations for the purposes of natural language processing applications has resulted in new insights in the properties of such relations and in concrete proposals on how to annotate them. A particularly ambitious and interesting effort of this kind is the Penn Discourse Treebank (PDTB), a corpus of 1 million words which is being annotated for discourse connectives and their arguments, more specifically for connectives such as but, for example, because, after, and when that are either realized lexically (explicit connectives) or that have no overt linguistic realization, but that can be inferred as a logical relation between pieces of discourse (implicit connectives). On the basis of the detailed PDTB annotations, which by now comprise a substantial corpus of linguistic data, it has become possible to revisit an open research question that had been raised repeatedly in the literature, albeit without yielding concrete results. This open research question concerns the similarities and differences between syntactic and semantic relations at the sentence level and at the discourse level.",
  "y": "differences"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_1",
  "x": "On the basis of the detailed PDTB annotations, which by now comprise a substantial corpus of linguistic data, it has become possible to revisit an open research question that had been raised repeatedly in the literature, albeit without yielding concrete results. This open research question concerns the similarities and differences between syntactic and semantic relations at the sentence level and at the discourse level. Webber (2006) and Lee et al. (2006) have addressed this very issue in the context of the PDTB annotations and have arrived at the following empirical generalizations: 1. While the arity of predicates at the sentential level can vary, e.g. one argument in the case of intransitive verbs, two in the case of transitives, three for ditransitives, etc., the arity of discourse connectives is fixed and consists of exactly two arguments. 2. While syntactic dependencies can be quite complex and may involve highly nested or even crossing dependencies of various kinds, dependencies expressed by discourse connectives tend to be much more limited, typically involving tree-like structures and not introducing structural ambiguities of scope or attachment. 3. More complex cases of discourse connectives that prima facie seem to involve crossing or partially overlapping arguments can be reduced to the independent discourse mechanisms of anaphora and attribution and thus do not introduce any added complexities. The third generalization is further elaborated by <cite>Webber et al. (2003)</cite> who distinguish between coordinating conjunctions such as and, or, so, and but and subordinating conjunctions such as although, whereas, and when on the one hand, and discourse adverbials such as then, otherwise, nevertheless, and instead on the other hand. It is the latter group, namely discourse adverbials, that, according to <cite>Webber et al. (2003)</cite> , should be considered as anaphors in very much the same way as other anaphoric expressions such as definite descriptions and pronouns. The purpose of this paper is to further examine and refine the above hypotheses by looking in some detail at a family of discourse connectives, all involving the notion of contrast.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_2",
  "x": "Webber (2006) and Lee et al. (2006) have addressed this very issue in the context of the PDTB annotations and have arrived at the following empirical generalizations: 1. While the arity of predicates at the sentential level can vary, e.g. one argument in the case of intransitive verbs, two in the case of transitives, three for ditransitives, etc., the arity of discourse connectives is fixed and consists of exactly two arguments. 2. While syntactic dependencies can be quite complex and may involve highly nested or even crossing dependencies of various kinds, dependencies expressed by discourse connectives tend to be much more limited, typically involving tree-like structures and not introducing structural ambiguities of scope or attachment. 3. More complex cases of discourse connectives that prima facie seem to involve crossing or partially overlapping arguments can be reduced to the independent discourse mechanisms of anaphora and attribution and thus do not introduce any added complexities. The third generalization is further elaborated by <cite>Webber et al. (2003)</cite> who distinguish between coordinating conjunctions such as and, or, so, and but and subordinating conjunctions such as although, whereas, and when on the one hand, and discourse adverbials such as then, otherwise, nevertheless, and instead on the other hand. It is the latter group, namely discourse adverbials, that, according to <cite>Webber et al. (2003)</cite> , should be considered as anaphors in very much the same way as other anaphoric expressions such as definite descriptions and pronouns. The purpose of this paper is to further examine and refine the above hypotheses by looking in some detail at a family of discourse connectives, all involving the notion of contrast. ---------------------------------- **THE DATA**",
  "y": "similarities"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_3",
  "x": "(6) South Africa stands in apparent contrast to the rest of the states considered here. B12(0020) (6) also provides an example of an adjectival premodifier that can modify contrast and that tends to function as an intensifier. Other such modifiers include profound, sharpest, strong, utter and clear. ---------------------------------- **DISCOURSE ANAPHORA** This section will focus on the discourse function of the adverbial phrase in contrast. Following <cite>Webber et al. (2003)</cite>, we will argue that it resembles other discourse adverbials such as then, otherwise, and nevertheless in that it crucially involves the notion of discourse anaphora. Discourse anaphora involves a relation between an anaphor, such as a pronoun or a temporal adverbial, and an antecedent that is present in the previous discourse or that can be inferred from it. In the case of pronouns, antecedents are typically NPs, while the antecedents of temporal adverbials can be time-denoting expressions, such as dates, events or states of affairs.",
  "y": "similarities"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_4",
  "x": "Following <cite>Webber et al. (2003)</cite>, we will argue that it resembles other discourse adverbials such as then, otherwise, and nevertheless in that it crucially involves the notion of discourse anaphora. Discourse anaphora involves a relation between an anaphor, such as a pronoun or a temporal adverbial, and an antecedent that is present in the previous discourse or that can be inferred from it. In the case of pronouns, antecedents are typically NPs, while the antecedents of temporal adverbials can be time-denoting expressions, such as dates, events or states of affairs. For pronouns, discourse anaphora can either involve coreference or more indirect referential relations which do not involve identity of reference with a previous discourse entity, but where the anaphor is merely associated with a previously mentioned discourse entity. Such cases of indirect referential relations include cases of bridging, as in (7), where the anaphor, in this case the receiver stands in a part-whole-relation to its antecedent -in this case a phone. (7) Myra darted to a phone and picked up the receiver. (Webber et al. (2003) , p. 555) Other-anaphora (Bierner and Webber (2000) Bierner (2001), Modjeska (2002)), as in (8), provides another instance of such an indirect referential relation. (8) Sue grabbed one phone, as Tom darted to the other phone.<cite> (Webber et al. (2003)</cite>, p. 555) Here the referent of the other phone can be inferred from the antecedent one phone.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_5",
  "x": "(10) It's a shame, then, that its gearchange is coarse and sloppy. In contrast, the Calibra's is light and quick, although the clutch action could be more progressive. A6W (0763) Here the elliptical the Calibra's is missing its nominal head, which is provided by the antecedent gearchange. Yet another anaphoric effect licensed by in contrast arises with respect to the notion of domain restriction, previously studied by, among many others, Lewis (1979) , Hinrichs (1988) and Hinrichs (1998), and von Fintel (1994) . (11) Few countries have satisfactory legislation on pesticides or the trained manpower to enforce it. In contrast, extensive use of pesticides in Europe, North America and Japan is backed by government legislation or voluntary schemes that provide farmers with detailed advice. B7G (0726) Note that the domain of the set of countries in the quantified NP few countries is subsequently narrowed so as to not include countries in Europe, North America and Japan. It is precisely the explicitly mentioned contrast that leads to this effect. <cite>Webber et al. (2003)</cite> observe that identification of the correct antecedent of a definite description such as the tower or this tower in (12a) or a discourse adverbial such as otherwise in (12b) may require reference to abstract discourse objects such as the result of stacking blocks (to form a tower) or the state of not wanting an apple as the logical antecedent of a definite description or of a discourse adverbial.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_6",
  "x": "Now close your eyes and try knocking the tower, this tower\u00a1 over with your nose.<cite> (Webber et al. (2003)</cite> , p. 552) b. Do you want an apple? Otherwise you can have a pear. (Webber et al. (2003), p. 552) Notice that the same kind of inference is required for contrast in example (13), providing further evidence for the anaphoric nature of this discourse connective. (13) Jack's heart lurched as he saw the ambulances and the busy, functional building and he immediately forgot everything they had been saying. \"I'll ask where he is,\" said Jamie Shepherd as they walked towards the reception desk. In contrast to the outside, the area was softly carpeted, softly lit, as if illness and death had to be cushioned away, made to look as if they didn't exist. BPD(0200) The referent of outside in (13) is never explicitly mentioned. Rather, outside refers back to the entire scene described before. Another type of inference that is sometimes necessitated by the in contrast connective concerns the operation of complementarity of reference as in (14). (14) Other speed-reducing devices may be added, such as regular shifts in the axis of the road, together with changes in the profile in the form of ramps and speed humps (Figure 4.3) .",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_7",
  "x": "Narrowings that allow a cycle to pass but not two cars are frequently added, often reinforced by the placement of trees, planters and street furniture. In contrast to the flowing design of fast roads, design elements are angular and of pedestrianscale, typified by low-level lamp posts which avoid the \"sea of light\" provided by high poles in traffic streets (Figure 4 .4). C8F (0297) In this text, which is on the topic of child safety, roads are never explicitly mentioned. Rather the concept of slow neighborhood roads can only be inferred from the description. The first explicit mention of the term road then refers to the opposite term fast roads. Comparison of in contrast with personal pronouns yields yet another similarity with other anaphoric expressions. Like with personal pronouns, the antecedent of in contrast can either occur across sentences, as in all of the examples considered so far, or it can occur intrasententially, as in example (15). (15) In contrast to his predecessors who worked at all hours of the day Macmillan tended to keep office hours. B0H (0476) Another property that distinguishes anaphoric discourse adverbials from structural connectives in the sense of <cite>Webber et al. (2003)</cite> , i.e. coordinating and subordinating conjunctions, concerns the type of dependencies that the arguments of the types of connectives can enter into.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_8",
  "x": "This naturally raises the question whether the semantics that has been proposed for this class of expressions can be naturally generalized to the semantics of in contrast. Following earlier proposals by Hinrichs (1986) and Kamp and Reyle (1993) , <cite>Webber et al. (2003)</cite> assume that the semantics of discourse adverbials such as then involves an anaphoric relation between two events. For example, the two clauses in (16) refer to individual events, which are put in the sequence-relation by the adverbial then. There are at least two difficulties associated with modelling the semantics of in contrast as a two-place relation between events: The scope of the two arguments of the contrast relation often extends beyond descriptions of individual events, as illustrated by examples such as (13), where the contrast involves sets of events and states of affairs. Thus, at the very least, one would have to generalize the semantics of in contrast to relations between sets of events and states of affairs, with relations between single events or states of affairs as a special case. However, it is difficult to see how such a modified representation could be suitably generalized to adequately model examples as in (18). (18) The Holsteins also tend to have much more white in the coat so that the white areas predominate and they could almost be described as white-and-blacks in contrast to the black-and-white Friesian type. B0K (0438) (18) explicitly contrasts two sets of individuals, cows in this case, rather than events or states of affairs. One could, of course, argue that the in contrast relation is simply polymorphic, referring either to relations between sets of events or states of affairs or to relations between (sets of) individuals or other entities such as locations, as in (13).",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_9",
  "x": "b. in-contrast((The-Holsteins, x [x also-tend-tohave-much-more-white-in-the-coat-so-thatthe-white-areas-predominate-and-they-couldalmost-be-described-as-white-and-blacks]), (the-black-and-white-Friesian-type, x \u00a1 [x alsotend-to-have-much-more-white-in-the-coat-sothat-the-white-areas-predominate-and-they-couldalmost-be-described-as-white-and-blacks])). In ( The account of in contrast which has been illustrated by the formulas in (19) and (20b) has two attractive properties: (i) from a theoretical perspective, it provides a unified analysis of the in contrast construction with and without a postmodifying prepositional phrase; (ii) by separating out the contrast pairs (as the first members of each argument pair) from their contrasting properties, it provides a transparent representation for applications such as information extraction and text summarization, which require tracking discourse entities and their relevant properties. Finally, it is worth reviewing the proposed analysis in light of the generalization put forth by Webber (2006) and by Lee et al. (2006) , namely that discourse connectives always denote two-place relations. The semantics of in contrast proposed in this section is consistent with this hypothesis since it assumes a two-place relation. However, notice that each of the two arguments is further structured into a contrast item and a contrast property. It is this highly structured character of the in-contrast relation that distinguishes this discourse connective from the much simpler two-place relations denoted by coordinating and subordinating conjunctions. The latter simply denote relations between events and/or states of affairs, namely those denoted by the two conjunct clauses. The semantics proposed for in-contrast, thus, provides further evidence for the distinction between coordinating and subordinating conjunctions and discourse adverbials that has been put forth by <cite>Webber et al. (2003)</cite> . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_10",
  "x": "It is this highly structured character of the in-contrast relation that distinguishes this discourse connective from the much simpler two-place relations denoted by coordinating and subordinating conjunctions. The latter simply denote relations between events and/or states of affairs, namely those denoted by the two conjunct clauses. The semantics proposed for in-contrast, thus, provides further evidence for the distinction between coordinating and subordinating conjunctions and discourse adverbials that has been put forth by <cite>Webber et al. (2003)</cite> . ---------------------------------- **CONCLUSION AND FUTURE WORK** This paper has presented a corpus-based study of the discourse connective in contrast. The corpus data were drawn from the British National Corpus (BNC) and were analyzed at the levels of syntax, discourse structure, and compositional semantics. Following <cite>Webber et al. (2003)</cite> , the paper argues that in contrast crucially involves discourse anaphora and, thus, resembles other discourse adverbials such as then, otherwise, and nevertheless. The compositional semantics proposed for other discourse connectives, however, does not straightforwardly generalize to in contrast, for which the notions of contrast pairs and contrast properties are essential.",
  "y": "differences"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_0",
  "x": "<cite>Artetxe et al. (2017)</cite> use a very small, automatically-generated seed lexicon of identical numerals as the initialization in an iterative self-learning framework to learn a linear mapping between monolingual embedding spaces; Zhang et al. (2017) use an adversarial training method to learn a similar mapping. Lample et al. (2018a) use a series of techniques to align monolingual embedding spaces in a completely unsupervised way; their method is used by Lample et al. (2018b) as the initialization for a completely unsupervised machine translation system. These recent advances in unsupervised bilingual lexicon induction show promise for use in low-resource contexts. However, none of them make use of linguistic features of the languages themselves (with the arguable exception of syntactic/semantic information encoded in the word embeddings). This is in contrast to work that predates many of these embedding-based methods that leveraged linguistic features such as edit distance and orthographic similarity: Dyer et al. (2011) and Berg-Kirkpatrick et al. (2010) investigate using linguistic features for word alignment, and Haghighi et al. (2008) use linguistic features for unsupervised bilingual lexicon induction. These features can help identify words with common ancestry (such as the English-Italian pair agile-agile) and borrowed words (macaronimaccheroni). The addition of linguistic features led to increased performance in these earlier models, especially for related languages, yet these features have not been applied to more modern methods. In this work, we extend the modern embeddingbased approach of <cite>Artetxe et al. (2017)</cite> with orthographic information in order to leverage similarities between related languages for increased accuracy in bilingual lexicon induction. ----------------------------------",
  "y": "background"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_1",
  "x": "Parallel data can be very expensive to create, so methods that require less of it or that can utilize more readily available data are desirable. One prevalent strategy involves creating multilingual word embeddings, where each language's vocabulary is embedded in the same latent space (Vuli\u0107 and Moens, 2013; Mikolov et al., 2013a; Artetxe et al., 2016) ; however, many of these methods still require a strong cross-lingual signal in the form of a large seed dictionary. More recent work has focused on reducing that constraint. Vuli\u0107 and Moens (2016) and Vulic and Korhonen (2016) use document-aligned data to learn bilingual embeddings instead of a seed dictionary. <cite>Artetxe et al. (2017)</cite> use a very small, automatically-generated seed lexicon of identical numerals as the initialization in an iterative self-learning framework to learn a linear mapping between monolingual embedding spaces; Zhang et al. (2017) use an adversarial training method to learn a similar mapping. Lample et al. (2018a) use a series of techniques to align monolingual embedding spaces in a completely unsupervised way; their method is used by Lample et al. (2018b) as the initialization for a completely unsupervised machine translation system. These recent advances in unsupervised bilingual lexicon induction show promise for use in low-resource contexts. However, none of them make use of linguistic features of the languages themselves (with the arguable exception of syntactic/semantic information encoded in the word embeddings). This is in contrast to work that predates many of these embedding-based methods that leveraged linguistic features such as edit distance and orthographic similarity: Dyer et al. (2011) and Berg-Kirkpatrick et al. (2010) investigate using linguistic features for word alignment, and Haghighi et al. (2008) use linguistic features for unsupervised bilingual lexicon induction.",
  "y": "motivation"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_2",
  "x": "However, none of them make use of linguistic features of the languages themselves (with the arguable exception of syntactic/semantic information encoded in the word embeddings). This is in contrast to work that predates many of these embedding-based methods that leveraged linguistic features such as edit distance and orthographic similarity: Dyer et al. (2011) and Berg-Kirkpatrick et al. (2010) investigate using linguistic features for word alignment, and Haghighi et al. (2008) use linguistic features for unsupervised bilingual lexicon induction. These features can help identify words with common ancestry (such as the English-Italian pair agile-agile) and borrowed words (macaronimaccheroni). The addition of linguistic features led to increased performance in these earlier models, especially for related languages, yet these features have not been applied to more modern methods. In this work, we extend the modern embeddingbased approach of <cite>Artetxe et al. (2017)</cite> with orthographic information in order to leverage similarities between related languages for increased accuracy in bilingual lexicon induction. ---------------------------------- **BACKGROUND** This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . Following their work, let X \u2208 R |Vs|\u00d7d and Z \u2208 R |Vt|\u00d7d be the word embedding matrices of two distinct languages, referred to respectively as the source and target, such that each row corresponds to the d-dimensional embedding of a single word.",
  "y": "extends"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_3",
  "x": "This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . Following their work, let X \u2208 R |Vs|\u00d7d and Z \u2208 R |Vt|\u00d7d be the word embedding matrices of two distinct languages, referred to respectively as the source and target, such that each row corresponds to the d-dimensional embedding of a single word. We refer to the ith row of one of these matrices as X i * or Z i * . The vocabularies for each language are V s and V t , respectively. Also let D \u2208 {0, 1} |Vs|\u00d7|Vt| be a binary matrix representing a dictionary such that D ij = 1 if the ith word in the source language is aligned with the jth word in the target language. We wish to find a mapping matrix W \u2208 R d\u00d7d that maps source embeddings onto their aligned target embeddings. <cite>Artetxe et al. (2017)</cite> define the optimal mapping matrix W * with the following equation, which minimizes the sum of the squared Euclidean distances between mapped source embeddings and their aligned target embeddings. By normalizing and mean-centering X and Z, and enforcing that W be an orthogonal matrix (W T W = I), the above formulation becomes equivalent to maximizing the dot product between the mapped source embeddings and target embeddings, such that",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_4",
  "x": "However, none of them make use of linguistic features of the languages themselves (with the arguable exception of syntactic/semantic information encoded in the word embeddings). This is in contrast to work that predates many of these embedding-based methods that leveraged linguistic features such as edit distance and orthographic similarity: Dyer et al. (2011) and Berg-Kirkpatrick et al. (2010) investigate using linguistic features for word alignment, and Haghighi et al. (2008) use linguistic features for unsupervised bilingual lexicon induction. These features can help identify words with common ancestry (such as the English-Italian pair agile-agile) and borrowed words (macaronimaccheroni). The addition of linguistic features led to increased performance in these earlier models, especially for related languages, yet these features have not been applied to more modern methods. In this work, we extend the modern embeddingbased approach of <cite>Artetxe et al. (2017)</cite> with orthographic information in order to leverage similarities between related languages for increased accuracy in bilingual lexicon induction. ---------------------------------- **BACKGROUND** This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . Following their work, let X \u2208 R |Vs|\u00d7d and Z \u2208 R |Vt|\u00d7d be the word embedding matrices of two distinct languages, referred to respectively as the source and target, such that each row corresponds to the d-dimensional embedding of a single word.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_5",
  "x": "**BACKGROUND** This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . Following their work, let X \u2208 R |Vs|\u00d7d and Z \u2208 R |Vt|\u00d7d be the word embedding matrices of two distinct languages, referred to respectively as the source and target, such that each row corresponds to the d-dimensional embedding of a single word. We refer to the ith row of one of these matrices as X i * or Z i * . The vocabularies for each language are V s and V t , respectively. Also let D \u2208 {0, 1} |Vs|\u00d7|Vt| be a binary matrix representing a dictionary such that D ij = 1 if the ith word in the source language is aligned with the jth word in the target language. We wish to find a mapping matrix W \u2208 R d\u00d7d that maps source embeddings onto their aligned target embeddings. <cite>Artetxe et al. (2017)</cite> define the optimal mapping matrix W * with the following equation, which minimizes the sum of the squared Euclidean distances between mapped source embeddings and their aligned target embeddings.",
  "y": "background"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_6",
  "x": "**BACKGROUND** This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . Following their work, let X \u2208 R |Vs|\u00d7d and Z \u2208 R |Vt|\u00d7d be the word embedding matrices of two distinct languages, referred to respectively as the source and target, such that each row corresponds to the d-dimensional embedding of a single word. We refer to the ith row of one of these matrices as X i * or Z i * . The vocabularies for each language are V s and V t , respectively. Also let D \u2208 {0, 1} |Vs|\u00d7|Vt| be a binary matrix representing a dictionary such that D ij = 1 if the ith word in the source language is aligned with the jth word in the target language. We wish to find a mapping matrix W \u2208 R d\u00d7d that maps source embeddings onto their aligned target embeddings. <cite>Artetxe et al. (2017)</cite> define the optimal mapping matrix W * with the following equation, which minimizes the sum of the squared Euclidean distances between mapped source embeddings and their aligned target embeddings.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_7",
  "x": "By normalizing and mean-centering X and Z, and enforcing that W be an orthogonal matrix (W T W = I), the above formulation becomes equivalent to maximizing the dot product between the mapped source embeddings and target embeddings, such that where Tr(\u00b7) is the trace operator, the sum of all diagonal entries. The optimal solution to this equation is W * = U V T , where X T DZ = U \u03a3V T is the singular value decomposition of X T DZ. This formulation requires a seed dictionary. To reduce the need for a large seed dictionary, <cite>Artetxe et al. (2017)</cite> propose an iterative, self-learning framework that determines W as above, uses it to calculate a new dictionary D, and then iterates until convergence. In the dictionary induction step, they set We propose two methods for extending this system using orthographic information, described in the following two sections. ---------------------------------- **ORTHOGRAPHIC EXTENSION OF WORD EMBEDDINGS**",
  "y": "extends"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_8",
  "x": "To reduce the need for a large seed dictionary, <cite>Artetxe et al. (2017)</cite> propose an iterative, self-learning framework that determines W as above, uses it to calculate a new dictionary D, and then iterates until convergence. In the dictionary induction step, they set We propose two methods for extending this system using orthographic information, described in the following two sections. ---------------------------------- **ORTHOGRAPHIC EXTENSION OF WORD EMBEDDINGS** This method augments the embeddings for all words in both languages before using them in the self-learning framework of <cite>Artetxe et al. (2017)</cite> . To do this, we append to each word's embedding a vector of length equal to the size of the union of the two languages' alphabets. Each position in this vector corresponds to a single letter, and its value is set to the count of that letter within the spelling of the word. This letter count vector is then scaled by a constant before being appended to the base word embedding.",
  "y": "background"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_9",
  "x": "This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . To reduce the need for a large seed dictionary, <cite>Artetxe et al. (2017)</cite> propose an iterative, self-learning framework that determines W as above, uses it to calculate a new dictionary D, and then iterates until convergence.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_10",
  "x": "The optimal solution to this equation is W * = U V T , where X T DZ = U \u03a3V T is the singular value decomposition of X T DZ. This formulation requires a seed dictionary. To reduce the need for a large seed dictionary, <cite>Artetxe et al. (2017)</cite> propose an iterative, self-learning framework that determines W as above, uses it to calculate a new dictionary D, and then iterates until convergence. In the dictionary induction step, they set We propose two methods for extending this system using orthographic information, described in the following two sections. ---------------------------------- **ORTHOGRAPHIC EXTENSION OF WORD EMBEDDINGS** This method augments the embeddings for all words in both languages before using them in the self-learning framework of <cite>Artetxe et al. (2017)</cite> . To do this, we append to each word's embedding a vector of length equal to the size of the union of the two languages' alphabets.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_11",
  "x": "These new matrices are used in place of X and Z in the self-learning process. ---------------------------------- **ORTHOGRAPHIC SIMILARITY ADJUSTMENT** This method modifies the similarity score for each word pair during the dictionary induction phase of the self-learning framework of <cite>Artetxe et al. (2017)</cite> , which uses the dot product of two words' embeddings to quantify similarity. We modify this similarity score by adding a measure of orthographic similarity, which is a function of the normalized string edit distance of the two words. The normalized edit distance is defined as the Levenshtein distance (L(\u00b7, \u00b7)) (Levenshtein, 1966) divided by the length of the longer word. The Levenshtein distance represents the minimum number of insertions, deletions, and substitutions required to transform one word into the other. The normalized edit distance function is denoted as NL(\u00b7, \u00b7). We define the orthographic similarity of two words w 1 and w 2 as log(2.0\u2212NL(w 1 , w 2 )).",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_12",
  "x": "These new matrices are used in place of X and Z in the self-learning process. ---------------------------------- **ORTHOGRAPHIC SIMILARITY ADJUSTMENT** This method modifies the similarity score for each word pair during the dictionary induction phase of the self-learning framework of <cite>Artetxe et al. (2017)</cite> , which uses the dot product of two words' embeddings to quantify similarity. We modify this similarity score by adding a measure of orthographic similarity, which is a function of the normalized string edit distance of the two words. The normalized edit distance is defined as the Levenshtein distance (L(\u00b7, \u00b7)) (Levenshtein, 1966) divided by the length of the longer word. The Levenshtein distance represents the minimum number of insertions, deletions, and substitutions required to transform one word into the other. The normalized edit distance function is denoted as NL(\u00b7, \u00b7). We define the orthographic similarity of two words w 1 and w 2 as log(2.0\u2212NL(w 1 , w 2 )).",
  "y": "extends"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_13",
  "x": "The algorithm works by computing all strings formed by k or fewer deletions from each target word, stores them in a hash table, then does the same for each source word and generates sourcetarget pairs that share an entry in the hash table. The complexity of this algorithm can be expressed as O(|V |l k ), where V = V t \u222a V s is the combined vocabulary and l is the length of the longest word in V . This is linear with respect to the vocabulary size, as opposed to the quadratic complexity required for computing the entire matrix. However, the algorithm is sensitive to both word length and the choice of k. In our experiments, we found that ignoring all words of length greater than 30 allowed the algorithm to complete very quickly while skipping less than 0.1% of the data. We also used small values of k (0 < k < 4), and used k = 1 for our final results, finding no significant benefit from using a larger value. ---------------------------------- **EXPERIMENTS** We use the datasets used by <cite>Artetxe et al. (2017)</cite> , consisting of three language pairs: EnglishItalian, English-German, and English-Finnish. The English-Italian dataset was introduced in Dinu and Baroni (2014) ; the other datasets were created by <cite>Artetxe et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_14",
  "x": "The algorithm works by computing all strings formed by k or fewer deletions from each target word, stores them in a hash table, then does the same for each source word and generates sourcetarget pairs that share an entry in the hash table. The complexity of this algorithm can be expressed as O(|V |l k ), where V = V t \u222a V s is the combined vocabulary and l is the length of the longest word in V . This is linear with respect to the vocabulary size, as opposed to the quadratic complexity required for computing the entire matrix. However, the algorithm is sensitive to both word length and the choice of k. In our experiments, we found that ignoring all words of length greater than 30 allowed the algorithm to complete very quickly while skipping less than 0.1% of the data. We also used small values of k (0 < k < 4), and used k = 1 for our final results, finding no significant benefit from using a larger value. ---------------------------------- **EXPERIMENTS** We use the datasets used by <cite>Artetxe et al. (2017)</cite> , consisting of three language pairs: EnglishItalian, English-German, and English-Finnish. The English-Italian dataset was introduced in Dinu and Baroni (2014) ; the other datasets were created by <cite>Artetxe et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_15",
  "x": "We also used small values of k (0 < k < 4), and used k = 1 for our final results, finding no significant benefit from using a larger value. ---------------------------------- **EXPERIMENTS** We use the datasets used by <cite>Artetxe et al. (2017)</cite> , consisting of three language pairs: EnglishItalian, English-German, and English-Finnish. The English-Italian dataset was introduced in Dinu and Baroni (2014) ; the other datasets were created by <cite>Artetxe et al. (2017)</cite> . Each dataset includes monolingual word embeddings (trained with word2vec (Mikolov et al., 2013b) ) for both languages and a bilingual dictionary, separated into a training and test set. We do not use the training set as the input dictionary to the system, instead using an automatically-generated dictionary consisting only of numeral identity translations (such as 2-2, 3-3, et cetera) as in <cite>Artetxe et al. (2017)</cite> . 1 However, because the methods presented in this work feature tunable hyperparameters, we use a portion of the training set as devel- Table 1 : Comparison of methods on test data. Scaling constants c e and c s were selected based on performance on development data over all three language pairs.",
  "y": "similarities uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_16",
  "x": "We did not filter these characters in this work. For our experiments with orthographic similarity adjustment, the heuristic identified approximately 2 million word pairs for each language pair out of a possible 40 billion, resulting in significant computation savings. and c s = 1 as our hyperparameters. The local optima were not identical for all three languages, but we felt that these values struck the best compromise among them. Table 1 compares our methods against the system of <cite>Artetxe et al. (2017)</cite> , using scaling factors selected based on development data results. Because approximately 20% of source-target pairs in the dictionary were identical, we also extended all systems to guess the identity translation if the source word appeared in the target vocabulary. This improved accuracy in most cases, with some exceptions for English-Italian. We also experimented with both methods together, and found that this was the best of the settings that did not include the identity translation component; with the identity component included, however, the embedding extension method alone was best for EnglishFinnish. The fact that Finnish is the only language here that is not in the Indo-European family (and has fewer words borrowed from English or its ancestors) may explain why the performance trends for English-Finnish were different than those of the other two language pairs. In addition to identifying orthographically similar words, the extension method is capable of learning a mapping between source and target letters, which could partially explain its improved performance over our edit distance method.",
  "y": "differences"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_17",
  "x": "The local optima were not identical for all three languages, but we felt that these values struck the best compromise among them. Table 1 compares our methods against the system of <cite>Artetxe et al. (2017)</cite> , using scaling factors selected based on development data results. Because approximately 20% of source-target pairs in the dictionary were identical, we also extended all systems to guess the identity translation if the source word appeared in the target vocabulary. This improved accuracy in most cases, with some exceptions for English-Italian. We also experimented with both methods together, and found that this was the best of the settings that did not include the identity translation component; with the identity component included, however, the embedding extension method alone was best for EnglishFinnish. The fact that Finnish is the only language here that is not in the Indo-European family (and has fewer words borrowed from English or its ancestors) may explain why the performance trends for English-Finnish were different than those of the other two language pairs. In addition to identifying orthographically similar words, the extension method is capable of learning a mapping between source and target letters, which could partially explain its improved performance over our edit distance method. Table 2 shows some correct translations from our system that were missed by the baseline. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_0",
  "x": "Answering detailed questions about an image is a type of task which requires more sophisticated patterns of reasoning, and there has been a rapid recent proliferation of computer vision approaches for tackling the visual question answering (Visual QA) task [6,<cite> 7]</cite> . Successful Visual QA architectures must be able Given a natural image and a textual question as input, our Visual QA architecture outputs an answer. It uses a hard attention mechanism that selects only the important visual features for the task for further processing. We base our architecture on the premise that the norm of the visual features correlates with their relevance, and that those feature vectors with high magnitudes correspond to image regions which contain important semantic content. to handle many objects and their complex relations while also integrating rich background knowledge, and attention has emerged as a promising strategy for achieving good performance<cite> [7,</cite> 8, 9, 10, 11, 12, 13, 14] . We recognize a broad distinction between types of attention in computer vision and machine learning -soft versus hard attention. Existing attention models<cite> [7,</cite> 8, 9, 10] are predominantly based on soft attention, in which all information is adaptively re-weighted before being aggregated. This can improve accuracy by isolating important information and avoiding interference from unimportant information. Learning becomes more data efficient as the complexity of the interactions among different pieces of information reduces; this, loosely speaking, allows for more unambiguous credit assignment.",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_1",
  "x": "Successful Visual QA architectures must be able Given a natural image and a textual question as input, our Visual QA architecture outputs an answer. It uses a hard attention mechanism that selects only the important visual features for the task for further processing. We base our architecture on the premise that the norm of the visual features correlates with their relevance, and that those feature vectors with high magnitudes correspond to image regions which contain important semantic content. to handle many objects and their complex relations while also integrating rich background knowledge, and attention has emerged as a promising strategy for achieving good performance<cite> [7,</cite> 8, 9, 10, 11, 12, 13, 14] . We recognize a broad distinction between types of attention in computer vision and machine learning -soft versus hard attention. Existing attention models<cite> [7,</cite> 8, 9, 10] are predominantly based on soft attention, in which all information is adaptively re-weighted before being aggregated. This can improve accuracy by isolating important information and avoiding interference from unimportant information. Learning becomes more data efficient as the complexity of the interactions among different pieces of information reduces; this, loosely speaking, allows for more unambiguous credit assignment. By contrast, hard attention, in which only a subset of information is selected for further processing, is much less widely used.",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_2",
  "x": "The perceptual effects can be so dramatic that prominent entities may not even rise to the level of awareness when the viewer is attending to other things in the scene [3, 4, 5] . Yet attention has not been a transformative force in computer vision, possibly because many standard computer vision tasks like detection, segmentation, and classification do not involve the sort of complex reasoning which attention is thought to facilitate. Answering detailed questions about an image is a type of task which requires more sophisticated patterns of reasoning, and there has been a rapid recent proliferation of computer vision approaches for tackling the visual question answering (Visual QA) task [6,<cite> 7]</cite> . Successful Visual QA architectures must be able Given a natural image and a textual question as input, our Visual QA architecture outputs an answer. It uses a hard attention mechanism that selects only the important visual features for the task for further processing. We base our architecture on the premise that the norm of the visual features correlates with their relevance, and that those feature vectors with high magnitudes correspond to image regions which contain important semantic content. to handle many objects and their complex relations while also integrating rich background knowledge, and attention has emerged as a promising strategy for achieving good performance<cite> [7,</cite> 8, 9, 10, 11, 12, 13, 14] . We recognize a broad distinction between types of attention in computer vision and machine learning -soft versus hard attention. Existing attention models<cite> [7,</cite> 8, 9, 10] are predominantly based on soft attention, in which all information is adaptively re-weighted before being aggregated.",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_3",
  "x": "There have been various efforts to address this shortcoming in visual attention [15] , attention to text [16] , and more general machine learning domains [17, 18, 19] , but this is still a very active area of research. Here we explore a simple approach to hard attention that bootstraps on an interesting phenomenon [20] in the feature representations of convolutional neural networks (CNNs): learned features often carry an easily accessible signal for hard attentional selection. In particular, selecting those feature vectors with the greatest L2-norm values proves to be a heuristic that can facilitate hard attention -and provide the performance and efficiency benefits associated with -without requiring specialized learning procedures (see Figure 1 ). This attentional signal results indirectly from a standard supervised task loss, and does not require explicit supervision to incentivize norms to be proportional to object presence, salience, or other potentially meaningful measures [20, 21] . We rely on a canonical Visual QA pipeline<cite> [7,</cite> 9, 22, 23, 24, 25] augmented with a hard attention mechanism that uses the L2-norms of the feature vectors to select subsets of the information for further processing. The first version, called the Hard Attention Network (HAN), selects a fixed number of feature vectors by choosing those with the top norms. The second version, called the Adaptive Hard Attention Network (AdaHAN), selects a variable number of feature vectors that depends on the input. Our results show that our algorithm can actually outperform comparable soft attention architectures on a challenging Visual QA task. This approach also produces interpretable hard attention masks, where the image regions which correspond to the selected features often contain semantically meaningful information, such as coherent objects.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_4",
  "x": "The second version, called the Adaptive Hard Attention Network (AdaHAN), selects a variable number of feature vectors that depends on the input. Our results show that our algorithm can actually outperform comparable soft attention architectures on a challenging Visual QA task. This approach also produces interpretable hard attention masks, where the image regions which correspond to the selected features often contain semantically meaningful information, such as coherent objects. We also show strong performance when combined with a form of non-local pairwise model [26, 25, 27, 28] . This algorithm computes features over pairs of input features and thus scale quadratically with number of vectors in the feature map, highlighting the importance of feature selection. ---------------------------------- **RELATED WORK** Visual question answering, or more broadly the Visual Turing Test, asks \"Can machines understand a visual scene only from answering questions?\" [6, 23, 29, 30, 31, 32] . Creating a good Visual QA dataset has proved non-trivial: biases in the early datasets [6, 22, 23, 33] rewarded algorithms for exploiting spurious correlations, rather than tackling the reasoning problem head-on<cite> [7,</cite> 34, 35] .",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_5",
  "x": "**RELATED WORK** Visual question answering, or more broadly the Visual Turing Test, asks \"Can machines understand a visual scene only from answering questions?\" [6, 23, 29, 30, 31, 32] . Creating a good Visual QA dataset has proved non-trivial: biases in the early datasets [6, 22, 23, 33] rewarded algorithms for exploiting spurious correlations, rather than tackling the reasoning problem head-on<cite> [7,</cite> 34, 35] . Thus, we focus on the recently-introduced VQA-CP <cite>[7]</cite> and CLEVR [34] datasets, which aim to reduce the dataset biases, providing a more difficult challenge for rich visual reasoning. One of the core challenges of Visual QA is the problem of grounding language: that is, associating the meaning of a language term with a specific perceptual input [36] . Many works have tackled this problem [37, 38, 39, 40] , enforcing that language terms be grounded in the image. In contrast, our algorithm does not directly use correspondence between modalities to enforce such grounding but instead relies on learning to find a discrete representation that captures the required information from the raw visual input, and question-answer pairs. The most successful Visual QA architectures build multimodal representations with a combined CNN+LSTM architecture [22, 33, 41] , and recently have begun including attention mechanisms inspired by soft and hard attention for image captioning [42] . However, only soft attention is used in the majority of Visual QA works<cite> [7,</cite> 8, 9, 10, 11, 12, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52] .",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_6",
  "x": "However, only soft attention is used in the majority of Visual QA works<cite> [7,</cite> 8, 9, 10, 11, 12, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52] . In these architectures, a full-frame CNN representation is used to compute a spatial weighting (attention) over the CNN grid cells. The visual representation is then the weighted-sum of the input tensor across space. The alternative is to select CNN grid cells in a discrete way, but due to many challenges in training non-differentiable architectures, such hard attention alternatives are severely under-explored. Notable exceptions include [6, 13, 14, 53, 54, 55] , but these run state-of-the-art object detectors or proposals to compute the hard attention maps. We argue that relying on such external tools is fundamentally limited: it requires costly annotations, and cannot easily adapt to new visual concepts that aren't previously labeled. Outside Visual QA and captioning, some prior work in vision has explored limited forms of hard attention. One line of work on discriminative patches builds a representation by selecting some patches and ignoring others, which has proved useful for object detection and classification [56, 57, 58] , and especially visualization [59] . However, such methods have recently been largely supplanted by end-to-end feature learning for practical vision problems.",
  "y": "motivation background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_7",
  "x": "These architectures maximize a conditional distribution over answers a, given questions q and images x: where A is a countable set of all possible answers. As is common in question answering<cite> [7,</cite> 9, 22, 23, 24] , the question is a sequence of words q = [q 1 , ..., q n ], while the output is reduced to a classification problem between a set of common answers (this is limited compared to approaches that generate answers [41] , but works better in practice). Our architecture for learning a mapping from image and question, to answer, is shown in Figure 2 . We encode the image with a CNN [62] (in our case, a pre-trained ResNet-101 [63] , or a small CNN trained from scratch), and encode the question to a fixed-length vector representation with an LSTM [64] . We compute a combined representation by copying the question representation to every spatial location in the CNN, and concatenating it with (or simply adding it to) the visual features, like previous Otherwise, we follow the canonical Visual QA pipeline<cite> [7,</cite> 9, 22, 23, 24, 25] . Questions and images are encoded into their vector representations. Next, the spatial encoding of the visual features is unraveled, and the question embedding is broadcasted and concatenated (or added) accordingly to form a multimodal representation of the inputs. Our attention mechanism selectively chooses a subset of the multimodal vectors that are next aggregated and processed by the answering module.",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_8",
  "x": "Our architecture for learning a mapping from image and question, to answer, is shown in Figure 2 . We encode the image with a CNN [62] (in our case, a pre-trained ResNet-101 [63] , or a small CNN trained from scratch), and encode the question to a fixed-length vector representation with an LSTM [64] . We compute a combined representation by copying the question representation to every spatial location in the CNN, and concatenating it with (or simply adding it to) the visual features, like previous Otherwise, we follow the canonical Visual QA pipeline<cite> [7,</cite> 9, 22, 23, 24, 25] . Questions and images are encoded into their vector representations. Next, the spatial encoding of the visual features is unraveled, and the question embedding is broadcasted and concatenated (or added) accordingly to form a multimodal representation of the inputs. Our attention mechanism selectively chooses a subset of the multimodal vectors that are next aggregated and processed by the answering module. work<cite> [7,</cite> 9, 22, 23, 24, 25] . After a few layers of combined processing, we apply attention over spatial locations, following previous works which often apply soft attention mechanisms<cite> [7,</cite> 8, 9, 10] at this point in the architecture. Finally, we aggregate features, using either sum-pooling, or relational [25, 27, 65] modules.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_10",
  "x": "After a few layers of combined processing, we apply attention over spatial locations, following previous works which often apply soft attention mechanisms<cite> [7,</cite> 8, 9, 10] at this point in the architecture. Finally, we aggregate features, using either sum-pooling, or relational [25, 27, 65] modules. We train the whole network end-to-end with a standard logistic regression loss over answer categories. ---------------------------------- **ATTENTION MECHANISMS** Here, we describe prior work on soft attention, and our approach to hard attention. Soft Attention. In most prior work, soft attention is implemented as a weighted mask over the spatial cells of the CNN representation. Let x := CN N (x), q := LST M (q) for image x and question q. We compute a weight w ij for every x ij (where i and j index spatial locations), using a neural network that takes x ij and q as input.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_11",
  "x": "We then examine AdaHAN, which adaptively chooses the number of attended cells, and briefly investigate the effect of network depth and pretraining. Finally, we present qualitative results, and also provide results on CLEVR to show the method's generality. ---------------------------------- **DATASETS** VQA-CP v2. This dataset <cite>[7]</cite> consists of about 121K (98K) images, 438K (220K) questions, and 4.4M (2.2M) answers in the train (test) set; and it is created so that the distribution of the answers between train and test splits differ, and hence the models cannot excessively rely on the language prior <cite>[7]</cite> . As expected, <cite>[7]</cite> show that performance of all Visual QA approaches they tested drops significantly between train to test sets. The dataset provides a standard traintest split, and also breaks questions into different question types: those where the answer is yes/no, those where the answer is a number, and those where the answer is something else. Thus, we report accuracy on each question type as well as the overall accuracy for each network architecture.",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_12",
  "x": "This dataset <cite>[7]</cite> consists of about 121K (98K) images, 438K (220K) questions, and 4.4M (2.2M) answers in the train (test) set; and it is created so that the distribution of the answers between train and test splits differ, and hence the models cannot excessively rely on the language prior <cite>[7]</cite> . As expected, <cite>[7]</cite> show that performance of all Visual QA approaches they tested drops significantly between train to test sets. The dataset provides a standard traintest split, and also breaks questions into different question types: those where the answer is yes/no, those where the answer is a number, and those where the answer is something else. Thus, we report accuracy on each question type as well as the overall accuracy for each network architecture. CLEVR. This synthetic dataset [34] consists of 100K images of 3D rendered objects like spheres and cylinders, and roughly 1m questions that were automatically generated with a procedural engine. While the visual task is relatively simple, solving this dataset requires reasoning over complex relationships between many objects. ---------------------------------- **EFFECT OF HARD ATTENTION**",
  "y": "motivation"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_13",
  "x": "These results are shown in the middle of Table 1 , where we can see that hard attention (48 entitties) actually boosts performance over an analogous model without hard attention. Finally, we compare standard soft attention baselines in the bottom of Table 1. In particular, we include previous results using a basic soft attention network<cite> [7,</cite> 9] , as well as our own re-implementation of the soft attention pooling algorithm presented in<cite> [7,</cite> 9] with the same features used in other experiments. Surprisingly, soft attention does not outperform basic sum pooling, even with careful implementation that outperforms the previously reported results with the same method on this dataset; in fact, it performs slightly worse. The nonlocal pairwise aggregation performs better than SAN on its own, although the best result includes hard attention. Our results overall are somewhat worse than the state-of-the-art <cite>[7]</cite> , but this is likely due to several architectural decisions not included here, such as a split pathway for different kinds of questions, special question embeddings, and the use of the question extractor. Table 2 : Comparison between different adaptive hard-attention techniques with average number of attended parts, and aggregation operation. We consider a simple summation, and the non-local pairwise aggregation. Since AdaHAN adaptively selects relevant features, based on the fixed threshold 1 w * h , we report here the average number of attended parts.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_14",
  "x": "Our results overall are somewhat worse than the state-of-the-art <cite>[7]</cite> , but this is likely due to several architectural decisions not included here, such as a split pathway for different kinds of questions, special question embeddings, and the use of the question extractor. Table 2 : Comparison between different adaptive hard-attention techniques with average number of attended parts, and aggregation operation. We consider a simple summation, and the non-local pairwise aggregation. Since AdaHAN adaptively selects relevant features, based on the fixed threshold 1 w * h , we report here the average number of attended parts. ---------------------------------- **ADAPTIVE HARD ATTENTION** Thus far, our experiments have dealt with networks that have a fixed threshold for all images. However, some images and questions may require reasoning about more entities than others. Therefore, we explore a simple adaptive method, where the network chooses how many cells to attend to for each image.",
  "y": "differences"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_15",
  "x": "We trained our models until we notice a saturation on the training set. Then we evaluate these models on the test set. Our tables show the performance of all the methods wrt. the second digits precision obtained by rounding. Table 1 shows SAN's [9] results reported by <cite>[7]</cite> together with our in-house implementation (denoted as \"ours\"). Our implementation has 2 attention hops, 1024 dimensional multimodal embedding size, a fixed learning rate 0.0001, and ResNet-101. In these experiments we pool the attended representations by weighted average with the attention weights. Our in-house implementation of the nonlocal pairwise mechanism strongly resembles implementations of [26] , and [27] . We use 2 heads, with embedding size 512.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_16",
  "x": "Since our network uses hard attention, which has zero gradients almost everywhere, one might suspect that it will become more difficult to train the lowerlevel features, or worse, that untrained features might prevent us from bootstrapping the attention mechanism. Therefore, we also trained HAN+sum (with 16% of the input cells) end-to-end together with a relatively small convolutional neural network initialized from scratch. We compare our method against our implementation of the SAN method trained using the same simple convolutional neural network. We call the models: simple-SAN, and simple-HAN. Analysis. In our experiments, simple-SAN achieves about 21% performance on the test set. Surprisingly, simple-HAN+sum achieves about 24% performance on the same split, on-par with the performance of normal SAN that uses more complex and deeper visual architecture [67] ; the results are reported by <cite>[7]</cite> . This result shows that the hard attention mechanism can indeed be tightly coupled within the training process, and that the whole procedure does not rely heavily on the properties of the ImageNet pre-trained networks. In a sense, we see that a discrete notion of entities also \"emerges\" through the learning process, leading to efficient training.",
  "y": "similarities"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_0",
  "x": "Besides improving readability and understandability of texts for readers, automated text segmentation is beneficial for NLP and IR tasks such as text summarization (Angheluta et al., 2002; Dias et al., 2007) and passage retrieval (Huang et al., 2003; Dias et al., 2007) . Whereas early approaches to unsupervised text segmentation measured the coherence of segments via raw term overlaps between sentences (Hearst, 1997; Choi, 2000) , more recent methods (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> addressed the issue of sparsity of term-based representations by replacing term-vectors with vectors of latent topics. A topical representation of text is, however, merely a vague approximation of its meaning. Considering that the goal of TS is to identify semantically coherent segments, we propose a TS algorithm aiming to directly capture the semantic relatedness between segments, instead of approximating it via topical similarity. We employ word embeddings (Mikolov et al., 2013 ) and a measure of semantic relatedness of short texts (\u0160ari\u0107 et al., 2012) to construct a relatedness graph of the text in which nodes denote sentences and edges are added between semantically related sentences. We then derive segments using the maximal cliques of such similarity graphs. The proposed algorithm displays competitive performance on the artifically-generated benchmark TS dataset (Choi, 2000) and, more importantly, outperforms the best-performing topic modeling-based TS method on a real-world dataset of political manifestos. ---------------------------------- **RELATED WORK**",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_1",
  "x": "Like the majority of TS methods (Hearst, 1994; Brants et al., 2002; Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> , in this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical TS, where each toplevel segment is further broken down (Yaari, 1997; Eisenstein, 2009 ). Hearst (1994 introduced TextTiling, one of the first unsupervised algorithms for linear text segmentation. She exploits the fact that words tend to be repeated in coherent segments and measures the similarity between paragraphs by comparing their sparse term-vectors. Choi (2000) introduced the probabilistic algorithm using matrix-based ranking and clustering to determine similarities between segments. Galley et al. (2003) combined contentbased information with acoustic cues in order to detect discourse shifts whereas Utiyama and Isahara (2001) and Fragkou et al. (2004) minimized different segmentation cost functions with dynamic programming. The first segmentation approach based on topic modeling (Brants et al., 2002) employed the probabilistic latent semantic analysis (pLSA) to derive latent representations of segments and determined the segmentation based on similarities of segments' latent vectors. More recent models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets (Choi, 2000; Galley et al., 2003) . Misra et al. (2009) used dynamic programming to find globally optimal segmentation over the set of LDA-based segment representations, whereas <cite>Riedl and Biemann (2012)</cite> introduced TopicTiling, an LDA-driven extension of Hearst's TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors). <cite>Riedl and Biemann (2012)</cite> show that TopicTiling outperforms at-that-time state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009 ) and that it is also faster than other LDA-based methods (Misra et al., 2009 ).",
  "y": "similarities"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_2",
  "x": "The first segmentation approach based on topic modeling (Brants et al., 2002) employed the probabilistic latent semantic analysis (pLSA) to derive latent representations of segments and determined the segmentation based on similarities of segments' latent vectors. More recent models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets (Choi, 2000; Galley et al., 2003) . Misra et al. (2009) used dynamic programming to find globally optimal segmentation over the set of LDA-based segment representations, whereas <cite>Riedl and Biemann (2012)</cite> introduced TopicTiling, an LDA-driven extension of Hearst's TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors). <cite>Riedl and Biemann (2012)</cite> show that TopicTiling outperforms at-that-time state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009 ) and that it is also faster than other LDA-based methods (Misra et al., 2009 ). In the most closely related work to ours, Malioutov and Barzilay (2006) proposed a graphbased TS approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof-words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments. Similarly, Ferret (2007) builds the similarity graph, only between words instead of between sentences, using sparse co-occurrence vectors as semantic representations for words. He then identifies topics by clustering the word similarity graph via the Shared Nearest Neighbor algorithm (Ert\u00f6z et al., 2004) . Unlike these works, we use the dense semantic representations of words and sentences (i.e., embeddings), which have been shown to outperform sparse semantic vectors on a range of NLP tasks. Also, instead of looking for minimal cuts in the relatedness graph, we exploit the maximal cliques of the relatedness graph between sentences to obtain the topic segments.",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_3",
  "x": "Hearst (1994 introduced TextTiling, one of the first unsupervised algorithms for linear text segmentation. She exploits the fact that words tend to be repeated in coherent segments and measures the similarity between paragraphs by comparing their sparse term-vectors. Choi (2000) introduced the probabilistic algorithm using matrix-based ranking and clustering to determine similarities between segments. Galley et al. (2003) combined contentbased information with acoustic cues in order to detect discourse shifts whereas Utiyama and Isahara (2001) and Fragkou et al. (2004) minimized different segmentation cost functions with dynamic programming. The first segmentation approach based on topic modeling (Brants et al., 2002) employed the probabilistic latent semantic analysis (pLSA) to derive latent representations of segments and determined the segmentation based on similarities of segments' latent vectors. More recent models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets (Choi, 2000; Galley et al., 2003) . Misra et al. (2009) used dynamic programming to find globally optimal segmentation over the set of LDA-based segment representations, whereas <cite>Riedl and Biemann (2012)</cite> introduced TopicTiling, an LDA-driven extension of Hearst's TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors). <cite>Riedl and Biemann (2012)</cite> show that TopicTiling outperforms at-that-time state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009 ) and that it is also faster than other LDA-based methods (Misra et al., 2009 ). In the most closely related work to ours, Malioutov and Barzilay (2006) proposed a graphbased TS approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof-words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments.",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_4",
  "x": "More recent models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets (Choi, 2000; Galley et al., 2003) . Misra et al. (2009) used dynamic programming to find globally optimal segmentation over the set of LDA-based segment representations, whereas <cite>Riedl and Biemann (2012)</cite> introduced TopicTiling, an LDA-driven extension of Hearst's TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors). <cite>Riedl and Biemann (2012)</cite> show that TopicTiling outperforms at-that-time state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009 ) and that it is also faster than other LDA-based methods (Misra et al., 2009 ). In the most closely related work to ours, Malioutov and Barzilay (2006) proposed a graphbased TS approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof-words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments. Similarly, Ferret (2007) builds the similarity graph, only between words instead of between sentences, using sparse co-occurrence vectors as semantic representations for words. He then identifies topics by clustering the word similarity graph via the Shared Nearest Neighbor algorithm (Ert\u00f6z et al., 2004) . Unlike these works, we use the dense semantic representations of words and sentences (i.e., embeddings), which have been shown to outperform sparse semantic vectors on a range of NLP tasks. Also, instead of looking for minimal cuts in the relatedness graph, we exploit the maximal cliques of the relatedness graph between sentences to obtain the topic segments. ----------------------------------",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_5",
  "x": "To allow for comparison with previous work, we evaluate GRAPHSEG on four subsets of the Choi dataset, differing in number of sentences the seg-2008, and 2012 U.S. elections ments contain. For the evaluation on the Choi dataset, the GRAPHSEG algorithm made use of the publicly available word embeddings built from a Google News dataset. 4 Both LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> and GRAPHSEG rely on corpus-derived word representations. Thus, we evaluated on the Manifesto dataset both the domainadapted and domain-unadapted variants of these methods. The domain-adapted variants of the models used the unlabeled domain corpus -a test set of 466 unlabeled political manifestos -to train the domain-specific word representations. This means that we obtain (1) in-domain topics for the LDAbased TopicTiling model of <cite>Riedl and Biemann (2012)</cite> and (2) domain-specific embeddings for the GRAPHSEG algorithm. On the Manifesto dataset we also evaluate a baseline that randomly (50% chance) starts a new segment at points m sentences apart, with m being set to half of the average length of gold segments. We evaluate the performance using two standard TS evaluation metrics -P k (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) . P k is the probability that two randomly drawn sentences mutually k sentences apart are classified incorrectly -either as belonging to the same segment when they are in different gold segments or as being in different segments when they are in the same gold segment.",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_6",
  "x": "---------------------------------- **EXPERIMENTAL SETTING** To allow for comparison with previous work, we evaluate GRAPHSEG on four subsets of the Choi dataset, differing in number of sentences the seg-2008, and 2012 U.S. elections ments contain. For the evaluation on the Choi dataset, the GRAPHSEG algorithm made use of the publicly available word embeddings built from a Google News dataset. 4 Both LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> and GRAPHSEG rely on corpus-derived word representations. Thus, we evaluated on the Manifesto dataset both the domainadapted and domain-unadapted variants of these methods. The domain-adapted variants of the models used the unlabeled domain corpus -a test set of 466 unlabeled political manifestos -to train the domain-specific word representations. This means that we obtain (1) in-domain topics for the LDAbased TopicTiling model of <cite>Riedl and Biemann (2012)</cite> and (2) domain-specific embeddings for the GRAPHSEG algorithm. On the Manifesto dataset we also evaluate a baseline that randomly (50% chance) starts a new segment at points m sentences apart, with m being set to half of the average length of gold segments.",
  "y": "uses"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_7",
  "x": "For the evaluation on the Choi dataset, the GRAPHSEG algorithm made use of the publicly available word embeddings built from a Google News dataset. 4 Both LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> and GRAPHSEG rely on corpus-derived word representations. Thus, we evaluated on the Manifesto dataset both the domainadapted and domain-unadapted variants of these methods. The domain-adapted variants of the models used the unlabeled domain corpus -a test set of 466 unlabeled political manifestos -to train the domain-specific word representations. This means that we obtain (1) in-domain topics for the LDAbased TopicTiling model of <cite>Riedl and Biemann (2012)</cite> and (2) domain-specific embeddings for the GRAPHSEG algorithm. On the Manifesto dataset we also evaluate a baseline that randomly (50% chance) starts a new segment at points m sentences apart, with m being set to half of the average length of gold segments. We evaluate the performance using two standard TS evaluation metrics -P k (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) . P k is the probability that two randomly drawn sentences mutually k sentences apart are classified incorrectly -either as belonging to the same segment when they are in different gold segments or as being in different segments when they are in the same gold segment. Following <cite>Riedl and Biemann (2012)</cite> , we set k to half of the document length divided by the number of gold segments.",
  "y": "uses"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_8",
  "x": "For the evaluation on the Choi dataset, the GRAPHSEG algorithm made use of the publicly available word embeddings built from a Google News dataset. 4 Both LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> and GRAPHSEG rely on corpus-derived word representations. Thus, we evaluated on the Manifesto dataset both the domainadapted and domain-unadapted variants of these methods. The domain-adapted variants of the models used the unlabeled domain corpus -a test set of 466 unlabeled political manifestos -to train the domain-specific word representations. This means that we obtain (1) in-domain topics for the LDAbased TopicTiling model of <cite>Riedl and Biemann (2012)</cite> and (2) domain-specific embeddings for the GRAPHSEG algorithm. On the Manifesto dataset we also evaluate a baseline that randomly (50% chance) starts a new segment at points m sentences apart, with m being set to half of the average length of gold segments. We evaluate the performance using two standard TS evaluation metrics -P k (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) . P k is the probability that two randomly drawn sentences mutually k sentences apart are classified incorrectly -either as belonging to the same segment when they are in different gold segments or as being in different segments when they are in the same gold segment. Following <cite>Riedl and Biemann (2012)</cite> , we set k to half of the document length divided by the number of gold segments.",
  "y": "uses"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_9",
  "x": "The GRAPHSEG algorithm has two parameters: (1) the sentence similarity treshold \u03c4 which is used when creating edges of the sentence relatedness graph and (2) the minimal segment size n, which we utilize to merge adjacent segments that are too small. In all experiments we use grid-search in a folded cross-validation setting to jointly optimize both parameters. In view of comparison with other models, the parameter optimization is justified be-3-5 6-8 9-11 3-11 Brants et al. (2002) 7. cause other models, e.g., TopicTiling<cite> (Riedl and Biemann, 2012)</cite> , also have parameters (e.g., number of topics for the topic model) which are optimized using cross-validation. ---------------------------------- **RESULTS AND DISCUSSION** In Table 2 we report the performance of GRAPH-SEG and prominent TS methods on the synthetic Choi dataset. GRAPHSEG performs competitively, outperforming all methods but (Fragkou et al., 2004) and domain-adapted versions of LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> . However, the approach by (Fragkou et al., 2004) uses the gold standard information -the average gold segment size -as input.",
  "y": "similarities"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_10",
  "x": "The GRAPHSEG algorithm has two parameters: (1) the sentence similarity treshold \u03c4 which is used when creating edges of the sentence relatedness graph and (2) the minimal segment size n, which we utilize to merge adjacent segments that are too small. In all experiments we use grid-search in a folded cross-validation setting to jointly optimize both parameters. In view of comparison with other models, the parameter optimization is justified be-3-5 6-8 9-11 3-11 Brants et al. (2002) 7. cause other models, e.g., TopicTiling<cite> (Riedl and Biemann, 2012)</cite> , also have parameters (e.g., number of topics for the topic model) which are optimized using cross-validation. ---------------------------------- **RESULTS AND DISCUSSION** In Table 2 we report the performance of GRAPH-SEG and prominent TS methods on the synthetic Choi dataset. GRAPHSEG performs competitively, outperforming all methods but (Fragkou et al., 2004) and domain-adapted versions of LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> . However, the approach by (Fragkou et al., 2004) uses the gold standard information -the average gold segment size -as input.",
  "y": "differences"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_11",
  "x": "**RESULTS AND DISCUSSION** In Table 2 we report the performance of GRAPH-SEG and prominent TS methods on the synthetic Choi dataset. GRAPHSEG performs competitively, outperforming all methods but (Fragkou et al., 2004) and domain-adapted versions of LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> . However, the approach by (Fragkou et al., 2004) uses the gold standard information -the average gold segment size -as input. On the other hand, the LDA-based models adapt their topic models on parts of the Choi dataset itself. Despite the fact that they use different documents for training the topic models from those used for evaluating segmentation quality, the evaluation is still tainted because snippets from the original documents appear in multiple artificial documents -some of which belong to the the training set and others to the test set, as admitted by <cite>Riedl and Biemann (2012)</cite> and this is why their reported performance on this dataset is overestimated. In Table 3 we report the results on the Manifesto dataset. Results of both TopicTiling and GRAPHSEG indicate that the realistic Manifesto dataset is much more difficult to segment than the artificial Choi dataset. The GRAPHSEG algorithm significantly outperforms the TopicTiling method (p < 0.05, Student's t-test).",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_12",
  "x": "In-domain training of word representations, topics for TopicTiling and word embeddings for GraphSeg, does not significantly improve the performance for neither of the two models. This result contrasts previous findings (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> in which the performance boost was credited to the indomain trained topics and supports our hypothesis that the performance boost of the LDA-based methods' with in-domain trained topics originates from information leakage between different portions of the synthetic Choi dataset. ---------------------------------- **CONCLUSION** In this work we presented GRAPHSEG, a novel graph-based algorithm for unsupervised text segmentation. GRAPHSEG employs word embeddings and extends a measure of semantic relatedness to construct a relatedness graph with edges established between semantically related sentences. The segmentation is then determined by the maximal cliques of the relatedness graph and improved by semantic comparison of adjacent segments. GRAPHSEG displays competitive performance compared to best-performing LDA-based methods on a synthetic dataset. However, we identify and discuss evaluation issues pertaining to LDA-based TS on this dataset.",
  "y": "differences"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_0",
  "x": "**INTRODUCTION** Pidgin English is one of the the most widely spoken languages in West Africa with roughly 75 million speakers estimated in Nigeria; and over 5 million speakers estimated in Ghana<cite> (Ogueji & Ahia, 2019)</cite> . 1 While there have been recent efforts in popularizing the monolingual Pidgin English as seen in the BBC Pidgin 2 , it remains under-resourced in terms of the available parallel corpus for machine translation. Similarly, this low-resource scenario extends to other domains in natural language generation (NLG) such as summarization, data-to-text and so on (Lebret et al., 2016; Su et al., 2018; Shen et al., 2019a; b; Zhao et al., 2019; Hong et al., 2019; de Souza et al., 2018) \u2212 where Pidgin English generation is largely under-explored. The scarcity is further aggravated when the pipeline language generation system includes other sub-modules that computes semantic textual similarity (Shen et al., 2017; Zhuang & Chang, 2017) , which exists solely in English. Previous works on unsupervised neural machine translation for Pidgin English constructed a monolingual corpus<cite> (Ogueji & Ahia, 2019)</cite> , and achieved a BLEU score of 5.18 from English to Pidgin. However, there is an issue of domain mismatch between down-stream NLG tasks and the trained machine translation system. This creates a caveat where the resulting English-to-Pidgin MT systems (trained on the domain of news and the Bible) cannot be directly used to translate out-domain English texts to Pidgin. An example of the English/pidgin text in the restaurant domain (Novikova et al., 2017) is displayed in Table 1 .",
  "y": "background"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_1",
  "x": "Pidgin English is one of the the most widely spoken languages in West Africa with roughly 75 million speakers estimated in Nigeria; and over 5 million speakers estimated in Ghana<cite> (Ogueji & Ahia, 2019)</cite> . 1 While there have been recent efforts in popularizing the monolingual Pidgin English as seen in the BBC Pidgin 2 , it remains under-resourced in terms of the available parallel corpus for machine translation. Similarly, this low-resource scenario extends to other domains in natural language generation (NLG) such as summarization, data-to-text and so on (Lebret et al., 2016; Su et al., 2018; Shen et al., 2019a; b; Zhao et al., 2019; Hong et al., 2019; de Souza et al., 2018) \u2212 where Pidgin English generation is largely under-explored. The scarcity is further aggravated when the pipeline language generation system includes other sub-modules that computes semantic textual similarity (Shen et al., 2017; Zhuang & Chang, 2017) , which exists solely in English. Previous works on unsupervised neural machine translation for Pidgin English constructed a monolingual corpus<cite> (Ogueji & Ahia, 2019)</cite> , and achieved a BLEU score of 5.18 from English to Pidgin. However, there is an issue of domain mismatch between down-stream NLG tasks and the trained machine translation system. This creates a caveat where the resulting English-to-Pidgin MT systems (trained on the domain of news and the Bible) cannot be directly used to translate out-domain English texts to Pidgin. An example of the English/pidgin text in the restaurant domain (Novikova et al., 2017) is displayed in Table 1 . Nevertheless, we argue that this domain-mismatch problem can be alleviated by using English text in the target-domain as a pivot language (Guo et al., 2019) .",
  "y": "background"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_2",
  "x": "**INTRODUCTION** Pidgin English is one of the the most widely spoken languages in West Africa with roughly 75 million speakers estimated in Nigeria; and over 5 million speakers estimated in Ghana<cite> (Ogueji & Ahia, 2019)</cite> . 1 While there have been recent efforts in popularizing the monolingual Pidgin English as seen in the BBC Pidgin 2 , it remains under-resourced in terms of the available parallel corpus for machine translation. Similarly, this low-resource scenario extends to other domains in natural language generation (NLG) such as summarization, data-to-text and so on (Lebret et al., 2016; Su et al., 2018; Shen et al., 2019a; b; Zhao et al., 2019; Hong et al., 2019; de Souza et al., 2018) \u2212 where Pidgin English generation is largely under-explored. The scarcity is further aggravated when the pipeline language generation system includes other sub-modules that computes semantic textual similarity (Shen et al., 2017; Zhuang & Chang, 2017) , which exists solely in English. Previous works on unsupervised neural machine translation for Pidgin English constructed a monolingual corpus<cite> (Ogueji & Ahia, 2019)</cite> , and achieved a BLEU score of 5.18 from English to Pidgin. However, there is an issue of domain mismatch between down-stream NLG tasks and the trained machine translation system. This creates a caveat where the resulting English-to-Pidgin MT systems (trained on the domain of news and the Bible) cannot be directly used to translate out-domain English texts to Pidgin. An example of the English/pidgin text in the restaurant domain (Novikova et al., 2017) is displayed in Table 1 .",
  "y": "background motivation"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_3",
  "x": "We employ the publicly available parallel data-to-text corpus E2E (Novikova et al., 2017) consisting of tabulated data and English descriptions in the restaurant domain. The training of the in-domain MT system is done with a two-step process: (1) We use the target-side English texts as the pivot, and train an unsupervised NMT (model unsup ) directly between in-domain English text and the available monolingual Pidgin corpus. (2) Next, we employ self-training (He et al., 2019) to create augmented parallel pairs to continue updating the system (model self ). ---------------------------------- **APPROACH** First phase of the approach requires training of an unsupervised NMT system similar to <cite>Ogueji & Ahia (2019)</cite> (PidginUNMT). Similar to <cite>Ogueji & Ahia (2019)</cite> , we train the cross-lingual model using FastText Bojanowski et al. (2017) on the combined Pidgin-English corpus. Next, we train an unsupervised NMT similar to Lample et al. (2017) ; Artetxe et al. (2017) ; <cite>Ogueji & Ahia (2019)</cite> between them to obtain model unsup . Then we further utilize model unsup to construct pseudo parallel corpus by predicting target Pidgin text given the English input.",
  "y": "similarities"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_4",
  "x": "We employ the publicly available parallel data-to-text corpus E2E (Novikova et al., 2017) consisting of tabulated data and English descriptions in the restaurant domain. The training of the in-domain MT system is done with a two-step process: (1) We use the target-side English texts as the pivot, and train an unsupervised NMT (model unsup ) directly between in-domain English text and the available monolingual Pidgin corpus. (2) Next, we employ self-training (He et al., 2019) to create augmented parallel pairs to continue updating the system (model self ). ---------------------------------- **APPROACH** First phase of the approach requires training of an unsupervised NMT system similar to <cite>Ogueji & Ahia (2019)</cite> (PidginUNMT). Similar to <cite>Ogueji & Ahia (2019)</cite> , we train the cross-lingual model using FastText Bojanowski et al. (2017) on the combined Pidgin-English corpus. Next, we train an unsupervised NMT similar to Lample et al. (2017) ; Artetxe et al. (2017) ; <cite>Ogueji & Ahia (2019)</cite> between them to obtain model unsup . Then we further utilize model unsup to construct pseudo parallel corpus by predicting target Pidgin text given the English input.",
  "y": "similarities"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_5",
  "x": "Similar to <cite>Ogueji & Ahia (2019)</cite> , we train the cross-lingual model using FastText Bojanowski et al. (2017) on the combined Pidgin-English corpus. Next, we train an unsupervised NMT similar to Lample et al. (2017) ; Artetxe et al. (2017) ; <cite>Ogueji & Ahia (2019)</cite> between them to obtain model unsup . Then we further utilize model unsup to construct pseudo parallel corpus by predicting target Pidgin text given the English input. We augment this dataset to the existing monolingual corpus. The self-training step involves further updating model unsup on the pseudo parallel corpus and non-parallel monolingual corpus to yield model self . ---------------------------------- **EXPERIMENTS AND RESULTS** We conduct experiments on the E2E corpus (Novikova et al., 2017) which amounts to roughly 42k samples in the training set. The monolingual Pidgin corpus contains 56,695 sentences and 32,925 unique words.",
  "y": "similarities"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_0",
  "x": "Researchers have explored both supervised and unsupervised techniques to address the problem of automatic keyphrase extraction. Supervised methods typically recast this problem as a binary classification task, where a model is trained on annotated data to determine whether a given phrase is a keyphrase or not (e.g., Frank et al. (1999) , Turney (2000; , Hulth (2003) , Medelyan et al. (2009)) . A disadvantage of supervised approaches is that they require a lot of training data and yet show bias towards the domain on which they are trained, undermining their ability to generalize well to new domains. Unsupervised approaches could be a viable alternative in this regard. The unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling (e.g., Tomokiyo and Hurst (2003) ), graph-based ranking (e.g., Zha (2002) ,<cite> Mihalcea and Tarau (2004)</cite> , Wan et al. (2007) , Wan and Xiao (2008) , Liu et al. (2009a) ), and clustering (e.g., Matsuo and Ishizuka (2004) , Liu et al. (2009b) ). While these methods have been shown to work well on a particular domain of text such as short paper abstracts and news articles, their effectiveness and portability across different domains have remained an unexplored issue. Worse still, each of them is based on a set of assumptions, which may only hold for the dataset on which they are evaluated. ---------------------------------- **CONSEQUENTLY, WE HAVE LITTLE UNDERSTANDING OF HOW EFFECTIVE THE STATE-OF THE-ART SYSTEMS WOULD BE ON A COMPLETELY NEW DATASET FROM A DIFFERENT DOMAIN.**",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_1",
  "x": "While keyphrases are excellent means for providing a concise summary of a document, recent research results have suggested that the task of automatically identifying keyphrases from a document is by no means trivial. Researchers have explored both supervised and unsupervised techniques to address the problem of automatic keyphrase extraction. Supervised methods typically recast this problem as a binary classification task, where a model is trained on annotated data to determine whether a given phrase is a keyphrase or not (e.g., Frank et al. (1999) , Turney (2000; , Hulth (2003) , Medelyan et al. (2009)) . A disadvantage of supervised approaches is that they require a lot of training data and yet show bias towards the domain on which they are trained, undermining their ability to generalize well to new domains. Unsupervised approaches could be a viable alternative in this regard. The unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling (e.g., Tomokiyo and Hurst (2003) ), graph-based ranking (e.g., Zha (2002) ,<cite> Mihalcea and Tarau (2004)</cite> , Wan et al. (2007) , Wan and Xiao (2008) , Liu et al. (2009a) ), and clustering (e.g., Matsuo and Ishizuka (2004) , Liu et al. (2009b) ). While these methods have been shown to work well on a particular domain of text such as short paper abstracts and news articles, their effectiveness and portability across different domains have remained an unexplored issue. Worse still, each of them is based on a set of assumptions, which may only hold for the dataset on which they are evaluated. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_2",
  "x": "Worse still, each of them is based on a set of assumptions, which may only hold for the dataset on which they are evaluated. ---------------------------------- **CONSEQUENTLY, WE HAVE LITTLE UNDERSTANDING OF HOW EFFECTIVE THE STATE-OF THE-ART SYSTEMS WOULD BE ON A COMPLETELY NEW DATASET FROM A DIFFERENT DOMAIN.** A few questions arise naturally. How would these systems perform on a different dataset with their original configuration? What could be the underlying reasons in case they perform poorly? Is there any system that can generalize fairly well across various domains? We seek to gain a better understanding of the state of the art in unsupervised keyphrase extraction by examining the aforementioned questions. More specifically, we compare five unsupervised keyphrase extraction algorithms on four corpora with varying domains and statistical characteristics. These algorithms represent the ma-jor directions in this research area, including TfIdf and four recently proposed systems, namely, TextRank <cite>(Mihalcea and Tarau, 2004)</cite> , SingleRank (Wan and Xiao, 2008) , ExpandRank (Wan and Xiao, 2008) , and a clustering-based approach (Liu et al., 2009b) .",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_3",
  "x": "More specifically, we compare five unsupervised keyphrase extraction algorithms on four corpora with varying domains and statistical characteristics. These algorithms represent the ma-jor directions in this research area, including TfIdf and four recently proposed systems, namely, TextRank <cite>(Mihalcea and Tarau, 2004)</cite> , SingleRank (Wan and Xiao, 2008) , ExpandRank (Wan and Xiao, 2008) , and a clustering-based approach (Liu et al., 2009b) . Since none of these systems (except TextRank) are publicly available, we reimplement all of them and make them freely available for research purposes. 1 To our knowledge, this is the first attempt to compare the performance of state-of-the-art unsupervised keyphrase extraction systems on multiple datasets. ---------------------------------- **CORPORA** Our four evaluation corpora belong to different domains with varying document properties. Table 1 provides an overview of each corpus. The DUC-2001 dataset (Over, 2001) , which is a collection of 308 news articles, is annotated by Wan and Xiao (2008) .",
  "y": "uses"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_4",
  "x": "Is there any system that can generalize fairly well across various domains? We seek to gain a better understanding of the state of the art in unsupervised keyphrase extraction by examining the aforementioned questions. More specifically, we compare five unsupervised keyphrase extraction algorithms on four corpora with varying domains and statistical characteristics. These algorithms represent the ma-jor directions in this research area, including TfIdf and four recently proposed systems, namely, TextRank <cite>(Mihalcea and Tarau, 2004)</cite> , SingleRank (Wan and Xiao, 2008) , ExpandRank (Wan and Xiao, 2008) , and a clustering-based approach (Liu et al., 2009b) . Since none of these systems (except TextRank) are publicly available, we reimplement all of them and make them freely available for research purposes. 1 To our knowledge, this is the first attempt to compare the performance of state-of-the-art unsupervised keyphrase extraction systems on multiple datasets. ---------------------------------- **CORPORA** Our four evaluation corpora belong to different domains with varying document properties.",
  "y": "uses"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_5",
  "x": "Table 1 provides an overview of each corpus. The DUC-2001 dataset (Over, 2001) , which is a collection of 308 news articles, is annotated by Wan and Xiao (2008) . We report results on all 308 articles in our evaluation. The Inspec dataset is a collection of 2,000 abstracts from journal papers including the paper title. Each document has two sets of keyphrases assigned by the indexers: the controlled keyphrases, which are keyphrases that appear in the Inspec thesaurus; and the uncontrolled keyphrases, which do not necessarily appear in the thesaurus. This is a relatively popular dataset for automatic keyphrase extraction, as it was first used by Hulth (2003) and later by<cite> Mihalcea and Tarau (2004)</cite> and Liu et al. (2009b) . In our evaluation, we use the set of 500 abstracts designated by these previous approaches as the test set and its set of uncontrolled keyphrases. Note that the average document length for this dataset is the smallest among all our datasets. The NUS Keyphrase Corpus (Nguyen and Kan, 2007) includes 211 scientific conference papers with lengths between 4 to 12 pages.",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_6",
  "x": "**UNSUPERVISED KEYPHRASE EXTRACTORS** A generic unsupervised keyphrase extraction system typically operates in three steps (Section 3.1), which will help understand the unsupervised systems explained in Section 3.2. ---------------------------------- **GENERIC KEYPHRASE EXTRACTOR** Step 1: Candidate lexical unit selection The first step is to filter out unnecessary word tokens from the input document and generate a list of potential keywords using heuristics. Commonly used heuristics include (1) using a stop word list to remove non-keywords (e.g., Liu et al. (2009b) ) and (2) allowing words with certain partof-speech tags (e.g., nouns, adjectives, verbs) to be considered candidate keywords<cite> (Mihalcea and Tarau (2004)</cite> , Liu et al. (2009a) , Wan and Xiao (2008) ). In all of our experiments, we follow Wan and Xiao (2008) and select as candidates words with the following Penn Treebank tags: NN, NNS, NNP, NNPS, and JJ, which are obtained using the Stanford POS tagger (Toutanova and Manning, 2000) . Table 1 : Corpus statistics for the four datasets used in this paper. A candidate word/phrase, typically a sequence of one or more adjectives and nouns, is extracted from the document initially and considered a potential keyphrase.",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_7",
  "x": "Step 3: Keyphrase formation In the final step, the ranked list of candidate words is used to form keyphrases. A candidate phrase, typically a sequence of nouns and adjectives, is selected as a keyphrase if (1) it includes one or more of the top-ranked candidate words<cite> (Mihalcea and Tarau (2004)</cite> , Liu et al. (2009b) ), or (2) the sum of the ranking scores of its constituent words makes it a top scoring phrase (Wan and Xiao, 2008) . ---------------------------------- **THE FIVE KEYPHRASE EXTRACTORS** As mentioned above, we re-implement five unsupervised approaches for keyphrase extraction. Below we provide a brief overview of each system. ---------------------------------- **TF-IDF** Tf-Idf assigns a score to each term t in a document d based on t's frequency in d (term frequency) and how many other documents include t (inverse document frequency) and is defined as:",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_8",
  "x": "---------------------------------- **TEXTRANK** In the TextRank algorithm <cite>(Mihalcea and Tarau, 2004)</cite> , a text is represented by a graph. Each vertex corresponds to a word type. A weight, w ij , is assigned to the edge connecting the two vertices, v i and v j , and its value is the number of times the corresponding word types co-occur within a window of W words in the associated text. The goal is to (1) compute the score of each vertex, which reflects its importance, and then (2) use the word types that correspond to the highestscored vertices to form keyphrases for the text. The score for v i , S(v i ), is initialized with a default value and is computed in an iterative manner until convergence using this recursive formula: where Adj(v i ) denotes v i 's neighbors and d is the damping factor set to 0.85 (Brin and Page, 1998) . Intuitively, a vertex will receive a high score if it has many high-scored neighbors.",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_9",
  "x": "where Adj(v i ) denotes v i 's neighbors and d is the damping factor set to 0.85 (Brin and Page, 1998) . Intuitively, a vertex will receive a high score if it has many high-scored neighbors. As noted before, after convergence, the T % top-scored vertices are selected as keywords. Adjacent keywords are then collapsed and output as a keyphrase. According to<cite> Mihalcea and Tarau (2004)</cite> , TextRank's best score on the Inspec dataset is achieved when only nouns and adjectives are used to create a uniformly weighted graph for the text under consideration, where an edge connects two word types only if they co-occur within a window of two words. Hence, our implementation of TextRank follows this configuration. ---------------------------------- **SINGLERANK** SingleRank (Wan and Xiao, 2008 ) is essentially a TextRank approach with three major differences.",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_10",
  "x": "TextRank and SingleRank setup Following<cite> Mihalcea and Tarau (2004)</cite> and Wan and Xiao (2008) , we set the co-occurrence window size for TextRank and SingleRank to 2 and 10, respectively, as these parameter values have yielded the best results for their evaluation datasets. ExpandRank setup Following Wan and Xiao (2008), we find the 5 nearest neighbors for each document from the remaining documents in the same corpus. The other parameters are set in the same way as in SingleRank. KeyCluster setup As argued by Liu et al. (2009b) , Wikipedia-based relatedness is computationally expensive to compute. As a result, we follow them by computing the co-occurrence-based relatedness instead, using a window of size 10. Then, we cluster the candidate words using spectral clustering, and use the frequent word list that they generously provided us to post-process the resulting keyphrases by filtering out those that are frequent unigrams. ---------------------------------- **RESULTS AND DISCUSSION** In an attempt to gain a better insight into the five unsupervised systems, we report their performance in terms of precision-recall curves for each of the four datasets (see Figure 1 ).",
  "y": "uses"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_11",
  "x": "We generate the curves for each system as follows. For Tf-Idf, SingleRank, and ExpandRank, we vary the number of keyphrases, N , predicted by each system. On average, TextRank performs much worse compared to Tf-Idf. This certainly gives more insight into TextRank since it was evaluated on Inspec only for T=33% by<cite> Mihalcea and Tarau (2004)</cite> .",
  "y": "differences"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_12",
  "x": "Our re-implementations Do our duplicated systems yield scores that match the original scores? Table 3 sheds light on this question. First, consider KeyCluster, where our score lags behind the original score by approximately 5%. An examination of Liu et al.'s (2009b) results reveals a subtle caveat in keyphrase extraction evaluations. In Inspec, not all gold-standard keyphrases appear in their associated document, and as a result, none of the five systems we consider in this paper can achieve a recall of 100. While<cite> Mihalcea and Tarau (2004)</cite> and our reimplementations use all of these gold-standard keyphrases in our evaluation, Hulth (2003) and Liu et al. address Table 3 : Original vs. re-implementation scores of TextRank 3 , and are confident that our implementation is correct. It is also worth mentioning that using our re-implementation of SingleRank, we are able to match the best scores reported by<cite> Mihalcea and Tarau (2004)</cite> on Inspec. We score 2 and 5 points less than Wan and Xiao's (2008) implementations of SingleRank and ExpandRank, respectively. We speculate that document pre-processing (e.g., stemming) has contributed to the discrepancy, but additional experiments are needed to determine the reason.",
  "y": "similarities"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_13",
  "x": "Our re-implementations Do our duplicated systems yield scores that match the original scores? Table 3 sheds light on this question. First, consider KeyCluster, where our score lags behind the original score by approximately 5%. An examination of Liu et al.'s (2009b) results reveals a subtle caveat in keyphrase extraction evaluations. In Inspec, not all gold-standard keyphrases appear in their associated document, and as a result, none of the five systems we consider in this paper can achieve a recall of 100. While<cite> Mihalcea and Tarau (2004)</cite> and our reimplementations use all of these gold-standard keyphrases in our evaluation, Hulth (2003) and Liu et al. address Table 3 : Original vs. re-implementation scores of TextRank 3 , and are confident that our implementation is correct. It is also worth mentioning that using our re-implementation of SingleRank, we are able to match the best scores reported by<cite> Mihalcea and Tarau (2004)</cite> on Inspec. We score 2 and 5 points less than Wan and Xiao's (2008) implementations of SingleRank and ExpandRank, respectively. We speculate that document pre-processing (e.g., stemming) has contributed to the discrepancy, but additional experiments are needed to determine the reason.",
  "y": "similarities"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_0",
  "x": "Since the benefits of tensor and matrix factorization are complementary, we then investigate two hybrid methods that combine the benefits of the two paradigms. We show that the combination can be fruitful: we handle ambiguously phrased relations, achieve gains in accuracy on real-world relations, and demonstrate that entity embeddings encode entity types. ---------------------------------- **INTRODUCTION** Distantly-supervised relation extraction has gained prominence as it utilizes automatically aligned data to train accurate extractors. Universal schema, in particular, has found impressive accuracy gains by (1) treating the distant-supervision as a knowledge-base (KB) containing both structured relations such as bornIn * First two authors contributed equally to the paper. and surface form relations such as \"was born in\" extracted from text, and (2) by completing the entries in such a KB using joint and compact encoding of the dependencies between the relations <cite>(Riedel et al., 2013</cite>; Fan et al., 2014; . Matrix factorization is at the core of this completion:<cite> Riedel et al. (2013)</cite> convert the KB into a binary matrix with entity-pairs forming the rows and relations forming the columns. Factorization of this matrix results in low-dimensional factors for entity-pairs and relations, which are able to effectively combine multiple evidence for each entity pair to predict unseen relations. An important shortcoming of this matrix factorization model for universal schema is that no information is shared between the rows that contain the same entity.",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_1",
  "x": "Matrix factorization is at the core of this completion:<cite> Riedel et al. (2013)</cite> convert the KB into a binary matrix with entity-pairs forming the rows and relations forming the columns. Factorization of this matrix results in low-dimensional factors for entity-pairs and relations, which are able to effectively combine multiple evidence for each entity pair to predict unseen relations. An important shortcoming of this matrix factorization model for universal schema is that no information is shared between the rows that contain the same entity. This can significantly impact accuracy on pairs of entities that are not mentioned together frequently, and for relations that depend crucially on fine-grained entity types, such as schoolAttended, nationality, and bookAuthor. On the other hand, tensor factorization for knowledge-base completion maintains perentity factors that combine evidence from all the relations an entity participates in, to predict its relations to other entities -a task known as link prediction (Nickel et al., 2012; Bordes et al., 2013) . These entity factors, as opposed to pairwise factors in matrix factorization, can be quite effective in identifying the latent, fine-grained entity types. Thus, in the light of the above problems of matrix factorization, the use of tensor factorization for universal schema is tempting. However, directly applying tensor factorization to universal schema has not been successful. Strong results were obtained only through a combination with matrix factorization predictions, and the use of predefined type information .",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_2",
  "x": "We show that the combination can be fruitful: we handle ambiguously phrased relations, achieve gains in accuracy on real-world relations, and demonstrate that entity embeddings encode entity types. ---------------------------------- **INTRODUCTION** Distantly-supervised relation extraction has gained prominence as it utilizes automatically aligned data to train accurate extractors. Universal schema, in particular, has found impressive accuracy gains by (1) treating the distant-supervision as a knowledge-base (KB) containing both structured relations such as bornIn * First two authors contributed equally to the paper. and surface form relations such as \"was born in\" extracted from text, and (2) by completing the entries in such a KB using joint and compact encoding of the dependencies between the relations <cite>(Riedel et al., 2013</cite>; Fan et al., 2014; . Matrix factorization is at the core of this completion:<cite> Riedel et al. (2013)</cite> convert the KB into a binary matrix with entity-pairs forming the rows and relations forming the columns. Factorization of this matrix results in low-dimensional factors for entity-pairs and relations, which are able to effectively combine multiple evidence for each entity pair to predict unseen relations. An important shortcoming of this matrix factorization model for universal schema is that no information is shared between the rows that contain the same entity. This can significantly impact accuracy on pairs of entities that are not mentioned together frequently, and for relations that depend crucially on fine-grained entity types, such as schoolAttended, nationality, and bookAuthor.",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_3",
  "x": "We also present improved accuracy on real-world relation extraction data, and demonstrate that the entity embeddings are effective at encoding entity types. ---------------------------------- **MATRIX AND TENSOR FACTORIZATION** In this section we introduce universal schemas and various factorization models that can be used to complete knowledge bases of such schemas. ---------------------------------- **UNIVERSAL SCHEMA** A universal schema is defined as the union of all OpenIE-like surface form patterns found in text and fixed canonical relations that exist in a knowledge base<cite> (Riedel et al., 2013)</cite> . The task here is to complete this schema by jointly reasoning over surface form patterns and relations. A successful approach to this joint reasoning is to embed both kinds of relations into the same low-dimensional embedding space, which can be achieved by matrix or tensor factorization methods.",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_4",
  "x": "**MATRIX FACTORIZATION WITH FACTORS OVER ENTITY-PAIRS** In matrix factorization for universal schema,<cite> Riedel et al. (2013)</cite> construct a sparse binary matrix of size |P| \u00d7 |R| whose rows are indexed by entity-pairs (a, b) \u2208 P and columns by surface form and Freebase relations s \u2208 R. Subsequently, generalized PCA (Collins et al., 2001 ) is used to find a rank-k factorization, i.e., with relation factors r \u2208 R |R|\u00d7k and entity-pair factors p \u2208 R |P|\u00d7k , the probability of a relation s and two entities a and b is: where \u03c3 is the sigmoid function. Using this factorization, similar entity-pairs and relations are embedded close to each other in a k-dimensional vector space. Since this model uses embeddings for pairs of entities, as opposed to per-entity embeddings, we refer to such models as pairwise models. Pairwise embeddings are especially suitable when working with universal schema data, since they can represent correlations between surface pattern relations and structured relations compactly. Furthermore, they combine multiple evidences specific to an entity-pair to predict a relation between them. Since the observed data matrix contains only true entries, the parameters are learned using Bayesian personalized Ranking (Rendle et al., 2009 ) that supports implicit feedback. Riedel et al. (2013) explore a number of variants of this factorization, including a neighborhood model that learns local classifiers, and an entity model that includes entity representations (we revisit this formulation in Section 2.3.4).",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_5",
  "x": "---------------------------------- **TRANSE** Another formulation that is based on entity representations is the translating embeddings model by Bordes et al. (2013) . The idea is that if a relation s between two entities a and b holds, that relation's vector representation r s should translate the representation e a to the second argument e b , i.e., In this work we use a variant of TransE in which different embeddings are learned for an entity for each argument position. ---------------------------------- **MODEL E** Furthermore, we isolate the entity factorization in<cite> Riedel et al. (2013)</cite> by viewing it as tensor factorization. In this model, each relation is assigned an embedding for each of its two arguments, i.e.,",
  "y": "extends"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_6",
  "x": "**MODEL E** Furthermore, we isolate the entity factorization in<cite> Riedel et al. (2013)</cite> by viewing it as tensor factorization. In this model, each relation is assigned an embedding for each of its two arguments, i.e., Although not explored in isolation by<cite> Riedel et al. (2013)</cite> , model E can be used on its own to predict relations between entities, even if they have not been observed to be in a relation. ---------------------------------- **COMBINED TENSOR AND MATRIX FACTORIZATION FOR UNIVERSAL SCHEMA** In the previous section, we provided background on matrix factorization with pairwise factors, followed by a tensor factorization based formulation of universal schema. Although matrix factorization performs well for universal schema<cite> (Riedel et al., 2013)</cite> , it is not robust to sparse data and does not capture latent entity types that can be crucial for accurate relation extraction. On the other hand, although tensor factorization models are able to compactly represent entity types using unary embeddings, they are unable to adequately represent the pair-specific information that is necessary for modeling relations.",
  "y": "extends"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_7",
  "x": "In this model, each relation is assigned an embedding for each of its two arguments, i.e., Although not explored in isolation by<cite> Riedel et al. (2013)</cite> , model E can be used on its own to predict relations between entities, even if they have not been observed to be in a relation. ---------------------------------- **COMBINED TENSOR AND MATRIX FACTORIZATION FOR UNIVERSAL SCHEMA** In the previous section, we provided background on matrix factorization with pairwise factors, followed by a tensor factorization based formulation of universal schema. Although matrix factorization performs well for universal schema<cite> (Riedel et al., 2013)</cite> , it is not robust to sparse data and does not capture latent entity types that can be crucial for accurate relation extraction. On the other hand, although tensor factorization models are able to compactly represent entity types using unary embeddings, they are unable to adequately represent the pair-specific information that is necessary for modeling relations. It is worth noting that tensor factorization for universal schema has been proposed by , who also observed that tensor factorization by itself performs poorly (even with additional type constraints), and the predictions need to be combined with matrix factorization to be accurate. In this section we will present the fundamental differences between matrix and tensor factorization, and examine a few hybrid models that can address these concerns.",
  "y": "motivation"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_8",
  "x": "**HYBRID FACTORIZATION MODELS** Since matrix and tensor factorization techniques are quite limited in their representations even on the simple, synthetic data, we now turn to hybrid matrix and Figure 3 : Overview of the Models: Some of the models explored in this work, showing pairwise (F) and unary (E) models, along with their combinations (FE and RFE), for computing P (s(a, b) ). tensor factorization models that represent entity types for universal schema. We describe two possible combinations, models FE and RFE, summarized in Figure 3 . Note that these approaches are distinct from collective factorization (Singh and Gordon, 2008) that can be used when extra entity information is available as unary relations. ---------------------------------- **COMBINED MODEL (FE)** As the direct combination of a pairwise model (Eq. 1) with an entity model (Eq. 5), we consider the FE model from<cite> Riedel et al. (2013)</cite> , i.e., the additive combination of the two: P (s(a, b)) = \u03c3(r s \u00b7 e ab + r s,1 \u00b7 e a + r s,2 \u00b7 e b ) (6) Both the matrix factorization model F and entity model E can de defined as special cases of this model, by setting r s,1/2 or r s to zero, respectively. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_9",
  "x": "---------------------------------- **COMBINED MODEL (FE)** As the direct combination of a pairwise model (Eq. 1) with an entity model (Eq. 5), we consider the FE model from<cite> Riedel et al. (2013)</cite> , i.e., the additive combination of the two: P (s(a, b)) = \u03c3(r s \u00b7 e ab + r s,1 \u00b7 e a + r s,2 \u00b7 e b ) (6) Both the matrix factorization model F and entity model E can de defined as special cases of this model, by setting r s,1/2 or r s to zero, respectively. ---------------------------------- **RECTIFIER MODEL (RFE)** A problem with combining the two models additively, as in FE, is that one model can easily override the other. For instance, even if the type constraints of a relation are violated, a high score by the pairwise model score might still yield a high prediction for that triplet. To alleviate this shortcoming, we experimented with rectifier units (Nair and Hinton, 2010) so that a score of model F or model E first needs to reach a certain threshold to influence the overall prediction for a triplet. Specifically, we use the smooth approximation of a rectifier \u2295(x) = log(1 + e x ) and define the probability for a triplet as follows:",
  "y": "motivation"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_10",
  "x": "**COMBINED MODEL (FE)** As the direct combination of a pairwise model (Eq. 1) with an entity model (Eq. 5), we consider the FE model from<cite> Riedel et al. (2013)</cite> , i.e., the additive combination of the two: P (s(a, b)) = \u03c3(r s \u00b7 e ab + r s,1 \u00b7 e a + r s,2 \u00b7 e b ) (6) Both the matrix factorization model F and entity model E can de defined as special cases of this model, by setting r s,1/2 or r s to zero, respectively. ---------------------------------- **RECTIFIER MODEL (RFE)** A problem with combining the two models additively, as in FE, is that one model can easily override the other. For instance, even if the type constraints of a relation are violated, a high score by the pairwise model score might still yield a high prediction for that triplet. To alleviate this shortcoming, we experimented with rectifier units (Nair and Hinton, 2010) so that a score of model F or model E first needs to reach a certain threshold to influence the overall prediction for a triplet. Specifically, we use the smooth approximation of a rectifier \u2295(x) = log(1 + e x ) and define the probability for a triplet as follows: ----------------------------------",
  "y": "extends"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_11",
  "x": "---------------------------------- **RECTIFIER MODEL (RFE)** A problem with combining the two models additively, as in FE, is that one model can easily override the other. For instance, even if the type constraints of a relation are violated, a high score by the pairwise model score might still yield a high prediction for that triplet. To alleviate this shortcoming, we experimented with rectifier units (Nair and Hinton, 2010) so that a score of model F or model E first needs to reach a certain threshold to influence the overall prediction for a triplet. Specifically, we use the smooth approximation of a rectifier \u2295(x) = log(1 + e x ) and define the probability for a triplet as follows: ---------------------------------- **PARAMETER ESTIMATION** As by<cite> Riedel et al. (2013)</cite> , we use a Bayesian personalized ranking objective (Rendle et al., 2009 ) to estimate parameters, i.e., for each observed training fact, we sample an unobserved fact for the same relation, and maximize their relative ranking using AdaGrad.",
  "y": "uses"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_12",
  "x": "---------------------------------- **UNIVERSAL SCHEMA RELATION EXTRACTION** With the promising results shown on synthetic data, we now turn to evaluation on real-world information extraction. In particular, we evaluate the models on universal schema for distantly-supervised relation extraction. Following the experiment setup of<cite> Riedel et al. (2013)</cite> , we instantiate the universal schema matrix over entity pairs and text/Freebase relations for New York Times data, and compare the performance using average precision of the presented models. Table 1 summarizes the performance of our models, as compared to existing approaches (see<cite> Riedel et al. (2013)</cite> for an overview). In particular, TR-R13 takes the output predictions of matrix factorization, and combines it with an entity-type aware RESCAL model . 3 Tensor factorization approaches perform poorly on this data. We present results for Model E, but other formulations such as PARAFAC, TransE, RESCAL, and Tucker2 achieved even lower accuracy; this is consistent with the results in .",
  "y": "uses"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_13",
  "x": "In particular, we evaluate the models on universal schema for distantly-supervised relation extraction. Following the experiment setup of<cite> Riedel et al. (2013)</cite> , we instantiate the universal schema matrix over entity pairs and text/Freebase relations for New York Times data, and compare the performance using average precision of the presented models. Table 1 summarizes the performance of our models, as compared to existing approaches (see<cite> Riedel et al. (2013)</cite> for an overview). In particular, TR-R13 takes the output predictions of matrix factorization, and combines it with an entity-type aware RESCAL model . 3 Tensor factorization approaches perform poorly on this data. We present results for Model E, but other formulations such as PARAFAC, TransE, RESCAL, and Tucker2 achieved even lower accuracy; this is consistent with the results in . Models that use the matrix factorization (F, FE, R13-F and RFE) are significantly better, but more importantly, the hybrid appraoch FE achieves the highest accuracy. It is unclear why RFE fails to provide similar gains, in particular, performing slightly worse than matrix factorization. Note that we are not introducing a new state-of-art here, the neighborhood model (NF) that achieves a higher accuracy is omitted for clarity.",
  "y": "uses"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_14",
  "x": [
   "In particular, TR-R13 takes the output predictions of matrix factorization, and combines it with an entity-type aware RESCAL model . 3 Tensor factorization approaches perform poorly on this data. We present results for Model E, but other formulations such as PARAFAC, TransE, RESCAL, and Tucker2 achieved even lower accuracy; this is consistent with the results in . Models that use the matrix factorization (F, FE, R13-F and RFE) are significantly better, but more importantly, the hybrid appraoch FE achieves the highest accuracy. It is unclear why RFE fails to provide similar gains, in particular, performing slightly worse than matrix factorization. Note that we are not introducing a new state-of-art here, the neighborhood model (NF) that achieves a higher accuracy is omitted for clarity. Table 2 : Nearest-Neighbors for a few randomlyselected entities based on their embeddings, demonstrating that similar entities are close to each other. ---------------------------------- **ENTITY EMBEDDINGS AND TYPES**"
  ],
  "y": "differences"
 },
 {
  "id": "7ddd5b18d774575ae7acb97ae9eb33_0",
  "x": "It is a popular tool in semantic parsing, and treebank creation efforts have been made for Turkish (\u00c7 ak\u0131c\u0131, 2005) , German (Hockenmaier, 2006) , English <cite>(Hockenmaier and Steedman, 2007)</cite> , Italian (Bos et al., 2009) , Chinese (Tse and Curran, 2010) , Arabic (Boxwell and Brew, 2010) , Japanese (Uematsu et al., 2013) , and Hindi (Ambati et al., 2018) . However, all of these treebanks were not directly annotated according to the CCG formalism, but automatically converted from phrase structure or dependency treebanks, which is an error-prone process. Direct annotation in CCG has so far mostly been limited to small datasets for seeding or testing semantic parsers (e.g., Artzi et al., 2015) , and no graphical annotation interface is available to support such efforts, making the annotation process difficult to scale. The only exceptions we are aware of are the Groningen Meaning Bank and the Parallel Meaning Bank (Abzianidze et al., 2017) , two annotation efforts which use a graphical user interface for annotating sentences with CCG derivations and other annotation layers, and which have produced CCG treebanks for English, German, Italian, and Dutch. However, these efforts are focused on semantics and have not released explicit guidelines for syntactic annotation. Their annotation tool is limited in that annotators only have control over lexical categories, not larger constituents. Even though CCG is a lexicalized formalism, where most decisions can be made on the lexical level, there is no full control over attachment phenomena in the lexicon. Moreover, these annotation tools are not open-source and cannot easily be deployed to support other annotation efforts. In this paper, we present an open-source, lightweight, easy-to-use graphical annotation tool that employs a statistical parser to create initial CCG derivations for sentences, and allows annotators to correct these annotations via lexical category constraints and span constraints.",
  "y": "background"
 },
 {
  "id": "7ddd5b18d774575ae7acb97ae9eb33_1",
  "x": "It is a popular tool in semantic parsing, and treebank creation efforts have been made for Turkish (\u00c7 ak\u0131c\u0131, 2005) , German (Hockenmaier, 2006) , English <cite>(Hockenmaier and Steedman, 2007)</cite> , Italian (Bos et al., 2009) , Chinese (Tse and Curran, 2010) , Arabic (Boxwell and Brew, 2010) , Japanese (Uematsu et al., 2013) , and Hindi (Ambati et al., 2018) . However, all of these treebanks were not directly annotated according to the CCG formalism, but automatically converted from phrase structure or dependency treebanks, which is an error-prone process. Direct annotation in CCG has so far mostly been limited to small datasets for seeding or testing semantic parsers (e.g., Artzi et al., 2015) , and no graphical annotation interface is available to support such efforts, making the annotation process difficult to scale. The only exceptions we are aware of are the Groningen Meaning Bank and the Parallel Meaning Bank (Abzianidze et al., 2017) , two annotation efforts which use a graphical user interface for annotating sentences with CCG derivations and other annotation layers, and which have produced CCG treebanks for English, German, Italian, and Dutch. However, these efforts are focused on semantics and have not released explicit guidelines for syntactic annotation. Their annotation tool is limited in that annotators only have control over lexical categories, not larger constituents. Even though CCG is a lexicalized formalism, where most decisions can be made on the lexical level, there is no full control over attachment phenomena in the lexicon. Moreover, these annotation tools are not open-source and cannot easily be deployed to support other annotation efforts. In this paper, we present an open-source, lightweight, easy-to-use graphical annotation tool that employs a statistical parser to create initial CCG derivations for sentences, and allows annotators to correct these annotations via lexical category constraints and span constraints.",
  "y": "motivation"
 },
 {
  "id": "7ddd5b18d774575ae7acb97ae9eb33_2",
  "x": "Adjudication Support Once two or more annotators have annotated a sentence, disagreements need to be discovered, and a final, authoritative version has to be created. Our tool supports this adjudication process through the special user account judge. This user can see the derivations of other annotators in a tabbed interface as shown in Figure 3 . In order to enable the judge to easily spot disagreements, categories that annotators disagree on are struck through, and constituents that annotators disagree on are dashed. ---------------------------------- **A QUADRILINGUAL PILOT CCG TREEBANK** To test the viability of creating multilingual CCG treebanks by direct annotation, we conducted an annotation experiment on 110 short sentences from the Tatoeba corpus (Tatoeba, 2019) , each in four translations (English, German, Italian, and Dutch). The main annotation guideline was to copy the annotation style of CCGrebank (Honnibal et al., 2010), a CCG treebank adapted from CCGbank <cite>(Hockenmaier and Steedman, 2007)</cite> , which is in turn based on the Penn Treebank (Marcus et al., 1993) . Since CCGrebank only covers English and lacks some constructions observed in our corpus, an annotation manual with more specific instructions was needed.",
  "y": "uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_0",
  "x": "Using semantic information to improve parsing performance has been an interesting research avenue since the early days of NLP, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical PP attachment experiments (Ratnaparkhi, 1994) . Although there have been some significant results (see Section 2), this issue continues to be elusive. In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships. We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) . We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> . <cite>Agirre et al. (2008)</cite> trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004 ) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003) , where related words, like scissors and knife cannot be generalized.",
  "y": "differences extends"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_1",
  "x": "Although there have been some significant results (see Section 2), this issue continues to be elusive. In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships. We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) . We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> . <cite>Agirre et al. (2008)</cite> trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004 ) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003) , where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser.",
  "y": "similarities"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_2",
  "x": "Given the basic nature of the semantic classes and word sense disambiguation algorithms used, we think there is ample room for future improvements. Using semantic information to improve parsing performance has been an interesting research avenue since the early days of NLP, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical PP attachment experiments (Ratnaparkhi, 1994) . Although there have been some significant results (see Section 2), this issue continues to be elusive. In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships. We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) . We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> . <cite>Agirre et al. (2008)</cite> trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004 ) on semantically-enriched input, where content words had been substituted with their semantic classes.",
  "y": "background"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_3",
  "x": "699 ---------------------------------- **INTRODUCTION** Using semantic information to improve parsing performance has been an interesting research avenue since the early days of NLP, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical PP attachment experiments (Ratnaparkhi, 1994) . Although there have been some significant results (see Section 2), this issue continues to be elusive. In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships. We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) .",
  "y": "differences extends"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_4",
  "x": "We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> . <cite>Agirre et al. (2008)</cite> trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003) , where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser. They tested the parsers in both a full parsing and a PP attachment context. The experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance. This work presented the first results over both WordNet and the Penn Treebank to show that semantic processing helps parsing. ---------------------------------- **RELATED WORK**",
  "y": "similarities uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_5",
  "x": "As our baseline parser, we use MaltParser (Nivre, 2006) . We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> . <cite>Agirre et al. (2008)</cite> trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003) , where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser. They tested the parsers in both a full parsing and a PP attachment context. The experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance. This work presented the first results over both WordNet and the Penn Treebank to show that semantic processing helps parsing. ----------------------------------",
  "y": "background"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_6",
  "x": "We used two different datasets: the full PTB and the Semcor/PTB intersection<cite> (Agirre et al. 2008</cite> ). The full PTB allows for comparison with the stateof-the-art, and we followed the usual train-test split. The Semcor/PTB intersection contains both gold-standard sense and parse tree annotations, and allows to set an upper bound of the relative impact of a given semantic representation on parsing. We use the same train-test split of <cite>Agirre et al. (2008)</cite> , with a total of 8,669 sentences containing 151,928 words partitioned into 3 sets: 80% training, 10% development and 10% test data. This dataset is available on request to the research community. We will evaluate the parser via Labeled Attachment Score (LAS). We will use Bikel's randomized parsing evaluation comparator to test the statistical significance of the results using word sense information, relative to the respective baseline parser using only standard features. We used PennConverter (Johansson and Nugues, 2007) to convert constituent trees in the Penn Treebank annotation style into dependency trees. Although in general the results from parsing Pennconverter's output are lower than with other conversions, Johansson and Nugues (2007) claim that this conversion is better suited for semantic processing, with a richer structure and a more finegrained set of dependency labels.",
  "y": "uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_7",
  "x": "To determine the best action at each step, the parser uses history-based feature models and SVM classifiers. One of the main reasons for using MaltParser for our experiments is that it easily allows the introduction of semantic information, adding new features, and incorporating them in the training model. ---------------------------------- **DATASET** We used two different datasets: the full PTB and the Semcor/PTB intersection<cite> (Agirre et al. 2008</cite> ). The full PTB allows for comparison with the stateof-the-art, and we followed the usual train-test split. The Semcor/PTB intersection contains both gold-standard sense and parse tree annotations, and allows to set an upper bound of the relative impact of a given semantic representation on parsing. We use the same train-test split of <cite>Agirre et al. (2008)</cite> , with a total of 8,669 sentences containing 151,928 words partitioned into 3 sets: 80% training, 10% development and 10% test data. This dataset is available on request to the research community.",
  "y": "similarities uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_8",
  "x": "We will experiment with the range of semantic representations used in <cite>Agirre et al. (2008)</cite> , all of which are based on WordNet 2.1. Words in WordNet (Fellbaum, 1998) are organized into sets of synonyms, called synsets (SS). Each synset in turn belongs to a unique semantic file (SF). There are a total of 45 SFs (1 for adverbs, 3 for adjectives, 15 for verbs, and 26 for nouns), based on syntactic and semantic categories. For example, noun semantic files (SF_N) differentiate nouns denoting acts or actions, and nouns denoting animals, among others. We experiment with both full synsets and SFs as instances of fine-grained and coarse-grained semantic representation, respectively. As an example of the difference in these two representations, knife in its tool sense is in the EDGE TOOL USED AS A CUTTING INSTRUMENT singleton synset, and also in the ARTIFACT SF along with thousands of other words including cutter. Note that these are the two extremes of semantic granularity in WordNet. As a hybrid representation, we also tested the effect of merging words with their corresponding SF (e.g. knife+ARTIFACT).",
  "y": "similarities uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_9",
  "x": "We tested the inclusion of several types of semantic information, in the form of WordNet semantic classes in a dependency parser, showing that: \u2022 Semantic information gives an improvement on a transition-based deterministic dependency parsing. \u2022 Feature combinations give an improvement over using a single feature. <cite>Agirre et al. (2008)</cite> used a simple method of substituting wordforms with semantic information, which only allowed using a single semantic feature. MaltParser allows the combination of several semantic features together with other features such as wordform, lemma or part of speech. Although tables 1 and 2 only show the best combination for each type of semantic information, this can be appreciated on GOLD and 1ST in Table 1 . Due to space reasons, we only have showed the best combination, but we can say that in general combining features gives significant increases over using a single semantic feature. \u2022 The present work presents a statistically significant improvement for the full treebank using WordNet-based semantic information for the first time. Our results extend those of <cite>Agirre et al. (2008)</cite> , which showed improvements on a subset of the PTB.",
  "y": "background"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_10",
  "x": "We tested the inclusion of several types of semantic information, in the form of WordNet semantic classes in a dependency parser, showing that: \u2022 Semantic information gives an improvement on a transition-based deterministic dependency parsing. \u2022 Feature combinations give an improvement over using a single feature. <cite>Agirre et al. (2008)</cite> used a simple method of substituting wordforms with semantic information, which only allowed using a single semantic feature. MaltParser allows the combination of several semantic features together with other features such as wordform, lemma or part of speech. Although tables 1 and 2 only show the best combination for each type of semantic information, this can be appreciated on GOLD and 1ST in Table 1 . Due to space reasons, we only have showed the best combination, but we can say that in general combining features gives significant increases over using a single semantic feature. \u2022 The present work presents a statistically significant improvement for the full treebank using WordNet-based semantic information for the first time. Our results extend those of <cite>Agirre et al. (2008)</cite> , which showed improvements on a subset of the PTB.",
  "y": "differences extends"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_0",
  "x": "This latter type of model caused a new revolution in NLP and led to popular language models like GPT-2 (Radford et al., 2018 (Radford et al., , 2019 and ELMo (Peters et al., 2018) . BERT <cite>(Devlin et al., 2019)</cite> improved over previous transformer models and recurrent networks by allowing the system to learn from input text in a bidirectional way, rather than only from left-to-right or the other way around. This model was later reimplemented, critically evaluated and improved in the RoBERTa model . These large-scale transformer models provide the advantage of being able to solve NLP tasks by having a common, expensive pre-training phase, followed by a smaller fine-tuning phase. The pretraining happens in an unsupervised way by providing large corpora of text in the desired language. The second phase only needs a relatively small annotated data set for fine-tuning to outperform previous popular approaches in one of a large number of possible language tasks. While language models are usually trained on English data, some multilingual models also exist. These are usually trained on a large quantity of text in different languages. For example, Multilingual-BERT is trained on a collection of corpora in 104 different languages <cite>(Devlin et al., 2019)</cite> , and generalizes language components well across languages (Pires et al., 2019) .",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_1",
  "x": "BERT <cite>(Devlin et al., 2019)</cite> improved over previous transformer models and recurrent networks by allowing the system to learn from input text in a bidirectional way, rather than only from left-to-right or the other way around. This model was later reimplemented, critically evaluated and improved in the RoBERTa model . These large-scale transformer models provide the advantage of being able to solve NLP tasks by having a common, expensive pre-training phase, followed by a smaller fine-tuning phase. The pretraining happens in an unsupervised way by providing large corpora of text in the desired language. The second phase only needs a relatively small annotated data set for fine-tuning to outperform previous popular approaches in one of a large number of possible language tasks. While language models are usually trained on English data, some multilingual models also exist. These are usually trained on a large quantity of text in different languages. For example, Multilingual-BERT is trained on a collection of corpora in 104 different languages <cite>(Devlin et al., 2019)</cite> , and generalizes language components well across languages (Pires et al., 2019) . However, models trained on data from one specific language usually improve the performance of multilingual models for this particular language (Martin et al., 2019; de Vries et al., 2019) .",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_2",
  "x": "Initially, transformers were introduced for use in machine translation, where they vastly improved state-of-the-art results for English to German in an efficient manner (Vaswani et al., 2017) . This transformer model architecture resulted in a new paradigm in NLP with the migration from sequence-to-sequence recurrent neural networks to transformer-based models by removing the recurrent component and only keeping attention. This cornerstone was used for BERT, a transformer model that obtained stateof-the-art results for eleven natural language processing tasks, such as question answering and natural language inference <cite>(Devlin et al., 2019)</cite> . BERT is pre-trained with large corpora of text using two unsupervised tasks. The first task is word masking (also called the Cloze task (Taylor, 1953) or masked language model (MLM)), where the model has to guess which word is masked in certain position in the text. The second task is next sentence prediction. This is done by predicting if two sentences are subsequent in the corpus, or if they are randomly sampled from the corpus. These tasks allowed the model to create internal representations about a language, which could thereafter be reused for different language tasks. This architecture has been shown to be a general language model that could be fine-tuned with little data in a relatively efficient way for a very distinct range of tasks and still outperform previous architectures <cite>(Devlin et al., 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_3",
  "x": "The first task is word masking (also called the Cloze task (Taylor, 1953) or masked language model (MLM)), where the model has to guess which word is masked in certain position in the text. The second task is next sentence prediction. This is done by predicting if two sentences are subsequent in the corpus, or if they are randomly sampled from the corpus. These tasks allowed the model to create internal representations about a language, which could thereafter be reused for different language tasks. This architecture has been shown to be a general language model that could be fine-tuned with little data in a relatively efficient way for a very distinct range of tasks and still outperform previous architectures <cite>(Devlin et al., 2019)</cite> . Transformer models are also capable of generating contextualized word embeddings. These contextualized embeddings were presented by Peters et al. (2018) and addressed the well known issue with a word's meaning being defined by its context (e.g. \"a stick\" versus \"let's stick to\"). This lack of context is something that traditional word embeddings like word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014) lack, whereas BERT automatically incorporates the context a word occurs in. Another advantage of transformer models is that attention allows them to better resolve coreferences between words .",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_4",
  "x": "A typical example for the importance of coreference resolution is \"The trophy doesnt fit in the brown suitcase because its too big.\", where the word \"it\" would refer to the the suitcase instead of the trophy if the last word was changed to \"small\" (Levesque et al., 2012) . Being able to resolve these coreferences is for example important for translating to languages with gender, as suitcase and trophy have different genders in French. Although BERT has been shown to be a useful language model, it has also received some scrutiny on the training and pre-processing of the language model. As mentioned before, BERT uses next sentence prediction (NSP) as one of its two training tasks. In NSP, the model has to predict whether two sentences follow each other in the training text, or are just randomly selected from the corpora. The authors of RoBERTa ) showed that while this task made the model achieve a better performance, it was not due to its intended reason, as it might merely predict relatedness rather than subsequent sentences. That<cite> Devlin et al. (2019)</cite> trained a better model when using NSP than without NSP is likely due to the model learning long-range dependencies in text from its inputs, which are longer than just the single sentence on itself. As such, the RoBERTa model uses only the MLM task, and uses multiple full sentences in every input. Other research improved the NSP task by instead making the model predict the correct order of two sentences, where the model thus has to predict whether the sentences occur in the given order in the corpus, or occur in flipped order (Lan et al., 2019) .",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_5",
  "x": "RobBERT shares its architecture with RoBERTa's base model, which itself is a replication and improvement over BERT . The architecture of our language model is thus equal to the original BERT model with 12 self-attention layers with 12 heads <cite>(Devlin et al., 2019)</cite> . One difference with the original BERT is due to the different pre-training task specified by RoBERTa, using only the MLM task and not the NSP task. The training thus only uses word masking, where the model has to predict which words were masked in certain positions of a given line of text. The training process uses the Adam optimizer (Kingma and Ba, 2017) with polynomial decay of the learning rate l r = 10 \u22126 and a ramp-up period of 1000 iterations, with parameters \u03b2 1 = 0.9 (a common default) and RoBERTa's default \u03b2 2 = 0.98. Additionally, we also used a weight decay of 0.1 as well as a small dropout of 0.1 to help prevent the model from overfitting (Srivastava et al., 2014) . We used a computing cluster in order to efficiently pre-train our model. More specifically, the pre-training was executed on a computing cluster with 20 nodes with 4 Nvidia Tesla P100 GPUs (16 GB VRAM each) and 2 nodes with 8 Nvidia V100 GPUs (having 32 GB VRAM each). This pretraining happened in fixed batches of 8192 sentences by rescaling each GPUs batch size depending on the number of GPUs available, in order to maximally utilize the cluster without blocking it entirely for other users.",
  "y": "similarities"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_6",
  "x": "As before, we fine-tuned the model twice: once with the full training set and once with a subset of 10k utterances from the training set for illustrating the benefits of pre-training on low-resource tasks. ZeroR (majority class) 66.70 mBERT <cite>(Devlin et al., 2019)</cite> 90.21 BERTje (de Vries et al., 2019) 94.94 RobBERT (ours) 98.03 RobBERT outperforms previous models as well as other BERT models both with as well as without fine-tuning (see Table 1 and Table 2 ). It is also able to reach similar performance using less data. The fact that zero-shot RobBERT outperforms other zero-shot BERT models is also an indication that the base model has internalised more knowledge about Dutch than the other two have. The reason RobBERT and other BERT models outperform the previous RNN-based approach is likely the transformers ability to deal better with coreference resolution , and by extension better in deciding which word the \"die\" or \"dat\" belongs to. ---------------------------------- **CODE** The training and evaluation code of this paper as well as the RobBERT model and the fine-tuned models are publicly available for download on https://github.com/iPieter/RobBERT.",
  "y": "background"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_0",
  "x": "To deal with phonological variability alternate pronunciations are included in the lexicon, and optional phonological rules are applied during training and recognition. The recognizer uses a time-synchronous graph-search strategy [16] for a first pass with a bigram back-off language model (LM) [10] . A trigram LM is used in a second acoustic decoding pass which makes use of the word graph generated using the bigram LM [6] . Experimental results are reported on the ARPA Wall Street Journal (WSJ) <cite>[19]</cite> and BREF [14] corpora, using for both corpora over 37k utterances for acoustic training and more than 37 million words of newspaper text for language model training. While the number of speakers is larger for WSJ, the total amount of acoustic training material is about the same (see Table 1 ). It is shown that for both corpora increasing the amount of training utterances by an order of magnitude reduces the word error by about 30%. The use of a trigram LM in a second pass also gives an error reduction of 20% to 30%. The combined error reduction is on the order of 50%. ----------------------------------",
  "y": "background"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_1",
  "x": "Language modeling entails incorporating constraints on the allowable sequences of words which form a sentence. Statistical n-gram models attempt to capture the syntactic and semantic constraints by estimating the frequencies of sequences of n words. In this work bigram and trigram language models are estimated on the training text material for each corpus. This data consists of 37M words of the WSJ 1 and 38M words of Le Monde. A backoff mechanism [10] is used to smooth the estimates of the probabilities of rare n-grams by relying on a lower order n-gram when there is insufficient training data, and to provide a means of modeling unobserved n-grams. Another advantage of the backoff mechanism is that LM size can be arbitrarily reduced by relying more on the backoff, by increasing the minimum number of required n-gram observations needed to include the n-gram. This property can be used in the first bigram decod1While we have built n-gram-backoff LMs directly from the 37M-word standardized WSJ training text material, in these experiments all results are reported using the 5k or 20k, bigram and tfigram backoff LMs provided by Lincoln Labs<cite> [ 19]</cite> as required by ARPA so as to be compatible with the other sites participating in the tests. ing pass to reduce computational requirements. The trigram langage model is used in the second pass of the decoding process:.",
  "y": "uses"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_2",
  "x": "In this work bigram and trigram language models are estimated on the training text material for each corpus. This data consists of 37M words of the WSJ 1 and 38M words of Le Monde. A backoff mechanism [10] is used to smooth the estimates of the probabilities of rare n-grams by relying on a lower order n-gram when there is insufficient training data, and to provide a means of modeling unobserved n-grams. Another advantage of the backoff mechanism is that LM size can be arbitrarily reduced by relying more on the backoff, by increasing the minimum number of required n-gram observations needed to include the n-gram. This property can be used in the first bigram decod1While we have built n-gram-backoff LMs directly from the 37M-word standardized WSJ training text material, in these experiments all results are reported using the 5k or 20k, bigram and tfigram backoff LMs provided by Lincoln Labs<cite> [ 19]</cite> as required by ARPA so as to be compatible with the other sites participating in the tests. ing pass to reduce computational requirements. The trigram langage model is used in the second pass of the decoding process:. In order to be able to constnact LMs for BREF, it was necessary to normalize the text material of Le Monde newpaper, which entailed a pre-treatment rather different from that used to normalize the WSJ texts <cite>[19]</cite> . The main differences are in the treatment of compound words, abbreviations, and case.",
  "y": "uses differences"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_3",
  "x": "It should be noted that this decoding strategy based on two forward passes can in fact be implemented in a single forward pass using one or two processors. We are using a two pass solution because it is conceptually simpler, and also due to memory constraints. ---------------------------------- **EXPERIMENTAL RESULTS** ---------------------------------- **WSJ:** The ARPA WSJ corpus <cite>[19]</cite> was designed to provide general-purpose speech data with large vocabularies. Text materials were selected to provide training and test data for 5k and 20k word, closed and open vocabularies, and with both verbalized (VP) and non-verbalized (NVP) punctuation. 41n our implementation, a word lattice differs from a word graph only because it includes word endpoint information.",
  "y": "background"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_4",
  "x": "Text materials were selected to provide training and test data for 5k and 20k word, closed and open vocabularies, and with both verbalized (VP) and non-verbalized (NVP) punctuation. 41n our implementation, a word lattice differs from a word graph only because it includes word endpoint information. The 20k open test is also referred to as a 64k test since all of the words in these sentences occur in the 63,495 most frequent words in the normalized WSJ text material<cite> [ 19]</cite> . Two sets of standard training material have been used for these experiments: The standard WSJ0 SI84 training data which include 7240 sentences from 84 speakers, and the standard set of 37,518 WSJ0/WSJ1 SI284 sentences from 284 speakers. Only the primary microphone data were used for training. The WSJ corpus provides a wealth of material that can be used for system development. We have worked primarily with the WSJ0-Dev (410 sentences, 10 speakers), and the WSJ1-Dev from spokes s5 and s6 (394 sentences, 10 speakers). Development of the word recognizer was done with the 5k closed vocabulary system in order to reduce the computational requirements. The Nov92 5k and 20k nvp test sets were used to assess progress during this development phase.",
  "y": "uses"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_5",
  "x": "Two sets of standard training material have been used for these experiments: The standard WSJ0 SI84 training data which include 7240 sentences from 84 speakers, and the standard set of 37,518 WSJ0/WSJ1 SI284 sentences from 284 speakers. Only the primary microphone data were used for training. The WSJ corpus provides a wealth of material that can be used for system development. We have worked primarily with the WSJ0-Dev (410 sentences, 10 speakers), and the WSJ1-Dev from spokes s5 and s6 (394 sentences, 10 speakers). Development of the word recognizer was done with the 5k closed vocabulary system in order to reduce the computational requirements. The Nov92 5k and 20k nvp test sets were used to assess progress during this development phase. The WSJ system was evaluated in the Nov92 ARPA evaluation test [17] for the 5k-closed vocabulary and in the Nov93 ARPA evaluation test [18] for the 5k and 64k hubs. Except when explicitly stated otherwise, all of the results reported for WSJ use the standard language models <cite>[19]</cite> . Using a set of 1084 CD models trained with the WSJ0 si84 training data, the word error is 6.6% on the Nov92 5k test data and 9.4% on the Nov93 test data.",
  "y": "uses"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_6",
  "x": "6 5This is in contrast to the WSJ texts which were selected so as to contain only words in the most frequent 64,000 words in the original text material. 6Another difference between BREF and WSJ0 is that the prompts for ---------------------------------- **DISCUSSION AND SUMMARY** The recognizer has been evaluated on 5k and 20k test data for the English and French languages using similar style corpora. It should be pointed out however, that although the Nov92 5k WSJ test data and the BREF 5k test data were closed-vocabulary, the conditions are not quite the same. For WSJ, paragraphs were selected ensuring not more than one word was out of the 5.6k most frequent words<cite> [ 19]</cite> , and these additional words were then included as part of the vocabulary. For BREF, a lexicon was first constructed containing the 5k/20k most frequent words, and sentences covered by this vocabulary were selected from the development test material. The situation was slightly different for the Nov93 5k test in that the prompt texts were not normalized, and therefore several OOV words (0.3%) occurred in the test data despite it being a closed-vocabulary test.",
  "y": "uses"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_0",
  "x": "Fukui et al.<cite> [6]</cite> propose multimodal compact bilinear pooling (MCB) to efficiently implement an outer product operator that combines visual and textual representations. Yu et al. [26] extend this pooling scheme by introducing a multi-modal factorized bilinear pooling approach (MFB) that improves the representational capacity of the bilinear operator. They achieve this by adding an initial step that efficiently expands the textual and visual embeddings to a high-dimensional space. In terms of structural innovations, Noh et al. [16] embed the textual question as an intermediate dynamic bilinear layer of a ConvNet that processes the visual information. Andreas et al. [2] propose a model that learns a set of task-specific neural modules that are jointly trained to answer visual questions. Following the successful introduction of soft attention in neural machine translation applications [3] , most modern VQA methods also incorporate a similar mechanism. The common approach is to use a one-way attention scheme, where the embedding of the question is used to generate a set of attention coefficients over a set of predefined image regions. These coefficients are then used to weight the embedding of the image regions to obtain a suitable descriptor [19, 21,<cite> 6,</cite> 25, 26] . More elaborated forms of attention has also been proposed.",
  "y": "background"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_1",
  "x": "Fukui et al.<cite> [6]</cite> propose multimodal compact bilinear pooling (MCB) to efficiently implement an outer product operator that combines visual and textual representations. Yu et al. [26] extend this pooling scheme by introducing a multi-modal factorized bilinear pooling approach (MFB) that improves the representational capacity of the bilinear operator. They achieve this by adding an initial step that efficiently expands the textual and visual embeddings to a high-dimensional space. In terms of structural innovations, Noh et al. [16] embed the textual question as an intermediate dynamic bilinear layer of a ConvNet that processes the visual information. Andreas et al. [2] propose a model that learns a set of task-specific neural modules that are jointly trained to answer visual questions. Following the successful introduction of soft attention in neural machine translation applications [3] , most modern VQA methods also incorporate a similar mechanism. The common approach is to use a one-way attention scheme, where the embedding of the question is used to generate a set of attention coefficients over a set of predefined image regions. These coefficients are then used to weight the embedding of the image regions to obtain a suitable descriptor [19, 21,<cite> 6,</cite> 25, 26] . More elaborated forms of attention has also been proposed.",
  "y": "background"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_2",
  "x": "Furthermore, they differ from our work in the method to integrate attention labels to a VQA model. Specifically, we take the mined grounding labels as weakly-supervised signals and denote two types of attention supervision, namely region-level and object-level labels. Figure 2 shows the main pipeline of our VQA model. We mostly build upon the MCB model in<cite> [6]</cite> , which exemplifies current state-of-the-art techniques for this problem. Our main innovation to this model is the addition of an Attention Supervision Module that incorporates visual grounding as an auxiliary task. Next we describe the main modules behind this model. Question Attention Module: Questions are tokenized and passed through an embedding layer, followed by an LSTM layer that generates the question features Q f \u2208 R T \u00d7D , where T is the maximum number of words in the tokenized version of the question and D is the dimensionality of the hidden state of the LSTM. Additionally, following [25] , a question attention mechanism is added that generates question attention coefficients C q \u2208 R T \u00d7G q , where G q is the so-called number of \"glimpses\". The purpose of G q is to allow the model to predict multiple attention maps so as to increase its expressiveness.",
  "y": "differences extends"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_3",
  "x": "Here, we use G q = 2. The weighted question features Q w \u2208 R G q D are then computed using a soft attention mechanism [3] , which is essentially a weighted sum of the T word features followed by a concatenation according to G q . ---------------------------------- **VQA MODEL STRUCTURE** Image Attention Module: Images are passed through an embedding layer consisting of a pre-trained ConvNet model, such as Resnet pretrained with the ImageNet dataset [10] . This generates image features I f \u2208 R C\u00d7H\u00d7W , where C, H and W are depth, height and width of the extracted feature maps. Fusion Module I is then used to generate a set of image attention coefficients. First, question features Q w are tiled as the same spatial shape of I f . Afterwards, the fusion module models the joint relationship J attn \u2208 R O\u00d7H\u00d7W between questions and images, mapping them to a common space of dimension O. In the simplest case, one can implement the fusion module using either concatenation or Hadamard product [1] , but more effective pooling schemes can be applied <cite>[6,</cite> 11, 25, 26] .",
  "y": "background"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_4",
  "x": "In general, it should both effectively capture the latent relationship between multi-modal features meanwhile be easy to optimize. The fusion results are then passed through an attention module that computes the visual atten- Classification Module: Using the compact representation of questions Q w and visual information V w , the classification module applies first the Fusion Module II that provides the feature representation of answers J ans \u2208 R L , where L is the latent answer space. Afterwards, it computes the logits over a set of predefined candidate answers. Following previous work<cite> [6]</cite> , we use as candidate outputs the top 3000 most frequent answers in the VQA dataset. At the end of this process, we obtain the highest scoring answer\u00c2. ---------------------------------- **ATTENTION SUPERVISION MODULE:** As a main novelty of the VQA model, we add an Image Attention Supervision Module as an auxiliary classification task, where ground-truth visual grounding labels C gt \u2208 R H\u00d7W \u00d7G v are used to guide the model to focus on meaningful parts of the image to answer each question.",
  "y": "similarities uses"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_5",
  "x": "As a main novelty of the VQA model, we add an Image Attention Supervision Module as an auxiliary classification task, where ground-truth visual grounding labels C gt \u2208 R H\u00d7W \u00d7G v are used to guide the model to focus on meaningful parts of the image to answer each question. To do that, we simply treat the generated attention coefficients C v as a probability distribution, and then compare it with the ground-truth using KL-divergence. Interestingly, we introduce two attention maps, corresponding to relevant region-level and objectlevel groundings, as shown in Figure 3 . Sections 4 and 5 provide details about our proposed method to obtain the attention labels and to train the resulting model, respectively. J a n s Figure 2 . Schematic diagram of the main parts of the VQA model. It is mostly based on the model presented in<cite> [6]</cite> . Main innovation is the Attention Supervision Module that incorporates visual grounding as an auxiliary task. This module is trained through the use of a set of image attention labels that are automatically mined from the Visual Genome dataset.",
  "y": "extends"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_6",
  "x": "We will make these labels publicly available. Here \"men\" in the region description is firstly lemmatized to be \"man\", whose aliases contain \"people\"; the word \"talking\" in the answer also contributes to the matching. So the selected regions have two matchings which is the most among all candidates. (b) Example object-level grounding from VG. Left: image with object instance labels; Right: our mined results. Note that in this case region-level grounding will give us the same result as in (a), but object-level grounding is clearly more localized. ---------------------------------- **IMPLEMENTATION DETAILS** We build the attention supervision on top of the opensourced implementation of MCB<cite> [6]</cite> and MFB [25] .",
  "y": "extends differences"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_7",
  "x": "There are more than 20 question types, covering a variety of topics and free-form answers. The dataset is split into training (82K images and 443K questions), validation (40K images and 214K questions), and testing (81K images and 448K questions) sets. The task is to predict a correct answer A given a corresponding image-question pair (I, Q). As a main advantage with respect to version 1.0 [9] , for every question VQA-2.0 includes complementary images that lead to different answers, reducing language bias by forcing the model to use the visual information. Visual Genome: The Visual Genome (VG) dataset [12] contains 108077 images, with an average of 17 QA pairs per image. We follow the processing scheme from<cite> [6]</cite> , where non-informative words in the questions and answers such as \"a\" and \"is\" are removed. Afterwards, (I, Q, A) triplets with answers to be single keyword and overlapped with VQA-2.0 dataset are included in our training set. This adds 97697 images and about 1 million questions to the training set. Besides the VQA data, VG also provides on average 50 region descriptions and 30 object instances per image.",
  "y": "similarities uses"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_8",
  "x": "Accuracy/% VQA-HAT VQA-X VQA-2.0 Human [5] 0.623 -80.62 PJ-X [17] 0.396 0.342 -MCB<cite> [6]</cite> 0 authors also collect 1374 \u00d7 3 = 4122 HAT maps for VQA-1.0 validation sets, where each of the 1374 (I, Q, A) were labeled by three different annotators, so one can compare the level of agreement among labels. We use VQA-HAT to evaluate visual grounding performance, by comparing the rank-correlation between human attention and model attention, as in [5, 17] . VQA-X: VQA-X dataset [17] contains 2000 labeled attention maps in VQA-2.0 validation sets. In contrast to VQA-HAT, VQA-X attention maps are in the form of instance segmentations, where annotators were asked to segment objects and/or regions that most prominently justify the answer. Hence the attentions are more specific and localized. We use VQA-X to evaluate visual grounding performance by comparing the rank-correlation, as in [5, 17 ]. ---------------------------------- **RESULTS** We evaluate the performance of our proposed method using two criteria: i) rank-correlation [20] to evaluate visual grounding and ii) accuracy to evaluate question answering.",
  "y": "similarities"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_0",
  "x": "Specifically, i) word embeddings are typically built from massive unlabeled datasets and thus OOVs are less likely to be encountered at test time, while ii) character embeddings offer further linguistically plausible fallback for the remaining OOVs through modeling intraword relations. Through these approaches, multilingual PoS tagging has seen tangible gains from neural methods in the recent years. ---------------------------------- **INTRODUCTION** In natural language processing, the deep learning revolution has shifted the focus from conventional hand-crafted symbolic representations to dense inputs, which are adequate representations learned automatically from corpora. However, particularly when working with low-resource languages, small amounts of symbolic lexical resources such as user-generated lexicons are often available even when gold-standard corpora are not. Recent work has shown benefits of combining conventional lexical information into neural cross-lingual part-ofspeech (PoS) tagging <cite>(Plank and Agi\u0107, 2018)</cite> . However, little is known on how complementary such additional information is, and to what extent improvements depend on the coverage and quality of these external resources. The contribution of this paper is in the analysis of the contributions of models' components (tagger transfer through annotation projection vs. the contribution of encoding lexical and morphosyntactic resources).",
  "y": "background"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_1",
  "x": "Sub-word information is often coupled with standard word embeddings to mitigate OOV issues. Specifically, i) word embeddings are typically built from massive unlabeled datasets and thus OOVs are less likely to be encountered at test time, while ii) character embeddings offer further linguistically plausible fallback for the remaining OOVs through modeling intraword relations. Through these approaches, multilingual PoS tagging has seen tangible gains from neural methods in the recent years. ---------------------------------- **INTRODUCTION** In natural language processing, the deep learning revolution has shifted the focus from conventional hand-crafted symbolic representations to dense inputs, which are adequate representations learned automatically from corpora. However, particularly when working with low-resource languages, small amounts of symbolic lexical resources such as user-generated lexicons are often available even when gold-standard corpora are not. Recent work has shown benefits of combining conventional lexical information into neural cross-lingual part-ofspeech (PoS) tagging <cite>(Plank and Agi\u0107, 2018)</cite> . However, little is known on how complementary such additional information is, and to what extent improvements depend on the coverage and quality of these external resources.",
  "y": "motivation"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_2",
  "x": "For Wiktionary, we use the freely available dictionaries from Li et al. (2012) . UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively). 1 The sizes of the dictionaries vary considerably, from a few thousand entries (e.g., for Hindi and Bulgarian) to 2M entries (Finnish Uni-Morph). We study the impact of smaller dictionary sizes in Section 4.1. The tagger we analyze in this paper is an extension of the base tagger, called distant supervision from disparate sources (DSDS) tagger <cite>(Plank and Agi\u0107, 2018)</cite> . It is trained on projected data and further differs from the base tagger by the integration of lexicon information. In particular, given a lexicon src, DSDS uses e src to embed the lexicon into an l-dimensional space, where e src is the concatenation of all embedded m properties of length l (empirically set, see Section 2.2), and a zero vector for words not in the lexicon. A property here is a possible PoS tag (for Wiktionary) or a morphological feature (for Unimorph). To integrate the type-level supervision, the lexicon embeddings vector is created and concatenated to the word and character-level representations for every token: w \u2022 cw \u2022 e.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_3",
  "x": "We use linguistic resources that are user-generated and available for many languages. The first is WIKTIONARY, a word type dictionary that maps words to one of the 12 Universal PoS tags (Li et al., 2012; Petrov et al., 2012) . The second resource is UNIMORPH, a morphological dictionary that provides inflectional paradigms for 350 languages (Kirov et al., 2016) . For Wiktionary, we use the freely available dictionaries from Li et al. (2012) . UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively). 1 The sizes of the dictionaries vary considerably, from a few thousand entries (e.g., for Hindi and Bulgarian) to 2M entries (Finnish Uni-Morph). We study the impact of smaller dictionary sizes in Section 4.1. The tagger we analyze in this paper is an extension of the base tagger, called distant supervision from disparate sources (DSDS) tagger <cite>(Plank and Agi\u0107, 2018)</cite> . It is trained on projected data and further differs from the base tagger by the integration of lexicon information.",
  "y": "extends"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_4",
  "x": "**EQUAL:** In an ideal setup, the dictionaries contain no disjoint tag sets, and larger amounts of equal tag sets or superset of the treebank data. This is particularly desirable for approaches that take lexical information as type-level supervision. ---------------------------------- **EXPERIMENTAL SETUP** In this section we describe the baselines, the data and the tagger hyperparameters. Data We use the 12 Universal PoS tags (Petrov et al., 2012) . The set of languages is motivated by accessibility to embeddings and dictionaries. We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by<cite> Plank and Agi\u0107 (2018)</cite> showing that DSDS provides a viable alternative.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_5",
  "x": "Data We use the 12 Universal PoS tags (Petrov et al., 2012) . The set of languages is motivated by accessibility to embeddings and dictionaries. We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by<cite> Plank and Agi\u0107 (2018)</cite> showing that DSDS provides a viable alternative. Annotation projection To build the taggers for new languages, we resort to annotation projection following<cite> Plank and Agi\u0107 (2018)</cite> . In particular, they employ the approach by Agi\u0107 et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences. The wide-coverage Watchtower corpus (WTC) by Agi\u0107 et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following<cite> Plank and Agi\u0107 (2018)</cite> . Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (Li et al., 2012) and retrofitting initialization. Hyperparameters We use the same setup as<cite> Plank and Agi\u0107 (2018)</cite> , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions. This ensures that our probing tasks always get the same input dimensionality: 64 (2x32) dimensions for cw, which is the same dimension as the off-theshelf word embeddings.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_6",
  "x": "In this section we describe the baselines, the data and the tagger hyperparameters. Data We use the 12 Universal PoS tags (Petrov et al., 2012) . The set of languages is motivated by accessibility to embeddings and dictionaries. We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by<cite> Plank and Agi\u0107 (2018)</cite> showing that DSDS provides a viable alternative. Annotation projection To build the taggers for new languages, we resort to annotation projection following<cite> Plank and Agi\u0107 (2018)</cite> . In particular, they employ the approach by Agi\u0107 et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences. The wide-coverage Watchtower corpus (WTC) by Agi\u0107 et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following<cite> Plank and Agi\u0107 (2018)</cite> . Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (Li et al., 2012) and retrofitting initialization. Hyperparameters We use the same setup as<cite> Plank and Agi\u0107 (2018)</cite> , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_7",
  "x": "The set of languages is motivated by accessibility to embeddings and dictionaries. We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by<cite> Plank and Agi\u0107 (2018)</cite> showing that DSDS provides a viable alternative. Annotation projection To build the taggers for new languages, we resort to annotation projection following<cite> Plank and Agi\u0107 (2018)</cite> . In particular, they employ the approach by Agi\u0107 et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences. The wide-coverage Watchtower corpus (WTC) by Agi\u0107 et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following<cite> Plank and Agi\u0107 (2018)</cite> . Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (Li et al., 2012) and retrofitting initialization. Hyperparameters We use the same setup as<cite> Plank and Agi\u0107 (2018)</cite> , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions. This ensures that our probing tasks always get the same input dimensionality: 64 (2x32) dimensions for cw, which is the same dimension as the off-theshelf word embeddings. Language-specific hyperparameters could lead to optimized models for each language.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_8",
  "x": "The wide-coverage Watchtower corpus (WTC) by Agi\u0107 et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following<cite> Plank and Agi\u0107 (2018)</cite> . Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (Li et al., 2012) and retrofitting initialization. Hyperparameters We use the same setup as<cite> Plank and Agi\u0107 (2018)</cite> , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions. This ensures that our probing tasks always get the same input dimensionality: 64 (2x32) dimensions for cw, which is the same dimension as the off-theshelf word embeddings. Language-specific hyperparameters could lead to optimized models for each language. However, we use identical settings for each language which worked well and is less expensive, following Bohnet et al. (2018) . For all experiments, we average over 3 randomly seeded runs, and provide mean accuracy. We use the off-the-shelf Polyglot word embeddings (Al-Rfou et al., 2013) . Word embedding initialization provides a consistent and considerable boost in this cross-lingual setup, up to 10% absolute improvements across 21 languages when only 500 projected training instances are available <cite>(Plank and Agi\u0107, 2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_9",
  "x": "Language-specific hyperparameters could lead to optimized models for each language. However, we use identical settings for each language which worked well and is less expensive, following Bohnet et al. (2018) . For all experiments, we average over 3 randomly seeded runs, and provide mean accuracy. We use the off-the-shelf Polyglot word embeddings (Al-Rfou et al., 2013) . Word embedding initialization provides a consistent and considerable boost in this cross-lingual setup, up to 10% absolute improvements across 21 languages when only 500 projected training instances are available <cite>(Plank and Agi\u0107, 2018)</cite> . Note that we em- pirically find it to be best to not update the word embeddings in this noisy training setup, as that results in better performance, see Section 4.4. ---------------------------------- **RESULTS** Table 1 presents our replication results, i.e., tagging accuracy for the 21 individual languages, with means over all languages and language families (for which at least two languages are available).",
  "y": "background"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_10",
  "x": "---------------------------------- **INCLUSION OF LEXICAL INFORMATION** Combining the best of two worlds results in the overall best tagging accuracy, confirming<cite> Plank and Agi\u0107 (2018)</cite> : Embedding lexical information into a neural tagger improves tagging accuracy from 83.4 to 84.1 (means over 21 languages). On 15 out of 21 languages, DSDS is the best performing model. On two languages, type constraints work the best (English and Greek). Retrofitting performs best only on one language (Persian); this is the language with the overall lowest performance. On three languages, Czech, French and Hungarian, the baseline remains the best model, none of the lexicon-enriching approaches works. We proceed to inspect these results in more detail. Analysis Overall, type-constraints improve the baseline but only slightly (83.4 vs 83.6).",
  "y": "similarities"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_11",
  "x": "Over all 21 languages there is a slight drop on true OOVs: -0.08, but this is a mean over all languages, for which results vary, making it important to look beyond the aggregate level. Over all languages except for Hungarian, the tagger, unsurprisingly, improves over tokens which are both in the lexicon and in the training data (see further discussion in Section 4). ---------------------------------- **DISCUSSION** Here we dig deeper into the effect of including lexical information by a) examining learning curves with increasing dictionary sizes, b) relating tag set properties to performance, and finally c) having a closer look at model internal representations, by comparing them to the representations of the base model that does not include lexical information. We hypothesize that when learning from dictionary-level supervision, information is propagated through the representation layers so as to generalize beyond simply relying on the respective external resources. ---------------------------------- **LEARNING CURVES** The lexicons we use so far are of different sizes (shown in Table 1 of<cite> Plank and Agi\u0107 (2018)</cite> ), spanning from 1,000 entries to considerable dictionaries of several hundred thousands entries.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_12",
  "x": "Therefore, we train the tagger with pre-trained embeddings on projected WTC data and freeze the word embeddings lookup layer during training. In recent years, natural language processing has witnessed a move towards deep learning approaches, in which automatic representation learning has become the de facto standard methodology (Collobert et al., 2011; Manning, 2015) . One of the first works that combines neural representations with semantic symbolic lexicons is the work on retrofitting (Faruqui et al., 2015) . The main idea is to use the relations defined in semantic lexicons to refine word embedding representations, such that words linked in the lexical resource are encouraged to be closer to each other in the distributional space. The majority of recent work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are obsolete for deep learning methods. They rely on end-to-end training without resorting to additional linguistic resources. Our study contributes to the increasing literature to show the utility of linguistic resources for deep learning models by providing a deep analysis of a recently proposed model <cite>(Plank and Agi\u0107, 2018)</cite> . Most prior work in this direction can be found on machine translation (Sennrich and Haddow, 2016; Chen et al., 2017; Li et al., 2017; Passban et al., 2018) , work on named entity recognition (Wu et al., 2018) and PoS tagging (Sagot and Mart\u00ednez Alonso, 2017) who use lexicons, but as n-hot features and without examining the crosslingual aspect. Somewhat complementary to evaluating the utility of linguistic resources empirically is the increasing body of work that uses linguistic insights to try to understand what properties neural-based representations capture (K\u00e1d\u00e1r et al., 2017; Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018) .",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_13",
  "x": "As recently shown (Agi\u0107 et al., 2017) , type-level information from dictionaries approximates PoS tagging accuracy in the absence of gold data for cross-lingual tagger evaluation. Their use of high-frequency word types inspired parts of our analysis. ---------------------------------- **CONCLUSIONS** We analyze DSDS, a recently-proposed lowresource tagger that symbiotically leverages neural representations and symbolic linguistic knowledge by integrating them in a soft manner. We replicated the results of<cite> Plank and Agi\u0107 (2018)</cite> , showing that the more implicit use of embedding user-generated dictionaries turns out to be more beneficial than approaches that rely more explicitly on symbolic knowledge, such a type constraints or retrofitting. By analyzing the reliance of DSDS on the linguistic knowledge, we found that the composition of the lexicon is more important than its size. Moreover, the tagger benefits from small dictionaries, as long as they do not contain tag set information contradictory to the evaluation data. Our quantitative analysis also sheds light on the internal representations, showing that they get more sensitive to the task.",
  "y": "uses extends"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_0",
  "x": "****CONSTRAINED SEMANTIC FORESTS FOR IMPROVED DISCRIMINATIVE SEMANTIC PARSING**** **ABSTRACT** In this paper, we present a model for improved discriminative semantic parsing. The model addresses an important limitation associated with our previous stateof-the-art discriminative semantic parsing model -the <cite>relaxed hybrid tree</cite> model by introducing our constrained semantic forests. We show that our model is able to yield new state-of-the-art results on standard datasets even with simpler features. Our system is available for download from http://statnlp.org/research/sp/. ---------------------------------- **INTRODUCTION** This paper addresses the problem of parsing natural language sentences into their corresponding semantic representations in the form of formal logical representations.",
  "y": "differences motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_1",
  "x": "---------------------------------- **INTRODUCTION** This paper addresses the problem of parsing natural language sentences into their corresponding semantic representations in the form of formal logical representations. Such a task is also known as semantic parsing (Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010) . One state-of-the-art model for semantic parsing is our recently introduced <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> , which performs integrated lexicon acquisition and semantic parsing within a single framework utilizing efficient algorithms for training and inference. <cite>The model</cite> allows natural language phrases to be recursively mapped to semantic units, where certain long-distance dependencies can be captured. <cite>It</cite> relies on representations called <cite>relaxed hybrid trees</cite> that can jointly represent both the sentences and semantics. <cite>The model</cite> is essentially discriminative, and allows rich features to be incorporated. Unfortunately, the <cite>relaxed hybrid tree</cite> model has an important limitation: <cite>it</cite> essentially does not allow certain sentence-semantics pairs to be jointly encoded using the proposed <cite>relaxed hybrid tree</cite> representations.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_2",
  "x": "This paper addresses the problem of parsing natural language sentences into their corresponding semantic representations in the form of formal logical representations. Such a task is also known as semantic parsing (Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010) . One state-of-the-art model for semantic parsing is our recently introduced <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> , which performs integrated lexicon acquisition and semantic parsing within a single framework utilizing efficient algorithms for training and inference. <cite>The model</cite> allows natural language phrases to be recursively mapped to semantic units, where certain long-distance dependencies can be captured. <cite>It</cite> relies on representations called <cite>relaxed hybrid trees</cite> that can jointly represent both the sentences and semantics. <cite>The model</cite> is essentially discriminative, and allows rich features to be incorporated. Unfortunately, the <cite>relaxed hybrid tree</cite> model has an important limitation: <cite>it</cite> essentially does not allow certain sentence-semantics pairs to be jointly encoded using the proposed <cite>relaxed hybrid tree</cite> representations. Thus, <cite>the model</cite> is unable to identify joint representations for certain sentence-semantics pairs during the training process, and is unable to produce desired outputs for certain inputs during the evaluation process. In this work, we propose a solution addressing the above limitation, which makes our model more robust.",
  "y": "motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_3",
  "x": "In this section, we provide a relatively brief discussion of prior work in semantic parsing. The hybrid tree model (Lu et al., 2008) and the Bayesian tree transducer based model (Jones et al., 2012) are generative frameworks, which essentially assume natural language and semantics are jointly generated from an underlying generative process. Such models are efficient, but are limited in their predictive power due to the simple independence assumptions made. On the other hand, discriminative models are able to exploit arbitrary features and are usually able to give better results. Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine translation problem, the UBL system (Kwiatkowski et al., 2010) which performs CCG-based semantic parsing using a log-linear model, as well as the <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> which extends the generative hybrid tree model. This extension results in a discriminative model that incorporates rich features and allows long-distance dependencies to be captured. The <cite>relaxed hybrid tree</cite> model has achieved the state-of-the-art results on standard benchmark datasets across different languages. Performing semantic parsing under other forms of supervision is also possible. Clarke et al. (2010) proposed a model that learns a semantic parser for answering questions without relying on semantic annotations.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_4",
  "x": "The hybrid tree model (Lu et al., 2008) and the Bayesian tree transducer based model (Jones et al., 2012) are generative frameworks, which essentially assume natural language and semantics are jointly generated from an underlying generative process. Such models are efficient, but are limited in their predictive power due to the simple independence assumptions made. On the other hand, discriminative models are able to exploit arbitrary features and are usually able to give better results. Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine translation problem, the UBL system (Kwiatkowski et al., 2010) which performs CCG-based semantic parsing using a log-linear model, as well as the <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> which extends the generative hybrid tree model. This extension results in a discriminative model that incorporates rich features and allows long-distance dependencies to be captured. The <cite>relaxed hybrid tree</cite> model has achieved the state-of-the-art results on standard benchmark datasets across different languages. Performing semantic parsing under other forms of supervision is also possible. Clarke et al. (2010) proposed a model that learns a semantic parser for answering questions without relying on semantic annotations. Goldwasser et al. (2011) presented a confidence-driven approach to semantic parsing based on self-training.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_6",
  "x": "---------------------------------- **<cite>RELAXED HYBRID TREES</cite>** We briefly discuss our previously proposed <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> in this section. <cite>The model</cite> is a discriminative semantic parsing model which extends the generative hybrid tree model (Lu et al., 2008) . Both systems are publicly available 1 . Let us use m to denote a complete semantic representation, n to denote a complete natural language sentence, and h to denote a complete latent structure that jointly represents both m and n. <cite>The model</cite> defines the conditional probability for observing a (m, h) pair for a given natural language sentence n using a log-linear approach: where \u039b is the set of parameters (weights of features) used by the model. its corresponding semantic representation. Typically, to limit the space of latent structures, certain assumptions have to be made to h. In our work, we assume that h must be from a space consisting of <cite>relaxed hybrid tree</cite> structures <cite>(Lu, 2014)</cite> .",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_7",
  "x": "where \u039b is the set of parameters (weights of features) used by the model. its corresponding semantic representation. Typically, to limit the space of latent structures, certain assumptions have to be made to h. In our work, we assume that h must be from a space consisting of <cite>relaxed hybrid tree</cite> structures <cite>(Lu, 2014)</cite> . The <cite>relaxed hybrid trees</cite> are analogous to the hybrid trees, which was earlier introduced as a generative framework. One major distinction between these two types of representations is that the <cite>relaxed hybrid tree</cite> representations are able to capture unbounded long-distance dependencies in a principled way. Such dependencies were unable to be captured by hybrid tree representations largely due to their generative settings. Figure 1 gives an example of a hybrid tree and a <cite>relaxed hybrid tree</cite> representation encoding the sentence w 1 w 2 w 3 w 4 w 5 w 6 w 7 w 8 w 9 w 10 and the se- In the hybrid tree structure, each word is strictly associated with a semantic unit. For example the word w 3 is associated with the semantic unit m b .",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_8",
  "x": "Such dependencies were unable to be captured by hybrid tree representations largely due to their generative settings. Figure 1 gives an example of a hybrid tree and a <cite>relaxed hybrid tree</cite> representation encoding the sentence w 1 w 2 w 3 w 4 w 5 w 6 w 7 w 8 w 9 w 10 and the se- In the hybrid tree structure, each word is strictly associated with a semantic unit. For example the word w 3 is associated with the semantic unit m b . In the <cite>relaxed hybrid tree</cite>, however, each word is not only directly associated with exactly one semantic unit m, but also indirectly associated with all other semantic units that are predecessors of m. For example, the word w 3 now is directly associated with m b , but is also indirectly associated with m a . These indirect associations allow the longdistance dependencies to be captured. Both the hybrid tree and <cite>relaxed hybrid tree</cite> models define patterns at each level of their latent structure which specify how the words and child semantic units are organized at each level. For example, within the semantic unit m a , we have a pattern wXw which states that we first have words that are directly associated with m a , followed by some words covered by its first child semantic unit, then another sequence of words directly associated with m a . ----------------------------------",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_9",
  "x": "In the hybrid tree structure, each word is strictly associated with a semantic unit. For example the word w 3 is associated with the semantic unit m b . In the <cite>relaxed hybrid tree</cite>, however, each word is not only directly associated with exactly one semantic unit m, but also indirectly associated with all other semantic units that are predecessors of m. For example, the word w 3 now is directly associated with m b , but is also indirectly associated with m a . These indirect associations allow the longdistance dependencies to be captured. Both the hybrid tree and <cite>relaxed hybrid tree</cite> models define patterns at each level of their latent structure which specify how the words and child semantic units are organized at each level. For example, within the semantic unit m a , we have a pattern wXw which states that we first have words that are directly associated with m a , followed by some words covered by its first child semantic unit, then another sequence of words directly associated with m a . ---------------------------------- **LIMITATIONS** One important difference between the hybrid tree representations and the <cite>relaxed hybrid tree</cite> representations is the exclusion of the pattern X in the latter.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_10",
  "x": "In the <cite>relaxed hybrid tree</cite>, however, each word is not only directly associated with exactly one semantic unit m, but also indirectly associated with all other semantic units that are predecessors of m. For example, the word w 3 now is directly associated with m b , but is also indirectly associated with m a . These indirect associations allow the longdistance dependencies to be captured. Both the hybrid tree and <cite>relaxed hybrid tree</cite> models define patterns at each level of their latent structure which specify how the words and child semantic units are organized at each level. For example, within the semantic unit m a , we have a pattern wXw which states that we first have words that are directly associated with m a , followed by some words covered by its first child semantic unit, then another sequence of words directly associated with m a . ---------------------------------- **LIMITATIONS** One important difference between the hybrid tree representations and the <cite>relaxed hybrid tree</cite> representations is the exclusion of the pattern X in the latter. This ensured <cite>relaxed hybrid trees</cite> with an infinite number of nodes were not considered <cite>(Lu, 2014)</cite> when computing the denominator term of Equation 1. In <cite>relaxed hybrid tree</cite>, H(n, m) was implemented as a packed forest representation for exponentially many possible <cite>relaxed hybrid trees</cite> where pattern X was excluded.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_11",
  "x": "These indirect associations allow the longdistance dependencies to be captured. Both the hybrid tree and <cite>relaxed hybrid tree</cite> models define patterns at each level of their latent structure which specify how the words and child semantic units are organized at each level. For example, within the semantic unit m a , we have a pattern wXw which states that we first have words that are directly associated with m a , followed by some words covered by its first child semantic unit, then another sequence of words directly associated with m a . ---------------------------------- **LIMITATIONS** One important difference between the hybrid tree representations and the <cite>relaxed hybrid tree</cite> representations is the exclusion of the pattern X in the latter. This ensured <cite>relaxed hybrid trees</cite> with an infinite number of nodes were not considered <cite>(Lu, 2014)</cite> when computing the denominator term of Equation 1. In <cite>relaxed hybrid tree</cite>, H(n, m) was implemented as a packed forest representation for exponentially many possible <cite>relaxed hybrid trees</cite> where pattern X was excluded. By allowing pattern X, we allow certain semantic units with no natural language word counter- part to exist in the joint <cite>relaxed hybrid tree</cite> representation.",
  "y": "motivation differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_13",
  "x": "In <cite>relaxed hybrid tree</cite>, H(n, m) was implemented as a packed forest representation for exponentially many possible <cite>relaxed hybrid trees</cite> where pattern X was excluded. By allowing pattern X, we allow certain semantic units with no natural language word counter- part to exist in the joint <cite>relaxed hybrid tree</cite> representation. This may lead to possible <cite>relaxed hybrid tree</cite> representations consisting of an infinite number of internal nodes (semantic units), as seen in Figure 3 (b) . When pattern X is allowed, both m a and m b are not directly associated with any natural language word, so we are able to further insert arbitrarily many (compatible) semantic units between the two units m a and m b while the resulting <cite>relaxed hybrid tree</cite> remains valid. Therefore we can construct a <cite>relaxed hybrid tree</cite> representation that contains the given natural language sentence w 1 w 2 with an infinite number of nodes. This issue essentially prevents us from computing the denominator term of Equation 1 since it involves an infinite number of possible m and h . To eliminate <cite>relaxed hybrid trees</cite> consisting of an infinite number of nodes, pattern X is disallowed in the <cite>relaxed hybrid trees</cite> model <cite>(Lu, 2014)</cite> . However, disallowing pattern X has led to other issues. Specifically, for certain semanticssentence pairs, it is not possible to find <cite>relaxed hybrid trees</cite> that jointly represent them.",
  "y": "motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_14",
  "x": "This may lead to possible <cite>relaxed hybrid tree</cite> representations consisting of an infinite number of internal nodes (semantic units), as seen in Figure 3 (b) . When pattern X is allowed, both m a and m b are not directly associated with any natural language word, so we are able to further insert arbitrarily many (compatible) semantic units between the two units m a and m b while the resulting <cite>relaxed hybrid tree</cite> remains valid. Therefore we can construct a <cite>relaxed hybrid tree</cite> representation that contains the given natural language sentence w 1 w 2 with an infinite number of nodes. This issue essentially prevents us from computing the denominator term of Equation 1 since it involves an infinite number of possible m and h . To eliminate <cite>relaxed hybrid trees</cite> consisting of an infinite number of nodes, pattern X is disallowed in the <cite>relaxed hybrid trees</cite> model <cite>(Lu, 2014)</cite> . However, disallowing pattern X has led to other issues. Specifically, for certain semanticssentence pairs, it is not possible to find <cite>relaxed hybrid trees</cite> that jointly represent them. In the example semantics-sentence pair given in Figure 3 (a) , it is not possible to find any <cite>relaxed hybrid tree</cite> that contains both the sentence and the semantics since each semantic unit which takes one argument must be associated with at least one word. On the other hand, it is still possible to find a hybrid tree representation for both the sentence and the semantics where pattern X is allowed (see Figure 3 (c) ).",
  "y": "motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_17",
  "x": "In practice, we can alleviate this issue by extending the lengths of the sentences. For example, we can append the special beginning-of-sentence symbol s and end-of-sentence symbol /s to all sentences to increase their lengths, allowing the <cite>relaxed hybrid trees</cite> to be constructed for certain sentence-semantics pairs with short sentences. However, such an approach does not resolve the theoretical limitation of <cite>the model</cite>. ---------------------------------- **CONSTRAINED SEMANTIC FORESTS** To address this limitation, we allow pattern X to be included when building our new discriminative semantic parsing model. However, as mentioned above, doing so will lead to latent structures (<cite>relaxed hybrid tree</cite> representations) of infinite heights. To resolve such an issue, we instead add an additional constraint -limiting the height of a semantic representation to a fixed constant c, where c is larger than the maximum height of all the trees appearing in the training set. Table 1 summarizes the list of patterns that our model considers.",
  "y": "motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_18",
  "x": "Table 1 summarizes the list of patterns that our model considers. This is essentially the same as those considered by the hybrid tree model. Our new objective function is as follows: where M refers to the set of all possible semantic trees whose heights are less than or equal to c, and H (n, m ) refers to the set of possible <cite>relaxed hybrid tree</cite> representations where the pattern X is allowed. The main challenge now becomes the computation of the denominator term in Equation 2, as the set M is still very large. To properly handle all such semantic trees in an efficient way, we introduce a constrained semantic forest (CSF) representation of M here. Such a constrained semantic forest is a packed forest representation of exponentially many possible unique semantic trees, where we set the height of the forest to c. By contrast, it was not possible in our previous <cite>relaxed hybrid tree</cite> model to introduce such a compact representation over all possible semantic trees. In our previous model's implementation, we directly constructed for each sentence n a different compact representation over all possible <cite>relaxed hybrid trees</cite> containing n. Setting the maximum height to c effectively guarantees that all semantic trees contained in the constrained semantic forest have a height no greater than c. We then constructed the (exponentially many) <cite>relaxed hybrid tree</cite> representations based on the constrained semantic forest M and each input sentence n. We used a single packed forest representation to represent all such <cite>relaxed hybrid tree</cite> representations.",
  "y": "uses motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_19",
  "x": "This is essentially the same as those considered by the hybrid tree model. Our new objective function is as follows: where M refers to the set of all possible semantic trees whose heights are less than or equal to c, and H (n, m ) refers to the set of possible <cite>relaxed hybrid tree</cite> representations where the pattern X is allowed. The main challenge now becomes the computation of the denominator term in Equation 2, as the set M is still very large. To properly handle all such semantic trees in an efficient way, we introduce a constrained semantic forest (CSF) representation of M here. Such a constrained semantic forest is a packed forest representation of exponentially many possible unique semantic trees, where we set the height of the forest to c. By contrast, it was not possible in our previous <cite>relaxed hybrid tree</cite> model to introduce such a compact representation over all possible semantic trees. In our previous model's implementation, we directly constructed for each sentence n a different compact representation over all possible <cite>relaxed hybrid trees</cite> containing n. Setting the maximum height to c effectively guarantees that all semantic trees contained in the constrained semantic forest have a height no greater than c. We then constructed the (exponentially many) <cite>relaxed hybrid tree</cite> representations based on the constrained semantic forest M and each input sentence n. We used a single packed forest representation to represent all such <cite>relaxed hybrid tree</cite> representations. This allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in <cite>(Lu, 2014)</cite> .",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_20",
  "x": "Such a constrained semantic forest is a packed forest representation of exponentially many possible unique semantic trees, where we set the height of the forest to c. By contrast, it was not possible in our previous <cite>relaxed hybrid tree</cite> model to introduce such a compact representation over all possible semantic trees. In our previous model's implementation, we directly constructed for each sentence n a different compact representation over all possible <cite>relaxed hybrid trees</cite> containing n. Setting the maximum height to c effectively guarantees that all semantic trees contained in the constrained semantic forest have a height no greater than c. We then constructed the (exponentially many) <cite>relaxed hybrid tree</cite> representations based on the constrained semantic forest M and each input sentence n. We used a single packed forest representation to represent all such <cite>relaxed hybrid tree</cite> representations. This allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in <cite>(Lu, 2014)</cite> . Optimization of the model parameters were done by using L-BFGS (Liu and Nocedal, 1989) , where the gradients were computed efficiently using an analogous dynamic programming algorithm. ---------------------------------- **EXPERIMENTS** Our experiments were conducted on the publicly available multilingual GeoQuery dataset. Various previous works on semantic parsing used this dataset for evaluations (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Jones et al., 2012) .",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_21",
  "x": "Setting the maximum height to c effectively guarantees that all semantic trees contained in the constrained semantic forest have a height no greater than c. We then constructed the (exponentially many) <cite>relaxed hybrid tree</cite> representations based on the constrained semantic forest M and each input sentence n. We used a single packed forest representation to represent all such <cite>relaxed hybrid tree</cite> representations. This allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in <cite>(Lu, 2014)</cite> . Optimization of the model parameters were done by using L-BFGS (Liu and Nocedal, 1989) , where the gradients were computed efficiently using an analogous dynamic programming algorithm. ---------------------------------- **EXPERIMENTS** Our experiments were conducted on the publicly available multilingual GeoQuery dataset. Various previous works on semantic parsing used this dataset for evaluations (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Jones et al., 2012) . The dataset consists of 880 natural language sentences where each sentence is coupled with a formal tree-structured semantic representation. The early version of this dataset was annotated with English only (Wong and Mooney, 2006; Kate and Mooney, 2006) , and Jones et al.",
  "y": "similarities"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_22",
  "x": "(2012) released a version that is annotated with three additional languages: German, Greek and Thai. To make our system directly comparable to previous works, we used the same train/test split used in those works (Jones et al., 2012; <cite>Lu, 2014</cite>) for evaluation. We also followed the standard approach for evaluating the correctness of an output semantic representation from our system. Specifically, we used a standard script to construct Prolog queries based on the outputs, and used the queries to retrieve answers from the GeoQuery database. Following previous works, we regarded an output semantic representation as correct if and only if it returned the same answers as the gold standard (Jones et al., 2012; <cite>Lu, 2014</cite>) . The results of our system as well as those of several previous systems are given in Table 2 . We compared our system's performance against those of several previous works. The WASP system (Wong and Mooney, 2006 ) is based on statistical machine translation technique while the HY-BRIDTREE+ system (Lu et al., 2008 ) is based on the generative hybrid tree model augmented with a discriminative re-ranking stage where certain global features are used. UBL-S (Kwiatkowski et al., 2010 ) is a CCG-based semantic parsing system.",
  "y": "uses"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_23",
  "x": "Following previous works, we regarded an output semantic representation as correct if and only if it returned the same answers as the gold standard (Jones et al., 2012; <cite>Lu, 2014</cite>) . The results of our system as well as those of several previous systems are given in Table 2 . We compared our system's performance against those of several previous works. The WASP system (Wong and Mooney, 2006 ) is based on statistical machine translation technique while the HY-BRIDTREE+ system (Lu et al., 2008 ) is based on the generative hybrid tree model augmented with a discriminative re-ranking stage where certain global features are used. UBL-S (Kwiatkowski et al., 2010 ) is a CCG-based semantic parsing system. TREETRANS (Jones et al., 2012) is the system based on tree transducers. <cite>RHT</cite> <cite>(Lu, 2014)</cite> is the discriminative semantic parsing system based on <cite>relaxed hybrid trees</cite>. In practice, we set c (the maximum height of a semantic representation) to 20 in our experi- ments, which we determined based on the heights of the semantic trees that appear in the training data. Results showed that our system consistently yielded higher results than all the previous systems, including our state-of-the-art <cite>relaxed hybrid tree</cite> system (<cite>the full model</cite>, when all the features are used), in terms of both accuracy score and F 1 -measure.",
  "y": "uses"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_24",
  "x": "Following previous works, we regarded an output semantic representation as correct if and only if it returned the same answers as the gold standard (Jones et al., 2012; <cite>Lu, 2014</cite>) . The results of our system as well as those of several previous systems are given in Table 2 . We compared our system's performance against those of several previous works. The WASP system (Wong and Mooney, 2006 ) is based on statistical machine translation technique while the HY-BRIDTREE+ system (Lu et al., 2008 ) is based on the generative hybrid tree model augmented with a discriminative re-ranking stage where certain global features are used. UBL-S (Kwiatkowski et al., 2010 ) is a CCG-based semantic parsing system. TREETRANS (Jones et al., 2012) is the system based on tree transducers. <cite>RHT</cite> <cite>(Lu, 2014)</cite> is the discriminative semantic parsing system based on <cite>relaxed hybrid trees</cite>. In practice, we set c (the maximum height of a semantic representation) to 20 in our experi- ments, which we determined based on the heights of the semantic trees that appear in the training data. Results showed that our system consistently yielded higher results than all the previous systems, including our state-of-the-art <cite>relaxed hybrid tree</cite> system (<cite>the full model</cite>, when all the features are used), in terms of both accuracy score and F 1 -measure.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_25",
  "x": "In practice, we set c (the maximum height of a semantic representation) to 20 in our experi- ments, which we determined based on the heights of the semantic trees that appear in the training data. Results showed that our system consistently yielded higher results than all the previous systems, including our state-of-the-art <cite>relaxed hybrid tree</cite> system (<cite>the full model</cite>, when all the features are used), in terms of both accuracy score and F 1 -measure. We would like to highlight two potential advantages of our new model over the old <cite>RHT</cite> model. First, our model is able to handle certain sentence-semantics pairs which could not be handled by <cite>RHT</cite> during both training and evaluation as discussed in Section 3.1. Second, our model considers the additional pattern X and therefore has the capability to capture more accurate dependencies between the words and semantic units. We note that in our experiments we used a small subset of the features used by our <cite>relaxed hybrid tree</cite> work. Specifically, we did not use any long-distance features, and also did not use any character-level features. As we have mentioned in <cite>(Lu, 2014)</cite> , although the <cite>RHT</cite> model is able to capture unbounded long-distance dependencies, for certain languages such as German such longdistance features appeared to be detrimental to the performance of the system (<cite>Lu, 2014</cite>, Table  4 ). Here in this work, we only used simple unigram features (concatenation of a semantic unit and an individual word that appears directly below that unit in the joint representation), pattern features (concatenation of a semantic unit and the pattern below that unit) as well as transition features (concatenation of two semantic units that form a parent-child relationship) described in <cite>(Lu, 2014)</cite> .",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_26",
  "x": "Results showed that our system consistently yielded higher results than all the previous systems, including our state-of-the-art <cite>relaxed hybrid tree</cite> system (<cite>the full model</cite>, when all the features are used), in terms of both accuracy score and F 1 -measure. We would like to highlight two potential advantages of our new model over the old <cite>RHT</cite> model. First, our model is able to handle certain sentence-semantics pairs which could not be handled by <cite>RHT</cite> during both training and evaluation as discussed in Section 3.1. Second, our model considers the additional pattern X and therefore has the capability to capture more accurate dependencies between the words and semantic units. We note that in our experiments we used a small subset of the features used by our <cite>relaxed hybrid tree</cite> work. Specifically, we did not use any long-distance features, and also did not use any character-level features. As we have mentioned in <cite>(Lu, 2014)</cite> , although the <cite>RHT</cite> model is able to capture unbounded long-distance dependencies, for certain languages such as German such longdistance features appeared to be detrimental to the performance of the system (<cite>Lu, 2014</cite>, Table  4 ). Here in this work, we only used simple unigram features (concatenation of a semantic unit and an individual word that appears directly below that unit in the joint representation), pattern features (concatenation of a semantic unit and the pattern below that unit) as well as transition features (concatenation of two semantic units that form a parent-child relationship) described in <cite>(Lu, 2014)</cite> . While additional features could potentially lead to better results, using simpler features would make our model more compact and more interpretable.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_27",
  "x": "TREETRANS (Jones et al., 2012) is the system based on tree transducers. <cite>RHT</cite> <cite>(Lu, 2014)</cite> is the discriminative semantic parsing system based on <cite>relaxed hybrid trees</cite>. In practice, we set c (the maximum height of a semantic representation) to 20 in our experi- ments, which we determined based on the heights of the semantic trees that appear in the training data. Results showed that our system consistently yielded higher results than all the previous systems, including our state-of-the-art <cite>relaxed hybrid tree</cite> system (<cite>the full model</cite>, when all the features are used), in terms of both accuracy score and F 1 -measure. We would like to highlight two potential advantages of our new model over the old <cite>RHT</cite> model. First, our model is able to handle certain sentence-semantics pairs which could not be handled by <cite>RHT</cite> during both training and evaluation as discussed in Section 3.1. Second, our model considers the additional pattern X and therefore has the capability to capture more accurate dependencies between the words and semantic units. We note that in our experiments we used a small subset of the features used by our <cite>relaxed hybrid tree</cite> work. Specifically, we did not use any long-distance features, and also did not use any character-level features.",
  "y": "uses"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_28",
  "x": "We would like to highlight two potential advantages of our new model over the old <cite>RHT</cite> model. First, our model is able to handle certain sentence-semantics pairs which could not be handled by <cite>RHT</cite> during both training and evaluation as discussed in Section 3.1. Second, our model considers the additional pattern X and therefore has the capability to capture more accurate dependencies between the words and semantic units. We note that in our experiments we used a small subset of the features used by our <cite>relaxed hybrid tree</cite> work. Specifically, we did not use any long-distance features, and also did not use any character-level features. As we have mentioned in <cite>(Lu, 2014)</cite> , although the <cite>RHT</cite> model is able to capture unbounded long-distance dependencies, for certain languages such as German such longdistance features appeared to be detrimental to the performance of the system (<cite>Lu, 2014</cite>, Table  4 ). Here in this work, we only used simple unigram features (concatenation of a semantic unit and an individual word that appears directly below that unit in the joint representation), pattern features (concatenation of a semantic unit and the pattern below that unit) as well as transition features (concatenation of two semantic units that form a parent-child relationship) described in <cite>(Lu, 2014)</cite> . While additional features could potentially lead to better results, using simpler features would make our model more compact and more interpretable. We summarized in Table 3 the number of features used in both the previous <cite>RHT</cite> system and our system across four different languages.",
  "y": "extends differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_29",
  "x": "As we have mentioned in <cite>(Lu, 2014)</cite> , although the <cite>RHT</cite> model is able to capture unbounded long-distance dependencies, for certain languages such as German such longdistance features appeared to be detrimental to the performance of the system (<cite>Lu, 2014</cite>, Table  4 ). Here in this work, we only used simple unigram features (concatenation of a semantic unit and an individual word that appears directly below that unit in the joint representation), pattern features (concatenation of a semantic unit and the pattern below that unit) as well as transition features (concatenation of two semantic units that form a parent-child relationship) described in <cite>(Lu, 2014)</cite> . While additional features could potentially lead to better results, using simpler features would make our model more compact and more interpretable. We summarized in Table 3 the number of features used in both the previous <cite>RHT</cite> system and our system across four different languages. It can be seen that our system only required about 2-3% of the Table 3 : Number of features involved for both the <cite>RHT</cite> system and our new system using constrained semantic forests, across four different languages. features used in the previous system. We also note that the training time for our model is longer than that of the <cite>relaxed hybrid tree</cite> model since the space for H (n, m ) is now much larger than the space for H(n, m ). In practice, to make the overall training process faster, we implemented a parallel version of the original <cite>RHT</cite> algorithm. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_30",
  "x": "While additional features could potentially lead to better results, using simpler features would make our model more compact and more interpretable. We summarized in Table 3 the number of features used in both the previous <cite>RHT</cite> system and our system across four different languages. It can be seen that our system only required about 2-3% of the Table 3 : Number of features involved for both the <cite>RHT</cite> system and our new system using constrained semantic forests, across four different languages. features used in the previous system. We also note that the training time for our model is longer than that of the <cite>relaxed hybrid tree</cite> model since the space for H (n, m ) is now much larger than the space for H(n, m ). In practice, to make the overall training process faster, we implemented a parallel version of the original <cite>RHT</cite> algorithm. ---------------------------------- **CONCLUSION** In this work, we presented an improved discriminative approach to semantic parsing.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_31",
  "x": "We summarized in Table 3 the number of features used in both the previous <cite>RHT</cite> system and our system across four different languages. It can be seen that our system only required about 2-3% of the Table 3 : Number of features involved for both the <cite>RHT</cite> system and our new system using constrained semantic forests, across four different languages. features used in the previous system. We also note that the training time for our model is longer than that of the <cite>relaxed hybrid tree</cite> model since the space for H (n, m ) is now much larger than the space for H(n, m ). In practice, to make the overall training process faster, we implemented a parallel version of the original <cite>RHT</cite> algorithm. ---------------------------------- **CONCLUSION** In this work, we presented an improved discriminative approach to semantic parsing. Our approach does not have the theoretical limitation associated with our previous state-of-the-art approach.",
  "y": "uses"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_0",
  "x": "Systems using the training set of 50K crowdsourced utterances from the E2E task achieved high semantic correctness, e.g. the BLEU score for our best system on the dev set was 0.72 [6] . However in the best case these models can only reproduce the 1 http://www.macs.hw.ac.uk/InteractionLab/E2E/ style of the training data, and in actuality the outputs have reduced stylistic variation, because when particular stylistic variations are less frequent, they are treated similarly to noise. Browns Cambridge is a pub, also it is a moderately priced italian place near Adriatic, also it is family friendly, you know and it's in the city centre. In subsequent work, we showed that we could augment the E2E training data with synthetically generated stylistic variants and train a neural generator to reproduce these variants, however the models can still only generate what they have seen in training<cite> [5]</cite> . Here, instead, we explore whether a model that is trained to achieve a single stylistic personality target can produce outputs that combine stylistic targets, to yield a novel style that is significantly different than what was seen in training, while still maintaining high semantic correctness. We first train each stylistic model with a single latent variable for supervision, for five different personality models, or voices, based on the Big Five theory of personality, namely the personality trait styles of EXTRAVERT, AGREEABLE, DISAGREEABLE, CONSCI-ENTIOUS, and UNCONSCIENTIOUS. Then, at generation time, we provide the model with combinations of the stylistic variables, i.e. we instruct the NNLG to generate multivoice outputs that combine EXTRAVERT with DISAGREEABLE, where such combined outputs never occurred in the training data. We first describe how we set up our dataset and neural models in Section 2, and then present our results in Section 3. We evaluate the multivoice outputs for both semantic fidelity and for similarities to and differences from the linguistic features that characterize the original training style.",
  "y": "background"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_1",
  "x": "The PERSONAGE corpus<cite> [5]</cite> provides a controlled environment for testing different models of neural generation and style generation. It consists of 88,500 restaurant domain utterances whose style varies according to models of personality, which were generated by an existing statistical NLG engine that has the capability of manipulating 67 different stylistic parameters [9] . Table 2 shows sample utterances that are output for the singlevoice models and for each of our multi- Table 2 : MultiVoice generation output and comparable singlevoice outputs for DISAGREEABLE, EXTRAVERT and CONSCIENTIOUS for the meaning representation in Figure 1 . We count the frequency of periods (Period Agg.) and expletives (Explet. Prag) for multivoice models that utilize DISAGREEABLE). voice models (described below) for the same MR. Each output corresponding to each single voice personality is controlled by a set of sentence planning parameters that vary for each personality. These parameters are discussed in Section 3 when we evaluate stylistic fidelity. What is important to note here is that each individual voice represents a distinct stylistic distribution in the training data. The corpus uses the MRs and training/test splits of the E2E Generation Challenge.",
  "y": "uses"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_2",
  "x": "Previous work shows that a simple model trained on the whole corpus of 88,855 utterances produces semantically correct outputs, but with reduced stylistic variation<cite> [5]</cite> , while a model that allocates a variable corresponding to a label for each style learns to reproduce the stylistic variation. This is interesting because each style variable (personality) actually encodes a set of 36 different stylistic parameters and their values: the model learns for example how the DISAGREEABLE personality tends to produce many shorter sentences in the output, as well as learning that it tends to use expletives like damn, e.g. see the outputs based on DISAGREEABLE personality in Table 2 . Model Description. Our NNLG model uses a single token to represent personality encoding, following the use of single language labels used in machine translation and other work on neural generation [10, <cite>5]</cite> . Figure 1 summarizes the model architecture. This model builds on the open-source sequence-tosequence (seq2seq) TGen system [11] , which is implemented in Tensorflow [12] . 2 The system is based on the seq2seq generation method with attention [14, 1<cite>5]</cite> , and uses a sequence of LSTMs [16] for the encoder and decoder, combined with beamsearch and an n-best list reranker for output tuning. The inputs to the model are dialog acts for each system action (such as inform) and a set of attribute slots (such as rating) and their values (such as high for attribute rating). To prepro- 2 We refer the reader to TGen publications [11, 13] for model details.",
  "y": "differences background"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_3",
  "x": "Model Description. Our NNLG model uses a single token to represent personality encoding, following the use of single language labels used in machine translation and other work on neural generation [10, <cite>5]</cite> . Figure 1 summarizes the model architecture. This model builds on the open-source sequence-tosequence (seq2seq) TGen system [11] , which is implemented in Tensorflow [12] . 2 The system is based on the seq2seq generation method with attention [14, 1<cite>5]</cite> , and uses a sequence of LSTMs [16] for the encoder and decoder, combined with beamsearch and an n-best list reranker for output tuning. The inputs to the model are dialog acts for each system action (such as inform) and a set of attribute slots (such as rating) and their values (such as high for attribute rating). To prepro- 2 We refer the reader to TGen publications [11, 13] for model details. cess the corpus of MR/utterance pairs, attributes that take on proper-noun values are delexicalized during training i.e. name and near. We encode personality as an additional dialog act, of type CONVERT with personality as the key and the target personality as the value (see Figure 1) .",
  "y": "uses"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_4",
  "x": "Our model differs from the TO-KEN model used in our previous work<cite> [5]</cite> because it is trained on unsorted inputs to allow us to add multiple CONVERT tags to the MR at generation time. Note that we do not train on multiple personalities, instead, we train one model that uses all the data, where each distinct single personality has a corresponding CONVERT(PERSONALITY = X) in the training instance. At generation time, we generate singlevoice data for all the test MRs (1,390 total realizations, 278 unique MRs, realized for each of 5 personalities). For the multivoice experiments, we generate 2 references per combination of two personalities for each of the 278 test MRs, since the order of the CONVERT tags matters. For a given order, the model produces a single output. We do not combine personalities that are exact opposites such as AGREEABLE and DISAGREEABLE, yielding 8 combinations. The multivoice test set consists of 4,448 total realizations (278 MRs and 8 \u00d7 2 outputs per MR). ---------------------------------- **RESULTS**",
  "y": "differences"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_5",
  "x": "We carefully evaluate the multivoice outputs for both semantic fidelity and for similarities to and differences from the linguistic features that characterize the original training style. We show that contrary to our predictions, the learned models do not always simply interpolate model parameters, but rather produce styles that are distinct, and novel from the personalities they were trained on. ---------------------------------- **INTRODUCTION** Natural language generators for task-oriented dialog should be able to vary the style of the output while still effectively realizing the system dialog actions and their associated semantics. The use of neural natural language generation (NNLG) for training the response generation component of conversational agents promises to simplify the process of producing high quality responses in new domains by relying on the neural architecture to automatically learn how to map an input meaning representation to an output utterance. However, there has been little investigation of NNLGs for dialog that can vary their response style, and we know of no experiments on models that can generate responses that are different in style from those seen during training, while still maintaining semantic fidelity to the input meaning representation. Instead, work on stylistic transfer has focused on tasks where only coarse-grained semantic fidelity is needed, such as controlling the sentiment of the utterance (positive or negative), or the topic or entity under discussion [1, 2, 3] . Consider for example a training instance for the restaurant domain consisting of a meaning representation (MR) from the End-to-End (E2E) Generation Challenge 1 and a sample output from one of our neural generation models in Figure 1 [4, <cite>5]</cite> .",
  "y": "background"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_0",
  "x": "Our result suggests that global learning with beam-search accommodates more complex models with richer features than a local model with greedy search and therefore enables higher accuracies. One interesting aspect of using a global model with beam-search is that it narrows down the contrast between \"local, greedy, transition-based parsing\" and \"global, exhaustive, graph-based parsing\" as exemplified by<cite> McDonald and Nivre (2007)</cite> . On the one hand, global beam-search parsing is more similar to global, exhaustive parsing than local, greedy parsing in the use of global models and non-greedy search. On the other hand, beam-search does not affect the fundamental transition-based parsing process, which allows the use of rich non-local features, and is very different from graph-based parsing. An interesting question is how such differences in models and algorithms affect empirical errors. McDonald and Nivre (2007) make a comparative analysis of local greedy transition-based MaltParser and global near-exhaustive graph-based MSTParser (McDonald and Pereira, 2006) using the CoNLL-X Shared Task data (Buchholz and Marsi, 2006) , showing that the parsers give near identical overall accuracies, but have very different error distributions according to various metrics. While MaltParser is more accurate on frequently occurring short sentences and dependencies, it performs worse on long sentences and dependencies due to search errors. We present empirical studies of the error distribution of global, beam-search transition-based dependency parsing, using ZPar (Zhang and Nivre, 2011) as a representative system. We follow<cite> McDonald and Nivre (2007)</cite> and perform a comparative error analysis of ZPar, MSTParser and MaltParser using the CoNLL-X shared task data.",
  "y": "background"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_1",
  "x": "We present empirical studies of the error distribution of global, beam-search transition-based dependency parsing, using ZPar (Zhang and Nivre, 2011) as a representative system. We follow<cite> McDonald and Nivre (2007)</cite> and perform a comparative error analysis of ZPar, MSTParser and MaltParser using the CoNLL-X shared task data. Our results show that beam-search im-proves the precision on long sentences and dependencies compared to greedy search, while the advantage of transition-based parsing on short dependencies is preserved. Under particular measures, such as precision for arcs at different levels of the trees, ZPar shows characteristics surprisingly similar to MSTParser. ---------------------------------- **ANALYZING THE EFFECT OF GLOBAL LEARNING AND BEAM-SEARCH** In this section we study the effects of global learning and beam-search on the accuracies of transition-based dependency parsing. Our experiments are performed using the Penn Treebank (PTB). We follow the standard approach to split PTB3 into training (sections 2-21), development (section 22) and final testing (section 23) sections.",
  "y": "uses"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_2",
  "x": "**THE PARSERS AND EVALUATION DATA** In this section we study the effect of global learning and beam-search on the error distributions of transition-based dependency parsing. We characterize the errors of ZPar and add it to the error comparison between MaltParser and MSTParser <cite>(McDonald and Nivre, 2007)</cite> . Following<cite> McDonald and Nivre (2007)</cite> we evaluate the parsers on the CoNLL-X Shared Task data (Buchholz and Marsi, 2006) , which include training and test sentences for 13 different languages. For each parser, we conjoin the outputs for all 13 languages in the same way as<cite> McDonald and Nivre (2007)</cite> , and calculate error distributions over the aggregated output. Accuracies are measured using the labeled attached score (LAS) evaluation metric, which is defined as the percentage of words (excluding punctuation) that are assigned both the correct head word and the correct arc label. To handle non-projectivity, pseudo-projective parsing (Nivre and Nilsson, 2005 ) is applied to ZPar and MaltParser, transforming non-projective trees into pseudo-projective trees in the training data, and post-processing pseudo-projective outputs by the parser to transform them into non-projective trees. MSTParser produces non-projective trees from projective trees by scorebased rearrangements of arcs. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_3",
  "x": "Yet another evidence for the support of more complex models by global learning and beamsearch is the work of Bohnet and Nivre (2012) , where non-projective parsing using online reordering (Nivre, 2009 ) and rich features led to significant improvements over greedy search (Nivre, 2009) , achieving state-of-the-art on a range of typologically diverse languages. 3 Characterizing the errors ---------------------------------- **THE PARSERS AND EVALUATION DATA** In this section we study the effect of global learning and beam-search on the error distributions of transition-based dependency parsing. We characterize the errors of ZPar and add it to the error comparison between MaltParser and MSTParser <cite>(McDonald and Nivre, 2007)</cite> . Following<cite> McDonald and Nivre (2007)</cite> we evaluate the parsers on the CoNLL-X Shared Task data (Buchholz and Marsi, 2006) , which include training and test sentences for 13 different languages. For each parser, we conjoin the outputs for all 13 languages in the same way as<cite> McDonald and Nivre (2007)</cite> , and calculate error distributions over the aggregated output. Accuracies are measured using the labeled attached score (LAS) evaluation metric, which is defined as the percentage of words (excluding punctuation) that are assigned both the correct head word and the correct arc label.",
  "y": "uses"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_4",
  "x": "We characterize the errors of ZPar and add it to the error comparison between MaltParser and MSTParser <cite>(McDonald and Nivre, 2007)</cite> . Following<cite> McDonald and Nivre (2007)</cite> we evaluate the parsers on the CoNLL-X Shared Task data (Buchholz and Marsi, 2006) , which include training and test sentences for 13 different languages. For each parser, we conjoin the outputs for all 13 languages in the same way as<cite> McDonald and Nivre (2007)</cite> , and calculate error distributions over the aggregated output. Accuracies are measured using the labeled attached score (LAS) evaluation metric, which is defined as the percentage of words (excluding punctuation) that are assigned both the correct head word and the correct arc label. To handle non-projectivity, pseudo-projective parsing (Nivre and Nilsson, 2005 ) is applied to ZPar and MaltParser, transforming non-projective trees into pseudo-projective trees in the training data, and post-processing pseudo-projective outputs by the parser to transform them into non-projective trees. MSTParser produces non-projective trees from projective trees by scorebased rearrangements of arcs. ---------------------------------- **ERROR DISTRIBUTIONS** We take a range of different perspectives to characterize the errors of ZPar, comparing them with those of MaltParser and MSTParser by measuring the accuracies against various types of metrics, including the size of the sentences and dependency arcs, the distance to the root of the dependency tree, and the number of siblings.",
  "y": "uses"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_5",
  "x": "One possible reason is that arcs near the bottom of the tree require comparatively fewer shift-reduce actions to build, and are therefore less prone to the propagation of search errors. Another important reason, as pointed out by<cite> McDonald and Nivre (2007)</cite> , is the default single-root mechanism by MaltParser: all words that have not been attached as a modifier when the shift-reduce process finishes are attached as modifiers to the pseudo-root. Although the vast majority of sentences have only one root-modifier, there is no global control for the number of root-modifiers in the greedy shift-reduce process, and each action is made locally and independently. As a result, MaltParser tends to over-predict root modifiers, leading to the comparatively low precision. Surprisingly, the precision curve of ZPar is much more similar to that of MSTParser than that of MaltParser, although ZPar is based on the same shift-reduce parsing process, and even has a similar default single-root mechanism as MaltParser. This result is perhaps the most powerful demonstration of the effect of global learning and beam-search compared to local learning and greedy search. The model which scores whole sequences of shift-reduce actions, plus the reduction of search error propagation, lead to significantly reduced over-prediction of rootmodifiers. In addition, rich features used by ZPar, such as the valency (number of modifiers for a head) and set of modifier labels for a head, can also be useful in reducing over-prediction of modifiers. Because of these, ZPar effectively pushes the predictions of difficult arcs down the tree, which is exactly the behavior of MSTParser.",
  "y": "background"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_6",
  "x": "The recall curves of the three parsers are similar, with ZPar having higher recall than MSTParser and MaltParser, particularly when the dependency size is greater than 2. This shows that particular gold-standard dependencies are hard for all parsers to build, but ZPar is better in recovering hard gold dependencies probably due to its rich features. To take another perspective, we compare the performance of the three parsers at different levels of a dependency tree by measuring accuracies for arcs relative to their distance to the root. Here the distance of an arc to the root is defined as the number of arcs in the path from the root to the modifier in the arc. Figure 4 shows the precision and recall of each system for arcs of varying distances to the root. Here the precision of MaltParser and MSTParser is very different, with MaltParser being more precise for arcs nearer to the leaves, but less precise for those nearer to the root. One possible reason is that arcs near the bottom of the tree require comparatively fewer shift-reduce actions to build, and are therefore less prone to the propagation of search errors. Another important reason, as pointed out by<cite> McDonald and Nivre (2007)</cite> , is the default single-root mechanism by MaltParser: all words that have not been attached as a modifier when the shift-reduce process finishes are attached as modifiers to the pseudo-root. Although the vast majority of sentences have only one root-modifier, there is no global control for the number of root-modifiers in the greedy shift-reduce process, and each action is made locally and independently.",
  "y": "similarities background"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_0",
  "x": "As a result, nearly all sense-tagged corpora in wide-spread use are created using trained annotators (Hovy et al., 2006; Passonneau et al., 2010) , which results in a knowledge acquisition bottleneck for training systems that require sense labels (Gale et al., 1992) . In other NLP areas, this bottleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turk (MTurk), a task commonly referred to as crowdsourcing. Recently, several works have proposed gathering sense annotations using crowdsourcing (Snow et al., 2008; Biemann and Nygaard, 2010; Passonneau et al., 2012b; Rumshisky et al., 2012) . However, these methods produce sense labels that are different from the commonly used sense inventories such as WordNet (Fellbaum, 1998) or OntoNotes (Hovy et al., 2006) . Furthermore, while Passonneau et al. (2012b) did use WordNet sense labels, they found the quality was well below that of trained experts. We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotation methodology itself, and (2) the restriction to single sense assignment. First, the choice in sense inventory plays an important role in gathering high-quality annotations; fine-grained inventories such as WordNet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010) . However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V\u00e9ronis, 1998; Murray and Green, 2004; <cite>Erk et al., 2009</cite>; Passonneau et al., 2012b) . Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question: if allowed to make labeling ambiguity explicit, will annotators agree?",
  "y": "background motivation"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_1",
  "x": "Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question: if allowed to make labeling ambiguity explicit, will annotators agree? Furthermore, we adopt the goal of<cite> Erk et al. (2009)</cite> , which enabled annotators to weight each sense by its applicability to the given context, thereby quantifying the ambiguity. This paper provides the following contributions. First, we demonstrate that the choice in annotation setup can significantly improve IAA and that the labels of untrained workers follow consistent patterns that enable creating high quality labeling from their aggregate. Second, we find that the sense labeling from crowdsourcing matches performance with annotators in a controlled setting. ---------------------------------- **RELATED WORK** Given the potential utility of a sense-labeled corpus, multiple studies have examined how to efficiently gather high quality sense annotations. Snow et al. (2008) had MTurk workers, referred to as Turkers, disambiguate uses of \"president.\" While they reported extremely high IAA (0.952), their analysis was only performed on a single word.",
  "y": "similarities"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_3",
  "x": "They highlight ambiguous and polysemous usages as a notable source of errors, which the present work directly addresses. In the most related work, Passonneau et al. (2012b) had Turkers annotate contexts using one or more senses, with the requirement that a worker labels all contexts. While they found that agreement between all workers was low, their annotations could be combined using the GLAD model (Whitehill et al., 2000) to obtain good performance, though not as good as trained annotators. ---------------------------------- **ANNOTATION METHODOLOGIES** We consider three methodologies for gathering sense labels: (1) the methodology of<cite> Erk et al. (2009)</cite> for gathering weighted labels, (2) a multistage strategy that uses both binary and Likert ratings, and (3) MaxDiff, a paired choice format. Likert Ratings Likert rating scales provide the most direct way of gathering weighted sense labels; Turkers are presented with all senses of a word and then asked to rate each on a numeric scale. We adopt the annotation guidelines of<cite> Erk et al. (2009)</cite> which used a five-point scale, ranging from 1 to 5, indicating the sense does not apply or that it matches the contextual usage exactly, respectively. Select and Rate Recent efforts in crowdsourcing have proposed multi-stage processes for accomplishing complex tasks, where efforts by one group of workers are used to create new subtasks for other workers to complete (Bernstein et al., 2010; Kittur et al., 2011; Kulkarni et al., 2012) .",
  "y": "uses"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_4",
  "x": "Second, while the present study analyzes words with 4-8 senses, we are ultimately interested in annotating highly polysemous words with tens of senses, which could present a significant cognitive burden for an annotator to rate concurrently. Here, the Select stage can potentially reduce the number of senses presented, leading to less cognitive burden in the Rate stage. Furthermore, as a pragmatic benefit, removing inapplicable senses reduces the visual space required for displaying the questions on the MTurk platform, which can improve annotation throughput. MaxDiff MaxDiff is an alternative to scale-based ratings in which Turkers are presented with a only subset of all of a word's senses and then asked to select (1) the sense option that best matches the mean-add.v ask.v win.v argument.n interest.n paper.n different.a important.a<cite> Erk et al. (2009)</cite> ing in the example context and (2) the sense option that least matches (Louviere, 1991) . In our setting, we presented three options at a time for words with fewer than seven senses, and four options for those with seven senses. For a single context, multiple subsets of the senses are presented and then their relative ranking is used to produce the numeric rating. The final applicability ratings were produced using a modification of the counting procedure of Orme (2009) . First, all sense ratings are computed as the number of times the sense was rated best minus the number of times rated least. Second, all negativelyrated senses are assigned score of 1, and all positively ratings are normalized to be (1, 5].",
  "y": "uses"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_5",
  "x": "Second, many studies using crowdsourcing combine the results into a single answer, thereby leveraging the wisdom of the crowds (Surowiecki, 2005) to smooth over inconsistencies in the data. Therefore, in the second experiment, we evaluate different methods of combining Turker responses into a single sense labeling, referred to as an aggregate labeling, and comparing that with the reference labeling. Third, we measure the replicability of the Turker annotations (Kilgarriff, 1999 ) using a sampling methodology. Two equally-sized sets of Turker annotations are created by randomly sampling without replacement from the full set of annotations for each item. IAA is calculated between the aggregate labelings computed from each set. This sampling is repeated 50 times and we report the mean IAA as a measure of the expected degree of replicability when annotating using different groups of Turkers. For the reference sense labeling, we use a subset of the GWS dataset of<cite> Erk et al. (2009)</cite> , where three annotators rated 50 instances each for eight words. For clarity, we refer to these individuals as the GWS annotators. Given a word usage in a sentence, GWS annotators rated the applicability of all WordNet 3.0 senses using the same Likert scale as described in Section 3.",
  "y": "uses"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_0",
  "x": "Traditional methods rely on linguistic or semantic features (Zhou et al., 2005; Jing and Zhai, 2007) , or kernels based on syntax or sequences (Bunescu and Mooney, 2005a,b; Plank and Moschitti, 2013) to represent sentences of relations. More recently, deep neural nets start to show promising results. Most rely on convolutional neural nets (Zeng et al., 2014 (Zeng et al., , 2015 Grishman, 2015, 2016;<cite> Fu et al., 2017)</cite> or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations. Our supervised base model will be similar to (Zhou et al., 2016) . Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016;<cite> Fu et al., 2017</cite> ) that require additional parsers. In order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation. They used their multi-task model to train on the bilingual ACE05 datasets and obtained improvement when there is less training available (10%-50%). Our experiments will show our multitask model can make significant improvement on the full training set. In terms of the regularization to the representation, Duong et al. (2015) used l2 regularization between the parameters of the same part of two models in multi-task learning.",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_1",
  "x": "More recently, deep neural nets start to show promising results. Most rely on convolutional neural nets (Zeng et al., 2014 (Zeng et al., , 2015 Grishman, 2015, 2016;<cite> Fu et al., 2017)</cite> or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations. Our supervised base model will be similar to (Zhou et al., 2016) . Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016;<cite> Fu et al., 2017</cite> ) that require additional parsers. In order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation. They used their multi-task model to train on the bilingual ACE05 datasets and obtained improvement when there is less training available (10%-50%). Our experiments will show our multitask model can make significant improvement on the full training set. In terms of the regularization to the representation, Duong et al. (2015) used l2 regularization between the parameters of the same part of two models in multi-task learning. Their method is a kind of soft-parameter sharing, which does not involve sharing any part of the model directly.",
  "y": "differences"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_2",
  "x": "---------------------------------- **SUPERVISED NEURAL RELATION EXTRACTION MODEL** The supervised neural model on a single dataset was introduced by Zeng et al. (2014) and followed by many others (Nguyen and Grishman, 2015; Zhou et al., 2016; Miwa and Bansal, 2016; Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> . We use a similar model as our base model. It takes word tokens, position of arguments and their entity types as input. Some work (Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> used extra syntax features as input. However, the parsers that produce syntax features could have errors and vary depending on the domain of text. The syntax features learned could also be too specific for a single dataset. Thus, we focus on learning representation from scratch, but also compare the models with extra features later in the experiments.",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_3",
  "x": "**SUPERVISED NEURAL RELATION EXTRACTION MODEL** The supervised neural model on a single dataset was introduced by Zeng et al. (2014) and followed by many others (Nguyen and Grishman, 2015; Zhou et al., 2016; Miwa and Bansal, 2016; Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> . We use a similar model as our base model. It takes word tokens, position of arguments and their entity types as input. Some work (Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> used extra syntax features as input. However, the parsers that produce syntax features could have errors and vary depending on the domain of text. The syntax features learned could also be too specific for a single dataset. Thus, we focus on learning representation from scratch, but also compare the models with extra features later in the experiments. The encoder is a bidirectional RNN with attention and the decoder is one hidden fully connected layer followed by a softmax output layer.",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_4",
  "x": "However, the parsers that produce syntax features could have errors and vary depending on the domain of text. The syntax features learned could also be too specific for a single dataset. Thus, we focus on learning representation from scratch, but also compare the models with extra features later in the experiments. The encoder is a bidirectional RNN with attention and the decoder is one hidden fully connected layer followed by a softmax output layer. In the input layer, we convert word tokens into word embeddings with pretrained word2vec (Mikolov et al., 2013) . For each token, we convert the distance to the two arguments of the example to two position embeddings. We also convert the entity types of the arguments to entity embeddings. The setup of word embedding and position embedding was introduced by Zeng et al. (2014) . The entity embedding (Nguyen and Grishman, 2016;<cite> Fu et al., 2017</cite> ) is included for arguments that are entities rather than common nouns.",
  "y": "similarities"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_5",
  "x": "We use GRU (Cho et al., 2014) as the RNN cell. W v and b v are the weights for the projection v i . v w is the word context vector, which works as a query of selecting important words. The importance of the word is computed as the similarity between v i and v w . The importance weight is then normalized through a softmax function. Then we obtain the high level summarization \u03c6(x) for the relation example. The decoder uses this high level representation as features for relation classification. It usually contains one hidden layer (Zeng et al., 2014; Nguyen and Grishman, 2016;<cite> Fu et al., 2017</cite> ) and a softmax output layer. We use the same structure which can be formalized as the following:",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_6",
  "x": "---------------------------------- **DATASETS** To apply the multi-task learning, we need at least two datasets. We pick ACE05 and ERE for our case study. The ACE05 dataset provides a cross-domain evaluation setting . It contains 6 domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and weblogs (wl). Previous work (Gormley et al., 2015; Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> set, and the other half of bc, cts and wl as the test sets. We followed their split of documents and their split of the relation types for asymmetric relations. The ERE dataset has a similar relation schema to ACE05, but is different in some annotation guidelines (Aguilar et al., 2014) .",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_7",
  "x": "---------------------------------- **AUGMENTATION BETWEEN ACE05 AND ERE** Training separately on the two corpora (row \"Supervised\" in Table 1 ), we obtain results on ACE05 comparable to previous work (Gormley et al., 2015) with substantially fewer features. With syntactic features as (Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> did, it could be further improved. In this paper, however, we want to focus on representation learning from scratch first. Our experiments focus on whether we can improve the representation with more sources of data. A common way to do so is pre-training. As a baseline, we pre-train the encoder of the supervised model on ERE and then fine-tune on ACE05, and vice versa (row \"Pretraining\" in Table 1 ). We observe improvement on both fine-tuned datasets.",
  "y": "future_work"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_0",
  "x": "Evaluated on a recent large scale dataset<cite> (Hermann et al., 2015)</cite> , our model exhibits better results than previous research, and we find that max-pooling is suited for modeling the accumulation of information on entities. Further analysis suggests that our model can put together multiple pieces of information encoded in different sentences to answer complicated questions. Our code for the model is available at https://github. com/soskek/der-network ---------------------------------- **INTRODUCTION** Machine reading systems (Poon et al., 2010; Richardson et al., 2013) can be tested on their ability to answer queries about contents of documents that they read, thus a central problem is how the information of documents should be organized in the system and retrieved by the queries. Recently, large scale datasets of document-queryanswer triples have been constructed from online newspaper articles and their summaries<cite> (Hermann et al., 2015)</cite> , by replacing named entities in the summaries with placeholders to form Cloze (Taylor, 1953 ) style questions ( Figure 1 ). These datasets have enabled training and testing of complicated neural network models of hypothesized machine readers<cite> (Hermann et al., 2015</cite>; Hill et al., 2015) .",
  "y": "uses"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_1",
  "x": "Recently, large scale datasets of document-queryanswer triples have been constructed from online newspaper articles and their summaries<cite> (Hermann et al., 2015)</cite> , by replacing named entities in the summaries with placeholders to form Cloze (Taylor, 1953 ) style questions ( Figure 1 ). These datasets have enabled training and testing of complicated neural network models of hypothesized machine readers<cite> (Hermann et al., 2015</cite>; Hill et al., 2015) . ( @entity1 ) @entity0 may be @entity2 in the popular @entity4 superhero films , but he recently dealt in some advanced bionic technology himself . @entity0 recently presented a robotic arm to young @entity7 , a @entity8 boy who is missing his right arm from just above his elbow . the arm was made by @entity12 , a \u2026! \" [X] \" star @entity0 presents a young child with a bionic arm! Figure 1 : A document-query-answer triple constructed from a news article and its bullet point summary. An entity in the summary (Robert Downey Jr.) is replaced by the placeholder [X] to form a query. All entities are anonymized to exclude world knowledge and focus on reading comprehension. ---------------------------------- **QUERY**",
  "y": "background"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_2",
  "x": "Further analysis suggests that our model can put together multiple pieces of information encoded in different sentences to answer complicated questions. Our code for the model is available at https://github. com/soskek/der-network ---------------------------------- **INTRODUCTION** Machine reading systems (Poon et al., 2010; Richardson et al., 2013) can be tested on their ability to answer queries about contents of documents that they read, thus a central problem is how the information of documents should be organized in the system and retrieved by the queries. Recently, large scale datasets of document-queryanswer triples have been constructed from online newspaper articles and their summaries<cite> (Hermann et al., 2015)</cite> , by replacing named entities in the summaries with placeholders to form Cloze (Taylor, 1953 ) style questions ( Figure 1 ). These datasets have enabled training and testing of complicated neural network models of hypothesized machine readers<cite> (Hermann et al., 2015</cite>; Hill et al., 2015) . ( @entity1 ) @entity0 may be @entity2 in the popular @entity4 superhero films , but he recently dealt in some advanced bionic technology himself .",
  "y": "background"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_3",
  "x": "All entities are anonymized to exclude world knowledge and focus on reading comprehension. ---------------------------------- **QUERY** In this paper, we hypothesize that a reader without world knowledge can only understand a named entity by dynamically constructing its meaning from the contexts. For example, in Figure 1 , a reader reading the sentence \"Robert Downey Jr. may be Iron Man . . . \" can only understand \"Robert Downey Jr.\" as something that \"may be Iron Man\" at this stage, given that it does not know Robert Downey Jr. a priori. Information about this entity can only be accumulated by its subsequent occurrence, such as \"Downey recently presented a robotic arm . . . \". Thus, named entities basically serve as anchors to link multiple pieces of information encoded in different sentences. This insight has been reflected by the anonymization process in construction of the dataset, in which coreferent entities (e.g. \"Robert Downey Jr.\" and \"Downey\") are replaced by randomly permuted abstract entity markers (e.g. \"@en-tity0\"), in order to prevent additional world knowledge from being attached to the surface form of the entities<cite> (Hermann et al., 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_4",
  "x": "This insight has been reflected by the anonymization process in construction of the dataset, in which coreferent entities (e.g. \"Robert Downey Jr.\" and \"Downey\") are replaced by randomly permuted abstract entity markers (e.g. \"@en-tity0\"), in order to prevent additional world knowledge from being attached to the surface form of the entities<cite> (Hermann et al., 2015)</cite> . We, however, take it as a strong motivation to implement a reader that dynamically builds meaning representations for each entity, by gathering and accumulating information on that entity as it reads a document (Section 2). Evaluation of our model, DER Network, exhibits better results than previous research (Section 3). In particular, we find that max-pooling of entity representations, which is intended to model the accumulation of information on entities, can drastically improve performance. Further analysis suggests that max-pooling can help our model draw multiple pieces of information from different sentences. ---------------------------------- **MODEL** Following<cite> Hermann et al. (2015)</cite> , our model estimates the conditional probability p(e|D, q), where q is a query and D is a document. A candidate answer for the query is denoted by e, which in this paper is any named entity.",
  "y": "motivation"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_5",
  "x": "We, however, take it as a strong motivation to implement a reader that dynamically builds meaning representations for each entity, by gathering and accumulating information on that entity as it reads a document (Section 2). Evaluation of our model, DER Network, exhibits better results than previous research (Section 3). In particular, we find that max-pooling of entity representations, which is intended to model the accumulation of information on entities, can drastically improve performance. Further analysis suggests that max-pooling can help our model draw multiple pieces of information from different sentences. ---------------------------------- **MODEL** Following<cite> Hermann et al. (2015)</cite> , our model estimates the conditional probability p(e|D, q), where q is a query and D is a document. A candidate answer for the query is denoted by e, which in this paper is any named entity. Our model can be factorized as:",
  "y": "uses"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_6",
  "x": "Evaluation of our model, DER Network, exhibits better results than previous research (Section 3). In particular, we find that max-pooling of entity representations, which is intended to model the accumulation of information on entities, can drastically improve performance. Further analysis suggests that max-pooling can help our model draw multiple pieces of information from different sentences. ---------------------------------- **MODEL** Following<cite> Hermann et al. (2015)</cite> , our model estimates the conditional probability p(e|D, q), where q is a query and D is a document. A candidate answer for the query is denoted by e, which in this paper is any named entity. Our model can be factorized as: in which u(q) is the learned meaning for the query and v(e; D, q) the dynamically constructed meaning for an entity, depending on the document D and the query q. We note that (1) is in contrast to the factorization used by<cite> Hermann et al. (2015)</cite>:",
  "y": "differences"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_7",
  "x": "More precisely, for each entity e, max-pooling takes the max value of each dimension of the vectors d e,c from all preceding contexts c (Figure 3) . Then, in a subsequent sentence c where the entity occurs again at index \u03c4 , we use the vector as input for the LSTMs in (3) and (4) for encoding the context. This vector x c,\u03c4 draws information from preceding contexts, and is regarded as the meaning of the entity e that the reader understands so far, before reading the sentence c. It is used in place of a vector previously randomly initialized as a notion of e, in the construction of the new dynamic entity representation d e,c . ---------------------------------- **3 EVALUATION** We use the CNN-QA dataset<cite> (Hermann et al., 2015)</cite> for evaluating our model's ability to answer questions about named entities. The dataset consists of (D, q, e)-triples, where the document D is taken from online news articles, and the query q is formed by hiding a named entity e in a summarizing bullet point of the document (Figure 1) . The training set has 90k articles and 380k queries, and both validation and test sets have 1k articles and 3k queries.",
  "y": "uses"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_8",
  "x": "9 Vector dimension: 300, Dropout: 0.3, Batch: 50, Optimization: RMSProp with momentum (Tieleman and Hinton, 2012; Graves, 2013) (momentum: 0.9, decay: 0.95), Learning rate: 1e-4 divided by 2.0 per epoch, Gradient clipping factor: 10. We initialize word vectors by uniform distribution [-0.05, 0.05] , and other matrix parameters by Gaussians of mean 0 and variance 2/(# rows + # columns). Hermann et al. (2015) and * * from Hill et al. (2015) . ( @entity1 ) @entity0 may be @entity2 in the popular @entity4 superhero films , but he recently dealt in some advanced bionic technology himself .! \u2026! @entity7 received his robotic arm in the summer , then later had it upgraded to resemble a \" @entity26 \" arm .! this past saturday , @entity7 received an even more impressive gift , from \" @entity2 \" himself .! \u2026! the actor showed the child two arms , one from @entity0 's movies and one for @entity7 : a real , working robotic @entity2 arm .! clear effects, suggesting that the attention mechanism plays a key role in our model. Combining these two techniques helps more. Further, we note that initializing our model with pre-trained word vectors 10 is helpful, though world knowledge of entities has been prevented by the anonymization process. This suggests that pre-trained word vectors may still bring extra linguistic knowledge encoded in ordinary words. Finally, we note that our model, full DER Network, shows the best results compared to several previous reader models<cite> (Hermann et al., 2015</cite>; Hill et al., 2015) , endorsing our approach as promising. The 99% confidence intervals of the results of full DER Network and the one initialized by word2vec on the test set were [0.700, 0.740] and [0.708, 0.749], respectively (measured by bootstrap tests).",
  "y": "differences"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_0",
  "x": "Task-oriented dialog systems, such as hotel booking or technical support service, help users to achieve specific goals with natural language. Compared with traditional pipeline solutions (Williams and Young, 2007; Young et al., 2013; Wen et al., 2017) , end-to-end approaches recently gain much attention (Zhao et al., 2017; Eric and Manning, 2017a; Lei et al., 2018) , because they directly map dialog history to the output responses and consequently reduce human effort for modular designs and hand-crafted state labels. To effectively incorporate KB information and perform knowledge- * Corresponding Author based reasoning, memory augmented models have been proposed (Bordes et al., 2017; Seo et al., 2017; Eric and Manning, 2017b; <cite>Madotto et al., 2018</cite>; Raghu et al., 2018; Reddy et al., 2019; Wu et al., 2019) . Bordes et al. (2017) and Seo et al. (2017) attended to retrieval models, lacking the ability of generation, while others incorporated the memory (i.e. end-to-end memory networks, abbreviated as MemNNs, Sukhbaatar et al. (2015) ) and copy mechanism (Gu et al., 2016 ) into a sequential generative architecture. However, most models tended to confound the dialog history with KB tuples and simply stored them into one memory. A shared memory forces the memory reader to reason over the two different types of data, which makes the task harder, especially when the memory is large. To explore this problem, Reddy et al. (2019) very recently proposed to separate memories for modeling dialog context and KB results. In this paper, we adopt working memory to interact with two longterm memories. Furthermore, compared to Reddy et al. (2019) , we leverage the reasoning ability of MemNNs to instantiate the external memories.",
  "y": "background"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_1",
  "x": "The encoder encodes the dialog history to obtain the high-level signal, a distributed intent vector. The WM consists of a Short-Term Storage system (STS) and a Central-EXE including an Attention Controller (Attn-Ctrl) and a rule-based word selection strategy. The Attn-Ctrl dynamically generates the attention control vector to query and reason over the two long memories and then stores three \"activated\" distributions into STS. Finally a generated token is selected from the STS under the word selection strategy at each decoder step. ---------------------------------- **MODEL DESCRIPTION** The symbols are defined in Table 1 , and more details can be found in the supplementary material. We omit the subscript E or S 2 , following <cite>Madotto et al. (2018)</cite> to define each pointer index set: Symbol Definition xi or yi a token in the dialog history or system response $ a special token used as a sentinel <cite>(Madotto et al., 2018 )</cite> X X = {x1, . . . , xn, $}, the dialog history Y Y = {y1, \u00b7 \u00b7 \u00b7 , ym}, the expected response bi one KB tuple, actually the corresponding entity B B = {b1, \u00b7 \u00b7 \u00b7 , b l , $}, the KB tuples P T RE = {ptrE,1, \u00b7 \u00b7 \u00b7 , ptrE,m}, dialog pointer index set.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_2",
  "x": "Symbol Definition xi or yi a token in the dialog history or system response $ a special token used as a sentinel <cite>(Madotto et al., 2018 )</cite> X X = {x1, . . . , xn, $}, the dialog history Y Y = {y1, \u00b7 \u00b7 \u00b7 , ym}, the expected response bi one KB tuple, actually the corresponding entity B B = {b1, \u00b7 \u00b7 \u00b7 , b l , $}, the KB tuples P T RE = {ptrE,1, \u00b7 \u00b7 \u00b7 , ptrE,m}, dialog pointer index set. P T RE supervised information for copying words in dialog history P T RS = {ptrS,1, \u00b7 \u00b7 \u00b7 , ptrS,m}, KB pointer index set. P T RS supervised information for copying entities in KB tuples Table 1 : Notation Table. where xb z \u2208 X or B is the dialog history or KB tuples according to the subscript (E or S) and n xb + 1 is the sentinel position index as n xb is equal to the dialog history length n or the number of KB triples l. The idea behind Eq. 1 is that we can obtain the positions of where to copy by matching the target text with the dialog history or KB information. Furthermore, we hope this provides the model with an accurate guidance of how to activate the two long-term memories. ---------------------------------- **MEMNN ENCODER** Here, on the context of our task, we give a brief description of K-hop MemNN with adjacent weight tying and more details can be found in (Sukhbaatar et al., 2015) . The memory of MemNN is represented by a set of trainable embedding matrices C = {C 1 , . . . , C K+1 }. Given input tokens in the dialog history X, MemNN first writes them into memories by Eq. 2 and then uses a query to iteratively read from them with multi hops to reason about the required response by Eq. 3 and Eq. 4.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_3",
  "x": "To incorporate the context information, we explore two context-aware transformation TRANS(\u00b7) by replacing Eq. 2 with A k i = TRANS(C k (x i )), which is defined as follows: where h i is the context-aware representation, and \u03c6 e is a trainable embedding function. We combine MemNNs with TRANS(\u00b7) to alleviate the OOV problem when reasoning about memory contents. ---------------------------------- **WORKING MEMORY DECODER** Inspired by the studies on the working memory, we design our decoder as an attentional control system for dialog generation which consists of the working memory and two long-term memories. As shown in Figure 1 , we adopt the E-MemNN to memorize the dialog history X as described in Section 2.1, and then store KB tuples into the S-MemNN without TRANS(\u00b7). We also incorporate additional temporal information and speaker information into dialog utterances as <cite>(Madotto et al., 2018)</cite> and adopt a (subject, relation, object) representation of KB information as (Eric and Manning, 2017b) . More details can be found in the supplementary material.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_4",
  "x": "More details can be found in the supplementary material. Having written dialog history and KB tuples into E-MemNN and S-MemNN, we then use the WM to interact with them (to query and reason over them) to generate the response. At each decoder step, the Attn-Ctrl, instantiated as a GRU, dynamically generates the query vector q t as follows: Here, query q t is used to access E-MemNN activating the final query q E = o K E , vocabulary distribution P vocab by Eq. 9 and copy distribution for dialog history P E\u00b7ptr . When querying S-MemNN, we consider the dialog history by using query q t = q E + q t and then obtain the copy distribution for KB entities P S\u00b7ptr . The two copy distributions are obtained by augmenting MemNNs with copy mechanism that is P E\u00b7ptr = p K E,t and P S\u00b7ptr = p K S,t . Now, three distributions, P vocab , P E\u00b7ptr and P S\u00b7ptr , are activated and moved into the STS, and then a proper word is generated from the activated distributions. We here use a rule-based word selection strategy by extending the sentinel idea in <cite>(Madotto et al., 2018)</cite> , which is shown in Figure 1 . If the expected word is not appearing either in the episodic memory or the semantic memory, the two copy pointers are trained to produce the sentinel token and our WMM2Seq generates the token from P vocab ; otherwise, the token is generated by copying from either the dialog history or KB tuples and this is done by comparing the two copy distributions.",
  "y": "extends"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_5",
  "x": "---------------------------------- **RESULTS AND ANALYSIS** We use Per-response/dialog Accuracy (Bordes et al., 2017) , BLEU (Papineni et al., 2002) and Entity F1 <cite>(Madotto et al., 2018)</cite> to compare the performance of different models. And the baseline models are Seq2Seq+Attn (Luong et al., 2015) , Pointer to Unknown (Ptr-Unk, Gulcehre et al. (2016) ), <cite>Mem2Seq</cite> <cite>(Madotto et al., 2018)</cite> , Hierarchical Pointer Generator Memory Network (HyP-MN, Raghu et al. (2018) ) and Global-to-Local Memory Pointer (GLMP, Wu et al. (2019) ). Automatic Evaluation: The results on the bAbI dialog dataset are given in Table 2 . We can see that our model does much better on the OOV situation and is on par with the best results on T5. Moreover, our model can perfectly issue API calls (task 1), update API calls (task 2) and provide extra information (task 4). As task 5 is a combination of tasks 1-4, our best performance on T5-OOV exhibits the powerful reasoning ability to the unseen dialog history and KB tuples. And this reasoning ability is also proved by the performance improvements on the DSTC2 dataset according to several metrics in Table 3 .",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_6",
  "x": "The learning rate is simply fixed to 0.001 and the dropout ratio is sampled from [0.1, 0.4]. Furthermore, we randomly mask some memory cells with the same dropout ratio to simulate the OOV situation for both episodic and semantic memories. The hyper-parameters for best models are given in the supplementary material. ---------------------------------- **RESULTS AND ANALYSIS** We use Per-response/dialog Accuracy (Bordes et al., 2017) , BLEU (Papineni et al., 2002) and Entity F1 <cite>(Madotto et al., 2018)</cite> to compare the performance of different models. And the baseline models are Seq2Seq+Attn (Luong et al., 2015) , Pointer to Unknown (Ptr-Unk, Gulcehre et al. (2016) ), <cite>Mem2Seq</cite> <cite>(Madotto et al., 2018)</cite> , Hierarchical Pointer Generator Memory Network (HyP-MN, Raghu et al. (2018) ) and Global-to-Local Memory Pointer (GLMP, Wu et al. (2019) ). Automatic Evaluation: The results on the bAbI dialog dataset are given in Table 2 . We can see that our model does much better on the OOV situation and is on par with the best results on T5.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_7",
  "x": "First, we remove the context-sensitive transformation TRANS(\u00b7) and then find significant performance degradation. This suggests that perceptual processes are a necessary step before storing perceptual information (the dialog history) into the episodic memory and it is important for the performance of working memory. Second, we find that WMM2Seq outperforms <cite>Mem2Seq</cite>, which uses a unified memory to store dialog history and KB information. We can safely conclude that the separation of context memory and KB memory benefits the performance, as WMM2Seq performs well with less parameters than <cite>Mem2Seq</cite> on task 5. Finally, we additionally analysis how the multi-hop attention mechanism helps by showing the performance differences between the hop K = 1 and the default hop K = 3. Though multi-hop attention strengthens the reasoning ability and improves the results, we find that the performance difference between the hops K = 1 and K = 3 is not so obvious as shown in <cite>(Madotto et al., 2018</cite>; Wu et al., 2019) . Furthermore, our model performs well even with one hop, which we mainly attribute to the reasoning ability of working memory. The separation of memories and stacking S-MemNN on E-MemNN also help a lot, because the whole external memory, consisting of the episodic and semantic memories, can be seen as a multi-hop (two-level) structure (the first level is the episode memory and the second level is the semantic memory). Attention Visualization: As an intuitive way to show the model's dynamics, attention weight visualization is also used to understand how the Central-EXE controls the access to the two long-term memories (E-MemNN and S-MemNN).",
  "y": "differences"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_8",
  "x": "Furthermore, there is no significant difference between the two kinds of the transformation TRANS(\u00b7). Ablation Study: To better understand the components used in our model, we report our ablation studies from three aspects. First, we remove the context-sensitive transformation TRANS(\u00b7) and then find significant performance degradation. This suggests that perceptual processes are a necessary step before storing perceptual information (the dialog history) into the episodic memory and it is important for the performance of working memory. Second, we find that WMM2Seq outperforms <cite>Mem2Seq</cite>, which uses a unified memory to store dialog history and KB information. We can safely conclude that the separation of context memory and KB memory benefits the performance, as WMM2Seq performs well with less parameters than <cite>Mem2Seq</cite> on task 5. Finally, we additionally analysis how the multi-hop attention mechanism helps by showing the performance differences between the hop K = 1 and the default hop K = 3. Though multi-hop attention strengthens the reasoning ability and improves the results, we find that the performance difference between the hops K = 1 and K = 3 is not so obvious as shown in <cite>(Madotto et al., 2018</cite>; Wu et al., 2019) . Furthermore, our model performs well even with one hop, which we mainly attribute to the reasoning ability of working memory.",
  "y": "differences extends"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_9",
  "x": "Furthermore, there is no significant difference between the two kinds of the transformation TRANS(\u00b7). Ablation Study: To better understand the components used in our model, we report our ablation studies from three aspects. First, we remove the context-sensitive transformation TRANS(\u00b7) and then find significant performance degradation. This suggests that perceptual processes are a necessary step before storing perceptual information (the dialog history) into the episodic memory and it is important for the performance of working memory. Second, we find that WMM2Seq outperforms <cite>Mem2Seq</cite>, which uses a unified memory to store dialog history and KB information. We can safely conclude that the separation of context memory and KB memory benefits the performance, as WMM2Seq performs well with less parameters than <cite>Mem2Seq</cite> on task 5. Finally, we additionally analysis how the multi-hop attention mechanism helps by showing the performance differences between the hop K = 1 and the default hop K = 3. Though multi-hop attention strengthens the reasoning ability and improves the results, we find that the performance difference between the hops K = 1 and K = 3 is not so obvious as shown in <cite>(Madotto et al., 2018</cite>; Wu et al., 2019) . Furthermore, our model performs well even with one hop, which we mainly attribute to the reasoning ability of working memory.",
  "y": "differences"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_10",
  "x": "We adopt <cite>Mem2Seq</cite> as the baseline for human evaluation considering its good performance and code release 3 . First we randomly select 100 samples from the DSTC2 test set, then generate the corresponding responses using WMM2Seq and <cite>Mem2Seq</cite>, and finally ask two human subjects to judge the quality of the generated responses according to the appropriateness and humanlikeness on a scale from 1 to 5. As shown in Table 4 , WMM2Seq outperforms <cite>Mem2Seq</cite> in both measures, which is coherent to the automatic evaluation. More details about human evaluation are reported in the supplementary material. ---------------------------------- **CONCLUSION** We leverage the knowledge from the psychological studies and propose our WMM2Seq for dialog response generation. First, the storage separation of the dialog history and KB information is very important and we explore two context-sensitive perceptual processes for the word-level representations of the dialog history. Second, working memory is adopted to interact with the long-term memories and then generate the responses.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_11",
  "x": "We adopt <cite>Mem2Seq</cite> as the baseline for human evaluation considering its good performance and code release 3 . First we randomly select 100 samples from the DSTC2 test set, then generate the corresponding responses using WMM2Seq and <cite>Mem2Seq</cite>, and finally ask two human subjects to judge the quality of the generated responses according to the appropriateness and humanlikeness on a scale from 1 to 5. As shown in Table 4 , WMM2Seq outperforms <cite>Mem2Seq</cite> in both measures, which is coherent to the automatic evaluation. More details about human evaluation are reported in the supplementary material. ---------------------------------- **CONCLUSION** We leverage the knowledge from the psychological studies and propose our WMM2Seq for dialog response generation. First, the storage separation of the dialog history and KB information is very important and we explore two context-sensitive perceptual processes for the word-level representations of the dialog history. Second, working memory is adopted to interact with the long-term memories and then generate the responses.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_12",
  "x": "Figure 2 shows the episodic and semantic memory attention vectors at the last hop for each generated token. Firstly, our model generates a different but still correct response as the customer wants a moderately priced restaurant in the west and does not care about the type of food. Secondly, the generated response has tokens from the vocabulary (e.g. \"is\" and \"a\"), dialog history (e.g. \"west\" and \"food\") and KB information (e.g. \"saint johns chop house\" and \"british\"), indicating that our model learns to interact well with the two long-term memories by two sentinels. Human Evaluation: Following the methods in (Eric and Manning, 2017b; Wu et al., 2019) , we report human evaluation of the generated responses in Table 4 . We adopt <cite>Mem2Seq</cite> as the baseline for human evaluation considering its good performance and code release 3 . First we randomly select 100 samples from the DSTC2 test set, then generate the corresponding responses using WMM2Seq and <cite>Mem2Seq</cite>, and finally ask two human subjects to judge the quality of the generated responses according to the appropriateness and humanlikeness on a scale from 1 to 5. As shown in Table 4 , WMM2Seq outperforms <cite>Mem2Seq</cite> in both measures, which is coherent to the automatic evaluation. More details about human evaluation are reported in the supplementary material. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_0",
  "x": "BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011) , summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; Stajner et al., 2015;<cite> Xu et al., 2016)</cite> , i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014) , BLEU became the main automatic metric for TS, despite its deficiencies (see \u00a72). Indeed, focusing on lexical simplification,<cite> Xu et al. (2016)</cite> argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used. In their experiments, BLEU failed to predict simplicity, but obtained a higher correlation with grammaticality and meaning preservation, relative to the SARI metric they proposed. In this paper, we further explore the applicability of BLEU for TS evaluation, examining BLEU's informativeness where sentence splitting is involved. Sentence splitting, namely the rewriting of a single sentence as multiple sentences while preserving its meaning, is the main structural simplification operation. It has been shown useful for MT preprocessing (Chandrasekar et al., 1996; Mishra et al., 2014; Li and Nenkova, 2015) and human comprehension (Mason and Kendall, 1979; Williams et al., 2003) , independently from other lexical and structural simplification operations. Sentence splitting is performed by many TS systems (Zhu et al., 2010; Woodsend and Lapata, 2011; Siddharthan and Angrosh, 2014; Gardent, 2014, 2016) . For example, 63% and 80% of the test sentences are split by the systems of Woodsend and Lapata (2011) and Zhu et al. (2010) , respectively (Narayan and Gardent, 2016) .",
  "y": "background"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_1",
  "x": "Along with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014) , BLEU became the main automatic metric for TS, despite its deficiencies (see \u00a72). Indeed, focusing on lexical simplification,<cite> Xu et al. (2016)</cite> argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used. In their experiments, BLEU failed to predict simplicity, but obtained a higher correlation with grammaticality and meaning preservation, relative to the SARI metric they proposed. In this paper, we further explore the applicability of BLEU for TS evaluation, examining BLEU's informativeness where sentence splitting is involved. Sentence splitting, namely the rewriting of a single sentence as multiple sentences while preserving its meaning, is the main structural simplification operation. It has been shown useful for MT preprocessing (Chandrasekar et al., 1996; Mishra et al., 2014; Li and Nenkova, 2015) and human comprehension (Mason and Kendall, 1979; Williams et al., 2003) , independently from other lexical and structural simplification operations. Sentence splitting is performed by many TS systems (Zhu et al., 2010; Woodsend and Lapata, 2011; Siddharthan and Angrosh, 2014; Gardent, 2014, 2016) . For example, 63% and 80% of the test sentences are split by the systems of Woodsend and Lapata (2011) and Zhu et al. (2010) , respectively (Narayan and Gardent, 2016) . Sentence splitting is also the focus of the recently proposed Split-and Rephrase sub-task (Narayan et al., 2017; Aharoni and Goldberg, 2018) , in which the automatic metric used is BLEU.",
  "y": "background"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_2",
  "x": "It has been shown useful for MT preprocessing (Chandrasekar et al., 1996; Mishra et al., 2014; Li and Nenkova, 2015) and human comprehension (Mason and Kendall, 1979; Williams et al., 2003) , independently from other lexical and structural simplification operations. Sentence splitting is performed by many TS systems (Zhu et al., 2010; Woodsend and Lapata, 2011; Siddharthan and Angrosh, 2014; Gardent, 2014, 2016) . For example, 63% and 80% of the test sentences are split by the systems of Woodsend and Lapata (2011) and Zhu et al. (2010) , respectively (Narayan and Gardent, 2016) . Sentence splitting is also the focus of the recently proposed Split-and Rephrase sub-task (Narayan et al., 2017; Aharoni and Goldberg, 2018) , in which the automatic metric used is BLEU. For exploring the effect of sentence splitting on BLEU scores, we compile a human-generated gold standard sentence splitting corpus -HSplit, which will also be useful for future studies of splitting in TS, and perform correlation analyses with human judgments. We consider two reference sets. First, we experiment with the most common set, proposed by<cite> Xu et al. (2016)</cite> , evaluating a variety of system outputs, as well as HSplit. The references in this setting explicitly emphasize lexical operations, and do not contain splitting or content deletion. 2 Second, we experiment with HSplit as the reference set, evaluating systems that focus on sentence splitting.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_3",
  "x": "In particular, Callison-Burch et al. (2006) showed that BLEU may not correlate in some cases with human judgments since a huge number of potential translations have the same BLEU score, and that correlation decreases when translation quality is low. Some of the reported shortcomings are relevant to monolingual translation, such as the impossibility to capture synonyms and paraphrases that are not in the reference set, or the uniform weighting of words. tural operations are involved (Nisioi et al., 2017; Sulem et al., 2018b) . ---------------------------------- **BLEU IN TS.** While BLEU is standardly used for TS evaluation (e.g.,<cite> Xu et al., 2016</cite>; Nisioi et al., 2017; Zhang and Lapata, 2017; Ma and Sun, 2017 ), only few works tested its correlation with human judgments. Using 20 source sentences from the PWKP test corpus (Zhu et al., 2010) with 5 simplified sentences for each of them, Wubben et al. (2012) reported positive correlation of BLEU with simplicity ratings, but no correlation with adequacy. T-BLEU (\u0160tajner et al., 2014), a variant of BLEU which uses lower n-grams when no overlapping 4-grams are found, was tested on outputs that applied only structural modifications to the source. It was found to have moderate positive correlation for meaning preservation, and positive but low correlation for grammaticality.",
  "y": "motivation"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_4",
  "x": "It was found to have moderate positive correlation for meaning preservation, and positive but low correlation for grammaticality. Correlation with simplicity was not considered in this experiment. Xu et al. (2016) focused on lexical simplification, finding that BLEU obtains reasonable correlation for grammaticality and meaning preservation but fails to capture simplicity, even when multiple references are used. To our knowledge, no previous work has examined the behavior of BLEU on sentence splitting, which we investigate here using a manually compiled gold standard. ---------------------------------- **GOLD-STANDARD SPLITTING CORPUS** In order to investigate the effect of correctly splitting sentences on the automatic metric scores, we build a parallel corpus, where each sentence is modified by 4 annotators, according to specific sentence splitting guidelines. We use the complex side of the test corpus of<cite> Xu et al. (2016)</cite> . 3 While Narayan et al. (2017) recently proposed the semi-automatically compiled WEB-SPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_5",
  "x": "3 While Narayan et al. (2017) recently proposed the semi-automatically compiled WEB-SPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split. This corpus enriches the set of references focused on lexical operations that were collected by<cite> Xu et al. (2016)</cite> for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (Narayan et al., 2017) . We use two sets of guidelines. In Set 1, annotators are required to split the original as much as possible, while preserving the sentence's gram-maticality, fluency and meaning. The guidelines include two sentence splitting examples. 4 In Set 2, annotators are encouraged to split only in cases where it simplifies the original sentence. That is, simplicity is implicit in Set 1 and explicit in Set 2. In both sets, the annotators are instructed to leave the source unchanged if splitting violates grammaticality, fluency or meaning preservation. 5 Each set of guidelines is used by two annotators, with native or native-like proficiency in English.",
  "y": "extends"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_6",
  "x": "---------------------------------- **EXPERIMENTS** ---------------------------------- **EXPERIMENTAL SETUP** Metrics. In addition to BLEU, 7 we also experiment with (1) iBLEU (Sun and Zhou, 2012) which was recently used for TS <cite>(Xu et al., 2016</cite>; and which takes into account the BLEU scores of the output against the input and against the references; (2) the Flesch-Kincaid Grade Level (FK; Kincaid et al., 1975 ), computed at the system level, which estimates the readability of the text with a lower value indicating higher 4 Examples are taken from Siddharthan (2006) . 5 Examples are not provided in the case of Set 2 so as not to give an a-priori notion of simplicity. The complete guidelines are found in the supplementary material. 6 Wilicoxon's signed rank test, p = 1.6 \u00b7 10 \u22125 for #Sents and p = 0.002 for SplitSents.",
  "y": "background"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_7",
  "x": "**EXPERIMENTAL SETUP** Metrics. In addition to BLEU, 7 we also experiment with (1) iBLEU (Sun and Zhou, 2012) which was recently used for TS <cite>(Xu et al., 2016</cite>; and which takes into account the BLEU scores of the output against the input and against the references; (2) the Flesch-Kincaid Grade Level (FK; Kincaid et al., 1975 ), computed at the system level, which estimates the readability of the text with a lower value indicating higher 4 Examples are taken from Siddharthan (2006) . 5 Examples are not provided in the case of Set 2 so as not to give an a-priori notion of simplicity. The complete guidelines are found in the supplementary material. 6 Wilicoxon's signed rank test, p = 1.6 \u00b7 10 \u22125 for #Sents and p = 0.002 for SplitSents. 7 System-level BLEU scores are computed using the multi-bleu Moses support tool. Sentence-level BLEU scores are computed using NLTK (Loper and Bird, 2002). readability; 8 (3) SARI<cite> (Xu et al., 2016)</cite> , which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_8",
  "x": "For \"Standard Reference Setting\", we consider both a case where evaluated systems do not perform any splittings on the test set (\"Systems/Corpora without Splits\"), and one where we evaluate these systems, along with the HSplit corpus, used in the role of system outputs (\"All Systems/Corpora\"). Systems include six MT-based simplification systems, including outputs of the state-of-the-art neural TS system of Nisioi et al. (2017) , in four variants: either default settings or initialization by word2vec, for each both the highest and the fourth ranked hypotheses in the beam are considered. 10 We further include Moses (Koehn et al., 2007) and SBMT-SARI<cite> (Xu et al., 2016)</cite> , a syntax-based MT system tuned against SARI, and the identity function (outputs are same as inputs). The case which evaluates outputs with sentence splitting additionally includes the four HSplit corpora and the HSplit average scores. For \"HSplit as Reference Setting\", we consider the outputs of six simplification systems whose main simplification operation is sentence splitting: DSS, DSS m , SEMoses, SEMoses m , SEMoses LM and SEMoses m LM , taken from (Sulem et al., 2018b) . Human Evaluation. We use the evaluation benchmark provided by Sulem et al. (2018b) , 11 including system outputs and human evaluation scores corresponding to the first 70 sentences of the test corpus of<cite> Xu et al. (2016)</cite> , and extend it to apply to HSplit as well. The evaluation of HSplit is carried out by 3 in-house native English annotators, who rated the different input-output pairs for the different systems according to 4 parameters: Grammaticality (G), Meaning preservation (M), Simplicity (S) and Structural Simplicity (StS). G and M are measured using a 1 to 5 scale.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_9",
  "x": "5 Examples are not provided in the case of Set 2 so as not to give an a-priori notion of simplicity. The complete guidelines are found in the supplementary material. 6 Wilicoxon's signed rank test, p = 1.6 \u00b7 10 \u22125 for #Sents and p = 0.002 for SplitSents. 7 System-level BLEU scores are computed using the multi-bleu Moses support tool. Sentence-level BLEU scores are computed using NLTK (Loper and Bird, 2002). readability; 8 (3) SARI<cite> (Xu et al., 2016)</cite> , which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems. For completeness, we also experiment with the negative Levenshtein distance to the source (-LD SC ), which serves as a measure of conservatism. 9 We explore two settings. In one (\"Standard Reference Setting\", \u00a74.2), we use two sets of references: the Simple Wikipedia reference (yielding BLEU-1ref and iBLEU-1ref), and 8 references obtained by crowdsourcing by<cite> Xu et al. (2016)</cite> (yielding BLEU-8ref, iBLEU-8ref and SARI-8ref).",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_10",
  "x": "Human Evaluation. We use the evaluation benchmark provided by Sulem et al. (2018b) , 11 including system outputs and human evaluation scores corresponding to the first 70 sentences of the test corpus of<cite> Xu et al. (2016)</cite> , and extend it to apply to HSplit as well. The evaluation of HSplit is carried out by 3 in-house native English annotators, who rated the different input-output pairs for the different systems according to 4 parameters: Grammaticality (G), Meaning preservation (M), Simplicity (S) and Structural Simplicity (StS). G and M are measured using a 1 to 5 scale. A -2 to +2 scale is used for measuring simplicity and structural simplicity. For computing the inter-annotator agreement of the whole benchmark (including the system outputs and the HSplit corpora), we follow Pavlick and Tetreault (2016) and randomly select, for each sentence, one annotator's rating to be the rating of Annotator 1 and the rounded average rating of the two other annotators to be the rating of Annotator 2. We then compute weighted quadratic \u03ba (Cohen, 1968) between Annotator 1 and 2. Repeating this process 1000 times, the obtained medians and 95% confidence intervals are 0.42 \u00b1 0.002 for G, 0.77 \u00b1 0.001 for M and 0.59 \u00b1 0.002 for S and StS. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_11",
  "x": "At the sentence level, considering 840 sentences (70 for each of the system/corpora), the G and M scores vary from 1 to 5 (\u03c3 equals 0.69 and 0.85 respectively), and the S and StS scores from -1 to 2 (\u03c3 equals 0.53 and 0.50). In the \"Systems/corpora without Splits\" case of the \"Standard Reference Setting\", where 7 systems/corpora are considered, the max-min difference at the system level are again 1.09 (\u03c3 = 0.36) and 1.23 (\u03c3 = 0.47) for G and M respectively. For S and StS, the differences are 0.45 and 0.49 (\u03c3 = 0.18). At the sentence level, considering 490 sentences (70 for each of the system/corpora), the G and M scores vary from 1 to 5 (\u03c3 equals 0.78 and 1.01 respectively), and the S and StS scores from -1 to 2 (\u03c3 equals 0.51 and 0.46). Comparing HSplit to Identity. Comparing the BLEU score on the input (the identity function) and on the HSplit corpora, we observe that the former yields much higher BLEU scores. Indeed, BLEU-1ref obtains 59.85 for the input and 43.90 for the HSplit corpora (averaged over the 4 HSplit corpora). BLEU-8ref obtains 94.63 for the input and 73.03 for HSplit. 12 The high scores obtained for Identity, also observed by<cite> Xu et al. (2016)</cite> , indicate that BLEU is a not a good predictor for relative simplicity to the input.",
  "y": "similarities"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_0",
  "x": "User comments play a central role in social media and online discussion fora. News portals and blogs often also allow their readers to comment to get feedback, engage their readers, and build customer loyalty. 1 User comments, however, and more generally user content can also be abusive (e.g., bullying, profanity, hate speech) (Cheng et al., 2015) . Social media are under pressure to combat abusive content, but so far rely mostly on user reports and tools that detect frequent words and phrases of reported posts. 2<cite> Wulczyn et al. (2017)</cite> estimated that only 17.9% of personal attacks in Wikipedia discussions were followed by moderator actions. News portals also suffer from abusive user comments, which damage their reputations and make them liable to fines, e.g., when hosting comments encouraging illegal actions. They often employ moderators, who are frequently overwhelmed, however, by the volume and abusiveness of comments. 3 Readers are disappointed when non-abusive comments do not appear quickly online because of moderation delays. Smaller news portals may be unable to employ moderators, and some are forced to shut down their comments sections entirely.",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_1",
  "x": "4 This is one of the largest publicly available datasets of moderated user comments. We also provide word embeddings pre-trained on 5.2M comments from the same portal. Furthermore, we experiment on the 'attacks' dataset of<cite> Wulczyn et al. (2017)</cite> , approx. 115K English Wikipedia talk page comments labeled as containing personal attacks or not. In a fully automatic scenario, there is no moderator and a system accepts or rejects comments. Although this scenario may be the only available one, e.g., when news portals cannot afford moderators, it is unrealistic to expect that fully automatic moderation will be perfect, because abusive comments may involve irony, sarcasm, harassment without profane phrases etc., which are particularly difficult for a machine to detect. When moderators are available, it is more realistic to develop semi- automatic systems aiming to assist, rather than replace the moderators, a scenario that has not been considered in previous work. In this case, comments for which the system is uncertain (Fig. 1 ) are shown to a moderator to decide; all other comments are accepted or rejected by the system. We discuss how moderation systems can be tuned, depending on the availability and workload of the moderators.",
  "y": "uses"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_3",
  "x": "1.45M training comments (covering Jan. 1, 2015 to Oct. 6, 2016 in the Gazzetta dataset; we call them G-TRAIN-L (Table 1) . Some experiments use only the first 100K comments of G-TRAIN-L, called G-TRAIN-S. An additional set of 60,900 comments (Oct. 7 to Nov. 11, 2016) was split to development (G-DEV, 29,700 comments), large test (G-TEST-L, 29,700), and small test set (G-TEST-S, 1,500). Gazzetta's moderators (2 full-time, plus journalists occasionally helping) are occasionally instructed to be stricter (e.g., during violent events). To get a more accurate view of performance in normal situtations, we manually re-moderated (labeled as 'accept' or 'reject') the comments of G-TEST-S, producing G-TEST-S-R. The reject ratio is approx. 30% in all subsets, except for G-TEST-S-R where it drops to 22%, because there are no occasions where the moderators were instructed to be stricter in G-TEST-S-R. Each G-TEST-S-R comment was re-moderated by five annotators. Krippendorff's (2004) alpha was 0.4762, close to the value (0.45) reported by<cite> Wulczyn et al. (2017)</cite> for the Wikipedia 'attacks' dataset. Using Cohen's Kappa (Cohen, 1960) , the mean pairwise agreement was 0.4749. The mean pairwise percentage of agreement (% of comments each pair of annotators agreed on) was 81.33%. Cohen's Kappa and Krippendorff's alpha lead to lower scores, because they account for agreement by chance, which is high when there is class imbalance (22% reject, 78% accept in G-TEST-S-R).",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_4",
  "x": "The Wikipedia 'attacks' dataset <cite>(Wulczyn et al., 2017)</cite> contains approx. 115K English Wikipedia talk page comments, which were labeled as containing personal attacks or not. Each comment was labeled by at least 10 annotators. Inter-annotator agreement, measured on a random sample of 1K comments using Krippendorff's (2004) alpha, was 0.45. The gold label of each comment is determined by the majority of annotators, leading to binary labels (accept, reject). Alternatively, the gold label is the percentage of annotators that labeled the comment as 'accept' (or 'reject'), leading to probabilistic labels. 7 The dataset is split in three parts (Table 1) : training (W-ATT-TRAIN, 69,526 comments), development (W-ATT-DEV, 23,160), and test (W-ATT-TEST, 23,178). In all three parts, the rejected comments are 12%, but this is an artificial ratio (Wulczyn et al. oversampled comments posted by banned users). By contrast, the ratio of rejected comments in all the Gazzetta subsets is the truly observed one. The Wikipedia comments are also longer (median length 38 tokens) compared to Gazzetta's (median length 25 tokens).",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_5",
  "x": [
   "The Wikipedia comments are also longer (median length 38 tokens) compared to Gazzetta's (median length 25 tokens). Wulczyn et al. (2017) also provide two additional datasets of English Wikipedia talk page comments, which are not used in this paper. The first one, called 'aggression' dataset, contains the same comments as the 'attacks' dataset, now labeled as 'aggressive' or not. The (probabilistic) labels of the 'attacks' and 'aggression' datasets are very highly correlated (0.8992 Spearman, 0.9718 Pearson) and we did not consider the aggression dataset any further. The second additional dataset, called 'toxicity' dataset, contains approx. 160K comments labeled as being toxic or not. Experiments we reported elsewhere (Pavlopoulos et al., 2017) show that results on the 'attacks' and 'toxicity' datasets are very similar; we do not include results on the latter in this paper to save space. ---------------------------------- **METHODS**"
  ],
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_6",
  "x": "Wulczyn et al. (2017) also provide two additional datasets of English Wikipedia talk page comments, which are not used in this paper. The first one, called 'aggression' dataset, contains the same comments as the 'attacks' dataset, now labeled as 'aggressive' or not. The (probabilistic) labels of the 'attacks' and 'aggression' datasets are very highly correlated (0.8992 Spearman, 0.9718 Pearson) and we did not consider the aggression dataset any further. The second additional dataset, called 'toxicity' dataset, contains approx. 160K comments labeled as being toxic or not. Experiments we reported elsewhere (Pavlopoulos et al., 2017) show that results on the 'attacks' and 'toxicity' datasets are very similar; we do not include results on the latter in this paper to save space. ---------------------------------- **METHODS** We experimented with an RNN operating on word embeddings, the same RNN enhanced with our attention mechanism (a-RNN), a vanilla convolutional neural network (CNN) also operating on word embeddings, the DETOX system of<cite> Wulczyn et al. (2017)</cite> , and a baseline that uses word lists.",
  "y": "uses"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_7",
  "x": "DETOX <cite>(Wulczyn et al., 2017)</cite> was the previous state of the art in comment moderation, in the sense that it had the best reported results on the Wikipedia datasets (Section 2.2), which were in turn the largest previous publicly available dataset of moderated user comments. 8 DETOX represents each comment as a bag of word n-grams (n \u2264 2, each comment becomes a bag containing its 1-grams and 2-grams) or a bag of character n-grams (n \u2264 5, each comment becomes a bag containing character 1-grams, . . . , 5-grams). DETOX can rely on a logistic regression (LR) or MLP classifier, and it can use binary or probabilistic gold labels (Section 2.2) during training. We used the DETOX implementation provided by Wulczyn et al. and the same grid search (and code) to tune the hyper-parameters of DETOX that select word or character n-grams, classifier (LR or MLP), and gold labels (binary or probabilistic). For Gazzetta, only binary gold labels were possible, since G-TRAIN-L and G-TRAIN-S have a single gold label per comment. Unlike Wulczyn et al., we tuned the hyper-parameters by evaluating (computing AUC and Spearman, Section 4) on a random 2% of held-out comments of W-ATT-TRAIN or G-TRAIN-S, instead of the development subsets, to be able to obtain more realistic results from the development sets while developing the methods. For both Wikipedia and Gazzetta, the tuning selected character n-grams, as in the work of Wulczyn et al. Also, for both Wikipedia and Gazzetta, it preferred LR to MLP, whereas Wulczyn et al. reported slightly higher performance 8 Two of the co-authors of<cite> Wulczyn et al. (2017)</cite> are with Jigsaw, who recently announced Perspective, a system to detect 'toxic' comments. Perspective is not the same as DETOX (personal communication), but we were unable to obtain scientific articles describing it. An API for Perspective is available at https://www.perspectiveapi.",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_8",
  "x": "**METHODS** We experimented with an RNN operating on word embeddings, the same RNN enhanced with our attention mechanism (a-RNN), a vanilla convolutional neural network (CNN) also operating on word embeddings, the DETOX system of<cite> Wulczyn et al. (2017)</cite> , and a baseline that uses word lists. ---------------------------------- **DETOX** DETOX <cite>(Wulczyn et al., 2017)</cite> was the previous state of the art in comment moderation, in the sense that it had the best reported results on the Wikipedia datasets (Section 2.2), which were in turn the largest previous publicly available dataset of moderated user comments. 8 DETOX represents each comment as a bag of word n-grams (n \u2264 2, each comment becomes a bag containing its 1-grams and 2-grams) or a bag of character n-grams (n \u2264 5, each comment becomes a bag containing character 1-grams, . . . , 5-grams). DETOX can rely on a logistic regression (LR) or MLP classifier, and it can use binary or probabilistic gold labels (Section 2.2) during training. We used the DETOX implementation provided by Wulczyn et al. and the same grid search (and code) to tune the hyper-parameters of DETOX that select word or character n-grams, classifier (LR or MLP), and gold labels (binary or probabilistic). For Gazzetta, only binary gold labels were possible, since G-TRAIN-L and G-TRAIN-S have a single gold label per comment.",
  "y": "uses"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_9",
  "x": "DETOX can rely on a logistic regression (LR) or MLP classifier, and it can use binary or probabilistic gold labels (Section 2.2) during training. We used the DETOX implementation provided by Wulczyn et al. and the same grid search (and code) to tune the hyper-parameters of DETOX that select word or character n-grams, classifier (LR or MLP), and gold labels (binary or probabilistic). For Gazzetta, only binary gold labels were possible, since G-TRAIN-L and G-TRAIN-S have a single gold label per comment. Unlike Wulczyn et al., we tuned the hyper-parameters by evaluating (computing AUC and Spearman, Section 4) on a random 2% of held-out comments of W-ATT-TRAIN or G-TRAIN-S, instead of the development subsets, to be able to obtain more realistic results from the development sets while developing the methods. For both Wikipedia and Gazzetta, the tuning selected character n-grams, as in the work of Wulczyn et al. Also, for both Wikipedia and Gazzetta, it preferred LR to MLP, whereas Wulczyn et al. reported slightly higher performance 8 Two of the co-authors of<cite> Wulczyn et al. (2017)</cite> are with Jigsaw, who recently announced Perspective, a system to detect 'toxic' comments. Perspective is not the same as DETOX (personal communication), but we were unable to obtain scientific articles describing it. An API for Perspective is available at https://www.perspectiveapi. com/, but we did not have access to the API at the time the experiments of this paper were carried out. for the MLP on W-ATT-DEV.",
  "y": "differences"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_12",
  "x": "When computing Spearman, the gold label is probabilistic (% of annotators that accepted the comment). The decisions of the systems are always probabilistic. Table 2 : Comment classification results. Scores reported by<cite> Wulczyn et al. (2017)</cite> are shown in brackets. always better than CNN and DETOX; there is no clear winner between CNN and DETOX. Furthermore, a-RNN is always better than RNN on Gazzetta comments, but not on Wikipedia comments, where RNN is overall slightly better according to Table 2 . Also, da-CENT is always worse than a-RNN and RNN, confirming that the hidden states (intuitively, context-aware word embeddings) of the RNN chain are important, even with the attention mechanism. Increasing the size of the Gazzetta training set (G-TRAIN-S to G-TRAIN-L) significantly improves the performance of all methods. The implementation of DETOX could not handle the size of G-TRAIN-L, which is why we do not report DETOX results for G-TRAIN-L. Notice, also, that the Wikipedia dataset is easier than the Gazzetta one (all methods perform better on Wikipedia comments, compared to Gazzetta).",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_13",
  "x": "When tuned for 100% coverage, comments for which the system is uncertain (gray zone) cannot be avoided and there are inevitably more misclassifications; the use of F 2 during threshold tuning places more emphasis on avoiding wrongly accepted comments, leading to high P accept (0.82), at the expense of wrongly rejected comments, i.e., sacrificing P reject (0.59). On the re-moderated G-TEST-S-R (similar diagrams, not shown), P accept , P reject become 0.96, 0.88 for coverage 50%, and 0.92, 0.48 for coverage 100%. We also repeated the annotator ensemble experiment of<cite> Wulczyn et al. (2017)</cite> on 8K randomly chosen comments of W-ATT-TEST (4K comments from random users, 4K comments from banned users). 19 The decisions of 10 randomly chosen annotators (possibly different per comment) were used to construct the gold label of each comment. The gold labels were then compared to the decisions of the systems and the decisions of an ensemble of k other annotators, k ranging from 1 to 10. Table 3 shows the mean AUC and Spearman scores, averaged over 25 runs of the experiment, along with standard errrors (in brackets). We conclude that RNN and a-RNN are as good as an ensemble of 7 human annotators; CNN is as good as 4 annotators; DETOX is as good as 4 in AUC and 3 annotators in Spearman correlation, which is consistent with the results of<cite> Wulczyn et al. (2017)</cite> . ---------------------------------- **SNIPPET HIGHLIGHTING EVALUATION**",
  "y": "uses background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_14",
  "x": "On the re-moderated G-TEST-S-R (similar diagrams, not shown), P accept , P reject become 0.96, 0.88 for coverage 50%, and 0.92, 0.48 for coverage 100%. We also repeated the annotator ensemble experiment of<cite> Wulczyn et al. (2017)</cite> on 8K randomly chosen comments of W-ATT-TEST (4K comments from random users, 4K comments from banned users). 19 The decisions of 10 randomly chosen annotators (possibly different per comment) were used to construct the gold label of each comment. The gold labels were then compared to the decisions of the systems and the decisions of an ensemble of k other annotators, k ranging from 1 to 10. Table 3 shows the mean AUC and Spearman scores, averaged over 25 runs of the experiment, along with standard errrors (in brackets). We conclude that RNN and a-RNN are as good as an ensemble of 7 human annotators; CNN is as good as 4 annotators; DETOX is as good as 4 in AUC and 3 annotators in Spearman correlation, which is consistent with the results of<cite> Wulczyn et al. (2017)</cite> . ---------------------------------- **SNIPPET HIGHLIGHTING EVALUATION** To investigate if the attention scores of a-RNN can highlight suspicious words, we focused on G-TEST-S-R, the only dataset with suspicious snippets annotated by humans.",
  "y": "similarities"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_15",
  "x": [
   "train a (token or characterbased) RNN language model per class (accept, reject), and use the probability ratio of the two models to accept or reject user comments. Experiments on the dataset of Djuric et al. (2015) , however, showed that their method (RNNLMs) performed worse than a combination of SVM and Naive Bayes classifiers (NBSVM) that used character and token n-grams. An LR classifier operating on DOC2VEC-like comment embeddings (Le and Mikolov, 2014) also performed worse than NBSVM. To surpass NBSVM, Mehdad et al. used an SVM to combine features from their three other methods (RNNLMs, LR with DOC2VEC, NBSVM). Wulczyn et al. (2017) experimented with character and word n-grams. We included their dataset and moderation system (DETOX) in our experiments. Waseem et al. (2016) used approx. 17K tweets annotated for hate speech. Their best results were obtained using an LR classifier with character n-grams (n = 1, . . . , 4), plus gender."
  ],
  "y": "similarities uses"
 },
 {
  "id": "929020618e8e1daa6a769f552a4655_0",
  "x": "These models are highly accurate for their use of sophisticated NLP features and machine intelligence to associate the extracted features to a proper readability level. Models proposed by Vajjala and Meurers [17] , Xia et al. <cite>[18]</cite> , and Mohammadi and Khasteh [19] are examples of state-of-the-art models for their target languages and target audience. These models are using Support Vector Machines trained on complex and proper feature sets extracted from related datasets. Still, their use of complicated and language-specific NLP features makes these models challenging to implement and heavily language-dependent. Furthermore, they do not offer any solution to the problem of finding the minimum portion of the text required to accurately assess the readability of a long text. The feature extraction task from a long text is computationally heavy, and minimizing the required length of the text to assess its readability is vital in large collections of documents. Utilizing the recent advances in deep learning and deep reinforcement learning, a new approach to text readability assessment is introduced in this study. Word-to-vec and frequency language model are used to represent the input text in order to eliminate the need for sophisticated NLP features. In addition to that, such text representation enables the applicability of the model on different languages using the word-to-vec and frequency language model of the target language.",
  "y": "background"
 },
 {
  "id": "929020618e8e1daa6a769f552a4655_1",
  "x": "Different models are required to assess the readability of English texts for the second language readers as a different set of characteristics of text is influential on its readability level for second language readers [35] . Xia et al. <cite>[18]</cite> has published a thorough study on second language text readability assessment. Similar to the study done by Vajjala and Meurers [17] , Xia has used an SVM classifier, and a set of NLP features consists of traditional, lexico-semantic, parse tree syntactic, language modeling, and discourse-based features. Many comparable studies have been carried out to create automated text readability assessment models for languages such as French [36] Russian [37] Germen [38] Chinese [39] Arabic [40] Portuguese [41] . This study is concentrated on the English and Persian languages as our test case for multilingual text readability assessment. The only known machine learning based text readability assessment model for the Persian language is the model proposed by Mohammadi and Khasteh [19] , which also uses an SVM model. In conclusion, machine learning models can obtain higher accuracies in contrast to the traditional formulas while being more straightforward to construct, assuming the existence of a useful dataset. In contrast, their use of a large number of sophisticated features makes them time-consuming and costly to implement, language-dependent, and less interpretable. The focus of this study is to reduce the need for intricate feature engineering and language dependency of text readability assessment models.",
  "y": "background"
 },
 {
  "id": "929020618e8e1daa6a769f552a4655_2",
  "x": "Prior to this study, distinct models have been used to assess the readability of English texts for second language readers. Since the DRL model eliminates the need for specific feature engineering for different types of text readability assessment, the proposed model can be applied to second language datasets without any modifications. To examine the proposed DRL model regarding this ability, it is applied to the Cambridge Exams dataset <cite>[18]</cite> . This dataset contains texts from the reading section of Cambridge English Exams, which is targeted for students at five readability levels (A2 to C2) of the Common European Framework of Reference (CEFR). This dataset contains 331 texts, which makes it a small dataset in comparison to the Weebit dataset. The automated feature extraction ability of the proposed model has also given the model the ability to be readily applied to other languages. As GloVe and language models are language-specific, the only necessary change is the use of the GloVe and frequency language model of the target language. These features are readily and freely available on the internet for many languages. The proposed model is evaluated on the Persian text readability dataset [19] .",
  "y": "uses"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_0",
  "x": "While incorporating existing knowledge (from curated knowledge bases) for the purpose of question-answering [12, 9, 16] is an important area of research, we consider the simpler setting where all the information is contained within the text itself -which is the approach taken by many recent memory based neural network models [17, <cite>18</cite>, 19, 20] . Recently, Henaff et. al. <cite>[18]</cite> proposed a dynamic memory based neural network for implicitly modeling the state of entities present in the text for question answering. However, <cite>this model</cite> lacks any module for relational reasoning. In response, we propose RelNet, which extends memoryaugmented neural networks with a relational memory to reason about relationships between multiple entities present within the text. Our end-to-end method reads text, and writes to both memory slots and edges between them. Intuitively, the memory slots correspond to entities and the edges correspond to relationships between entities, each represented as a vector. The only supervision signal for our method comes from answering questions on the text. We demonstrate the utility of the model through experiments on the bAbI tasks [19] and find that the model achieves smaller mean error across the tasks than the best previously published result [<cite>18</cite>] in the 10k examples regime and achieves 0% error on 11 of the 20 tasks.",
  "y": "background motivation"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_1",
  "x": "For the task of question-answering, we instead make an attempt at an end-to-end approach which directly models the entities and relations in the text as memory slots. While incorporating existing knowledge (from curated knowledge bases) for the purpose of question-answering [12, 9, 16] is an important area of research, we consider the simpler setting where all the information is contained within the text itself -which is the approach taken by many recent memory based neural network models [17, <cite>18</cite>, 19, 20] . Recently, Henaff et. al. <cite>[18]</cite> proposed a dynamic memory based neural network for implicitly modeling the state of entities present in the text for question answering. However, <cite>this model</cite> lacks any module for relational reasoning. In response, we propose RelNet, which extends memoryaugmented neural networks with a relational memory to reason about relationships between multiple entities present within the text. Our end-to-end method reads text, and writes to both memory slots and edges between them. Intuitively, the memory slots correspond to entities and the edges correspond to relationships between entities, each represented as a vector. The only supervision signal for our method comes from answering questions on the text.",
  "y": "motivation"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_2",
  "x": "While incorporating existing knowledge (from curated knowledge bases) for the purpose of question-answering [12, 9, 16] is an important area of research, we consider the simpler setting where all the information is contained within the text itself -which is the approach taken by many recent memory based neural network models [17, <cite>18</cite>, 19, 20] . Recently, Henaff et. al. <cite>[18]</cite> proposed a dynamic memory based neural network for implicitly modeling the state of entities present in the text for question answering. However, <cite>this model</cite> lacks any module for relational reasoning. In response, we propose RelNet, which extends memoryaugmented neural networks with a relational memory to reason about relationships between multiple entities present within the text. Our end-to-end method reads text, and writes to both memory slots and edges between them. Intuitively, the memory slots correspond to entities and the edges correspond to relationships between entities, each represented as a vector. The only supervision signal for our method comes from answering questions on the text. We demonstrate the utility of the model through experiments on the bAbI tasks [19] and find that the model achieves smaller mean error across the tasks than the best previously published result [<cite>18</cite>] in the 10k examples regime and achieves 0% error on 11 of the 20 tasks.",
  "y": "motivation"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_3",
  "x": "The only supervision signal for our method comes from answering questions on the text. We demonstrate the utility of the model through experiments on the bAbI tasks [19] and find that the model achieves smaller mean error across the tasks than the best previously published result [<cite>18</cite>] in the 10k examples regime and achieves 0% error on 11 of the 20 tasks. ---------------------------------- **RELNET MODEL** We describe the RelNet model in this section. The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to <cite>Recurrent Entity Networks</cite> [<cite>18</cite>] and then equip it with an additional relational memory. There are three main components to the model: 1) input encoder 2) dynamic memory, and 3) output module. We will describe these three modules in details.",
  "y": "differences"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_4",
  "x": "We model the dynamic memory in a fashion similar to <cite>Recurrent Entity Networks</cite> [<cite>18</cite>] and then equip it with an additional relational memory. There are three main components to the model: 1) input encoder 2) dynamic memory, and 3) output module. We will describe these three modules in details. The input encoder and output module implementations are similar to the <cite>Entity Network</cite> [<cite>18</cite>] and main novelty lies in the dynamic memory. We describe the operations executed by the network for a single example consisting of a document with T sentences, where each sentence consists of a sequence of words represented with K-dimensional word embeddings {e 1 , . . . , e N }, a question on the document represented as another sequence of words and an answer to the question. ---------------------------------- **INPUT ENCODER:** The input at each time point is a sentence from the document which can be encoded into a fixed vector representation using some encoding mechanism, such as a recurrent neural network. We use a simple encoder with a learned multiplicative mask [<cite>18</cite>, 17] :",
  "y": "similarities extends"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_5",
  "x": "We model the dynamic memory in a fashion similar to <cite>Recurrent Entity Networks</cite> [<cite>18</cite>] and then equip it with an additional relational memory. There are three main components to the model: 1) input encoder 2) dynamic memory, and 3) output module. We will describe these three modules in details. The input encoder and output module implementations are similar to the <cite>Entity Network</cite> [<cite>18</cite>] and main novelty lies in the dynamic memory. We describe the operations executed by the network for a single example consisting of a document with T sentences, where each sentence consists of a sequence of words represented with K-dimensional word embeddings {e 1 , . . . , e N }, a question on the document represented as another sequence of words and an answer to the question. ---------------------------------- **INPUT ENCODER:** The input at each time point is a sentence from the document which can be encoded into a fixed vector representation using some encoding mechanism, such as a recurrent neural network. We use a simple encoder with a learned multiplicative mask [<cite>18</cite>, 17] :",
  "y": "similarities extends"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_6",
  "x": "We model the dynamic memory in a fashion similar to <cite>Recurrent Entity Networks</cite> [<cite>18</cite>] and then equip it with an additional relational memory. There are three main components to the model: 1) input encoder 2) dynamic memory, and 3) output module. We will describe these three modules in details. The input encoder and output module implementations are similar to the <cite>Entity Network</cite> [<cite>18</cite>] and main novelty lies in the dynamic memory. We describe the operations executed by the network for a single example consisting of a document with T sentences, where each sentence consists of a sequence of words represented with K-dimensional word embeddings {e 1 , . . . , e N }, a question on the document represented as another sequence of words and an answer to the question. ---------------------------------- **INPUT ENCODER:** The input at each time point is a sentence from the document which can be encoded into a fixed vector representation using some encoding mechanism, such as a recurrent neural network. We use a simple encoder with a learned multiplicative mask [<cite>18</cite>, 17] :",
  "y": "uses"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_7",
  "x": "The memory consists of two parts: entity memory and relational memory. The entity memory is organized as a key-value memory network [12] , where the keys are global embeddings updated during training time but not during inference, and the value memory slot is a dynamic memory for each example (document, question) whose values are updated while reading the document. The memory thus consists of D memory slots {m 1 , . . . , m D } (each is a vector of dimension K) and associated keys {k 1 , . . . , k D } (again vectors of dimension K). At time t, after reading the sentence t into a vector representation s t , a gating mechanism decides the set of memories to be updated (< \u00b7, \u00b7 > denotes inner product): Intuitively the memory slots can be thought of as entities. Indeed, Henaff et. al. <cite>[18]</cite> found that if <cite>they</cite> tie the key vectors to entities in the text then the memories contain information about the state of those entities. The update in (1) essentially does a soft selection of memory slots based on cosine distance in the embedding space. Note that there can be multiple entites in a sentence hence a sigmoid operation is more suitable, and it is also more scalable <cite>[18]</cite> .",
  "y": "background"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_8",
  "x": "The memory thus consists of D memory slots {m 1 , . . . , m D } (each is a vector of dimension K) and associated keys {k 1 , . . . , k D } (again vectors of dimension K). At time t, after reading the sentence t into a vector representation s t , a gating mechanism decides the set of memories to be updated (< \u00b7, \u00b7 > denotes inner product): Intuitively the memory slots can be thought of as entities. Indeed, Henaff et. al. <cite>[18]</cite> found that if <cite>they</cite> tie the key vectors to entities in the text then the memories contain information about the state of those entities. The update in (1) essentially does a soft selection of memory slots based on cosine distance in the embedding space. Note that there can be multiple entites in a sentence hence a sigmoid operation is more suitable, and it is also more scalable <cite>[18]</cite> . After selecting the set of memories, there is an update step which stores information in the corresponding memory slots: where PReLU is a parametric Rectified linear unit [21] , and U , V and W are k \u00d7 k parameter matrices.",
  "y": "background"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_9",
  "x": "The relational memories are updated as follows. First, a gating mechanism decides the set of active relational memories: where g m i , g m j select the relational memory slot based on the active entity slots and the last sigmoid gate decides whether the corresponding relational memory needs to be updated based on the current input sentence. After selecting the set of active relational memory, we update the contents of the relational memory:r ij \u2190 P ReLU (Ar ij + Bs t ) r ij \u2190 r ij + g r ij \u2299r ij (4) where again A, B are k \u00d7 k parameter matrices. Note that for updates (3)- (4) we use a different encoding mask to obtain the sentence representation for relations. Similar to [<cite>18</cite>] , we normalize the memories after each update step (that is after reading each sentence). This acts as a forget step and does not cause the memory to explode. The full memory consists of the entity memory slots {h j } and the relational memory slots {r ij }. Output Module This is a standard attention module used in memory networks [17, <cite>18</cite>] .",
  "y": "similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_10",
  "x": "After selecting the set of active relational memory, we update the contents of the relational memory:r ij \u2190 P ReLU (Ar ij + Bs t ) r ij \u2190 r ij + g r ij \u2299r ij (4) where again A, B are k \u00d7 k parameter matrices. Note that for updates (3)- (4) we use a different encoding mask to obtain the sentence representation for relations. Similar to [<cite>18</cite>] , we normalize the memories after each update step (that is after reading each sentence). This acts as a forget step and does not cause the memory to explode. The full memory consists of the entity memory slots {h j } and the relational memory slots {r ij }. Output Module This is a standard attention module used in memory networks [17, <cite>18</cite>] . The question is encoded as a K dimensional vector q using the same encoding mechanism as the sentences (though with a separate learned mask). We first concatenate the relational memory vectors with the corresponding entity vectors, and project the resulting memory vector to k dimension. Then attention on these projected memories, conditioned on the vector q, yields the final answer:",
  "y": "similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_11",
  "x": "The question is encoded as a K dimensional vector q using the same encoding mechanism as the sentences (though with a separate learned mask). We first concatenate the relational memory vectors with the corresponding entity vectors, and project the resulting memory vector to k dimension. Then attention on these projected memories, conditioned on the vector q, yields the final answer: where y is the predicted answer, and C, H, Z are parameter matrices. ---------------------------------- **RELATED WORK** There is a long line of work in textual question-answering systems [22, 23] . Recent successful approaches use memory based neural networks for question answering [24, 19, 25, 20, <cite>18</cite>] . Our model is also a memory network based model and is also related to the neural turing machine [26] .",
  "y": "background"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_12",
  "x": "We first concatenate the relational memory vectors with the corresponding entity vectors, and project the resulting memory vector to k dimension. Then attention on these projected memories, conditioned on the vector q, yields the final answer: where y is the predicted answer, and C, H, Z are parameter matrices. ---------------------------------- **RELATED WORK** There is a long line of work in textual question-answering systems [22, 23] . Recent successful approaches use memory based neural networks for question answering [24, 19, 25, 20, <cite>18</cite>] . Our model is also a memory network based model and is also related to the neural turing machine [26] . As described previously, the model is closely related to the <cite>Recurrent Entity Networks</cite> model [<cite>18</cite>] which describes an end-to-end approach to model entities in text but does not directly model relations.",
  "y": "background"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_14",
  "x": "Graph based neural network models [33, 34, 35] have been proposed which take graph data as an input. The relational memory however does not rely on a specified graph structure and such models can potentially be used for multi-hop reasoning over the relational memory. ---------------------------------- **EXPERIMENTS** We evaluate the model's performance on the bAbI tasks [19] , a collection of 20 question answering tasks which have become a benchmark for evaluating memory-augmented neural networks. We Task <cite>EntNet</cite> [<cite>18</cite>] compare the performance with the <cite>Recurrent Entity Networks model (EntNet)</cite> <cite>[18]</cite> . Performance is measured in terms of mean percentage error on the tasks. Training Details: We used Adam and did a grid search for the learning rate in {0.01, 0.005, 0.001} and choose a fixed learning rate of 0.005 based on performance on the validation set, and clip the gradient norm at 2. We keep all other details similar to <cite>[18]</cite> for a fair comparison.",
  "y": "similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_15",
  "x": "We Task <cite>EntNet</cite> [<cite>18</cite>] compare the performance with the <cite>Recurrent Entity Networks model (EntNet)</cite> <cite>[18]</cite> . Performance is measured in terms of mean percentage error on the tasks. Training Details: We used Adam and did a grid search for the learning rate in {0.01, 0.005, 0.001} and choose a fixed learning rate of 0.005 based on performance on the validation set, and clip the gradient norm at 2. We keep all other details similar to <cite>[18]</cite> for a fair comparison. embedding dimensions were fixed to be 100, models were trained for a maximum of 250 epochs with minibatches size of 32 for all tasks except 3 for which the batch size was 16. The document sizes were limited to most recent 70 sentences for all tasks, except for task 3 for which it was limited to 130. The RelNet models were run for 5 times with random seed on each task and the model with best validation performance was chosen as the final model. The baseline <cite>EntNet</cite> model was run for 10 times for each task <cite>[18]</cite> . The results are shown in Table 1 .",
  "y": "similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_16",
  "x": "Training Details: We used Adam and did a grid search for the learning rate in {0.01, 0.005, 0.001} and choose a fixed learning rate of 0.005 based on performance on the validation set, and clip the gradient norm at 2. We keep all other details similar to <cite>[18]</cite> for a fair comparison. embedding dimensions were fixed to be 100, models were trained for a maximum of 250 epochs with minibatches size of 32 for all tasks except 3 for which the batch size was 16. The document sizes were limited to most recent 70 sentences for all tasks, except for task 3 for which it was limited to 130. The RelNet models were run for 5 times with random seed on each task and the model with best validation performance was chosen as the final model. The baseline <cite>EntNet</cite> model was run for 10 times for each task <cite>[18]</cite> . The results are shown in Table 1 . The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the <cite>EntNet</cite> model <cite>[18]</cite> . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the <cite>EntNet</cite> model achieves 0% error on 7 of the tasks.",
  "y": "differences"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_17",
  "x": "The document sizes were limited to most recent 70 sentences for all tasks, except for task 3 for which it was limited to 130. The RelNet models were run for 5 times with random seed on each task and the model with best validation performance was chosen as the final model. The baseline <cite>EntNet</cite> model was run for 10 times for each task <cite>[18]</cite> . The results are shown in Table 1 . The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the <cite>EntNet</cite> model <cite>[18]</cite> . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the <cite>EntNet</cite> model achieves 0% error on 7 of the tasks. ---------------------------------- **CONCLUSION** We demonstrated an end-to-end trained neural network augmented with a structured memory representation which can reason about entities and relations for question answering.",
  "y": "differences"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_0",
  "x": "Multi-task learning (MTL) is an important machine learning paradigm that aims at improving the generalization performance of a task using other related tasks. Such framework has been widely studied by Thrun (1996) ; Caruana (1997) ; Evgeniou & Pontil (2004) ; Ando & Zhang (2005) ; Argyriou et al. (2007) ; Kumar & III (2012) , among many others. In the context of deep neural networks, MTL has been applied successfully to various problems ranging from language (Liu et al., 2015) , to vision (Donahue et al., 2014) , and speech (Heigold et al., 2013; Huang et al., 2013) . Recently, sequence to sequence (seq2seq) learning, proposed by Kalchbrenner & Blunsom (2013) , Sutskever et al. (2014) , and Cho et al. (2014) , emerges as an effective paradigm for dealing with variable-length inputs and outputs. seq2seq learning, at its core, uses recurrent neural networks to map variable-length input sequences to variable-length output sequences. While relatively new, the seq2seq approach has achieved state-of-the-art results in not only its original application -machine translation - (Luong et al., 2015b; Jean et al., 2015a; Luong et al., 2015a; Jean et al., 2015b; Luong & Manning, 2015) , but also image caption generation , and constituency parsing<cite> (Vinyals et al., 2015a)</cite> . Despite the popularity of multi-task learning and sequence to sequence learning, there has been little work in combining MTL with seq2seq learning. To the best of our knowledge, there is only one recent publication by Dong et al. (2015) which applies a seq2seq models for machine translation, where the goal is to translate from one language to multiple languages. In this work, we propose three MTL approaches that complement one another: (a) the one-to-many approach -for tasks that can have an encoder in common, such as translation and parsing; this applies to the multi-target translation setting in (Dong et al., 2015) as well, (b) the many-to-one approach -useful for multisource translation or tasks in which only the decoder can be easily shared, such as translation and image captioning, and lastly, (c) the many-to-many approach -which share multiple encoders and decoders through which we study the effect of unsupervised learning in translation.",
  "y": "background"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_1",
  "x": "Recently, sequence to sequence (seq2seq) learning, proposed by Kalchbrenner & Blunsom (2013) , Sutskever et al. (2014) , and Cho et al. (2014) , emerges as an effective paradigm for dealing with variable-length inputs and outputs. seq2seq learning, at its core, uses recurrent neural networks to map variable-length input sequences to variable-length output sequences. While relatively new, the seq2seq approach has achieved state-of-the-art results in not only its original application -machine translation - (Luong et al., 2015b; Jean et al., 2015a; Luong et al., 2015a; Jean et al., 2015b; Luong & Manning, 2015) , but also image caption generation , and constituency parsing<cite> (Vinyals et al., 2015a)</cite> . Despite the popularity of multi-task learning and sequence to sequence learning, there has been little work in combining MTL with seq2seq learning. To the best of our knowledge, there is only one recent publication by Dong et al. (2015) which applies a seq2seq models for machine translation, where the goal is to translate from one language to multiple languages. In this work, we propose three MTL approaches that complement one another: (a) the one-to-many approach -for tasks that can have an encoder in common, such as translation and parsing; this applies to the multi-target translation setting in (Dong et al., 2015) as well, (b) the many-to-one approach -useful for multisource translation or tasks in which only the decoder can be easily shared, such as translation and image captioning, and lastly, (c) the many-to-many approach -which share multiple encoders and decoders through which we study the effect of unsupervised learning in translation. We show that syntactic parsing and image caption generation improves the translation quality between English (Sutskever et al., 2014) and (right) constituent parsing<cite> (Vinyals et al., 2015a)</cite> . and German by up to +1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F 1 .",
  "y": "motivation"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_2",
  "x": "In the context of deep neural networks, MTL has been applied successfully to various problems ranging from language (Liu et al., 2015) , to vision (Donahue et al., 2014) , and speech (Heigold et al., 2013; Huang et al., 2013) . Recently, sequence to sequence (seq2seq) learning, proposed by Kalchbrenner & Blunsom (2013) , Sutskever et al. (2014) , and Cho et al. (2014) , emerges as an effective paradigm for dealing with variable-length inputs and outputs. seq2seq learning, at its core, uses recurrent neural networks to map variable-length input sequences to variable-length output sequences. While relatively new, the seq2seq approach has achieved state-of-the-art results in not only its original application -machine translation - (Luong et al., 2015b; Jean et al., 2015a; Luong et al., 2015a; Jean et al., 2015b; Luong & Manning, 2015) , but also image caption generation , and constituency parsing<cite> (Vinyals et al., 2015a)</cite> . Despite the popularity of multi-task learning and sequence to sequence learning, there has been little work in combining MTL with seq2seq learning. To the best of our knowledge, there is only one recent publication by Dong et al. (2015) which applies a seq2seq models for machine translation, where the goal is to translate from one language to multiple languages. In this work, we propose three MTL approaches that complement one another: (a) the one-to-many approach -for tasks that can have an encoder in common, such as translation and parsing; this applies to the multi-target translation setting in (Dong et al., 2015) as well, (b) the many-to-one approach -useful for multisource translation or tasks in which only the decoder can be easily shared, such as translation and image captioning, and lastly, (c) the many-to-many approach -which share multiple encoders and decoders through which we study the effect of unsupervised learning in translation. We show that syntactic parsing and image caption generation improves the translation quality between English (Sutskever et al., 2014) and (right) constituent parsing<cite> (Vinyals et al., 2015a)</cite> . and German by up to +1.5 BLEU points over strong single-task baselines on the WMT benchmarks.",
  "y": "differences"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_3",
  "x": "Recently, Bahdanau et al. (2015) proposes an attention mechanism, a way to provide seq2seq models with a random access memory, to handle long input sequences. This is accomplished by setting s in Eq. (1) to be the set of encoder hidden states already computed. On the decoder side, at each time step, the attention mechanism will decide how much information to retrieve from that memory by learning where to focus, i.e., computing the alignment weights for all input positions. Recent work such as Jean et al., 2015a; Luong et al., 2015a; <cite>Vinyals et al., 2015a)</cite> has found that it is crucial to empower seq2seq models with the attention mechanism. ---------------------------------- **MULTI-TASK SEQUENCE-TO-SEQUENCE LEARNING** We generalize the work of Dong et al. (2015) to the multi-task sequence-to-sequence learning setting that includes the tasks of machine translation (MT), constituency parsing, and image caption generation. Depending which tasks involved, we propose to categorize multi-task seq2seq learning into three general settings.",
  "y": "background"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_5",
  "x": "Following Luong et al. (2015a) , we use the 50K most frequent words for each language from the training corpus. 1 These vocabularies are then shared with other tasks, except for parsing in which the target \"language\" has a vocabulary of 104 tags. We use newstest2013 (3000 sentences) as a validation set to select our hyperparameters, e.g., mixing coefficients. For testing, to be comparable with existing results in (Luong et al., 2015a) For the unsupervised tasks, we use the English and German monolingual corpora from WMT'15. 4 Since in our experiments, unsupervised tasks are always coupled with translation tasks, we use the same validation and test sets as the accompanied translation tasks. For constituency parsing, we experiment with two types of corpora: 1. a small corpus -the widely used Penn Tree Bank (PTB) dataset (Marcus et al., 1993) and, 2. a large corpus -the high-confidence (HC) parse trees provided by<cite> Vinyals et al. (2015a)</cite> . The two parsing tasks, however, are evaluated on the same validation (section 22) and test (section 23) sets from the PTB data. Note also that the parse trees have been linearized following<cite> Vinyals et al. (2015a)</cite> .",
  "y": "uses"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_6",
  "x": "1. a small corpus -the widely used Penn Tree Bank (PTB) dataset (Marcus et al., 1993) and, 2. a large corpus -the high-confidence (HC) parse trees provided by<cite> Vinyals et al. (2015a)</cite> . The two parsing tasks, however, are evaluated on the same validation (section 22) and test (section 23) sets from the PTB data. Note also that the parse trees have been linearized following<cite> Vinyals et al. (2015a)</cite> . Lastly, for image caption generation, we use a dataset of image and caption pairs provided by Vinyals et al. (2015b) . ---------------------------------- **TRAINING DETAILS** In all experiments, following Sutskever et al. (2014) and Luong et al. (2015b) , we train deep LSTM models as follows: (a) we use 4 LSTM layers each of which has 1000-dimensional cells and embeddings, 5 (b) parameters are uniformly initialized in [-0.06, 0.06] , (c) we use a mini-batch size of 128, (d) dropout is applied with probability of 0.2 over vertical connections (Pham et al., 2014) , (e) we use SGD with a fixed learning rate of 0.7, (f) input sequences are reversed, and lastly, (g) we use a simple finetuning schedule -after x epochs, we halve the learning rate every y epochs. The values x and y are referred as finetune start and finetune cycle in Table 1 together with the number of training epochs per task. As described in Section 3, for each multi-task experiment, we need to choose one task to be the reference task (which corresponds to \u03b1 1 = 1).",
  "y": "uses"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_7",
  "x": "---------------------------------- **LARGE TASKS WITH SMALL TASKS** In this setting, we want to understand if a small task such as PTB parsing can help improve the performance of a large task such as translation. Since the parsing task maps from a sequence of English words to a sequence of parsing tags<cite> (Vinyals et al., 2015a)</cite> , only the encoder can be shared with an English\u2192German translation task. As a result, this is a one-to-many MTL scenario ( \u00a73.1). To our surprise, the results in Table 2 suggest that by adding a very small number of parsing minibatches (with mixing ratio 0.01, i.e., one parsing mini-batch per 100 translation mini-batches), we can improve the translation quality substantially. More concretely, our best multi-task model yields a gain of +1.5 BLEU points over the single-task baseline. It is worth pointing out that as shown in Table 2 , our single-task baseline is very strong, even better than the equivalent non-attention model reported in (Luong et al., 2015a) . Larger mixing coefficients, however, overfit the small PTB corpus; hence, achieve smaller gains in translation quality.",
  "y": "background"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_8",
  "x": "For parsing, as<cite> Vinyals et al. (2015a)</cite> have shown that attention is crucial to achieve good parsing performance when training on the small PTB corpus, we do not set a high bar for our attention-free systems in this setup (better performances are reported in Section 4.3.3). Nevertheless, the parsing results in Table 2 indicate that MTL is also beneficial for parsing, yielding an improvement of up to +8.9 F 1 points over the baseline. 6 It would be interesting to study how MTL can be useful with the presence of the attention mechanism, which we leave for future work. ---------------------------------- **TASK** ---------------------------------- **LARGE TASKS WITH MEDIUM TASKS** We investigate whether the same pattern carries over to a medium task such as image caption generation. Since the image caption generation task maps images to a sequence of English words Xu et al., 2015) , only the decoder can be shared with a German\u2192English translation task.",
  "y": "similarities"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_9",
  "x": "Our first set of experiments is almost the same as the one-to-many setting in Section 4.3.1 which combines translation, as the reference task, with parsing. The only difference is in terms of parsing Table 2 . Reference tasks are in italic with mixing ratios in parentheses. The average results of 2 runs are in mean (stddev) format. data. Instead of using the small Penn Tree Bank corpus, we consider a large parsing resource, the high-confidence (HC) corpus, which is provided by<cite> Vinyals et al. (2015a)</cite> . As highlighted in Table 4 , the trend is consistent; MTL helps boost translation quality by up to +0.9 BLEU points. Table 4 : English\u2192German WMT'14 translation -shown are perplexities (ppl) and BLEU scores of various translation models. Our multi-task systems combine translation and parsing on the highconfidence corpus together.",
  "y": "uses"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_10",
  "x": "Best results are bolded. ---------------------------------- **TASK TRANSLATION** The second set of experiments shifts the attention to parsing by having it as the reference task. We show in Table 5 results that combine parsing with either (a) the English autoencoder task or (b) the English\u2192German translation task. Our models are compared against the best attention-based systems in<cite> (Vinyals et al., 2015a)</cite> , including the state-of-the-art result of 92.8 F 1 . Before discussing the multi-task results, we note a few interesting observations. First, very small parsing perplexities, close to 1.1, can be achieved with large training data. 7 Second, our baseline system can obtain a very competitive F 1 score of 92.2, rivaling<cite> Vinyals et al. (2015a)</cite> 's systems.",
  "y": "uses"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_11",
  "x": "Before discussing the multi-task results, we note a few interesting observations. First, very small parsing perplexities, close to 1.1, can be achieved with large training data. 7 Second, our baseline system can obtain a very competitive F 1 score of 92.2, rivaling<cite> Vinyals et al. (2015a)</cite> 's systems. This is rather surprising since our models do not use any attention mechanism. A closer look into these models reveal that there seems to be an architectural difference:<cite> Vinyals et al. (2015a)</cite> use 3-layer LSTM with 256 cells and 512-dimensional embeddings; whereas our models use 4-layer LSTM with 1000 cells and 1000-dimensional embeddings. This further supports findings in (Jozefowicz et al., 2016) that larger networks matter for sequence models. For the multi-task results, while autoencoder does not seem to help parsing, translation does. At the mixing ratio of 0.05, we obtain a non-negligible boost of 0.2 F 1 over the baseline and with 92.4 F 1 , our multi-task system is on par with the best single system reported in<cite> (Vinyals et al., 2015a)</cite> . Furthermore, by ensembling 6 different multi-task models (trained with the translation task at mixing ratios of 0.1, 0.05, and 0.01), we are able to establish a new state-of-the-art result in English constituent parsing with 93.0 F 1 score.",
  "y": "similarities"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_12",
  "x": "We show in Table 5 results that combine parsing with either (a) the English autoencoder task or (b) the English\u2192German translation task. Our models are compared against the best attention-based systems in<cite> (Vinyals et al., 2015a)</cite> , including the state-of-the-art result of 92.8 F 1 . Before discussing the multi-task results, we note a few interesting observations. First, very small parsing perplexities, close to 1.1, can be achieved with large training data. 7 Second, our baseline system can obtain a very competitive F 1 score of 92.2, rivaling<cite> Vinyals et al. (2015a)</cite> 's systems. This is rather surprising since our models do not use any attention mechanism. A closer look into these models reveal that there seems to be an architectural difference:<cite> Vinyals et al. (2015a)</cite> use 3-layer LSTM with 256 cells and 512-dimensional embeddings; whereas our models use 4-layer LSTM with 1000 cells and 1000-dimensional embeddings. This further supports findings in (Jozefowicz et al., 2016) that larger networks matter for sequence models. For the multi-task results, while autoencoder does not seem to help parsing, translation does.",
  "y": "differences"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_13",
  "x": "Before discussing the multi-task results, we note a few interesting observations. First, very small parsing perplexities, close to 1.1, can be achieved with large training data. 7 Second, our baseline system can obtain a very competitive F 1 score of 92.2, rivaling<cite> Vinyals et al. (2015a)</cite> 's systems. This is rather surprising since our models do not use any attention mechanism. A closer look into these models reveal that there seems to be an architectural difference:<cite> Vinyals et al. (2015a)</cite> use 3-layer LSTM with 256 cells and 512-dimensional embeddings; whereas our models use 4-layer LSTM with 1000 cells and 1000-dimensional embeddings. This further supports findings in (Jozefowicz et al., 2016) that larger networks matter for sequence models. For the multi-task results, while autoencoder does not seem to help parsing, translation does. At the mixing ratio of 0.05, we obtain a non-negligible boost of 0.2 F 1 over the baseline and with 92.4 F 1 , our multi-task system is on par with the best single system reported in<cite> (Vinyals et al., 2015a)</cite> . Furthermore, by ensembling 6 different multi-task models (trained with the translation task at mixing ratios of 0.1, 0.05, and 0.01), we are able to establish a new state-of-the-art result in English constituent parsing with 93.0 F 1 score.",
  "y": "similarities"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_0",
  "x": "In recent work, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected from a full-fledged SRL system. SPs introduce semantic generalizations on the type of arguments preferred by the predicates and, thus, they are expected to improve results on infrequent and unknown words. The positive effect was especially relevant for out-of-domain data. In this paper we advance (<cite>Zapirain et al., 2009</cite>) in two directions: (1) We learn separate SPs for prepositions and verbs, showing improvement over using SPs for verbs alone. (2) We integrate the information of several SP models in a state-of-the-art SRL system (SwiRL 1 ) and show significant improvements in SR classification. The key for the improvement lies in a metaclassifier, trained to select among the predictions provided by several role classification models. ---------------------------------- **SPS FOR SR CLASSIFICATION** SPs have been widely believed to be an important knowledge source when parsing and performing SRL, especially role classification.",
  "y": "background"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_1",
  "x": "Indeed, the SRL evaluation exercises at CoNLL-2004 (Carreras and M\u00e0rquez, 2005 observed that all systems showed a significant performance degradation (\u223c10 F 1 points) when applied to test data from a different genre of that of the training set. Pradhan et al. (2008) showed that this performance degradation is essentially caused by the argument classification subtask, and suggested the lexical data sparseness as one of the main reasons. The same authors studied the contribution of the different feature types in SRL and concluded that the lexical features were the most salient features in argument classification (Pradhan et al., 2007) . In recent work, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected from a full-fledged SRL system. SPs introduce semantic generalizations on the type of arguments preferred by the predicates and, thus, they are expected to improve results on infrequent and unknown words. The positive effect was especially relevant for out-of-domain data. In this paper we advance (<cite>Zapirain et al., 2009</cite>) in two directions: (1) We learn separate SPs for prepositions and verbs, showing improvement over using SPs for verbs alone. (2) We integrate the information of several SP models in a state-of-the-art SRL system (SwiRL 1 ) and show significant improvements in SR classification. The key for the improvement lies in a metaclassifier, trained to select among the predictions provided by several role classification models.",
  "y": "extends"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_2",
  "x": "Since lexical features tend to be sparse, SRL systems are prone to overfit the training data and generalize poorly to new corpora. Indeed, the SRL evaluation exercises at CoNLL-2004 (Carreras and M\u00e0rquez, 2005 observed that all systems showed a significant performance degradation (\u223c10 F 1 points) when applied to test data from a different genre of that of the training set. Pradhan et al. (2008) showed that this performance degradation is essentially caused by the argument classification subtask, and suggested the lexical data sparseness as one of the main reasons. The same authors studied the contribution of the different feature types in SRL and concluded that the lexical features were the most salient features in argument classification (Pradhan et al., 2007) . In recent work, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected from a full-fledged SRL system. SPs introduce semantic generalizations on the type of arguments preferred by the predicates and, thus, they are expected to improve results on infrequent and unknown words. The positive effect was especially relevant for out-of-domain data. In this paper we advance (<cite>Zapirain et al., 2009</cite>) in two directions: (1) We learn separate SPs for prepositions and verbs, showing improvement over using SPs for verbs alone. (2) We integrate the information of several SP models in a state-of-the-art SRL system (SwiRL 1 ) and show significant improvements in SR classification.",
  "y": "motivation"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_3",
  "x": "Still, present parsers and SRL systems use just lexical features, which can be seen as the most simple form of SP, where the headword needs to be seen in the training data, and otherwise the SP is not satisfied. Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters. In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. More recently, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored. In this work we apply a subset of the SP methods proposed in (<cite>Zapirain et al., 2009</cite> ). These methods can be split in two main families, depending on the resource used to compute similarity: WordNetbased methods and distributional methods. Both families define a similarity score between a word (the headword of the argument to be classified) and a set of words (the headwords of arguments of a given role). WordNet-based similarity: One of the models that we used is based on Resnik's similarity measure (1993), referring to it as res. The other model is an in-house method (<cite>Zapirain et al., 2009</cite> ), referred as <cite>wn</cite>, which only takes into account the depth of the most common ancestor, and returns SPs that are as specific as possible.",
  "y": "motivation background"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_4",
  "x": "---------------------------------- **SPS FOR SR CLASSIFICATION** SPs have been widely believed to be an important knowledge source when parsing and performing SRL, especially role classification. Still, present parsers and SRL systems use just lexical features, which can be seen as the most simple form of SP, where the headword needs to be seen in the training data, and otherwise the SP is not satisfied. Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters. In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. More recently, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored. In this work we apply a subset of the SP methods proposed in (<cite>Zapirain et al., 2009</cite> ). These methods can be split in two main families, depending on the resource used to compute similarity: WordNetbased methods and distributional methods.",
  "y": "extends"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_5",
  "x": "These methods can be split in two main families, depending on the resource used to compute similarity: WordNetbased methods and distributional methods. Both families define a similarity score between a word (the headword of the argument to be classified) and a set of words (the headwords of arguments of a given role). WordNet-based similarity: One of the models that we used is based on Resnik's similarity measure (1993), referring to it as res. The other model is an in-house method (<cite>Zapirain et al., 2009</cite> ), referred as <cite>wn</cite>, which only takes into account the depth of the most common ancestor, and returns SPs that are as specific as possible. Distributional similarity: Following (<cite>Zapirain et al., 2009</cite>) we considered both first order and second order similarity. In first order similarity, the similarity of two words was computed using the cosine (or Jaccard measure) of the co-occurrence vectors of the two words. Co-occurrence vectors where constructed using freely available software (Pad\u00f3 and Lapata, 2007) run over the British National Corpus. We used the optimal parameters (Pad\u00f3 and Lapata, 2007, p. 179 ). We will refer to these similarities as sim cos and sim Jac , respectively.",
  "y": "uses"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_6",
  "x": "For each headword, we select the role (r) of the verb (c) which fits best the head word (w), where the goodness of fit (SP sim (v, r, w)) is modeled using one of the similarity models above, between the headword w and the headwords seen in training data for role r of verb v. This selection rule is formalized as follows: In <cite>our previous work</cite> (<cite>Zapirain et al., 2009</cite> ), we modelled SPs for pairs of predicates (verbs) and arguments, independently of the fact that the argument is a core argument (typically a noun) or an adjunct argument (typically a prepositional phrase). In contrast, (Litkowski and Hargraves, 2005) show that prepositions have SPs of their own, especially when functioning as adjuncts. We therefore decided to split SPs according to whether the potential argument is a Prepositional Phrase (PP) or a Noun Phrase (NP). For NPs, which tend to be core arguments 2 , we use the SPs of the verb (as formalized above). For PPs, which have an even distribution between core and adjunct arguments, we use the SPs of the prepositions alone, ignoring the verbs. Implementation wise, this means that in Eq. (1), we change v for p, where p is the preposition heading the PP. ---------------------------------- **EXPERIMENTS WITH SPS IN ISOLATION**",
  "y": "background"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_7",
  "x": "When using SPs alone, we only use the headwords of the arguments, and each argument is classified independently of the rest. For each headword, we select the role (r) of the verb (c) which fits best the head word (w), where the goodness of fit (SP sim (v, r, w)) is modeled using one of the similarity models above, between the headword w and the headwords seen in training data for role r of verb v. This selection rule is formalized as follows: In <cite>our previous work</cite> (<cite>Zapirain et al., 2009</cite> ), we modelled SPs for pairs of predicates (verbs) and arguments, independently of the fact that the argument is a core argument (typically a noun) or an adjunct argument (typically a prepositional phrase). In contrast, (Litkowski and Hargraves, 2005) show that prepositions have SPs of their own, especially when functioning as adjuncts. We therefore decided to split SPs according to whether the potential argument is a Prepositional Phrase (PP) or a Noun Phrase (NP). For NPs, which tend to be core arguments 2 , we use the SPs of the verb (as formalized above). For PPs, which have an even distribution between core and adjunct arguments, we use the SPs of the prepositions alone, ignoring the verbs. Implementation wise, this means that in Eq. (1), we change v for p, where p is the preposition heading the PP. ----------------------------------",
  "y": "differences extends"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_0",
  "x": "We show that this extension has a very small impact on training times, while obtaining better alignments in terms of BLEU scores. ---------------------------------- **INTRODUCTION** Word alignment is at the basis of most statistical machine translation. The models that are generally used are often slow to train, and have a large number of parameters. <cite>Dyer et al. (2013)</cite> present a simple reparameterization of IBM Model 2 that is very fast to train, and achieves results similar to IBM Model 4. While this model is very effective, it also has a very low number of parameters, and as such doesn't have a large amount of expressive power. For one thing, it forces the model to consider alignments on both sides of the diagonal equally likely. However, it isn't clear that this is the case, as for some languages an alignment to earlier or later in the sentence (above or below the diagonal) could be common, due to word order differences.",
  "y": "background"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_1",
  "x": "However, using the reparameterization in<cite> (Dyer et al., 2013)</cite> would leave the model simple enough even with a relatively large amount of word classes. word class, and so can have different gradients for alignment probability over the english words. If the model has learned that prepositions and nouns are more likely to align to words later in the sentence, it could have a lower lambda for both word classes, resulting in a less steep slope. If we also split lambda into two variables, we can get alignment probabilities as shown above for the Dutch word 'de', where aligning to one side of the diagonal is made more likely for some word classes. Finally, instead of just having one side of the diagonal less steep than the other, it may be useful to instead move the peak of the alignment probability function off the diagonal, while keeping it equally likely. In Figure 2 , this is done for the past participle 'gezien'. We will present a simple model for adding the above extensions to achieve the above (splitting the parameter, adding an offset and conditioning the parameters on the POS tag of the target word) in section 2, results on a set of experiments in section 3 and present our conclusions in section 4. ---------------------------------- **METHODS**",
  "y": "background"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_2",
  "x": "In Figure 2 , this is done for the past participle 'gezien'. We will present a simple model for adding the above extensions to achieve the above (splitting the parameter, adding an offset and conditioning the parameters on the POS tag of the target word) in section 2, results on a set of experiments in section 3 and present our conclusions in section 4. ---------------------------------- **METHODS** We make use of a modified version of Model 2, from <cite>Dyer et al. (2013)</cite> , which has an alignment model that is parameterised in its original form solely on the variable \u03bb. Specifically, the probability of a sentence e given a sentence f is given as: here, m is the length of the target sentence e, n the same for source sentence f , \u03b4 is the alignment model and \u03b8 is the translation model. In this paper we are mainly concerned with the alignment model \u03b4. In the original formulation (with a minor tweak to ensure symmetry through the center), this function is defined as:",
  "y": "extends"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_3",
  "x": "We denote these parameters as \u03bb and \u03b3 for below and above the diagonal respectively. Further, the offset is denoted as \u03c9. we change the definition of h(\u00b7) to the following instead: + \u03c9 otherwise j \u2193 is the point closest to or on the diagonal here, calculated as: Here, \u03c9 can range from \u22121 to 1, and thus the calculation for the diagonal j \u2193 is clamped to be in a valid range for alignments. As the partition function (Z(\u00b7)) used in<cite> (Dyer et al., 2013)</cite> consists of 2 calculations for each target position i, one for above and one for below the diagonal, we can simply substitute \u03b3 for the geometric series calculations in order to use different parameters for each: where j \u2191 is j \u2193 + 1. ---------------------------------- **OPTIMIZING THE PARAMETERS**",
  "y": "background"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_4",
  "x": "The first derivative of L with respect to \u03bb at a single target word becomes: And similar for finding the first derivative with respect to \u03b3, but summing from j \u2191 to n instead. The first derivative with respect to \u03c9 then, is: Where h (\u00b7) is the first derivative of h(\u00b7) with respect to \u03c9. For obtaining this derivative, the arithmetico-geometric series (Fernandez et al., 2006) was originally used as an optimization, and for the gradient with respect to omega a geometric series should suffice, as an optimization, as there is no conditioning on the source words. This is not done in the current work however, so timing results will not be directly comparable to those found in<cite> (Dyer et al., 2013)</cite> . Conditioning on the POS of the target words then becomes as simple as using a different \u03bb, \u03b3, and \u03c9 for each POS tag in the input, and calculating a separate derivative for each of them, using only the derivatives at those target words that use the POS tag. A minor detail is to keep a count of alignment positions used for finding the derivative for each different parameter, and normalizing the resulting derivatives with those counts, so the step size can be kept constant across POS tags. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_5",
  "x": "A minor detail is to keep a count of alignment positions used for finding the derivative for each different parameter, and normalizing the resulting derivatives with those counts, so the step size can be kept constant across POS tags. ---------------------------------- **EMPIRICAL RESULTS** The above described model is evaluated with experiments on a set of 3 language pairs, on which AER scores and BLEU scores are computed. We use similar corpora as used in<cite> (Dyer et al., 2013)</cite> : a French-English corpus made up of Europarl version 7 and news-commentary corpora, the ArabicEnglish parallel data consisting of the non-UN portions of the NIST training corpora, and the FBIS Chinese-English corpora. The models that are compared are the original reparameterization of Model 2, a version where \u03bb is split around the diagonal (split), one where pos tags are used, but \u03bb is not split around the diagonal (pos), one where an offset is used, but parameters aren't split about the diagonal (offset), one that's split about the diagonal and uses pos tags used as in<cite> (Dyer et al., 2013)</cite> , with stepsize for updates to \u03bb and \u03b3 during gradient ascent is 1000, and that for \u03c9 is 0.03, decaying after every gradient descent step by 0.9, using 8 steps every iteration. Both \u03bb and \u03b3 are initialised to 6, and \u03c9 is initialised to 0. For these experiments the pos and pos & split use POS tags generated using the Stanford POS tagger (Toutanova and Manning, 2000) , using the supplied models for all of the languages used in the experiments. For comparison, Model 4 is trained for 5 iterations using 5 iterations each of Model 1 and Model 3 as initialization, using GIZA++ (Och and Ney, 2003) .",
  "y": "similarities"
 },
 {
  "id": "9770f647c0b406462f4b941f136748_0",
  "x": "The main limitation of their algorithm is that it was developed specifically for adjectives and that the question of its application to other grammatical categories has not been solved <cite>(Turney & Littman, 2003)</cite> . If several other techniques have been proposed to determine affective valence from corpora, only a few of them have been designed to work with relatively small corpora (ten million words or fewer), a necessary property for building specific affective lexicons. Two techniques that fulfil this condition are described below. ---------------------------------- **SO-LSA** The technique proposed by<cite> Turney and Littman (2003)</cite> tries to infer semantic orientation from semantic association in a corpus. It is based on the semantic proximity between a target word and fourteen benchmarks: seven with positive valence and seven with negative valence (see Table 1 ). A word is considered as positive if it is closer to the positive benchmarks and further away from the negative benchmarks. Turney and Littman proposed two techniques for estimating the strength of the semantic association between words on the basis of corpora.",
  "y": "motivation"
 },
 {
  "id": "9770f647c0b406462f4b941f136748_1",
  "x": "If several other techniques have been proposed to determine affective valence from corpora, only a few of them have been designed to work with relatively small corpora (ten million words or fewer), a necessary property for building specific affective lexicons. Two techniques that fulfil this condition are described below. ---------------------------------- **SO-LSA** The technique proposed by<cite> Turney and Littman (2003)</cite> tries to infer semantic orientation from semantic association in a corpus. It is based on the semantic proximity between a target word and fourteen benchmarks: seven with positive valence and seven with negative valence (see Table 1 ). A word is considered as positive if it is closer to the positive benchmarks and further away from the negative benchmarks. Turney and Littman proposed two techniques for estimating the strength of the semantic association between words on the basis of corpora. The first technique estimates the semantic proximity between a word and a benchmark on the basis of the frequency with which they co-occur.",
  "y": "background"
 },
 {
  "id": "9770f647c0b406462f4b941f136748_3",
  "x": "The average correlation between the ratings provided by two participants was 0.60. A detailed presentation of the procedure used to build the materials is given in Bestgen, Fairon and Kevers (2004) . ---------------------------------- **METHOD** The two techniques described above were used in this experiment. The fourteen SO-LSA benchmarks chosen by<cite> Turney and Littman (2003)</cite> were translated into 2 Each sentence was automatically modified so as to replace the name and the description of the function of every individual by a generic first name of adequate sex (Mary, John, etc.) in order to prevent the judges being influenced by their prior positive or negative opinion about these people. French (bon, gentil, excellent, positif, heureux, correct et sup\u00e9rieur: mauvais, m\u00e9chant, m\u00e9diocre, n\u00e9gatif, malheureux, faux et inf\u00e9rieur) . For DI-LSA, a French lexicon made up of 3000 words evaluated on the pleasant-unpleasant scale was used (Hogenraad et al., 1995) . A minimum of thirty judges rated the words on a seven-point scale from 'very unpleasant' (1) Table 3 : Emotional valences of several sentences.",
  "y": "similarities"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_0",
  "x": "In particular, the technique proposed by <cite>Yuan et al. (2016)</cite> returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released. This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow). Our study showed that similar results can be obtained with much less data than hinted at by <cite>Yuan et al. (2016)</cite> . Detailed analyses shed light on the strengths and weaknesses of this method. First, adding more unannotated training data is useful, but is subject to diminishing returns. Second, the model can correctly identify both popular and unpopular meanings. Finally, the limited sense coverage in the annotated datasets is a major limitation. All code and trained models are made freely available. ----------------------------------",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_1",
  "x": "****A DEEP DIVE INTO WORD SENSE DISAMBIGUATION WITH LSTM**** **ABSTRACT** LSTM-based language models have been shown effective in Word Sense Disambiguation (WSD). In particular, the technique proposed by <cite>Yuan et al. (2016)</cite> returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released. This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow). Our study showed that similar results can be obtained with much less data than hinted at by <cite>Yuan et al. (2016)</cite> . Detailed analyses shed light on the strengths and weaknesses of this method. First, adding more unannotated training data is useful, but is subject to diminishing returns. Second, the model can correctly identify both popular and unpopular meanings.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_2",
  "x": "****A DEEP DIVE INTO WORD SENSE DISAMBIGUATION WITH LSTM**** **ABSTRACT** LSTM-based language models have been shown effective in Word Sense Disambiguation (WSD). In particular, the technique proposed by <cite>Yuan et al. (2016)</cite> returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released. This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow). Our study showed that similar results can be obtained with much less data than hinted at by <cite>Yuan et al. (2016)</cite> . Detailed analyses shed light on the strengths and weaknesses of this method. First, adding more unannotated training data is useful, but is subject to diminishing returns. Second, the model can correctly identify both popular and unpopular meanings.",
  "y": "differences"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_3",
  "x": "---------------------------------- **INTRODUCTION** Word Sense Disambiguation (WSD) is a long-established task in the NLP community (see Navigli (2009) for a survey) which goal is to annotate lemmas in text with the most appropriate meaning from a lexical database like WordNet (Fellbaum, 1998) . Many approaches have been proposed -the more popular ones include the usage of Support Vector Machine (SVM) (Zhong and Ng, 2010) , SVM combined with unsupervised trained embeddings (Iacobacci et al., 2016; Rothe and Sch\u00fctze, 2017) , and graph-based approaches (Agirre et al., 2014; Weissenborn et al., 2015) . In recent years, there has been a surge in interest in using Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to perform WSD (Raganato et al., 2017b; Melamud et al., 2016) . These approaches are characterized by their high performance, simplicity and their ability to extract a lot of information from raw text. Among the best-performing ones is the approach by <cite>Yuan et al. (2016)</cite> , in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small sense-annotated datasets to achieve state-of-the-art performance in all-words WSD. Even though the results obtained by <cite>Yuan et al. (2016)</cite> outperform the previous state-of-the-art, neither the used datasets nor the constructed models are available to the community. This is unfortunate because this makes the re-application of this technique a non-trivial process, and it hinders further studies for understanding which limitations prevent even higher accuracies.",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_4",
  "x": "These approaches are characterized by their high performance, simplicity and their ability to extract a lot of information from raw text. Among the best-performing ones is the approach by <cite>Yuan et al. (2016)</cite> , in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small sense-annotated datasets to achieve state-of-the-art performance in all-words WSD. Even though the results obtained by <cite>Yuan et al. (2016)</cite> outperform the previous state-of-the-art, neither the used datasets nor the constructed models are available to the community. This is unfortunate because this makes the re-application of this technique a non-trivial process, and it hinders further studies for understanding which limitations prevent even higher accuracies. These could be, for instance, of algorithmic nature or relate to the input (either size or quality), and a deeper understanding is crucial for enabling further improvements. In addition, some details are not reported, and this could prevent other attempts from replicating the results. To address these issues, we reimplemented <cite>Yuan et al. (2016)</cite> 's method with the goal of: 1) reproducing and making available the code, trained models, and results and 2) understanding which are the main factors that constitute the strengths and weaknesses of this method. While a full replication is not possible due to the unavailability of the original data, we nevertheless managed to reproduce their approach with other public text corpora, and this allowed us to perform a deeper investigation on the performance of this technique. This investigation aimed at understanding how sensitive the WSD approach is w.r.t.",
  "y": "motivation"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_5",
  "x": "To address these issues, we reimplemented <cite>Yuan et al. (2016)</cite> 's method with the goal of: 1) reproducing and making available the code, trained models, and results and 2) understanding which are the main factors that constitute the strengths and weaknesses of this method. While a full replication is not possible due to the unavailability of the original data, we nevertheless managed to reproduce their approach with other public text corpora, and this allowed us to perform a deeper investigation on the performance of this technique. This investigation aimed at understanding how sensitive the WSD approach is w.r.t. the amount of unannotated data (i.e., raw text) used for training, model complexity, how biased the method is towards the choice of the most frequent senses (MFS), and identifying limitations that cannot be overcome with bigger unannotated datasets. The contribution of this paper is thus two-fold: On the one hand, we present a reproduction study whose results are publicly available and hence can be freely used by the community. Notice that the lack of available models has been explicitly mentioned, in a recent work, as the cause for the missing comparison of this technique with other competitors (Raganato et al., 2017b, footnote 10) . On the other hand, we present other experiments to shed more light on the value of this and similar methods. We anticipate some conclusions. First, a positive result is that we were able to reproduce the method from <cite>Yuan et al. (2016)</cite> and obtain similar results to the ones originally published.",
  "y": "extends"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_6",
  "x": "The contribution of this paper is thus two-fold: On the one hand, we present a reproduction study whose results are publicly available and hence can be freely used by the community. Notice that the lack of available models has been explicitly mentioned, in a recent work, as the cause for the missing comparison of this technique with other competitors (Raganato et al., 2017b, footnote 10) . On the other hand, we present other experiments to shed more light on the value of this and similar methods. We anticipate some conclusions. First, a positive result is that we were able to reproduce the method from <cite>Yuan et al. (2016)</cite> and obtain similar results to the ones originally published. However, to our surprise, these results were obtained using a much smaller corpus of 1.8 billion tokens (Gigaword), which is less than 2% of the data used in the original study. In addition, we observe that the amount of unannotated data is important, but that the relationship between its size and the improvement is not linear, meaning that exponentially more unannotated data is needed in order to improve the performance. Moreover, we show that the percentage of correct sense assignments is more balanced w.r.t sense popularity, meaning that the system has a less-strong bias towards the most-frequent sense (MFS) and is better at recognizing both popular and unpopular meanings. Finally, we show that the limited sense coverage in the annotated datasets is a major limitation, as shown by the fact that resulting model does not have a representation for more than 30% of the meanings which should have been considered for disambiguating the test sets.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_7",
  "x": "The contribution of this paper is thus two-fold: On the one hand, we present a reproduction study whose results are publicly available and hence can be freely used by the community. Notice that the lack of available models has been explicitly mentioned, in a recent work, as the cause for the missing comparison of this technique with other competitors (Raganato et al., 2017b, footnote 10) . On the other hand, we present other experiments to shed more light on the value of this and similar methods. We anticipate some conclusions. First, a positive result is that we were able to reproduce the method from <cite>Yuan et al. (2016)</cite> and obtain similar results to the ones originally published. However, to our surprise, these results were obtained using a much smaller corpus of 1.8 billion tokens (Gigaword), which is less than 2% of the data used in the original study. In addition, we observe that the amount of unannotated data is important, but that the relationship between its size and the improvement is not linear, meaning that exponentially more unannotated data is needed in order to improve the performance. Moreover, we show that the percentage of correct sense assignments is more balanced w.r.t sense popularity, meaning that the system has a less-strong bias towards the most-frequent sense (MFS) and is better at recognizing both popular and unpopular meanings. Finally, we show that the limited sense coverage in the annotated datasets is a major limitation, as shown by the fact that resulting model does not have a representation for more than 30% of the meanings which should have been considered for disambiguating the test sets.",
  "y": "differences"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_8",
  "x": "Rothe and Sch\u00fctze (2017) use word embeddings as a starting point and then rely on the formal constraints in a lexical resource to create synset embeddings. Recently, there has been a surge in WSD approaches that use unannotated data but do not consider synset relations. One example is provided by Iacobacci et al. (2016) , who investigated the role of word embeddings as features in a WSD system. Four methods (concatenation, average, fractional decay, and exponential decay) are used to extract features from the sentential context using word embeddings. The features are then added to the default feature set of IMS (Zhong and Ng, 2010) . Moreover, Raganato et al. (2017b) present a number of end-to-end neural WSD architectures. The best performing one is based on a bidirectional Long Short-Term Memory (BLSTM) with attention and two auxiliary loss functions (part-of-speech and the WordNet coarse-grained semantic labels). Melamud et al. (2016) also make use of unannotated data to train a BLSTM. The work by <cite>Yuan et al. (2016)</cite> , which we consider in this paper, belongs to this last category.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_9",
  "x": "**WSD WITH LANGUAGE MODELS** The method proposed by <cite>Yuan et al. (2016)</cite> performs WSD by annotating each lemma in a text with one WordNet synset that is associated with its meaning. Broadly speaking, the disambiguation is done by: 1) constructing a language model from a large unannotated dataset; 2) extracting sense embeddings from this model using a much smaller annotated dataset; 3) relying on the sense embeddings to make predictions on the lemmas in unseen sentences. Each operation is described below. Constructing Language Models. Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997 ) is a celebrated recurrent neural network architecture that has proven to be effective in many natural language processing tasks (Sutskever et al., 2014; Dyer et al., 2015; He et al., 2017, among others) . Different from previous architectures, LSTM is equipped with trainable gates that control the flow of information, allowing the neural networks to learn both short-and long-range dependencies. In <cite>Yuan et al. (2016)</cite> , the first operation consists of constructing an LSTM language model to capture the meaning of words in context. They use an LSTM network with a single hidden layer of h nodes.",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_10",
  "x": "---------------------------------- **WSD WITH LANGUAGE MODELS** The method proposed by <cite>Yuan et al. (2016)</cite> performs WSD by annotating each lemma in a text with one WordNet synset that is associated with its meaning. Broadly speaking, the disambiguation is done by: 1) constructing a language model from a large unannotated dataset; 2) extracting sense embeddings from this model using a much smaller annotated dataset; 3) relying on the sense embeddings to make predictions on the lemmas in unseen sentences. Each operation is described below. Constructing Language Models. Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997 ) is a celebrated recurrent neural network architecture that has proven to be effective in many natural language processing tasks (Sutskever et al., 2014; Dyer et al., 2015; He et al., 2017, among others) . Different from previous architectures, LSTM is equipped with trainable gates that control the flow of information, allowing the neural networks to learn both short-and long-range dependencies. In <cite>Yuan et al. (2016)</cite> , the first operation consists of constructing an LSTM language model to capture the meaning of words in context.",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_11",
  "x": "3. The procedure invokes a subroutine to choose one of the n senses for the context vector c t . It selects the sense whose vector is closest to c t using cosine as the similarity function. Label Propagation. <cite>Yuan et al. (2016)</cite> argue that the averaging procedure is suboptimal because of two reasons. First, the distribution of occurrences of senses is unknown whereas averaging is only suitable for spherical clusters. Second, averaging reduces the representation of occurrences of each sense to a single vector and therefore ignores sense prior. For this reason, they propose to use label propagation for inference as an alternative to averaging. Label propagation (Zhu and Ghahramani, 2002 ) is a classic semi-supervised algorithm that has been employed in WSD (Niu et al., 2005) and other NLP tasks (Chen et al., 2006; Zhou, 2011) . The procedure involves predicting senses for not only the target cases but also for unannotated words queried from a corpus.",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_12",
  "x": "The corpus consists of 1.8 billion tokens in 4.1 million documents, originated from four major news agencies. We leave the study of bigger corpora for future work. For the training of the sense embeddings, we use the same two corpora used by <cite>Yuan et al. (2016)</cite>: 1. SemCor (Miller et al., 1993 ) is a corpus containing approximately 240,000 sense annotated words. The tagged documents originate from the Brown corpus (Francis and Kucera, 1979) and cover various genres. 2. OMSTI (Taghipour and Ng, 2015) contains one million sense annotations automatically tagged by exploiting the English-Chinese part of the parallel MultiUN corpus (Eisele and Chen, 2010) . A list of English translations were manually created for each WordNet sense. If the Chinese translation of an English word matches one of the manually curated translations for a WordNet sense, that sense is selected. Implementation. We used the BeautifulSoup HTML parser to extract plain text from the Gigaword corpus.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_13",
  "x": "P: SemCor indicates that sense distributions from SemCor are used in the system architecture. Three scorers are used: \"framework\" refers to the WSD evaluation framework from Raganato et al. (2017a) ; \"mapping to WN3.0\" refers to the evaluation used by <cite>Yuan et al. (2016)</cite> while \"competition\" refers to the scorer provided by the competition itself (e.g., semeval2013). 1,644 test instances in total, which are all nouns. The application of the MFS baseline on this dataset yields an F 1 score of 63.0%. ---------------------------------- **RESULTS** In this section, we report our reproduction of the results of <cite>Yuan et al. (2016)</cite> and additional experiments to gain a deeper insight into the strengths and weaknesses of the approach. These experiments focus on the performance on the most-and less-frequent senses, coverage of the annotated dataset and the consequent impact on the overall predictions, the granularity of the sense representation, and the impact of the unannotated data and model complexity on the accuracy of WSD. Reproduction results.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_14",
  "x": "---------------------------------- **RESULTS** In this section, we report our reproduction of the results of <cite>Yuan et al. (2016)</cite> and additional experiments to gain a deeper insight into the strengths and weaknesses of the approach. These experiments focus on the performance on the most-and less-frequent senses, coverage of the annotated dataset and the consequent impact on the overall predictions, the granularity of the sense representation, and the impact of the unannotated data and model complexity on the accuracy of WSD. Reproduction results. We trained the LSTM model with the best reported settings in <cite>Yuan et al. (2016)</cite> (hidden layer size h = 2048, embedding dimensionality p = 512) using a machine equipped with an Intel Xeon E5-2650, 256GB of RAM, 8TB of disk space, and two nVIDIA GeForce GTX 1080 Ti GPUs. During our training, one epoch took about one day to finish with TensorFlow fully utilizing one GPU. The whole training process took four months. We tested the performance of the downstream WSD task three times during the training and observed that the best performance is obtained at the 65 th epoch, despite a later model producing a lower negative log-likelihood.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_15",
  "x": "Three scorers are used: \"framework\" refers to the WSD evaluation framework from Raganato et al. (2017a) ; \"mapping to WN3.0\" refers to the evaluation used by <cite>Yuan et al. (2016)</cite> while \"competition\" refers to the scorer provided by the competition itself (e.g., semeval2013). 1,644 test instances in total, which are all nouns. The application of the MFS baseline on this dataset yields an F 1 score of 63.0%. ---------------------------------- **RESULTS** In this section, we report our reproduction of the results of <cite>Yuan et al. (2016)</cite> and additional experiments to gain a deeper insight into the strengths and weaknesses of the approach. These experiments focus on the performance on the most-and less-frequent senses, coverage of the annotated dataset and the consequent impact on the overall predictions, the granularity of the sense representation, and the impact of the unannotated data and model complexity on the accuracy of WSD. Reproduction results. We trained the LSTM model with the best reported settings in <cite>Yuan et al. (2016)</cite> (hidden layer size h = 2048, embedding dimensionality p = 512) using a machine equipped with an Intel Xeon E5-2650, 256GB of RAM, 8TB of disk space, and two nVIDIA GeForce GTX 1080 Ti GPUs.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_16",
  "x": "During our training, one epoch took about one day to finish with TensorFlow fully utilizing one GPU. The whole training process took four months. We tested the performance of the downstream WSD task three times during the training and observed that the best performance is obtained at the 65 th epoch, despite a later model producing a lower negative log-likelihood. Thus, we used the model produced at the 65 th epoch for our experiments below. Table 1 presents the results using the test sets senseval2 and semeval2013, respectively. The top part of the table presents our reproduction results, the middle part reports the results from <cite>Yuan et al. (2016)</cite> , while the bottom part reports a representative sample of the other state-of-the-art approaches. It should be noted that with the test set semeval2013, all scorers use WordNet 3.0, therefore the performance of the various methods can be directly compared. However, not all answers in senseval2 can be mapped to WN3.0 and we do not know how <cite>Yuan et al. (2016)</cite> handled these cases. In the WSD evaluation framework (Moro et al., 2014 ) that we selected for evaluation, these cases were either re-annotated or removed.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_17",
  "x": "During our training, one epoch took about one day to finish with TensorFlow fully utilizing one GPU. The whole training process took four months. We tested the performance of the downstream WSD task three times during the training and observed that the best performance is obtained at the 65 th epoch, despite a later model producing a lower negative log-likelihood. Thus, we used the model produced at the 65 th epoch for our experiments below. Table 1 presents the results using the test sets senseval2 and semeval2013, respectively. The top part of the table presents our reproduction results, the middle part reports the results from <cite>Yuan et al. (2016)</cite> , while the bottom part reports a representative sample of the other state-of-the-art approaches. It should be noted that with the test set semeval2013, all scorers use WordNet 3.0, therefore the performance of the various methods can be directly compared. However, not all answers in senseval2 can be mapped to WN3.0 and we do not know how <cite>Yuan et al. (2016)</cite> handled these cases. In the WSD evaluation framework (Moro et al., 2014 ) that we selected for evaluation, these cases were either re-annotated or removed.",
  "y": "similarities"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_19",
  "x": "n represents the number of considered instances. al., 2016; Melamud et al., 2016) . However, the gap with the graph-based approach of Weissenborn et al. (2015) is still significant. When we use both SemCor and OMSTI for the annotated data, our results drop 0.02 point for senseval2, whereas they increase by almost 0.01 for semeval2013. Different from <cite>Yuan et al. (2016)</cite>, we did not observe improvement by using label propagation (comparing T: SemCor, U: OMSTI against T:SemCor without propagation). However, the performance of the label propagation strategy is still competitive on both test sets. Most-vs. less-frequent-sense instances. The original paper only analyses the performance on the whole test sets.",
  "y": "differences"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_20",
  "x": "n represents the number of considered instances. al., 2016; Melamud et al., 2016) . However, the gap with the graph-based approach of Weissenborn et al. (2015) is still significant. When we use both SemCor and OMSTI for the annotated data, our results drop 0.02 point for senseval2, whereas they increase by almost 0.01 for semeval2013. Different from <cite>Yuan et al. (2016)</cite>, we did not observe improvement by using label propagation (comparing T: SemCor, U: OMSTI against T:SemCor without propagation). However, the performance of the label propagation strategy is still competitive on both test sets. Most-vs. less-frequent-sense instances. The original paper only analyses the performance on the whole test sets.",
  "y": "similarities"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_21",
  "x": "less-frequent-sense instances. The original paper only analyses the performance on the whole test sets. We extend this analysis by looking at the performance for disambiguating the most frequent-sense (MFS) and less-frequent-sense (LFS) instances. The first type of instances are the ones for which the correct link is the most-frequent sense, whereas the second subset consists of the remaining ones. This analysis is important because it is well-known that the simple strategy of always choosing the MFS is a strong baseline in WSD, thus there is a tendency for WSD systems to overfit towards the MFS (Postma et al., 2016) . Table 2 shows that the method by <cite>Yuan et al. (2016)</cite> does not overfit towards the MFS to the same extent as other supervised systems since the recall on LFS instances is still quite high 0.41 (a lower recall on LFS instances than on MFS ones is expected due to the reduced training data for them). On semeval13, the recall on LFS is already relatively high using only SemCor (0.33), and reaches 0.38 when using both SemCor and OMSTI. For comparison, the default system IMS (Zhong and Ng, 2010) trained on SemCor only obtains an R lfs of 0.15 on semeval13 (Postma et al., 2016) and only reaches 0.33 with a large amount of annotated data. Finally, our implementation of the label propagation does seem to slightly overfit towards the MFS.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_23",
  "x": "Figure 2a shows the effect of unannotated data volume on WSD performance. The data points at 100 billion (10 11 ) tokens correspond to <cite>Yuan et al. (2016)</cite> 's reported results. As might be expected, a bigger corpus leads to more meaningful context vectors and therefore higher performance on WSD. However, the amount of data needed for 1% of improvement in F 1 grows exponentially fast (notice that the horizontal axis is in log scale). Extrapolating from this graph, to get a performance of 0.8 F 1 by adding more unannotated data, one would need a corpus of 10 12 tokens. This observation also applies to the balance of the sense assignment. Using only 25% of the unannotated data already yields a recall of 35% on the less-frequent senses. In addition, one might expect to push the performance further by increasing the capacity of the LSTM models. To evaluate this possibility, we performed an experiment in which we varied the sizes of LSTM models trained on 100% of the GigaWord corpus and evaluated against senseval2 and semeval2013, respectively.",
  "y": "uses similarities"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_24",
  "x": "To evaluate this possibility, we performed an experiment in which we varied the sizes of LSTM models trained on 100% of the GigaWord corpus and evaluated against senseval2 and semeval2013, respectively. Figure 2b suggests that it is possible but one would need exponentially bigger models. Finally, Reimers and Gurevych (2017) have showed that it is crucial to report the distribution of test scores instead of only one score as this practice might lead to wrong conclusions. As pointed out at the beginning of Section 5, our biggest models take months to train, making training multiple versions of them impractical. However, we trained our smallest model (h = 100, p = 10) ten times and our second smallest model (h = 256, p = 64) five times and observed that as the number of parameters increased, the standard deviation of F 1 decreased from 0.008 to 0.003. We, therefore, believe random fluctuation does not affect the interpretation of the results. ---------------------------------- **CONCLUSIONS** This paper reports the results of a reproduction study of the model proposed by <cite>Yuan et al. (2016)</cite> and an additional analysis to gain a deeper understanding of the impact of various factors on its performance.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_25",
  "x": "We, therefore, believe random fluctuation does not affect the interpretation of the results. ---------------------------------- **CONCLUSIONS** This paper reports the results of a reproduction study of the model proposed by <cite>Yuan et al. (2016)</cite> and an additional analysis to gain a deeper understanding of the impact of various factors on its performance. A number of interesting conclusions can be drawn from our results. First, we observed that we do not need a very large unannotated dataset to achieve state-of-the-art all-words WSD performance since we used the Gigaword corpus, which is two orders of magnitude smaller than <cite>Yuan et al. (2016)</cite> 's proprietary corpus, and got similar performance on senseval2 and semeval2013. A more detailed analysis hints that adding more unannotated data and increasing model capacity are subject to diminishing returns. Moreover, we observed that this approach has a more balanced sense assignment than other techniques, as shown by the relatively good performance on less-frequent-sense instances. In addition, we identified that the limited sense coverage in annotated dataset places a potentially upper bound for the overall performance.",
  "y": "differences"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_0",
  "x": "We highlight the importance of utilizing syntactic structure to cluster documents by author, and demonstrate experimental results that show the method we outline performs on par with state-of-the-art techniques. Additionally, we argue that this feature set outperforms previous methods in cases where authors consciously emulate each other's style or are otherwise rhetorically similar. ---------------------------------- **INTRODUCTION** Unsupervised authorial clustering is the process of partitioning n documents written by k distinct authors into k groups of documents segmented by authorship. Nothing is assumed about each document except that it was written by a single author. Koppel et al. (2011) formulated this problem in a paper focused on clustering five books from the Hebrew Bible. They also consider a 'multi-author document' version of the problem: decomposing sentences from a single composite document generated by merging randomly sampled chunks of text from k authors. Akiva and Koppel (2013) followed that work with an expanded method, and<cite> Aldebei et al. (2015)</cite> have since presented an improved technique in the 'multi-author document' context by exploiting posterior probabilities of a Naive-Bayesian Model. We consider only the case of clustering n documents written by k authors because we believe that, in most cases of authorial decomposition, there is some minimum size of text (a 'document'), for which it can be reliably asserted that only a single author is present.",
  "y": "background"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_1",
  "x": "---------------------------------- **INTRODUCTION** Unsupervised authorial clustering is the process of partitioning n documents written by k distinct authors into k groups of documents segmented by authorship. Nothing is assumed about each document except that it was written by a single author. Koppel et al. (2011) formulated this problem in a paper focused on clustering five books from the Hebrew Bible. They also consider a 'multi-author document' version of the problem: decomposing sentences from a single composite document generated by merging randomly sampled chunks of text from k authors. Akiva and Koppel (2013) followed that work with an expanded method, and<cite> Aldebei et al. (2015)</cite> have since presented an improved technique in the 'multi-author document' context by exploiting posterior probabilities of a Naive-Bayesian Model. We consider only the case of clustering n documents written by k authors because we believe that, in most cases of authorial decomposition, there is some minimum size of text (a 'document'), for which it can be reliably asserted that only a single author is present. Furthermore, this formulation precludes results dependent on a random document generation procedure. In this paper, we argue that the biblical clustering done by Koppel et al. (2011) and by<cite> Aldebei et al. (2015)</cite> do not represent a grouping around true authorship within the Bible, but rather around common topics or shared style.",
  "y": "differences"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_2",
  "x": "To cluster documents by true authorship, we propose that considering part-of-speech (POS) ngrams as features most distinctly identifies an individual writer. The use of syntactic structure in authorial research has been studied before. Baayen et al. (1996) introduced syntactic information measures for authorship attribution and Stamatatos (2009) argued that POS information could reflect a more reliable authorial fingerprint than lexical information. Both Zheng et al. (2006) and Layton et al. (2013) propose that syntactic feature sets are reliable predictors for authorial attribution, and Tschuggnall and Specht (2014) demonstrates, with modest success, authorial decomposition using pq-grams extracted from sentences' syntax trees. We found that by combining the feature set of POS n-grams with a clustering approach similar to the one presented by Akiva (2013) , our method of decomposition attains higher accuracy than Tschuggnall's method, which also considers grammatical style. Additionally, in cases where authors are rhetorically similar, our framework outperforms techniques outlined by Akiva (2013) and <cite>Aldebei (2015)</cite> , which both rely on word occurrences as features. This paper is organized as follows: section 2 outlines our proposed framework, section 3 clari-fies our method in detail through an example, section 4 contains results, section 5 tests an explanation of our results, and section 6 concludes our findings and discusses future work. ---------------------------------- **OUR FRAMEWORK**",
  "y": "differences"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_3",
  "x": "**CLARIFYING DETAILS WITH NYT COLUMNS** We shall describe a clustering of New York Times columns to clarify our framework. The NYT cor- pus is used both because the author of each document is known with certainty and because it is a canonical dataset that has served as a benchmark for both Akiva and Koppel (2013) and<cite> Aldebei et al. (2015)</cite> . The corpus is comprised of texts from four columnists: Gail Collins (274 documents), Maureen Dowd (298 documents), Thomas Friedman (279 documents), and Paul Krugman (331 documents). Each document is approximately the same length and the columnists discuss a variety of topics. Here we consider the binary (k = 2) case of clustering the set of 629 Dowd and Krugman documents into two groups. In step one, the documents are converted into their 'POS-translated' form as previously outlined. Each document is represented as a frequency vector that reflects all 3, 4, and 5-grams that appear in the 'POS-translated' corpus. This range of ngrams was determined through validation of different values for n across several datasets.",
  "y": "similarities"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_4",
  "x": "**OBAMA-MCCAIN & EZEKIEL-JEREMIAH** In order to confirm our framework is accurate over a variety of documents, we considered campaign speeches from the 2008 presidential election. Collecting 27 speeches from President Obama and 20 from Senator McCain, we expected our technique to excel in this context. We found instead that our method performed exceptionally poorly, clustering these speeches with only 74.2% accuracy. Indeed, we were further surprised to discover that by adjusting our framework to be similar to that presented in Akiva and Koppel (2013) and<cite> Aldebei et al. (2015)</cite> -by replacing POS n-grams with ordinary word occurrences in step one -our framework performed very well, clustering at 95.3%. Similarly, our framework performed poorly on the Books of Ezekiel and Jeremiah from the Hebrew Bible. Using the English-translated King James Version, and considering each chapter as an individual document, our framework clusters the 48 chapters of Ezekiel and the 52 chapters of Jeremiah at 54.7%. Aldebei et al. (2015) reports 98.0% on this dataset, and when considering the original English text instead of the POS-translated text, our framework achieves 99.0%. The simultaneous success of word features and failure of POS features on these two datasets seemed to completely contradict our previous results.",
  "y": "similarities"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_0",
  "x": "<cite>[9]</cite> in <cite>their work</cite> pointed out that hate speech is different from offensive language and released a data set of 25k tweets with the goal of distinguishing hate speech from offensive language. Stop saying dumb blondes with pretty faces as you need a pretty face to pull them off !!! #mkr In Islam women must be locked in their houses and Muslims claim this is treating them well Table 1 : Tweets from [10] data set demonstrating online abuse They find that racist and homophobic tweets are more likely to be classified as hate speech but sexist tweets are generally classified as offensive. [4] introduced a large, hand-coded corpus of online harassment data for studying the nature of harassing comments and the culture of trolling. Keeping these motivations in mind, we make the following salient contributions: \u2022 We build a deep context-aware attention-based model for abusive behavior detection on Twitter . To the best of our knowledge ours is the first work that exploits context aware attention for this task. \u2022 Our model is robust and achieves consistent performance gains in all the three abusive data sets \u2022 We show how context aware attention helps in focusing on certain abusive keywords when used in specific context and improve the performance of abusive behavior detection . ---------------------------------- **RELATED WORK**",
  "y": "background"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_1",
  "x": "2) Deep Learning models which learn feature representations on their own. [10] released the popular data set of 16k tweets annotated as belonging to sexism, racism or none class 1 , and provided a feature engineered model for detection of abuse in their corpus. <cite>[9]</cite> use a similar handcrafted feature engineered model to identify offensive language and distinguish it from hate speech. [2] in their work, experiment with multiple deep learning architectures for the task of hate speech detection on Twitter using the same data set by [10] . Their best-reported F1-score is achieved using Long Short Term Memory Networks (LSTM) + Gradient Boosting. On the data set released by [10] , [5] experiment with a two-step approach of detecting abusive language first and then classifying them into specific types i.e. racist, sexist or none. They achieve best results using a Hybrid Convolution Neural Network (CNN) with the intuition that character level input would counter the purposely or mistakenly misspelled words and made-up vocabularies. [6] in their work ran experiments on the Gazetta dataset and the DETOX system ( [12] ) and show that a Recurrent Neural Network (RNN) coupled with deep, classification-specific attention outperforms the previous state of the art in abusive comment moderation. In their more recent work [7] explored how user embeddings, user-type embeddings, and user type biases can improve their previous RNN based model on the Gazetta dataset. Attentive neural networks have been shown to perform well on a variety of NLP tasks ( [13] , [11] ).",
  "y": "background"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_3",
  "x": "**DATA SETS** We have used the 3 benchmark data sets for abusive content detection on Twitter. At the time of the experiment, the [10] data set had a total of 15,844 tweets out of which 1,924 were labelled as belonging to racism, 3,058 as sexism and 10,862 as none. The <cite>[9]</cite> data set had a total of 25,112 tweets out of which 1498 were labelled as hate speech, 19,326 as offensive language and 4,288 as neither. For the [4] data set, there were 20,362 tweets out of which 5,235 were positive harassment examples and 15,127 were negative. We call [10] data set as D1 , <cite>[9]</cite> data set as <cite>D2</cite> and [4] as D3 For tweet tokenization, we use Ekphrasis which is a text processing tool built specially from social platforms such as Twitter. [3] use a big collection of Twitter messages (330M) to generate word embeddings, with a vocabulary size of 660K words, using GloVe ( [8] ). We use these pre-trained word embeddings for initializing the first layer (embedding layer) of our neural networks. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_4",
  "x": "---------------------------------- **DATA SETS** We have used the 3 benchmark data sets for abusive content detection on Twitter. At the time of the experiment, the [10] data set had a total of 15,844 tweets out of which 1,924 were labelled as belonging to racism, 3,058 as sexism and 10,862 as none. The <cite>[9]</cite> data set had a total of 25,112 tweets out of which 1498 were labelled as hate speech, 19,326 as offensive language and 4,288 as neither. For the [4] data set, there were 20,362 tweets out of which 5,235 were positive harassment examples and 15,127 were negative. We call [10] data set as D1 , <cite>[9]</cite> data set as <cite>D2</cite> and [4] as D3 For tweet tokenization, we use Ekphrasis which is a text processing tool built specially from social platforms such as Twitter. [3] use a big collection of Twitter messages (330M) to generate word embeddings, with a vocabulary size of 660K words, using GloVe ( [8] ). We use these pre-trained word embeddings for initializing the first layer (embedding layer) of our neural networks.",
  "y": "uses"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_5",
  "x": "[3] use a big collection of Twitter messages (330M) to generate word embeddings, with a vocabulary size of 660K words, using GloVe ( [8] ). We use these pre-trained word embeddings for initializing the first layer (embedding layer) of our neural networks. ---------------------------------- **RESULTS** The network is trained at a learning rate of 0.001 for 10 epochs, with a dropout of 0.2 to prevent over-fitting. The results are averaged over 10-fold cross-validations for D1 and D3 and 5 fold cross-validations for <cite>D2</cite> because <cite>[9]</cite> reported results using 5 fold CV. Because of class imbalance in all our data sets, we report weighted F1 scores. Table 3 shows our results in detail. We compare our model with the best models reported in each paper.",
  "y": "motivation"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_6",
  "x": "Table 3 : Data sets and the results of different models. We reproduced the results for each model on three of the data sets. ---------------------------------- **MODELS** We also share some examples from the three data sets in Figure  2 which our BiLSTM attention model could not classify correctly. On closer investigation we find that most cases where our model fails are instances where annotation is either noisy or the difference between classes are very blurred and subtle. The first tweet is a tweet from [10] , the second tweet is a tweet from from <cite>[9]</cite> data set and the third from the [4] ---------------------------------- **WHY CONTEXTUAL ATTENTION?**",
  "y": "differences"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_0",
  "x": "****STRUCTURAL CORRESPONDENCE LEARNING FOR PARSE DISAMBIGUATION**** **ABSTRACT** The paper presents an application of Structural Correspondence Learning (SCL)<cite> (Blitzer et al., 2006)</cite> for domain adaptation of a stochastic attribute-value grammar (SAVG). So far, SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis<cite> (Blitzer et al., 2006</cite>; ). An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007), however, without any clear conclusions. We report on our exploration of applying SCL to adapt a syntactic disambiguation model and show promising initial results on Wikipedia domains. ---------------------------------- **INTRODUCTION** Many current, effective natural language processing systems are based on supervised Machine Learning techniques.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_1",
  "x": "The paper presents an application of Structural Correspondence Learning (SCL)<cite> (Blitzer et al., 2006)</cite> for domain adaptation of a stochastic attribute-value grammar (SAVG). So far, SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis<cite> (Blitzer et al., 2006</cite>; ). An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007), however, without any clear conclusions. We report on our exploration of applying SCL to adapt a syntactic disambiguation model and show promising initial results on Wikipedia domains. ---------------------------------- **INTRODUCTION** Many current, effective natural language processing systems are based on supervised Machine Learning techniques. The parameters of such systems are estimated to best reflect the characteristics of the training data, at the cost of portability: a system will be successful only as long as the training material resembles the input that the model gets. Therefore, whenever we have access to a large amount of labeled data from some \"source\" (out-of-domain), but we would like a model that performs well on some new \"target\" domain (Gildea, 2001; Daum\u00e9 III, 2007) , we face the problem of domain adaptation.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_2",
  "x": "**INTRODUCTION** Many current, effective natural language processing systems are based on supervised Machine Learning techniques. The parameters of such systems are estimated to best reflect the characteristics of the training data, at the cost of portability: a system will be successful only as long as the training material resembles the input that the model gets. Therefore, whenever we have access to a large amount of labeled data from some \"source\" (out-of-domain), but we would like a model that performs well on some new \"target\" domain (Gildea, 2001; Daum\u00e9 III, 2007) , we face the problem of domain adaptation. The need for domain adaptation arises in many NLP tasks: Part-of-Speech tagging, Sentiment Analysis, Semantic Role Labeling or Statistical Parsing, to name but a few. For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus (Gildea, 2001 ). The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daum\u00e9 III and Marcu, 2006; Daum\u00e9 III, 2007;<cite> Blitzer et al., 2006</cite>; McClosky et al., 2006; . We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daum\u00e9 III, 2007) : supervised and semi-supervised. In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daum\u00e9 III, 2007) , besides the labeled source data, we have access to a comparably small, but labeled amount of target data.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_3",
  "x": "The parameters of such systems are estimated to best reflect the characteristics of the training data, at the cost of portability: a system will be successful only as long as the training material resembles the input that the model gets. Therefore, whenever we have access to a large amount of labeled data from some \"source\" (out-of-domain), but we would like a model that performs well on some new \"target\" domain (Gildea, 2001; Daum\u00e9 III, 2007) , we face the problem of domain adaptation. The need for domain adaptation arises in many NLP tasks: Part-of-Speech tagging, Sentiment Analysis, Semantic Role Labeling or Statistical Parsing, to name but a few. For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus (Gildea, 2001 ). The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daum\u00e9 III and Marcu, 2006; Daum\u00e9 III, 2007;<cite> Blitzer et al., 2006</cite>; McClosky et al., 2006; . We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daum\u00e9 III, 2007) : supervised and semi-supervised. In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daum\u00e9 III, 2007) , besides the labeled source data, we have access to a comparably small, but labeled amount of target data. In contrast, semi-supervised domain adaptation<cite> (Blitzer et al., 2006</cite>; McClosky et al., 2006; is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain data. Semi-supervised adaptation is a much more realistic situation, while at the same time also considerably more difficult.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_5",
  "x": "**MOTIVATION AND PRIOR WORK** While several authors have looked at the supervised adaptation case, there are less (and especially less successful) studies on semi-supervised domain adaptation (McClosky et al., 2006;<cite> Blitzer et al., 2006</cite>; . Of these, McClosky et al. (2006) deal specifically with selftraining for data-driven statistical parsing. They show that together with a re-ranker, improvements are obtained. Similarly, Structural Correspondence Learning<cite> (Blitzer et al., 2006</cite>; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification. In contrast, report on \"frustrating\" results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. \"no team was able to improve target domain performance substantially over a state of the art baseline\". In the same shared task, an attempt was made to apply SCL to domain adaptation for data-driven dependency parsing (Shimizu and Nakagawa, 2007 ). The system just ended up at rank 7 out of 8 teams.",
  "y": "background motivation"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_6",
  "x": "They show that together with a re-ranker, improvements are obtained. Similarly, Structural Correspondence Learning<cite> (Blitzer et al., 2006</cite>; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification. In contrast, report on \"frustrating\" results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. \"no team was able to improve target domain performance substantially over a state of the art baseline\". In the same shared task, an attempt was made to apply SCL to domain adaptation for data-driven dependency parsing (Shimizu and Nakagawa, 2007 ). The system just ended up at rank 7 out of 8 teams. However, based on annotation differences in the datasets and a bug in their system (Shimizu and Nakagawa, 2007) , their results are inconclusive. 1 Thus, the effectiveness of SCL is rather unexplored for parsing. So far, most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea, 2001; Roark and Bacchiani, 2003; McClosky et al., 2006; Shimizu and Nakagawa, 2007) , i.e. systems employing (constituent or dependency based) treebank grammars (Charniak, 1996) . Parse selection constitutes an important part of many parsing systems (Johnson et al., 1999; Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006 ).",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_7",
  "x": "The few studies on adapting disambiguation models (Hara et al., 2005; Plank and van Noord, 2008) have focused exclusively on the supervised scenario. Therefore, the direction we explore in this study is semi-supervised domain adaptation for parse disambiguation. We examine the effectiveness of Structural Correspondence Learning (SCL)<cite> (Blitzer et al., 2006)</cite> for this task, a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis. The system used in this study is Alpino, a wide-coverage Stochastic Attribute Value Grammar (SAVG) for Dutch (van Noord and Malouf, 2005; van Noord, 2006) . For our empirical eval-uation we explore Wikipedia as primary test and training collection. In the sequel, we first introduce the parsing system. Section 4 reviews Structural Correspondence Learning and shows our application of SCL to parse selection, including all our design choices. In Section 5 we present the datasets, introduce the process of constructing target domain data from Wikipedia, and discuss interesting initial empirical results of this ongoing study. 3 Background: Alpino parser Alpino (van Noord and Malouf, 2005; van Noord, 2006 ) is a robust computational analyzer for Dutch that implements the conceptual two-stage parsing approach.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_8",
  "x": "We will refer to this set of features as original features. They are used to train the baseline model on the given labeled source data. ---------------------------------- **STRUCTURAL CORRESPONDENCE LEARNING** ---------------------------------- **SCL** (Structural Correspondence Learning)<cite> (Blitzer et al., 2006</cite>; Blitzer, 2008 ) is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains. Before describing the algorithm in detail, let us illustrate the intuition behind SCL with an example, borrowed from . Suppose we have a Sentiment Analysis system trained on book reviews (domain A), and we would like to adapt it to kitchen appliances (domain B).",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_9",
  "x": "**SCL** (Structural Correspondence Learning)<cite> (Blitzer et al., 2006</cite>; Blitzer, 2008 ) is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains. Before describing the algorithm in detail, let us illustrate the intuition behind SCL with an example, borrowed from . Suppose we have a Sentiment Analysis system trained on book reviews (domain A), and we would like to adapt it to kitchen appliances (domain B). Features such as \"boring\" and \"repetitive\" are common ways to express negative sentiment in A, while \"not working\" or \"defective\" are specific to B. If there are features across the domains, e.g. \"don't buy\", with which the domain specific features are highly correlated with, then we might tentatively align those features. Therefore, the key idea of SCL is to identify automatically correspondences among features from different domains by modeling their correlations with pivot features. Pivots are features occurring frequently and behaving similarly in both domains<cite> (Blitzer et al., 2006)</cite> . They are inspired by auxiliary problems from Ando and Zhang (2005) . Non-pivot features that correspond with many of the same pivot-features are assumed to correspond.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_10",
  "x": "Non-pivot features that correspond with many of the same pivot-features are assumed to correspond. Intuitively, if we are able to find good correspondences among features, then the augmented labeled source domain data should transfer better to a target domain (where no labeled data is available)<cite> (Blitzer et al., 2006)</cite> . The outline of the algorithm is given in Figure 1 . The first step is to identify m pivot features occurring frequently in the unlabeled data of both 4. Apply SVD to W : :h,:] are the h top left singular vectors of W . 5. Apply projection x s \u03b8 and train a predictor on the original and new features obtained through the projection. domains. Then, a binary classifier is trained for each pivot feature (pivot predictor) of the form: \"Does pivot feature l occur in this instance?\". The pivots are masked in the unlabeled data and the aim is to predict them using non-pivot features.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_11",
  "x": "Positive entries in the weight vector indicate that a non-pivot is highly correlated with the respective pivot feature. Step 3 is to arrange the m weight vectors in a matrix W , where a column corresponds to a pivot predictor weight vector. Applying the projection W T x (where x is a training instance) would give us m new features, however, for \"both computational and statistical reasons\"<cite> (Blitzer et al., 2006</cite>; Ando and Zhang, 2005 ) a low-dimensional approximation of the original feature space is computed by applying Singular Value Decomposition (SVD) on W (step 4). Let \u03b8 = U T h\u00d7n be the top h left singular vectors of W (with h a dimension parameter and n the number of non-pivot features). The resulting \u03b8 is a projection onto a lower dimensional space R h , parameterized by h. The final step of SCL is to train a linear predictor on the augmented labeled source data x, \u03b8x . In more detail, the original feature space x is augmented with h new features obtained by applying the projection \u03b8x. In this way, we can learn weights for domain-specific features, which otherwise would not have been observed. If \u03b8 contains meaningful correspondences, then the pre-dictor trained on the augmented data should transfer well to the new domain.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_12",
  "x": "A property of the pivot predictors is that they can be trained from unlabeled data, as they represent properties of the input. So far, pivot features on the word level were used<cite> (Blitzer et al., 2006</cite>; Blitzer, 2008) , e.g. \"Does the bigram not buy occur in this document?\" (Blitzer, 2008) . Pivot features are the key ingredient for SCL, and they should align well with the NLP task. For PoS tagging and Sentiment Analysis, features on the word level are intuitively well-related to the problem at hand. For the task of parse disambiguation based on a conditional model this is not the case. Hence, we actually introduce an additional and new layer of abstraction, which, we hypothesize, aligns well with the task of parse disambiguation: we first parse the unlabeled data. In this way we obtain full parses for given sentences as produced by the grammar, allowing access to more abstract representations of the underlying pivot predictor training data (for reasons of efficiency, we here use only the first generated parse as training data for the pivot predictors, rather than n-best). Thus, instead of using word-level features, our features correspond to properties of the generated parses: application of grammar rules (r1,r2 features), dependency relations (dep), PoS tags (f1,f2), syntactic features (s1), precedence (mf ), bilexical preferences (z), apposition (appos) and further features for unknown words, temporal phrases, coordination (h,in year and p1, respectively). This allows us to get a possibly noisy, but more abstract representation of the underlying data.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_13",
  "x": "As pivot features should be common across domains, here we restrict our pivots to be of the type r1,p1,s1 (the most frequently occurring feature types). In more detail, r1 indicates which grammar rule applied, p1 whether coordination conjuncts are parallel, and s1 whether topicalization or long-distance dependencies occurred. We count how often each feature appears in the parsed source and target domain data, and select those r1,p1,s1 features as pivot features, whose count is > t, where t is a specified threshold. In all our experiments, we set t = 5000. In this way we obtained on average 360 pivot features, on the datasets described in Section 5. Predictive features As pointed out by<cite> Blitzer et al. (2006)</cite> , each instance will actually contain features which are totally predictive of the pivot features (i.e. the pivot itself). In our case, we additionally have to pay attention to 'more specific' features, e.g. r2 is a feature that extends r1, in the sense that it incorporates more information than its parent (i.e. which grammar rules applied in the construction of daughter nodes). It is crucial to remove these predictive features when creating the training data for the pivot predictors. Following<cite> Blitzer et al. (2006)</cite> (which follow Ando and Zhang (2005)), we only use positive entries in the pivot predictors weight vectors to compute the SVD.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_14",
  "x": "Predictive features As pointed out by<cite> Blitzer et al. (2006)</cite> , each instance will actually contain features which are totally predictive of the pivot features (i.e. the pivot itself). In our case, we additionally have to pay attention to 'more specific' features, e.g. r2 is a feature that extends r1, in the sense that it incorporates more information than its parent (i.e. which grammar rules applied in the construction of daughter nodes). It is crucial to remove these predictive features when creating the training data for the pivot predictors. Following<cite> Blitzer et al. (2006)</cite> (which follow Ando and Zhang (2005)), we only use positive entries in the pivot predictors weight vectors to compute the SVD. Thus, when constructing the matrix W , we disregard all negative entries in W and compute the SVD (W = U DV T ) on the resulting non-negative sparse matrix. This sparse representation saves both time and space. ---------------------------------- **MATRIX AND SVD** ----------------------------------",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_15",
  "x": "This sparse representation saves both time and space. ---------------------------------- **MATRIX AND SVD** ---------------------------------- **FURTHER PRACTICAL ISSUES OF SCL** In practice, there are more free parameters and model choices (Ando and Zhang, 2005; Ando, 2006;<cite> Blitzer et al., 2006</cite>; Blitzer, 2008) besides the ones discussed above. Feature normalization and feature scaling. Blitzer et al. (2006) found it necessary to normalize and scale the new features obtained by the projection \u03b8, in order to \"allow them to receive more weight from a regularized discriminative learner\". For each of the features, they centered them by subtracting out the mean and normalized them to unit variance (i.e. x \u2212 mean/sd).",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_16",
  "x": "For each of the features, they centered them by subtracting out the mean and normalized them to unit variance (i.e. x \u2212 mean/sd). They then rescaled the features by a factor \u03b1 found on heldout data: \u03b1\u03b8x. Restricted Regularization. When training the supervised model on the augmented feature space x, \u03b8x ,<cite> Blitzer et al. (2006)</cite> only regularize the weight vector of the original features, but not the one for the new low-dimensional features. This was done to encourage the model to use the new low-dimensional representation rather than the higher-dimensional original representation (Blitzer, 2008) . Dimensionality reduction by feature type. An extension suggested in Ando and Zhang (2005) is to compute separate SVDs for blocks of the matrix W corresponding to feature types (as illustrated in Figure 2 ), and then to apply separate projection for every type. Due to the positive results in Ando (2006),<cite> Blitzer et al. (2006)</cite> include this in their standard setting of SCL and report results using block SVDs only. ----------------------------------",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_17",
  "x": "They then rescaled the features by a factor \u03b1 found on heldout data: \u03b1\u03b8x. Restricted Regularization. When training the supervised model on the augmented feature space x, \u03b8x ,<cite> Blitzer et al. (2006)</cite> only regularize the weight vector of the original features, but not the one for the new low-dimensional features. This was done to encourage the model to use the new low-dimensional representation rather than the higher-dimensional original representation (Blitzer, 2008) . Dimensionality reduction by feature type. An extension suggested in Ando and Zhang (2005) is to compute separate SVDs for blocks of the matrix W corresponding to feature types (as illustrated in Figure 2 ), and then to apply separate projection for every type. Due to the positive results in Ando (2006),<cite> Blitzer et al. (2006)</cite> include this in their standard setting of SCL and report results using block SVDs only. ---------------------------------- **EXPERIMENTS AND RESULTS**",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_18",
  "x": "Optionally, filter out certain pages In our empirical setup, we followed<cite> Blitzer et al. (2006)</cite> and tried to balance the size of source and target data. Thus, depending on the size of the resulting target domain dataset, and the \"broadness\" of the categories involved in creating it, we might wish to filter out certain pages. We implemented a filter mechanism that excludes pages of a certain category (e.g. a supercategory that is hypothesized to be \"too broad\"). Alternatively, we might have used a filter mechanism that excludes certain pages directly. In our experiments, we always included pages that are directly related to a page of interest, and those that shared a subcategory. Of course, the page itself is not included in that dataset. With regard to supercategories, we usually included all pages having a category c \u2208 super categories(p), unless stated otherwise. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_19",
  "x": "The results show a (sometimes) small but consistent increase in absolute performance on all testsets over the baseline system (up to +0.26 absolute CA score), as well as an increase in \u03c6 measure (absolute error reduction). This corresponds to a relative error reduction of up to 7.29%. Thus, our first instantiation of SCL for parse disambiguation indeed shows promising results. We can confirm that changing the dimensionality parameter h has rather little effect (Table 4) , which is in line with previous findings (Ando and Zhang, 2005;<cite> Blitzer et al., 2006)</cite> . Thus we might fix the parameter and prefer smaller dimensionalities, which saves space and time. Note that these results were obtained without any of the additional normalization, rescaling, feature-specific regularization, or block SVD issues, etc. (discussed in section 4.2). We used the same Gaussian regularization term (\u03c3 2 =1000) for all features (original and new features), and did not perform any feature normalization or rescaling. This means our current instantiation of SCL is an actually simplified version of the original SCL algorithm, applied to parse disambiguation.",
  "y": "similarities"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_20",
  "x": "Of course, our results are preliminary and, rather than warranting many definite conclusions, encourage further exploration of SCL and related semi-supervised adaptation techniques. ---------------------------------- **ADDITIONAL EMPIRICAL RESULTS** In the following, we describe additional results obtained by extensions and/or refinements of our current SCL instantiation. Feature normalization. We also tested feature normalization (as described in Section 4.2). While<cite> Blitzer et al. (2006)</cite> found it necessary to normalize (and scale) the projection features, we did not observe any improvement by normalizing them (actually, it slightly degraded performance in our case). Thus, we found this step unnecessary, and currently did not look at this issue any further. A look at \u03b8 To gain some insight of which kind of correspondences SCL learned in our case, we started to examine the rows of \u03b8.",
  "y": "differences"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_21",
  "x": "6.65%) than the model with no feature split (no block SVDs), thus obtaining a relative error reduction of 6.65% over the baseline. The same figure also shows what happens if we remove a specific feature type at a time; the apposition features contribute the most on this Prince domain. As a fact, one third of the sentences in the Prince testset contain constructions with appositions (e.g. about film-, album-and song titles). ---------------------------------- **CONCLUSIONS AND FUTURE WORK** The paper presents an application of Structural Correspondence Learning (SCL) to parse disam- Figure 5 : Results of dimensionality reduction by feature type, h = 25; block SVD included all 9 feature types; the right part shows the accuracy when one feature type was removed. biguation. While SCL has been successfully applied to PoS tagging and Sentiment Analysis<cite> (Blitzer et al., 2006</cite>; , its effectiveness for parsing was rather unexplored. The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results, even without the many additional extensions on the feature level as done in<cite> Blitzer et al. (2006)</cite> .",
  "y": "motivation background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_22",
  "x": "**CONCLUSIONS AND FUTURE WORK** The paper presents an application of Structural Correspondence Learning (SCL) to parse disam- Figure 5 : Results of dimensionality reduction by feature type, h = 25; block SVD included all 9 feature types; the right part shows the accuracy when one feature type was removed. biguation. While SCL has been successfully applied to PoS tagging and Sentiment Analysis<cite> (Blitzer et al., 2006</cite>; , its effectiveness for parsing was rather unexplored. The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results, even without the many additional extensions on the feature level as done in<cite> Blitzer et al. (2006)</cite> . We exploited Wikipedia as primary resource, both for collecting unlabeled target domain data, as well as test suite for empirical evaluation. On the three examined datasets, SCL slightly but constantly outperformed the baseline. Applying SCL involves many design choices and practical issues, which we tried to depict here in detail. A novelty in our application is that we first actually parse the unlabeled data from both domains.",
  "y": "differences"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_0",
  "x": "**INTRODUCTION** The task of paraphrase generation has many important applications in NLP. It can be used to generate adversarial examples of input text, which can then be used to train neural networks so that they become less susceptible to adversarial attack (Iyyer et al., 2018) . For knowledge-based QA systems, a paraphrasing step can produce multiple variations of a user query and match them with knowledge base assertions, enhancing recall (Yin et al., 2015; Fader et al., 2014) . Relation extraction can also benefit from incorporating paraphrase generation into its processing pipeline (Romano et al., 2006) . Manually annotating translation references is expensive, and automatically generating references through paraphrasing has been shown to be effective for evaluation of machine translation (Zhou et al., 2006; Kauchak and Barzilay, 2006) . Datasets used for paraphrase generation include QUORA 1 , TWITTER (Lan et al., 2017) and MSCOCO (Lin et al., 2014) . Previous work on paraphrase generation that used these datasets (Wang et al., 2019;<cite> Gupta et al., 2018</cite>; Li et al., 1 https://data.quora.com/ First-Quora-Dataset-Release-Question-Pairs 2018; Prakash et al., 2016) chose BLEU (Papineni et al., 2002) , METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006) as evaluation metrics. In this paper, we find that simply using the input sentence as output in an unsupervised manner (i.e. fully parroting the input) significantly outperforms the state-of-the-art on two metrics for TWITTER, and on one metric for QUORA.",
  "y": "background"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_1",
  "x": "The QUORA dataset contains 149,263 paraphrase sentence pairs (positive examples) and 255,027 non-paraphrase sentence pairs (negative examples). Having both positive and negative ex-amples makes it appealing for research on paraphrase generation (Gupta et al., 2018; Li et al., 2018) and identification (Lan and Xu, 2018) . After processing the dataset, there are 149,650 unique sentences that have reference paraphrases. <cite>Gupta et al. (2018)</cite> sampled 4K sentences as their test set, but did not specify which sentences they used. Li et al.(2018) sampled 30K sentences as their test set, also not specifying which sentences they used. To avoid selecting a subset of data that is biased in favor of our method, we perform evaluation on the entire QUORA dataset. Although we evaluate on the entire dataset, the size of our training set is zero due to the fully unsupervised nature of full and partial parroting. We group sentences by the number of reference paraphrases they have, and plot the relative counts in Appendix A. It can be seen that over 64% of entries have only a single reference paraphrase, which is problematic because even if a paraphrase of good quality is generated for any one of these entries, BLEU, METEOR and TER scores could still be inferior if the generated paraphrase differs too much from the single reference paraphrase. Previous paraphrase generation work on QUORA (Gupta et al., 2018; Li et al., 2018) did not mention removing these entries, thus we include them in our experiments for fair comparison.",
  "y": "background"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_2",
  "x": "MSCOCO. This is an image captioning dataset, with multiple captions provided for a single image (Lin et al., 2014) . There have been multiple works which use it as a paraphrase generation dataset by treating captions of the same image as paraphrases (Wang et al., 2019;<cite> Gupta et al., 2018</cite>; Prakash et al., 2016) . The training and testing sets are available, containing 331,163 and 162,016 input sentences respectively. However, relevance scores for captions of the same image score only 3.38 out of 5 under human evaluation (in contrast, the score is 4.82 for QUORA)<cite> (Gupta et al., 2018)</cite> , due to the fact that different captions for the same image often vary in the semantic information conveyed. This makes the use of MSCOCO as a paraphrase generation dataset questionable. We plot the number of reference paraphrases in Appendix A. ---------------------------------- **EXPERIMENTS**",
  "y": "background"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_3",
  "x": "However, relevance scores for captions of the same image score only 3.38 out of 5 under human evaluation (in contrast, the score is 4.82 for QUORA)<cite> (Gupta et al., 2018)</cite> , due to the fact that different captions for the same image often vary in the semantic information conveyed. This makes the use of MSCOCO as a paraphrase generation dataset questionable. We plot the number of reference paraphrases in Appendix A. ---------------------------------- **EXPERIMENTS** We evaluate the performance of full parroting on all three datasets and compare with state-of-the-art models. We also study the performance of partial parroting. Whereas full parroting does not modify the input sentence, partial parroting replaces or cuts some of the input words. We try three different modes of choosing words to be cut or replaced: from the sentence head, from the tail or sampled randomly.",
  "y": "background"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_4",
  "x": "Furthermore, the score deviation between different samples is small. Consequently, although the exact test sets used by<cite> (Gupta et al., 2018)</cite> and (Li et al., 2018) are not available, it is logical to assume that parroting performance would still exceed or be on par with the state-of-the-art on those test sets. Partial parroting. We also introduce lexical variation into our parroting method by replacing or cutting words of the input sentence. For replacement, we substitute input words with an outof-vocabulary word not found in any of the input sentence's reference paraphrases. Paraphrase generation models are usually allowed to generate words which exist in reference paraphrases; we purposely use out-of-vocabulary words to give harsher scores to our method. words from the start of input sentences. For QUORA, when over 10% of the input sentence has been modified by being cut off, partial parroting underperforms the state-of-the-art by only 3.8% on METEOR. For TWITTER, the same form of partial parroting (cutting off words) still outperforms the state-of-the-art on BLEU when input sentences are modified by 42% , and does the same on METEOR when the input is modified by 56%.",
  "y": "future_work"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_0",
  "x": "The importance of concepts is implemented by assigning weights w i to each concept i with binary variable c i , yielding the following coverage maximization objective, subject to the appropriate constraints: In proposing bigrams as concepts for their system,<cite> Gillick and Favre (2009)</cite> explain that: [c]oncepts could be words, named entities, syntactic subtrees or semantic relations, for example. While deeper semantics make more appealing concepts, their extraction and weighting are much more error-prone. Any error in concept extraction can result in a biased objective function, leading to poor sentence selection. (Gillick and Favre, 2009) Several authors, e.g., Woodsend and Lapata (2012) , and Li et al. (2013) , have followed<cite> Gillick and Favre (2009)</cite> in assuming that bigrams would lead to better practical performance than more syntactic or semantic concepts, even though bigrams serve as only an approximation of these. In this paper, we revisit this assumption and evaluate the maximum coverage objective for extractive text summarization with syntactic and semantic concepts. Specifically, we replace bigram concepts with new ones based on syntactic dependencies, semantic frames, as well as named entities. We show that using such concepts can lead to significant improvements in text summarization performance outside of the newswire domain.",
  "y": "background"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_1",
  "x": "State-of-the-art approaches to extractive summarization are based on the notion of coverage maximization (Berg-Kirkpatrick et al., 2011) . The assumption is that a good summary is a selection of sentences from the document that contains as many of the important concepts as possible. The importance of concepts is implemented by assigning weights w i to each concept i with binary variable c i , yielding the following coverage maximization objective, subject to the appropriate constraints: In proposing bigrams as concepts for their system,<cite> Gillick and Favre (2009)</cite> explain that: [c]oncepts could be words, named entities, syntactic subtrees or semantic relations, for example. While deeper semantics make more appealing concepts, their extraction and weighting are much more error-prone. Any error in concept extraction can result in a biased objective function, leading to poor sentence selection. (Gillick and Favre, 2009) Several authors, e.g., Woodsend and Lapata (2012) , and Li et al. (2013) , have followed<cite> Gillick and Favre (2009)</cite> in assuming that bigrams would lead to better practical performance than more syntactic or semantic concepts, even though bigrams serve as only an approximation of these. In this paper, we revisit this assumption and evaluate the maximum coverage objective for extractive text summarization with syntactic and semantic concepts.",
  "y": "background"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_2",
  "x": "Bigrams.<cite> Gillick and Favre (2009)</cite> proposed to use bigrams as concepts, and to weight their contribution to the objective function in Equation (1) by the frequency with which they occur in the document. Some pre-processing is first carried out to these bigrams: all bigrams consisting uniquely of stop-words are removed from consideration, and each word is stemmed. They also require bigrams to occur with a minimal frequency (cf. Section 3.2). Named entities. We consider three new types of concepts, all suggested, but subsequently rejected by<cite> Gillick and Favre (2009</cite> Semantic frames. The intuition behind our use of frame semantics is that a summary should represent the most central semantic frames (Fillmore, 1982; Fillmore et al., 2003) present in the corresponding document-indeed, we consider these frames to be actual types of concepts. We extract frame names from sentences for a further type of concepts under consideration. We use SE-MAFOR 3 to augment documents with semantic frames. ----------------------------------",
  "y": "background"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_4",
  "x": "---------------------------------- **BASELINE AND SYSTEMS** Our baseline is the bigram-based extraction summarization system of<cite> Gillick and Favre (2009)</cite> , icsisumm 7 . Their system was originally intended for multi-document update summarization, and summaries are extracted from document sentences that share more than k content words with some query. We follow this approach for the TAC08 data. For ECHR and WIKIPEDIA, the task is single document summarization, and the now irrelevant topic-document intersection preprocessing step is eliminated. The original system uses the GNU linear programming kit 8 with a time limit of 100 seconds. For all experiments presented in this paper, we double this time limit; we experimented with longer time limits on the development set for the ECHR data, without any performance improvements. Once the summarizer reaches the time limit, a summary is output based on the current feasible solution, whether the solution is optimal or not.",
  "y": "uses"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_5",
  "x": "The original system takes several important input parameters. 1. Summary length, for TAC08, is specified by the TAC 2008 conference guidelines as 100 words. For WIKIPEDIA and ECHR, we have access to training sets which gave an average summary length of around 335 and 805 words respectively, which we take as the standard output summary length. 2. Concept count cut-off is the minimum frequency of concepts from the document (set) that qualifies them for consideration in coverage maximization. For bigrams of the original system on TAC08, there are two types of document sets: 'A' and 'B'. For 'A' type documents,<cite> Gillick and Favre (2009)</cite> set this threshold to 3 and for 'B' type documents, they set this to 4. For WIKIPEDIA and ECHR, we take the bigram threshold to be 4. In our extension of the system to other concepts, we do not use any threshold.",
  "y": "background"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_6",
  "x": "**RESULTS** We evaluate output summaries using ROUGE-1, ROUGE-2, and ROUGE-SU4 (Lin, 2004) , with no stemming and retaining all stopwords. These measures have been shown to correlate best with human judgments in general, but among the automatic measures, ROUGE-1 and ROUGE-2 also correlate best with the Pyramid (Nenkova and Passonneau, 2004; Nenkova et al., 2007) and Responsiveness manual metrics (Louis and Nenkova, 2009) . Moreover, ROUGE-1 has been shown to best reflect human-automatic summary comparisons (Owczarzak et al., 2012) . For single concept systems, the results are shown in Table 1 , and concept combination system results are given in Table 2 . We first note that our runs of the current distribution of icsisumm yield significantly worse ROUGE-2 results than reported in<cite> (Gillick and Favre, 2009</cite> ) (see Table 1 , BIGRAMS): 0.081 compared to 0.110 respectively. On the TAC08 data, we observe no improvements over the baseline BIGRAM system for any ROUGE metric here. Hence,<cite> Gillick and Favre (2009)</cite> were right in their assumption that syntactic and semantic concepts would not lead to performance improvements, when restricting ourselves to this dataset. However, when we change domain to the legal judgments or Wikipedia articles, using syntactic and semantic concepts leads to significant gains across all the ROUGE metrics.",
  "y": "differences"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_0",
  "x": "Macro Acc STA EVE REP GENI GENA QUE IMP CRF<cite> (Friedrich et al., 2016)</cite> 66 ---------------------------------- **IMPACT OF GENRE** Considering that MASC+Wiki is rich in written genres, we additionally conduct cross-genre classification experiments, where we use one genre of documents for testing and the other genres of documents for training. The purpose of cross-genre experiment is to see whether the model can work robustly across genres. Table 4 shows cross-genre experimental results of our neural network models on the training set of MASC+Wiki by treating each genre as one crossvalidation fold. As we expected, both the macroaverage F1-score and class-wise F1 scores are lower compared with the results in Table 2 where in-genre data were used for model training as well. But the performance drop on the paragraph-level models is little, which clearly outperform the previous system<cite> (Friedrich et al., 2016)</cite> and the baseline model by a large margin. As shown in Table 5, benefited from modeling wider contexts and common SE label patterns, our full paragraphlevel model improves performance across almost all the genres. The high performance in the crossgenre setting demonstrates the robustness of our paragraph-level model across genres.",
  "y": "differences"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_1",
  "x": "Capabilities to categorize a clause based on the type of situation entity (e.g., events, states and generic statements) the clause introduces to the discourse can benefit many NLP applications. Observing that the situation entity type of a clause depends on discourse functions the clause plays in a paragraph and the interpretation of discourse functions depends heavily on paragraph-wide contexts, we propose to build context-aware clause representations for predicting situation entity types of clauses. Specifically, we propose a hierarchical recurrent neural network model to read a whole paragraph at a time and jointly learn representations for all the clauses in the paragraph by extensively modeling context influences and inter-dependencies of clauses. Experimental results show that our model achieves the state-of-the-art performance for clause-level situation entity classification on the genrerich MASC+Wiki corpus, which approaches human-level performance. ---------------------------------- **INTRODUCTION** Clauses in a paragraph play different discourse and pragmatic roles and have different aspectual properties (Smith, 1997; Verkuyl, 2013) accordingly. We aim to categorize a clause based on its aspectual property and more specifically, based on the type of Situation Entity (SE) 1 (e.g., events, states, generalizing statements and generic statements) the clause introduces to the discourse, following the recent work by<cite> (Friedrich et al., 2016)</cite> . Understanding SE types of clauses is beneficial for many NLP tasks, including discourse mode identi-fication 2 (Smith, 2003 (Smith, , 2005 ), text summarization, information extraction and question answering.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_2",
  "x": "Experimental results show that our model achieves the state-of-the-art performance for clause-level situation entity classification on the genrerich MASC+Wiki corpus, which approaches human-level performance. ---------------------------------- **INTRODUCTION** Clauses in a paragraph play different discourse and pragmatic roles and have different aspectual properties (Smith, 1997; Verkuyl, 2013) accordingly. We aim to categorize a clause based on its aspectual property and more specifically, based on the type of Situation Entity (SE) 1 (e.g., events, states, generalizing statements and generic statements) the clause introduces to the discourse, following the recent work by<cite> (Friedrich et al., 2016)</cite> . Understanding SE types of clauses is beneficial for many NLP tasks, including discourse mode identi-fication 2 (Smith, 2003 (Smith, , 2005 ), text summarization, information extraction and question answering. The situation entity type of a clause reflects discourse roles the clause plays in a paragraph and discourse role interpretation depends heavily on paragraph-wide contexts. Recently, <cite>Friedrich et al. (2016)</cite> used insightful syntactic-semantic features extracted from the target clause itself for SE type classification, which has achieved good performance across several genres when evaluated on the newly created large dataset MASC+Wiki. In addition, <cite>Friedrich et al. (2016)</cite> implemented a sequence labeling model with conditional random fields (CRF) (Lafferty et al., 2001 ) for finetuning a sequence of predicted SE types.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_3",
  "x": "In addition, <cite>Friedrich et al. (2016)</cite> implemented a sequence labeling model with conditional random fields (CRF) (Lafferty et al., 2001 ) for finetuning a sequence of predicted SE types. However, other than leveraging common SE label patterns (e.g., GENERIC clauses tend to cluster together.), this approach largely ignored the wider contexts a clause appears in when predicting its SE type. To further improve the performance and robustness of situation entity type classification, we argue that we should consider influences of wider contexts more extensively, not only by fine-tuning a sequence of SE type predictions, but also in deriving clause representations and obtaining precise individual SE type predictions. For example, we distinguish GENERIC statements from GENER-ALIZING statements depending on if a clause expresses general information over classes or kinds instead of specific individuals. We recognize the latter two clauses in the following paragraph as GENERALIZING because both clauses describe situations related to the Amazon river: (1): [Today, the Amazon river is experiencing a crisis of overfishing. ] STATE [Both subsistence fishers and their commercial rivals compete in netting large quantities of pacu,] GENERALIZING [which bring good prices at markets in Brazil and abroad.] GENERALIZING If we ignore the wider context, the second clause can be wrongly recognized as GENERIC easily since \"fishers\" usually refer to one general class rather than specific individuals. However, considering the background introduced in first clause, \"fishers\" here actually refer to the fishers who fish on Amazon river which become specific individuals immediately. Therefore, we aim to build context-aware clause representations dynamically which are informed by their paragraph-wide contexts.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_4",
  "x": "Recently, <cite>Friedrich et al. (2016)</cite> used insightful syntactic-semantic features extracted from the target clause itself for SE type classification, which has achieved good performance across several genres when evaluated on the newly created large dataset MASC+Wiki. In addition, <cite>Friedrich et al. (2016)</cite> implemented a sequence labeling model with conditional random fields (CRF) (Lafferty et al., 2001 ) for finetuning a sequence of predicted SE types. However, other than leveraging common SE label patterns (e.g., GENERIC clauses tend to cluster together.), this approach largely ignored the wider contexts a clause appears in when predicting its SE type. To further improve the performance and robustness of situation entity type classification, we argue that we should consider influences of wider contexts more extensively, not only by fine-tuning a sequence of SE type predictions, but also in deriving clause representations and obtaining precise individual SE type predictions. For example, we distinguish GENERIC statements from GENER-ALIZING statements depending on if a clause expresses general information over classes or kinds instead of specific individuals. We recognize the latter two clauses in the following paragraph as GENERALIZING because both clauses describe situations related to the Amazon river: (1): [Today, the Amazon river is experiencing a crisis of overfishing. ] STATE [Both subsistence fishers and their commercial rivals compete in netting large quantities of pacu,] GENERALIZING [which bring good prices at markets in Brazil and abroad.] GENERALIZING If we ignore the wider context, the second clause can be wrongly recognized as GENERIC easily since \"fishers\" usually refer to one general class rather than specific individuals. However, considering the background introduced in first clause, \"fishers\" here actually refer to the fishers who fish on Amazon river which become specific individuals immediately.",
  "y": "background motivation"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_5",
  "x": "Our paragraph-level model derive clause representations by modeling interdependencies between clauses within a paragraph. In order to further improve SE type classification performance, we also add an extra CRF layer at the top of our paragraph-level model to fine-tune a sequence of SE type predictions over clauses<cite> (Friedrich et al., 2016)</cite> , which however is not our contribution. Experimental results show that our paragraphlevel neural network model greatly improves the performance of SE type classification on the same MASC+Wiki<cite> (Friedrich et al., 2016)</cite> corpus and achieves robust performance close to human level. In addition, the CRF layer further improves the SE type classification results, but by a small margin. We hypothesize that situation entity type patterns across clauses may have been largely captured by allowing the preceding and following clauses to influence semantic representation building for a clause in the paragraph-level neural net model. ---------------------------------- **RELATED WORK** ---------------------------------- **LINGUISTIC CATEGORIES OF SE TYPES**",
  "y": "differences"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_6",
  "x": "For example, we distinguish GENERIC statements from GENER-ALIZING statements depending on if a clause expresses general information over classes or kinds instead of specific individuals. We recognize the latter two clauses in the following paragraph as GENERALIZING because both clauses describe situations related to the Amazon river: (1): [Today, the Amazon river is experiencing a crisis of overfishing. ] STATE [Both subsistence fishers and their commercial rivals compete in netting large quantities of pacu,] GENERALIZING [which bring good prices at markets in Brazil and abroad.] GENERALIZING If we ignore the wider context, the second clause can be wrongly recognized as GENERIC easily since \"fishers\" usually refer to one general class rather than specific individuals. However, considering the background introduced in first clause, \"fishers\" here actually refer to the fishers who fish on Amazon river which become specific individuals immediately. Therefore, we aim to build context-aware clause representations dynamically which are informed by their paragraph-wide contexts. Specifically, we propose a hierarchical recurrent neural network model to read a whole paragraph at a time and jointly learn representations for all the clauses in the paragraph. Our paragraph-level model derive clause representations by modeling interdependencies between clauses within a paragraph. In order to further improve SE type classification performance, we also add an extra CRF layer at the top of our paragraph-level model to fine-tune a sequence of SE type predictions over clauses<cite> (Friedrich et al., 2016)</cite> , which however is not our contribution.",
  "y": "extends"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_7",
  "x": "---------------------------------- **RELATED WORK** ---------------------------------- **LINGUISTIC CATEGORIES OF SE TYPES** The situation entity types annotated in the MASC+Wiki corpus<cite> (Friedrich et al., 2016)</cite> were initially introduced by Smith (2003) , which were then extended by (Palmer et al., 2007; Friedrich and Palmer, 2014b) . The situation entity types can be divided into the following broad categories: \u2022 Eventualities (EVENT, STATE and RE-PORT): for clauses representing actual happenings and world states. STATE and EVENT are two fundamental aspectual classes of a clause (Siegel and McKeown, 2000) which can be distinguished by the semantic property of dynamism. REPORT is a subtype of EVENT for quoted speech.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_8",
  "x": "Palmer et al. (2007) first implemented a maximum entropy model for SE type classification relying on words, POS tags and some linguistic cues as main features. This work used a relatively small dataset (around 4300 clauses) and did not achieve satisfied performance (around 50% of accuracy). To bridge the gap, <cite>Friedrich et al. (2016)</cite> created a much larger dataset MASC+Wiki (more than 40,000 clauses) and achieved better SE type classification performance (around 75% accuracy) by using rich features extracted from the target clause. The feature sets include POS tags, Brown cluster features, syntactic and semantic features of the main verb and main referent as well as features indicating the aspectual nature of a clause. <cite>Friedrich et al. (2016)</cite> further improved the performance by implementing a sequence labeling (CRF) model to fine-tune a sequence of SE type predictions and noted that much of the performance gain came from modeling the label pattern that GENERIC clauses often occur together. In contrast, we focus on deriving dynamic clause representations informed by paragraph-level contexts and model context influences more extensively. Becker et al. (2017) proposed a GRU based neural network model that predicts the SE type for one clause each time, by encoding the content of the target clause using a GRU and incorporating several sources of context information, including contents and labels of preceding clauses as well as genre information, using additional separate GRUs (Chung et al., 2014) . This model is different from our approach that processes one paragraph (with a sequence of clauses) at a time and extensively models inter-dependencies of clauses. Other related tasks include predicting aspectual classes of verbs (Friedrich and Palmer, 2014a) , classifying genericity of noun phrases (Reiter and Frank, 2010) and predicting clause habituality (Friedrich and Pinkal, 2015) .",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_9",
  "x": "---------------------------------- **SITUATION ENTITY (SE) TYPE CLASSIFICATION** Although situation entities have been well-studied in linguistics, there were only several previous works focusing on data-driven SE type classification using computational methods. Palmer et al. (2007) first implemented a maximum entropy model for SE type classification relying on words, POS tags and some linguistic cues as main features. This work used a relatively small dataset (around 4300 clauses) and did not achieve satisfied performance (around 50% of accuracy). To bridge the gap, <cite>Friedrich et al. (2016)</cite> created a much larger dataset MASC+Wiki (more than 40,000 clauses) and achieved better SE type classification performance (around 75% accuracy) by using rich features extracted from the target clause. The feature sets include POS tags, Brown cluster features, syntactic and semantic features of the main verb and main referent as well as features indicating the aspectual nature of a clause. <cite>Friedrich et al. (2016)</cite> further improved the performance by implementing a sequence labeling (CRF) model to fine-tune a sequence of SE type predictions and noted that much of the performance gain came from modeling the label pattern that GENERIC clauses often occur together. In contrast, we focus on deriving dynamic clause representations informed by paragraph-level contexts and model context influences more extensively.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_10",
  "x": "**SITUATION ENTITY (SE) TYPE CLASSIFICATION** Although situation entities have been well-studied in linguistics, there were only several previous works focusing on data-driven SE type classification using computational methods. Palmer et al. (2007) first implemented a maximum entropy model for SE type classification relying on words, POS tags and some linguistic cues as main features. This work used a relatively small dataset (around 4300 clauses) and did not achieve satisfied performance (around 50% of accuracy). To bridge the gap, <cite>Friedrich et al. (2016)</cite> created a much larger dataset MASC+Wiki (more than 40,000 clauses) and achieved better SE type classification performance (around 75% accuracy) by using rich features extracted from the target clause. The feature sets include POS tags, Brown cluster features, syntactic and semantic features of the main verb and main referent as well as features indicating the aspectual nature of a clause. <cite>Friedrich et al. (2016)</cite> further improved the performance by implementing a sequence labeling (CRF) model to fine-tune a sequence of SE type predictions and noted that much of the performance gain came from modeling the label pattern that GENERIC clauses often occur together. In contrast, we focus on deriving dynamic clause representations informed by paragraph-level contexts and model context influences more extensively. Becker et al. (2017) proposed a GRU based neural network model that predicts the SE type for one clause each time, by encoding the content of the target clause using a GRU and incorporating several sources of context information, including contents and labels of preceding clauses as well as genre information, using additional separate GRUs (Chung et al., 2014) .",
  "y": "background differences"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_11",
  "x": "**FINE-TUNE SITUATION ENTITY PREDICTIONS WITH A CRF LAYER** Previous studies (Friedrich et al., 2016; Becker et al., 2017) show that there exist common SE label patterns between adjacent clauses. For example, <cite>Friedrich et al. (2016)</cite> reported the fact that GENERIC sentences usually occur together in a paragraph. Following<cite> (Friedrich et al., 2016)</cite> , in order to capture SE label patterns in our hierarchical recurrent neural network model, we add a CRF layer at the top of the softmax prediction layer (shown in figure 2 ) to fine-tune predicted situation entity types. The CRF layer will update a state-transition matrix, which can effectively adjust the current label depending on its preceding and following labels. Both the training and decoding procedures of the CRF layer can be conducted efficiently using the Viterbi algorithm. With the CRF layer, the model jointly assigns a sequence of SE labels, one label per clause, by considering individual clause representations as well as common SE label patterns. ---------------------------------- **PARAMETER SETTINGS AND MODEL TRAINING**",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_12",
  "x": "---------------------------------- **FINE-TUNE SITUATION ENTITY PREDICTIONS WITH A CRF LAYER** Previous studies (Friedrich et al., 2016; Becker et al., 2017) show that there exist common SE label patterns between adjacent clauses. For example, <cite>Friedrich et al. (2016)</cite> reported the fact that GENERIC sentences usually occur together in a paragraph. Following<cite> (Friedrich et al., 2016)</cite> , in order to capture SE label patterns in our hierarchical recurrent neural network model, we add a CRF layer at the top of the softmax prediction layer (shown in figure 2 ) to fine-tune predicted situation entity types. The CRF layer will update a state-transition matrix, which can effectively adjust the current label depending on its preceding and following labels. Both the training and decoding procedures of the CRF layer can be conducted efficiently using the Viterbi algorithm. With the CRF layer, the model jointly assigns a sequence of SE labels, one label per clause, by considering individual clause representations as well as common SE label patterns. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_13",
  "x": "All our proposed models were implemented with Pytorch 6 and converged to the best result within 40 epochs. Note that to diminish the effects of randomness in training neural network models and report stable experimental results, we ran each of the proposed models as well as our own baseline models ten times and reported the averaged performance across the ten runs. ---------------------------------- **EVALUATION** ---------------------------------- **DATASET AND PREPROCESSING** The MASC+Wiki Corpus: We evaluated our neural network model on the MASC+Wiki corpus 7<cite> (Friedrich et al., 2016)</cite> (Friedrich et al., 2016; Becker et al., 2017) , we used the same 80:20 traintest split with balanced genre distributions. Preprocessing: As described in<cite> (Friedrich et al., 2016)</cite> , texts were split into clauses using SPADE (Soricut and Marcu, 2003) . There are 4,784 paragraphs in total in the corpus; and on average, each paragraph contains 9.6 clauses.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_14",
  "x": "All our proposed models were implemented with Pytorch 6 and converged to the best result within 40 epochs. Note that to diminish the effects of randomness in training neural network models and report stable experimental results, we ran each of the proposed models as well as our own baseline models ten times and reported the averaged performance across the ten runs. ---------------------------------- **EVALUATION** ---------------------------------- **DATASET AND PREPROCESSING** The MASC+Wiki Corpus: We evaluated our neural network model on the MASC+Wiki corpus 7<cite> (Friedrich et al., 2016)</cite> (Friedrich et al., 2016; Becker et al., 2017) , we used the same 80:20 traintest split with balanced genre distributions. Preprocessing: As described in<cite> (Friedrich et al., 2016)</cite> , texts were split into clauses using SPADE (Soricut and Marcu, 2003) . There are 4,784 paragraphs in total in the corpus; and on average, each paragraph contains 9.6 clauses.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_17",
  "x": "**EXPERIMENTAL RESULTS** Following the previous work<cite> (Friedrich et al., 2016)</cite> on the same task and dataset, we report accuracy and macro-average F1-score across SE types on the test set of MASC+Wiki. The first section of Table 3 shows the results of the previous works. The second section shows the result of our implemented clause-level Bi-LSTM baseline, which already outperforms the previous best model. This result proves the effectiveness of the Bi-LSTM + max pooling approach in clause representation learning (Conneau et al., 2017) . The third section reports the performance of the paragraph-level models that uses paragraph-wide contexts as input. Compared with the baseline clause-level Bi-LSTM model, the basic paragraphlevel model achieves 3.5% and 3.3% of performance gains in macro-average F1-score and accuracy respectively. Building on top of the basic paragraph-level model, the CRF layer further improves the SE type prediction performance slightly by 0.4% and 0.7% in macro-average F1-score and accuracy respectively. Therefore, our full model with the CRF layer achieves the state-of-the-art performance on the MASC+Wiki corpus.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_18",
  "x": "Compared with the baseline clause-level Bi-LSTM model, the basic paragraphlevel model achieves 3.5% and 3.3% of performance gains in macro-average F1-score and accuracy respectively. Building on top of the basic paragraph-level model, the CRF layer further improves the SE type prediction performance slightly by 0.4% and 0.7% in macro-average F1-score and accuracy respectively. Therefore, our full model with the CRF layer achieves the state-of-the-art performance on the MASC+Wiki corpus. ---------------------------------- **ANALYSIS** ---------------------------------- **10-FOLD CROSS-VALIDATION** We noticed that the previous work<cite> (Friedrich et al., 2016)</cite> did not publish the class-wise performance of their model on the test set, instead, they reported the detailed performance on the training set using 10-fold cross-validation. For direct comparisons, we also report our 10-fold cross-validation results 8 on the training set of MASC+Wiki.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_19",
  "x": "Compared with the baseline clause-level Bi-LSTM model, the basic paragraphlevel model achieves 3.5% and 3.3% of performance gains in macro-average F1-score and accuracy respectively. Building on top of the basic paragraph-level model, the CRF layer further improves the SE type prediction performance slightly by 0.4% and 0.7% in macro-average F1-score and accuracy respectively. Therefore, our full model with the CRF layer achieves the state-of-the-art performance on the MASC+Wiki corpus. ---------------------------------- **ANALYSIS** ---------------------------------- **10-FOLD CROSS-VALIDATION** We noticed that the previous work<cite> (Friedrich et al., 2016)</cite> did not publish the class-wise performance of their model on the test set, instead, they reported the detailed performance on the training set using 10-fold cross-validation. For direct comparisons, we also report our 10-fold cross-validation results 8 on the training set of MASC+Wiki.",
  "y": "uses"
 },
 {
  "id": "a01a6ab7cf13c7916b1b3823a4b4de_0",
  "x": "Since a Japanese version already exists, we could extract from it additional information to help with the transliteration process. Importantly, since our article is about an American guitarist, we would explicitly want to start with the English (original) version of the name, and treat other languages as extra data, rather than vice versa. In order to effectively incorporate the otherlanguage data, we apply SVM re-ranking in a manner that has previously been shown to provide significant improvement for grapheme-to-phoneme conversion (Bhargava and Kondrak, 2011) . This method is flexible enough to incorporate multiple languages; it employs features based on character alignments between potential outputs and existing transliterations from other languages, as well as scores of these alignments, which serve as a measure of similarity. We apply this approach on top of the same DIRECTL+ system as submitted last year <cite>(Jiampojamarn et al., 2010b)</cite> for English-to-Hindi machine transliteration. Compared to the base DI-RECTL+ performance, we are able to achieve significantly better results, with a relative performance increase of over 10%. We also achieve improvements without supplemental transliterations by simply apply the same approach with another system's output as extra data. We furthermore experiment with romanization for Hindi data as well as different alignment length settings for English-toChinese transliteration. This paper presents methods, methodology, and results for the above experiments.",
  "y": "background"
 },
 {
  "id": "a01a6ab7cf13c7916b1b3823a4b4de_1",
  "x": "**BASE SYSTEMS** Our principal base system that generates the n-best output lists is DIRECTL+, which has produced excellent results in the NEWS 2010 Shared Task on Transliteration <cite>(Jiampojamarn et al., 2010b)</cite> . For re-ranking, note that training a re-ranker requires training data where the base system scores are representative of unseen data so that the re-ranker does not simply learn to follow the base system; we therefore split the training data into ten folds and perform a sort-of cross validation with DIRECTL+. This provides us with usable training data for reranking. We tune the SVM's hyperparameter based on performance on the provided development data, and use the best DIRECTL+ settings established in the NEWS 2010 Shared Task <cite>(Jiampojamarn et al., 2010b)</cite> . Armed with optimal parameter settings, we combine the training and development data into a single set used to train our final DIRECTL+ system. We also repeat the cross-validation process for training the re-ranker. We also apply the SVM re-ranking approach to system combination. In this case, we additionally train another system-here we use SE-QUITUR (Bisani and Ney, 2008 )-for English-toHindi transliteration.",
  "y": "uses background"
 },
 {
  "id": "a01a6ab7cf13c7916b1b3823a4b4de_2",
  "x": "The use of other corpora here requires that these results be submitted as a nonstandard run. Note that, because there is not complete coverage for the English-to-Hindi test data, we simply submit the base system's results as-is in cases where there is no transliteration available from other languages. ---------------------------------- **BASE SYSTEMS** Our principal base system that generates the n-best output lists is DIRECTL+, which has produced excellent results in the NEWS 2010 Shared Task on Transliteration <cite>(Jiampojamarn et al., 2010b)</cite> . For re-ranking, note that training a re-ranker requires training data where the base system scores are representative of unseen data so that the re-ranker does not simply learn to follow the base system; we therefore split the training data into ten folds and perform a sort-of cross validation with DIRECTL+. This provides us with usable training data for reranking. We tune the SVM's hyperparameter based on performance on the provided development data, and use the best DIRECTL+ settings established in the NEWS 2010 Shared Task <cite>(Jiampojamarn et al., 2010b)</cite> . Armed with optimal parameter settings, we combine the training and development data into a single set used to train our final DIRECTL+ system.",
  "y": "extends"
 },
 {
  "id": "a01a6ab7cf13c7916b1b3823a4b4de_4",
  "x": "There are three lines of research that are relevant to the work we have presented in this paper: (1) DI-RECTL+ and SEQUITUR for machine transliteration; (2) applying multiple languages; and (3) system combination. For the NEWS 2009 and 2010 Shared Tasks, the discriminative DIRECTL+ system that incorporates many-to-many alignments, online maxmargin training and a phrasal decoder was shown to function well as a general string transduction tool; while originally designed for grapheme-tophoneme conversion, it produced excellent results for machine transliteration (Jiampojamarn et al., 2009;<cite> Jiampojamarn et al., 2010b)</cite> , leading us to re-use it here. Finch and Sumita (2010) also submitted a top-performing system that was based in part on SEQUITUR, which is a generative system based on joint n-gram modelling (Bisani and Ney, 2008) . In this paper, we applied multiple transliteration languages to a single transliteration task. While our method is based on SVM re-ranking with similar features as to those used in the base system (Bhargava and Kondrak, 2011) , there have been other explorations into incorporating other language data, particularly when data are scarce. Zhang et al. (2010) , for example, apply a pivoting approach to machine transliteration, and similarly Khapra et al. (2010) propose to transliterate through \"bridge\" languages. Along similar lines, Kumaran et al. (2010a) find increases in accuracy using a linear-combination-of-scores system that combined the outputs of a direct transliteration system with a system that transliterated through a third language. For statistical machine translation, Cohn and Lapata (2007) also explore the use of a third language. Finally, we also touched briefly on system combination: we applied the SVM re-ranking method to combining the outputs of both DIRECTL+ and SEQUITUR, in particular treating DIRECTL+ as the base system and using SEQUITUR's best outputs to re-rank DIRECTL+'s output lists.",
  "y": "uses background"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_0",
  "x": "Understanding which components are noncompositional within an MWE is important in NLP applications in which semantic information is required. For example, when searching for spelling bee, we may also be interested in documents about spelling, but not those which contain only bee. For research project, on the other hand, we are likely to be interested in documents which contain either research or project in isolation, and for swan song, we are only going to be interested in documents which contain the phrase swan song, and not just swan or song. In this paper, we propose an unsupervised approach based on Wikitionary for predicting which components of a given MWE have a compositional usage. Experiments over two widely-used datasets show that our approach outperforms stateof-the-art methods. ---------------------------------- **RELATED WORK** Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) , or the prediction of the compositionality of MWE types (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) . The identification of non-compositional MWE tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional (generally non-MWE) and non-compositional MWE usage.",
  "y": "background"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_1",
  "x": "For research project, on the other hand, we are likely to be interested in documents which contain either research or project in isolation, and for swan song, we are only going to be interested in documents which contain the phrase swan song, and not just swan or song. In this paper, we propose an unsupervised approach based on Wikitionary for predicting which components of a given MWE have a compositional usage. Experiments over two widely-used datasets show that our approach outperforms stateof-the-art methods. ---------------------------------- **RELATED WORK** Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) , or the prediction of the compositionality of MWE types (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) . The identification of non-compositional MWE tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009 ) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or applicable across multiple constructions using features similar to those used in all-words word sense disambiguation (Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) . The prediction of the compositionality of MWE types has traditionally been couched as a binary classification task (compositional or non-compositional: Baldwin et al. (2003) , Bannard (2006) ), but more recent work has moved towards a regression setup, where the degree of the compositionality is predicted on a continuous scale (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) .",
  "y": "background"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_2",
  "x": "**RELATED WORK** Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) , or the prediction of the compositionality of MWE types (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) . The identification of non-compositional MWE tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009 ) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or applicable across multiple constructions using features similar to those used in all-words word sense disambiguation (Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) . The prediction of the compositionality of MWE types has traditionally been couched as a binary classification task (compositional or non-compositional: Baldwin et al. (2003) , Bannard (2006) ), but more recent work has moved towards a regression setup, where the degree of the compositionality is predicted on a continuous scale (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) . In either case, the modelling has been done either over the whole MWE (Reddy et al., 2011;<cite> Salehi and Cook, 2013)</cite> , or relative to each component within the MWE (Baldwin et al., 2003; Bannard, 2006) . In this paper, we focus on the binary classification of MWE types relative to each component of the ---------------------------------- **MWE.**",
  "y": "background"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_3",
  "x": "---------------------------------- **MWE.** The work that is perhaps most closely related to this paper is that of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , who use translation data to predict the compositionality of a given MWE relative to each of its components, and then combine those scores to derive an overall compositionality score. In both cases, translations of the MWE and its components are sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014) , and if there is greater similarity between the translated components and MWE in a range of languages, the MWE is predicted to be more compositional. The basis of the similarity calculation is unsupervised, using either string similarity<cite> (Salehi and Cook, 2013)</cite> or distributional similarity (Salehi et al., 2014) . However, the overall method is supervised, as training data is used to select the languages to aggregate scores across for a given MWE construction. To benchmark our method, we use two of the same datasets as these two papers, and repurpose the best-performing methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) for classification of the compositionality of each MWE component. ---------------------------------- **METHODOLOGY**",
  "y": "similarities"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_4",
  "x": "---------------------------------- **MWE.** The work that is perhaps most closely related to this paper is that of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , who use translation data to predict the compositionality of a given MWE relative to each of its components, and then combine those scores to derive an overall compositionality score. In both cases, translations of the MWE and its components are sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014) , and if there is greater similarity between the translated components and MWE in a range of languages, the MWE is predicted to be more compositional. The basis of the similarity calculation is unsupervised, using either string similarity<cite> (Salehi and Cook, 2013)</cite> or distributional similarity (Salehi et al., 2014) . However, the overall method is supervised, as training data is used to select the languages to aggregate scores across for a given MWE construction. To benchmark our method, we use two of the same datasets as these two papers, and repurpose the best-performing methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) for classification of the compositionality of each MWE component. ---------------------------------- **METHODOLOGY**",
  "y": "uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_5",
  "x": "**MWE.** The work that is perhaps most closely related to this paper is that of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , who use translation data to predict the compositionality of a given MWE relative to each of its components, and then combine those scores to derive an overall compositionality score. In both cases, translations of the MWE and its components are sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014) , and if there is greater similarity between the translated components and MWE in a range of languages, the MWE is predicted to be more compositional. The basis of the similarity calculation is unsupervised, using either string similarity<cite> (Salehi and Cook, 2013)</cite> or distributional similarity (Salehi et al., 2014) . However, the overall method is supervised, as training data is used to select the languages to aggregate scores across for a given MWE construction. To benchmark our method, we use two of the same datasets as these two papers, and repurpose the best-performing methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) for classification of the compositionality of each MWE component. ---------------------------------- **METHODOLOGY** Our basic method relies on analysis of lexical overlap between the component words and the definitions of the MWE in Wiktionary, in the manner of Lesk (1986) .",
  "y": "uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_6",
  "x": "A fine clay, rich in kaolinite, used in ceramics, paper-making, etc. This method is compatible with the three definition-based similarity methods described above, and indicated by the +SYN suffix (e.g. FIRSTDEF+SYN is FIRSTDEF with synonymbased expansion). ---------------------------------- **TRANSLATIONS** A third information source in Wiktionary that can be used to predict compositionality is sense-level translation data. Due to the user-generated nature of Wiktionary, the set of languages for which 1 Although the recall of these tags is low (Muzny and Zettlemoyer, 2013 translations are provided varies greatly across lexical entries. Our approach is to take whatever translations happen to exist in Wiktionary for a given MWE, and where there are translations in that language for the component of interest, use the LCSbased method of<cite> Salehi and Cook (2013)</cite> to measure the string similarity between the translation of the MWE and the translation of the components. Unlike<cite> Salehi and Cook (2013)</cite> , however, we do not use development data to select the optimal set of languages in a supervised manner, and instead simply take the average of the string similarity scores across the available languages. In the case of more than one translation in a given language, we use the maximum string similarity for each pairing of MWE and component translation.",
  "y": "uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_7",
  "x": "A third information source in Wiktionary that can be used to predict compositionality is sense-level translation data. Due to the user-generated nature of Wiktionary, the set of languages for which 1 Although the recall of these tags is low (Muzny and Zettlemoyer, 2013 translations are provided varies greatly across lexical entries. Our approach is to take whatever translations happen to exist in Wiktionary for a given MWE, and where there are translations in that language for the component of interest, use the LCSbased method of<cite> Salehi and Cook (2013)</cite> to measure the string similarity between the translation of the MWE and the translation of the components. Unlike<cite> Salehi and Cook (2013)</cite> , however, we do not use development data to select the optimal set of languages in a supervised manner, and instead simply take the average of the string similarity scores across the available languages. In the case of more than one translation in a given language, we use the maximum string similarity for each pairing of MWE and component translation. Unlike the definition and synonym-based approach, the translation-based approach will produce real rather than binary values. To combine the two approaches, we discretise the scores given by the translation approach. In the case of disagreement between the two approaches, we label the given MWE as non-compositional. This results in higher recall and lower precision for the task of detecting compositionality.",
  "y": "differences"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_8",
  "x": "In the case of MWE compositionality analysis, our primary concern is lexical coverage in Wiktionary, i.e., what proportion of a representative set of MWEs is contained in Wiktionary. We measure lexical coverage relative to the two datasets used in this research (described in detail in Section 4), namely 90 English noun compounds (ENCs) and 160 English verb particle constructions (EVPCs). In each case, we calculated the proportion of the dataset that is found in Wiktionary, Wiktionary+Wikipedia (where we back off to a Wikipedia document in the case that a MWE is not found in Wiktionary) and WordNet (Fellbaum, 1998) . The results are found in Table 1 , and indicate perfect coverage in Wiktionary+Wikipedia for the ENCs, and very high coverage for the EVPCs. In both cases, the coverage of WordNet is substantially lower, although still respectable, at around 90%. ---------------------------------- **DATASETS** As mentioned above, we evaluate our method over the same two datasets as<cite> Salehi and Cook (2013)</cite> (which were later used, in addition to a third dataset of German noun compounds, in Salehi et al. (2014) ): (1) 90 binary English noun compounds (ENCs, e.g. spelling bee or swimming pool); and (2) 160 English verb particle constructions (EVPCs, e.g. stand up and give away). Our results are not directly comparable with those of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , however, who evaluated in terms of a regression task, modelling the overall compositionality of the MWE.",
  "y": "similarities"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_9",
  "x": "As mentioned above, we evaluate our method over the same two datasets as<cite> Salehi and Cook (2013)</cite> (which were later used, in addition to a third dataset of German noun compounds, in Salehi et al. (2014) ): (1) 90 binary English noun compounds (ENCs, e.g. spelling bee or swimming pool); and (2) 160 English verb particle constructions (EVPCs, e.g. stand up and give away). Our results are not directly comparable with those of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , however, who evaluated in terms of a regression task, modelling the overall compositionality of the MWE. In our case, the task setup is a binary classification task relative to each of the two components of the MWE. The ENC dataset was originally constructed by Reddy et al. (2011) , and annotated on a continuous [0, 5] scale for both overall compositionality and the component-wise compositionality of each of the modifier and head noun. The sampling was random in an attempt to make the dataset balanced, with 48% of compositional English noun compounds, of which 51% are compositional in the first component and 60% are compositional in the second component. We generate discrete labels by discretising the component-wise compositionality scores based on the partitions [0, 2.5] and (2.5, 5]. On average, each NC in this dataset has 1.4 senses (definitions) in Wiktionary. The EVPC dataset was constructed by Bannard (2006) , and manually annotated for compositionality on a binary scale for each of the head verb and particle. For the 160 EVPCs, 76% are verb-compositional and 48% are particlecompositional.",
  "y": "similarities differences"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_10",
  "x": "The EVPC dataset was constructed by Bannard (2006) , and manually annotated for compositionality on a binary scale for each of the head verb and particle. For the 160 EVPCs, 76% are verb-compositional and 48% are particlecompositional. On average, each EVPC in this dataset has 3.0 senses (definitions) in Wiktionary. ---------------------------------- **EXPERIMENTS** The baseline for each dataset takes the form of looking for a user-annotated idiom tag in the Wiktionary lexical entry for the MWE: if there is an idiomatic tag, both components are considered to be non-compositional; otherwise, both components are considered to be compositional. We expect this method to suffer from low precision for two reasons: first, the guidelines given to the annotators of our datasets might be different from what Wiktionary contributors assume to be an idiom. Second, the baseline method assumes that for any non-compositional MWE, all components must be equally non-compositional, despite the wealth of MWEs where one or more components are compositional (e.g. from the Wiktionary guidelines for idiom inclusion, 3 computer chess, basketball player, telephone box). We also compare our method with: (1) \"LCS\", the string similarity-based method of<cite> Salehi and Cook (2013)</cite> , in which 54 languages are used; (2) \"DS\", the monolingual distributional similarity method of Salehi et al. (2014) ; (3) \"DS+DSL2\", the multilingual distributional similarity method of Salehi et al. (2014) , including supervised language selection for a given dataset, based on crossvalidation; and (4) \"LCS+DS+DSL2\", whereby the first three methods are combined using a supervised support vector regression model.",
  "y": "uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_11",
  "x": "Tables 2 and 3 provide the results when our proposed method for detecting non-compositionality is applied to the ENC and EVPC datasets, respectively. The inclusion of translation data was found to improve all of precision, recall and F-score across the board for all of the proposed methods. For reasons of space, results without translation data are therefore omitted from the paper. Overall, the simple unsupervised methods proposed in this paper are comparable with the unsupervised and supervised state-of-the-art methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , with ITAG achieving the highest F-score for the ENC dataset and for the verb components of the EVPC dataset. The inclusion of synonyms boosts results in most cases. When we combine each of our proposed methods with the string and distributional similarity methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , we see substantial improvements over the comparable combined method of \"LCS+DS+DSL2\" in most cases, demonstrating both the robustness of the proposed methods and their complementarity with the earlier methods. It is important to reinforce that the proposed methods make no language-specific assumptions and are therefore applicable to any type of MWE and any language, with the only requirement being that the MWE of interest be listed in the Wiktionary for ---------------------------------- **ERROR ANALYSIS**",
  "y": "uses differences"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_12",
  "x": "The inclusion of translation data was found to improve all of precision, recall and F-score across the board for all of the proposed methods. For reasons of space, results without translation data are therefore omitted from the paper. Overall, the simple unsupervised methods proposed in this paper are comparable with the unsupervised and supervised state-of-the-art methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , with ITAG achieving the highest F-score for the ENC dataset and for the verb components of the EVPC dataset. The inclusion of synonyms boosts results in most cases. When we combine each of our proposed methods with the string and distributional similarity methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , we see substantial improvements over the comparable combined method of \"LCS+DS+DSL2\" in most cases, demonstrating both the robustness of the proposed methods and their complementarity with the earlier methods. It is important to reinforce that the proposed methods make no language-specific assumptions and are therefore applicable to any type of MWE and any language, with the only requirement being that the MWE of interest be listed in the Wiktionary for ---------------------------------- **ERROR ANALYSIS** We analysed all items in each dataset where the system score differed from that of the human annotators.",
  "y": "uses"
 },
 {
  "id": "a229630a81020951ec0be27f54885a_0",
  "x": "3 Cost-sensitive UCE classification According to the problem of UCE filtering, a cost-sensitive classification is required. Each learning algorithm can be biased to prefer some kind of missclassification errors to others. A popular technique for doing this is resampling the training collection by multiplying the number of instances of the preferred class by the cost ratio. Also, the unpreferred class can be downsampled by eliminating some instances. The software package we use for our experiments applies both methods depending on the algorithm tested. We have tested four learning algorithms: Naive Bayes (NB), C4.5, PART and k-nearest neighbor (kNN), all implemented in the Weka package (Witten and Frank, 1999) . The version of Weka used in this work is Weka 3.0.1. The algorithms used can be biased to prefer the mistake of classify a UCE message as not UCE to the opposite, assigning a penalty to the second kind of errors. Following<cite> (Androutsopoulos et al., 2000)</cite> , we have assigned 9 and 999 (9 and 999 times more important) penalties to the missclassification of legitimate messages as UCE.",
  "y": "uses"
 },
 {
  "id": "a229630a81020951ec0be27f54885a_1",
  "x": "**EVALUATION AND RESULTS** The experiments results are summarized in the Table 1 , 2 and 3. The learning algorithms Naive Bayes (NB), 5-Nearest Neighbor (5NN), C4.5 and PART were tested on words (-W), heuristic features (-H), and both (-WH). The kNN algorithm was tested with values of k equal to 1, 2, 5 and 8, being 5 the optimal number of neighbors. We present the weighted accuracy (wacc), and also the recall (rec) and precision (pre) for the class UCE. Weighted accuracy is a measure that weights higher the hits and misses for the preferred class. Recall and precision for the UCE class show how effective the filter is blocking UCE, and what is its effectiveness letting legitimate messages pass the filter, respectively<cite> (Androutsopoulos et al., 2000)</cite> . In Table 1 , no costs were used. Tables 2 and  3 show the results of our experiments for cost ratios of 9 and 999.",
  "y": "background"
 },
 {
  "id": "a229630a81020951ec0be27f54885a_2",
  "x": "However, for the cost value of 999, both algorithms degrade to the trivial rejector: they prefer to classify every message as legitimate in order to avoid highly penalized errors. With these results, neither of these algorithms seems useful for autonomous classification of UCE as stated by Androutsopoulos, since this cost ratio represents a scenario in which UCE messages are deleted without notifying the user of the UCE filter. Nevertheless, PART-WH shows competitive performance for a cost ratio of 9. Its numbers are comparable to those shown in a commercial study by the top performing Brightmail filtering system (Mariano, 2000) , which reaches a UCE recall of 0.73, and a precision close to 1.0, and it is manually updated. Naive Bayes has not shown high variability with respect to costs. This is probably due to the sampling method, which only slightly affects to the estimation of probabilities (done by approximation to a normal distribution). In (Sahami et al., 1998; <cite>Androutsopoulos et al., 2000)</cite> , the method followed is the variation of the probability threshold, which leads to a high variation of results. In future experiments, we plan to apply the uniform method MetaCost (Domingos, 1999) to the algorithms tested in this work, for getting more comparable results. With respect to the use of heuristics, we can see that this information alone is not competitive, but it can improve classification based on words.",
  "y": "background"
 },
 {
  "id": "a229630a81020951ec0be27f54885a_3",
  "x": "Naive Bayes has not shown high variability with respect to costs. This is probably due to the sampling method, which only slightly affects to the estimation of probabilities (done by approximation to a normal distribution). In (Sahami et al., 1998; <cite>Androutsopoulos et al., 2000)</cite> , the method followed is the variation of the probability threshold, which leads to a high variation of results. In future experiments, we plan to apply the uniform method MetaCost (Domingos, 1999) to the algorithms tested in this work, for getting more comparable results. With respect to the use of heuristics, we can see that this information alone is not competitive, but it can improve classification based on words. The improvement shown in our experiments is modest, due to the heuristics used. We are not able to add other heuristics in this case because the Spambase collection comes in a preprocessed fashion. For future experiments, we will use the collection from<cite> (Androutsopoulos et al., 2000)</cite> , which is in raw form. This fact will enable us to search for more powerful heuristics.",
  "y": "future_work"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_0",
  "x": "First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated. ---------------------------------- **INTRODUCTION** Distributed representations of words in the form of word embeddings Pennington et al., 2014) and contextualized word embeddings (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2018; McCann et al., 2017; Radford et al., 2019) have led to huge performance improvement on many NLP tasks. However, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human-produced data<cite> (Bolukbasi et al., 2016</cite>; Caliskan et al., 2017) . In this work, we extend these analyses to the ELMo contextualized word embeddings. Our work provides a new intrinsic analysis of how ELMo represents gender in biased ways.",
  "y": "motivation background"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_1",
  "x": "For word representations,<cite> Bolukbasi et al. (2016)</cite> and Caliskan et al. (2017) show that word embeddings encode societal biases about gender roles and occupations, e.g. engineers are stereotypically men, and nurses are stereotypically women. As a consequence, downstream applications that use these pretrained word embeddings also reflect this bias. For example, Zhao et al. (2018a) and Rudinger et al. (2018) show that coreference resolution systems relying on word embeddings encode such occupational stereotypes. In concurrent work, May et al. (2019) measure gender bias in sentence embeddings, but their evaluation is on the aggregation of word representations. In contrast, we analyze bias in contextualized word representations and its effect on a downstream task. To mitigate bias from word embeddings,<cite> Bolukbasi et al. (2016)</cite> propose a post-processing method to project out the bias subspace from the pre-trained embeddings. Their method is shown to reduce the gender information from the embeddings of gender-neutral words, and, remarkably, maintains the same level of performance on different downstream NLP tasks. Zhao et al. (2018b) further propose a training mechanism to separate gender information from other factors. However, Gonen and Goldberg (2019) argue that entirely removing bias is difficult, if not impossible, and the gender bias information can be often recovered.",
  "y": "motivation background"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_2",
  "x": "For example, Zhao et al. (2018a) and Rudinger et al. (2018) show that coreference resolution systems relying on word embeddings encode such occupational stereotypes. In concurrent work, May et al. (2019) measure gender bias in sentence embeddings, but their evaluation is on the aggregation of word representations. In contrast, we analyze bias in contextualized word representations and its effect on a downstream task. To mitigate bias from word embeddings,<cite> Bolukbasi et al. (2016)</cite> propose a post-processing method to project out the bias subspace from the pre-trained embeddings. Their method is shown to reduce the gender information from the embeddings of gender-neutral words, and, remarkably, maintains the same level of performance on different downstream NLP tasks. Zhao et al. (2018b) further propose a training mechanism to separate gender information from other factors. However, Gonen and Goldberg (2019) argue that entirely removing bias is difficult, if not impossible, and the gender bias information can be often recovered. This paper investigates a natural follow-up question: What are effective bias mitigation techniques for contextualized embeddings? ----------------------------------",
  "y": "background"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_3",
  "x": "**GEOMETRY OF GENDER** Next, we analyze the gender subspace in ELMo. We first sample 400 sentences with at least one gendered word (e.g., he or she from the OntoNotes 5.0 dataset (Weischedel et al., 2012) and generate the corresponding gender-swapped variants (changing he to she and vice-versa). We then calculate the difference of ELMo embeddings between occupation words in corresponding sentences and conduct principal component analysis for all pairs of sentences. Figure 1 shows there are two principal components for gender in ELMo, in contrast to GloVe which only has one<cite> (Bolukbasi et al., 2016)</cite> . The two principal components in ELMo seem to represent the gender from the contextual information (Contextual Gender) as well as the gender embedded in the word itself (Occupational Gender). To visualize the gender subspace, we pick a few sentence pairs from WinoBias (Zhao et al., 2018a) . Each sentence in the corpus contains one gendered pronoun and two occupation words, such as \"The developer corrected the secretary because she made a mistake\" and also the same sentence with the opposite pronoun (he). In Figure 1 on the right, we project the ELMo embeddings of occupation words that are co-referent with the pronoun (e.g. secretary in the above example) for when the pronoun is male (blue dots) and female (orange dots) on the two principal components from the PCA analysis.",
  "y": "differences"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_4",
  "x": "Previous work (Zhao et al., 2018a) evaluated the systems based on GloVe embeddings but here we evaluate a state-of-the-art system that trained on the OntoNotes corpus with ELMo embeddings . ---------------------------------- **BIAS MITIGATION METHODS** Next, we describe two methods for mitigating bias in ELMo for the purpose of coreference resolution: (1) a train-time data augmentation approach and (2) a test-time neutralization approach. Zhao et al. (2018a) propose a method to reduce gender bias in coreference resolution by augmenting the training corpus for this task. Data augmentation is performed by replacing gender revealing entities in the OntoNotes dataset with words indicating the opposite gender and then training on the union of the original data and this swapped data. In addition, they find it useful to also mitigate bias in supporting resources and therefore replace standard GloVe embeddings with bias mitigated word embeddings from<cite> Bolukbasi et al. (2016)</cite> . We evaluate the performance of both aspects of this approach.",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_0",
  "x": "These tools are vital for the performance of machine learning (ML) approaches to Arabic SA: traditionally, ML approaches use a \"bag of words\" (BOW) model (e.g. Wilson et al. (2009) ). However, for morphologically rich languages, such as Arabic, a mixture of stemmed tokens and morphological features have shown to outperform BOW approaches (Abdul-Mageed et al., 2011; Mourad and Darwish, 2013) , accounting for the fact that Arabic contains a very large number of inflected words. In addition (or maybe as a result), there is much less interest from the research community in tackling the challenge of Arabic SA for social media. As such, there are much fewer open resources available, such as annotated data sets or sentiment lexica. We therefore explore an alternative approach to Arabic SA on social media, using off-the-shelf Machine Translation systems to translate Arabic tweets into English and then use a state-of-the-art sentiment classifier <cite>(Socher et al., 2013)</cite> to assign sentiment labels. To the best of our knowledge, this is the first study to measure the impact of automatically translated data on the accuracy of sentiment analysis of Arabic tweets. In particular, we address the following research questions: 1. How does off-the-shelf MT on Arabic social data influence SA performance? 2. Can MT-based approaches be a viable alternative to improve sentiment classification performance on Arabic tweets? 3.",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_1",
  "x": "The authors report an accuracy score of 64.75% on the English held-out test set. For the other languages, reported accuracy scores ranged between 60 -62%. Hence, they conclude that it is possible to obtain high quality training data using MT, which is an encouraging result to motivate our approach. Wan (2009) proposes a co-training approach to tackle the lack of Chinese sentiment corpora by employing Google Translate as publicly available machine translation (MT) service to translate a set of annotated English reviews into Chinese. Using a held-out test set, the best reported accuracy score was at 81.3% with SVM on binary classification task: positive vs negative. Our approach differs from the ones described, in that we use automatic MT to translate Arabic tweets into English and then perform SA using a stateof-the-art SA classifier for English <cite>(Socher et al., 2013)</cite> . Most importantly, we empirically benchmark its performance towards previous SA approaches, including lexicon-based, fully supervised and distant supervision SA. tweets from the Twitter public stream. We restrict the language of all retrieved tweets to Arabic by setting the language parameter to ar.",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_2",
  "x": "In order to obtain the English translation of our Twitter data-set, we employ two common and freelyavailable MT systems: Google Translate and Microsoft Translator Service. We then use the Stanford Sentiment Classifier (SSC) developed by<cite> Socher et al. (2013)</cite> to automatically assign sentiment labels (positive, negative) to translated tweets. The classifier is based on a deep learning (DL) approach, using recursive neural models to capture syntactic dependencies and compositionality of sentiments. Socher et al. (2013) show that this model significantly outperforms previous standard models, such as Na\u00efve Bayes (NB) and Support Vector Machines (SVM) with an accuracy score of 85.4% for binary classification (positive vs. negative) at sentence level 2 . The authors observe that the recursive models work well on shorter text while BOW features with NB and SVM perform well only on longer sentences. Using<cite> Socher et al. (2013)</cite> 's approach for directly training a sentiment classifier will require a larger training data-set, which is not available yet for Ara-bic 3 . ---------------------------------- **BASELINE SYSTEMS** We benchmark the MT-approach against three baseline systems representing current standard approaches to SA: a lexicon-based approach, a fully supervised machine learning approach and a distant supervision approach (also see Section 2).",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_3",
  "x": "The data set will be released as part of this submission. ---------------------------------- **MT-BASED APPROACH** In order to obtain the English translation of our Twitter data-set, we employ two common and freelyavailable MT systems: Google Translate and Microsoft Translator Service. We then use the Stanford Sentiment Classifier (SSC) developed by<cite> Socher et al. (2013)</cite> to automatically assign sentiment labels (positive, negative) to translated tweets. The classifier is based on a deep learning (DL) approach, using recursive neural models to capture syntactic dependencies and compositionality of sentiments. Socher et al. (2013) show that this model significantly outperforms previous standard models, such as Na\u00efve Bayes (NB) and Support Vector Machines (SVM) with an accuracy score of 85.4% for binary classification (positive vs. negative) at sentence level 2 . The authors observe that the recursive models work well on shorter text while BOW features with NB and SVM perform well only on longer sentences. Using<cite> Socher et al. (2013)</cite> 's approach for directly training a sentiment classifier will require a larger training data-set, which is not available yet for Ara-bic 3 .",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_4",
  "x": "The reverse it true for the DS and fully supervised baselines, which find it hard to identify negative tweets. This is in line with results reported by Refaee and Rieser (2014b) which evaluate DS approaches to Arabic SA. Only the lexiconapproach is balanced between the positive and negative class. Note that our ML baseline systems as well as the English SA classifier by<cite> Socher et al. (2013)</cite> are trained on balanced data sets, i.e. we can assume no prior bias towards one class. ---------------------------------- **PLANNED CONTRASTS** ---------------------------------- **ERROR ANALYSIS** The above results highlight the potential of an MTbased approach to SA for languages that lack a large Table 4 : Examples of misclassified tweets training data-set annotated for sentiment analysis, such as Arabic.",
  "y": "similarities"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_5",
  "x": "5. Example 5 represents a case of correctly translated sentiment-bearing words (love, life), but failed to translate surrounding text ('Ashan' and 'Amtlat'). Bautin et al. (2008) point out that this type of contextual information loss is one of the main challenges of MT-based SA. 6. Example 6 represents a case of a correctly translated tweet, but with an incorrectly assigned sentiment label. We assume that this is due to changes in sentence structure introduced by the MT system. Balahur and Turchi (2013) state that word ordering is one of the most prominent causes of SA misclassification. In order to confirm this hypothesis, we manually corrected sentence structure before feeding it into the SA classifier. This approach led to the correct SA label, and thus, confirmed that the cause of the problem is word-ordering. Note that the Stanford SA system pays particular attention to sentence structure due to its \"deep\" architecture that adds to the model the feature of being sensitive to word ordering <cite>(Socher et al., 2013)</cite> . In future work, we will verify this by comparing these results to other high performing English SA tools (see for example Abbasi et al. (2014) In sum, one of the major challenges of this approach seems to be the use of Arabic dialects in social media, such as Twitter.",
  "y": "future_work"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_6",
  "x": "This is especially true for tweets as they tend to be less formal resulting in issues like misspelling and individual spelling variations. However, with more resources being released for informal Arabic and Arabic dialects, e.g. (Cotterell and Callison-Burch, 2014; Refaee and Rieser, 2014a) , we assume that off-the-shelf MT systems will improve their performance in the near future. ---------------------------------- **CONCLUSION** This paper is the first to investigate and empirically evaluate the performance of Machine Translation (MT)-based Sentiment Analysis (SA) for Arabic Tweets. In particular, we make use of off-theshelf MT tools, such as Google and Microsoft MT, to translate Arabic Tweets into English. We then use the Stanford Sentiment Classifier <cite>(Socher et al., 2013)</cite> to automatically assign sentiment labels (positive, negative) to translated tweets. In contrast to previous work, we benchmark this approach on a gold-standard test set of 937 manually annotated tweets and compare its performance to standard SA approaches, including lexicon-based, supervised and distant supervision approaches. We find that MT approaches reach a comparable performance or significantly outperform more resourceintense standard approaches.",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_0",
  "x": "On the other hand, <cite>Noraset et al. (2017)</cite> attempted to generate a definition of a word from its word embedding induced from massive text, followed by Gadetsky et al. (2018) that refers to a local context to define a polysemous word with a local context by choosing relevant dimensions of their embeddings. Although these research efforts revealed that both local and global contexts of words are useful in generating their definitions, none of these studies exploited both local and global contexts directly. In this study, we tackle a task of describing (defining) a phrase when given its local context as (Ni and Wang, 2017) , while allowing access to other usage examples via word embeddings trained from massive text (global contexts) (Noraset et al., 2017; Gadetsky et al., 2018) . We present LOG-Cad, a neural network-based description generator (Figure 1 ) to directly solve this task. Given a word with its context, our generator takes advantage of the target word's embedding, pre-trained from massive text (global contexts), while also encoding the given local context, combining both to generate a natural language description. The local and global contexts complement one another and are both essential; global contexts are crucial when local contexts are short and vague, while the local context is crucial when the target phrase is polysemous, rare, or unseen. Considering various contexts where we need definitions of phrases, we evaluated our method with four datasets including WordNet<cite> (Noraset et al., 2017)</cite> for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slangs, and a newlycreated Wikipedia dataset for entities. Experimental results demonstrate the effectiveness of our method against the three baselines stated above (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) . Our contributions are as follows:",
  "y": "background"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_1",
  "x": "Paraphrasing can suggest other ways of describing a word while keeping its meaning, but those paraphrases are generally context-insensitive and may not be sufficient for understanding. To address this problem, Ni and Wang (2017) has proposed a task of describing a phrase in a given context. However, they follow the strict assumption that the target phrase is unknown and there is only a single local context available for the phrase, which makes the task of generating an accurate and coherent definition difficult (perhaps as difficult as a human comprehending the phrase itself). On the other hand, <cite>Noraset et al. (2017)</cite> attempted to generate a definition of a word from its word embedding induced from massive text, followed by Gadetsky et al. (2018) that refers to a local context to define a polysemous word with a local context by choosing relevant dimensions of their embeddings. Although these research efforts revealed that both local and global contexts of words are useful in generating their definitions, none of these studies exploited both local and global contexts directly. In this study, we tackle a task of describing (defining) a phrase when given its local context as (Ni and Wang, 2017) , while allowing access to other usage examples via word embeddings trained from massive text (global contexts) (Noraset et al., 2017; Gadetsky et al., 2018) . We present LOG-Cad, a neural network-based description generator (Figure 1 ) to directly solve this task. Given a word with its context, our generator takes advantage of the target word's embedding, pre-trained from massive text (global contexts), while also encoding the given local context, combining both to generate a natural language description. The local and global contexts complement one another and are both essential; global contexts are crucial when local contexts are short and vague, while the local context is crucial when the target phrase is polysemous, rare, or unseen.",
  "y": "motivation"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_2",
  "x": "In this study, we tackle a task of describing (defining) a phrase when given its local context as (Ni and Wang, 2017) , while allowing access to other usage examples via word embeddings trained from massive text (global contexts) (Noraset et al., 2017; Gadetsky et al., 2018) . We present LOG-Cad, a neural network-based description generator (Figure 1 ) to directly solve this task. Given a word with its context, our generator takes advantage of the target word's embedding, pre-trained from massive text (global contexts), while also encoding the given local context, combining both to generate a natural language description. The local and global contexts complement one another and are both essential; global contexts are crucial when local contexts are short and vague, while the local context is crucial when the target phrase is polysemous, rare, or unseen. Considering various contexts where we need definitions of phrases, we evaluated our method with four datasets including WordNet<cite> (Noraset et al., 2017)</cite> for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slangs, and a newlycreated Wikipedia dataset for entities. Experimental results demonstrate the effectiveness of our method against the three baselines stated above (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) . Our contributions are as follows: \u2022 We set up a general task of defining phrases given their contexts. This task is a generalization of three related tasks (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) and involves various situations where we need definitions of phrases.",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_3",
  "x": "In this study, we tackle a task of describing (defining) a phrase when given its local context as (Ni and Wang, 2017) , while allowing access to other usage examples via word embeddings trained from massive text (global contexts) (Noraset et al., 2017; Gadetsky et al., 2018) . We present LOG-Cad, a neural network-based description generator (Figure 1 ) to directly solve this task. Given a word with its context, our generator takes advantage of the target word's embedding, pre-trained from massive text (global contexts), while also encoding the given local context, combining both to generate a natural language description. The local and global contexts complement one another and are both essential; global contexts are crucial when local contexts are short and vague, while the local context is crucial when the target phrase is polysemous, rare, or unseen. Considering various contexts where we need definitions of phrases, we evaluated our method with four datasets including WordNet<cite> (Noraset et al., 2017)</cite> for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slangs, and a newlycreated Wikipedia dataset for entities. Experimental results demonstrate the effectiveness of our method against the three baselines stated above (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) . Our contributions are as follows: \u2022 We set up a general task of defining phrases given their contexts. This task is a generalization of three related tasks (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) and involves various situations where we need definitions of phrases.",
  "y": "differences"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_4",
  "x": "Local & Global Contexts for Description Generation In this paper, we refer to the explicit contextual information included in a single sentence as \"local context,\" and the implicit contextual information in the word/phrase embedding trained in an unsupervised manner on largescale corpora as \"global context. \" Previous work on the definition generation task<cite> (Noraset et al., 2017)</cite> has shown that global contexts can be useful clues when generating definitions of unknown words. The intuition behind their method is that words with similar meanings tend to have similar definitions in a dictionary. This can be seen as an extension of the Distributional Hypothesis (Harris, 1954; Firth, 1957) , which states words that share semantic meanings tend to appear in similar contexts. Additionally, work on the WSD task (Navigli, 2009) , novel sense detection (Erk, 2006; Lau et al., 2014) , and the non-standard word explanation task (Ni and Wang, 2017) have revealed that local contexts surrounding the word can help disambiguate its sense. Based on these studies, we propose to incorporate both local and global contexts to describe an unknown expression. Model Figure 1 shows an illustration of our LOG-CaD model. Similarly to the standard encoder-decoder model with attention (Bahdanau et al., 2015; Luong and Manning, 2016) , it consists of two modules: a context encoder and a description decoder. The challenge here is that the decoder needs to be conditioned not only on the local context, but also on its global context.",
  "y": "background"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_5",
  "x": "Model Figure 1 shows an illustration of our LOG-CaD model. Similarly to the standard encoder-decoder model with attention (Bahdanau et al., 2015; Luong and Manning, 2016) , it consists of two modules: a context encoder and a description decoder. The challenge here is that the decoder needs to be conditioned not only on the local context, but also on its global context. To incorporate the different types of contexts, we propose to use a GATE function<cite> (Noraset et al., 2017)</cite> to dynamically control how the global and local contexts influence the generation of the description. We use bi-directional and uni-directional LSTMs (Hochreiter and Schmidhuber, 1997) as our context encoder and description decoder (Figure 1 ), respectively. Given a sentence X and a phrase X trg , the context encoder generates a sequence of continuous vectors where x i denotes the word embedding of word x i . Then, the description decoder computes the conditional probability of a description Y with Eq. (1), which can be approximated with another LSTM as where s t is a hidden state of the decoder LSTM, and y t\u22121 is a jointly-trained word embedding of the previous output word y t\u22121 .",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_6",
  "x": "Considering the fact that the local context can be relatively long (e.g. around 20 words on average in the Wikipedia dataset that will be introduced in the next section) it is hard for a decoder to focus on important words in local contexts. In order to deal with this problem, ATTENTION(\u00b7) function in Eq. (4) decides which words in the local context X to focus on at each time step. d t can be computed with an attention mechanism (Luong and Manning, 2016) as where U h and U s are matrices that map the encoder and decoder hidden states into a common space, respectively. In order to capture prefixes and suffixes in X trg , we construct character-level CNNs (Eq. (5)) following<cite> (Noraset et al., 2017)</cite> . Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following <cite>Noraset et al. (2017)</cite> , we set the kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg . In addition to the local context and the character-information, we also utilize the global context obtained from massive text. We achieve this by two different strategies proposed by <cite>Noraset et al. (2017)</cite> . First, we feed phrase embedding x trg to initialize the decoder as",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_7",
  "x": "In order to capture prefixes and suffixes in X trg , we construct character-level CNNs (Eq. (5)) following<cite> (Noraset et al., 2017)</cite> . Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following <cite>Noraset et al. (2017)</cite> , we set the kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg . In addition to the local context and the character-information, we also utilize the global context obtained from massive text. We achieve this by two different strategies proposed by <cite>Noraset et al. (2017)</cite> . First, we feed phrase embedding x trg to initialize the decoder as Here, phrase embedding x trg is calculated by simply summing up all the embeddings of words that consistute the phrase X trg . Note that we use a random-initialized vector if no pre-trained embedding is available for the words in X trg . As described in the previous section, we use both local and global contexts. In order to capture the interaction between two contexts and the description decoder, we adopt a GATE(\u00b7) function (Eq. (6) Table 2 : Domains, expressions to be described, and the coverage of pre-trained word embeddings of the four datasets.",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_8",
  "x": "6 For all of these datasets, a given word or phrase has an inventory of senses with corresponding definitions and usage examples. These definitions are regarded as ground-truth descriptions. Datasets To evaluate our model on the word description task on WordNet, we followed <cite>Noraset et al. (2017)</cite> and extracted data from WordNet 7 using the dict-definition 8 toolkit. Each entry in the data consists of three elements: (1) a word, (2) its definition, and (3) a usage example of the word. We split this dataset to obtain Train, Validation, and Test sets. If a word has multiple definitions/examples, we treat them as different entries. Note that the words are mutually exclusive across the three sets. The only difference between our dataset and theirs is that we extract the tuples only if the words have their usage examples in WordNet. Since not all entries in WordNet have usage examples, our dataset is a small subset of (Noraset et al., 2017 ) (see Table 1 ).",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_9",
  "x": "7 https://wordnet.princeton.edu/ 8 https://github.com/NorThanapon/dict-definition 9 GoogleNews-vectors-negative300.bin.gz at https://code.google.com/archive/p/word2vec/ Table 3 : Hyperparameters of the models 2017). If the expression to be described consists of multiple words, its phrase embedding is calculated by simply summing up all the CBOW vectors of words in the phrase, such as \"sonic\" and \"boom.\" (See Figure 1) . If pre-trained CBOW embeddings are unavailable, we instead use a special [UNK] vector (which is randomly initialized with a uniform distribution) as word embeddings. Note that our pre-trained embeddings only cover 26.79% of the words in the expressions to be described in our Wikipedia dataset, while it covers all words in WordNet dataset (See Table 2 ). Even if no reliable word embeddings are available, all models can capture the character information through character-level CNNs (See Figure 1) . ---------------------------------- **MODELS** We implemented four methods including three baselines: (1) Global, (2) Local, (3) I-Attention, and our proposed model, (4) LOGCaD. The Global model is our reimplementation of the strongest model (S + G + CH) in<cite> (Noraset et al., 2017)</cite> . It can access the embedding (global context) of the phrase to be described, but has no ability to read the usage examples (local context).",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_10",
  "x": "Our task avoids these difficulties in WSD by directly generating descriptions for phrases or words with their contexts. It also allows us to flexibly tailor a fine-grained definition for the specific context. Paraphrasing (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010) (or text simplification (Siddharthan, 2014) ) can be used to rephrase words with unknown senses. However, the target of paraphrase acquisition are words (or phrases) with no specified context. Although several studies (Connor and Roth, 2007; Max, 2009; Max et al., 2012) consider sub-sentential (context-sensitive) paraphrases, they do not intend to obtain a definition-like description as a paraphrase of a word. Recently, <cite>Noraset et al. (2017)</cite> introduced a task of generating a definition sentence of a word from its pre-trained embedding. Since their task does not take local contexts of words as inputs, their method cannot generate an appropriate definition for a polysemous word for a specific context. To cope with this problem, Gadetsky et al. (2018) have proposed a definition generation method that works with polysemous words in dictionaries. They present a model that utilizes local context to filter out the unrelated meanings from a pre-trained word embedding in a specific context.",
  "y": "background"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_11",
  "x": "Since their task does not take local contexts of words as inputs, their method cannot generate an appropriate definition for a polysemous word for a specific context. To cope with this problem, Gadetsky et al. (2018) have proposed a definition generation method that works with polysemous words in dictionaries. They present a model that utilizes local context to filter out the unrelated meanings from a pre-trained word embedding in a specific context. While their method use local context only for disambiguating the meanings that are mixed up in word embeddings, the information from local contexts cannot be utilized if the pre-trained embeddings are unavailable or unreliable. On the other hand, our method can fully utilize the local context through an attentional mechanism, even if the reliable word embeddings are unavailable. Focusing on non-standard English words (or phrases), Ni and Wang (2017) generated their explanations solely from sentences with those words. Their model does not take advantage of global contexts (word embeddings induced from massive text) as was used in <cite>Noraset et al. (2017)</cite> . Our task of describing phrases with its given context is a generalization of these three tasks (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) , and the proposed method naturally utilizes both local and global contexts of a word in question. ----------------------------------",
  "y": "background"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_12",
  "x": "On the other hand, our method can fully utilize the local context through an attentional mechanism, even if the reliable word embeddings are unavailable. Focusing on non-standard English words (or phrases), Ni and Wang (2017) generated their explanations solely from sentences with those words. Their model does not take advantage of global contexts (word embeddings induced from massive text) as was used in <cite>Noraset et al. (2017)</cite> . Our task of describing phrases with its given context is a generalization of these three tasks (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) , and the proposed method naturally utilizes both local and global contexts of a word in question. ---------------------------------- **CONCLUSIONS** This paper sets up a task of generating a natural language description for a word/phrase with a specific context, aiming to help us acquire unknown word senses when reading text. We approached this task by using a variant of encoder-decoder models that capture the given local context by an encoder and global contexts by the target word's embedding induced from massive text. Experimental results on three existing datasets and one novel dataset built from Wikipedia dataset confirmed that the use of both local and global contexts is the key to generating appropriate contextsensitive description in various situations.",
  "y": "uses"
 },
 {
  "id": "a6296d02f21ca5887c7686a2cbe56c_0",
  "x": "---------------------------------- **INTRODUCTION** Confusion network decoding has been applied in combining outputs from multiple machine translation systems. The earliest approach in (Bangalore et al., 2001 ) used edit distance based multiple string alignment (MSA) (Durbin et al., 1988) to build the confusion networks. The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts <cite>(Rosti et al., 2007)</cite> . The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment. The confusion networks are built around a \"skeleton\" hypothesis. The skeleton hypothesis defines the word order of the decoding output. Usually, the 1-best hypotheses from each system are considered as possible skeletons.",
  "y": "background"
 },
 {
  "id": "a6296d02f21ca5887c7686a2cbe56c_1",
  "x": "The earliest approach in (Bangalore et al., 2001 ) used edit distance based multiple string alignment (MSA) (Durbin et al., 1988) to build the confusion networks. The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts <cite>(Rosti et al., 2007)</cite> . The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment. The confusion networks are built around a \"skeleton\" hypothesis. The skeleton hypothesis defines the word order of the decoding output. Usually, the 1-best hypotheses from each system are considered as possible skeletons. Using the pair-wise hypothesis alignment, the confusion networks are built in two steps. First, all hypotheses are aligned against the skeleton independently. Second, the confusion networks are created from the union of these alignments.",
  "y": "extends"
 },
 {
  "id": "a6296d02f21ca5887c7686a2cbe56c_2",
  "x": "The skeleton hypothesis defines the word order of the decoding output. Usually, the 1-best hypotheses from each system are considered as possible skeletons. Using the pair-wise hypothesis alignment, the confusion networks are built in two steps. First, all hypotheses are aligned against the skeleton independently. Second, the confusion networks are created from the union of these alignments. The incremental hypothesis alignment algorithm combines these two steps. All words from the previously aligned hypotheses are available, even if not present in the skeleton hypothesis, when aligning the following hypotheses. As in <cite>(Rosti et al., 2007)</cite> , confusion networks built around all skeletons are joined into a lattice which is expanded and rescored with language models. System weights and language model weights are tuned to optimize the quality of the decoding output on a development set.",
  "y": "similarities uses"
 },
 {
  "id": "a6296d02f21ca5887c7686a2cbe56c_3",
  "x": "and the confidences for other systems are set to zeros. Each deletion will generate a new NULL word arc unless one exists at the corresponding position in the network. The NULL word arc confidences are adjusted as in the case of a match or a substitution depending on whether the NULL word arc exists or not. Finally, each insertion will generate a new node and two word arcs at the corresponding position in the network. The first word arc will have the inserted word with the confidence set as in the case of a substitution and the second word arc will have a NULL word with confidences set by assuming all previously aligned hypotheses and the skeleton generated the NULL word arc. After all hypotheses have been added into the confusion network, the system specific word arc confidences are scaled to sum to one over all arcs between 1 2 3 4 5 6 I (3) like (3) kites (1) NULL (2) NULL (1) big (1) blue (2) balloons (2) Figure 2: Network after incremental TER alignment. each set of two consecutive nodes. Other scores for the word arc are set as in <cite>(Rosti et al., 2007)</cite> . ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "a6b450d1113e0e6d3d2813c09d12a8_0",
  "x": "Thus, they are not well suited for estimating SR. Therefore, we apply the Wikipedia category graph as a knowledge source for SR measures. We show that Wikipedia based SR measures yield better correlation with human judgments on SR datasets than GermaNet measures. However, using Wikipedia also leads to a performance drop on SS datasets, as knowledge about classical taxonomic relations is not explicitly modeled. Therefore, we combine GermaNet with Wikipedia, and yield substantial improvements over measures operating on a single knowledge source. ---------------------------------- **DATASETS** Several German datasets for evaluation of SS or SR have been created so far (see Table 1 ). <cite>Gurevych (2005)</cite> conducted experiments with a German translation of an English dataset (Rubenstein and Goodenough, 1965) , but argued that the dataset (Gur65) is too small (it contains only 65 noun pairs), and does not model SR.",
  "y": "background"
 },
 {
  "id": "a6b450d1113e0e6d3d2813c09d12a8_1",
  "x": "**SEMANTIC RELATEDNESS MEASURES** Semantic wordnet based measures Lesk (1986) introduced a measure (Les) based on the number of word overlaps in the textual definitions (or glosses) of two terms, where higher overlap means higher similarity. As GermaNet does not contain glosses, this measure cannot be employed. <cite>Gurevych (2005)</cite> proposed an alternative algorithm (PG) generating surrogate glosses by using a concept's relations within the hierarchy. Following the description in Budanitsky and Hirst (2006) , we further define several measures using the taxonomy structure. PL is the taxonomic path length l(c 1 , c 2 ) between two concepts c1 and c2. LC normalizes the path length with the depth of the taxonomy. Res computes SS as the information content (IC) of the lowest common subsumer (lcs) of two concepts, while JC combines path based and IC features. 1 Lin is derived from information theory.",
  "y": "background"
 },
 {
  "id": "a6b450d1113e0e6d3d2813c09d12a8_2",
  "x": "Best values for each dataset and knowledge source are in bold. We use the P G measure in optimal configuration as reported by <cite>Gurevych (2005)</cite> . For the Les measure, we give the results for considering: (i) only the first paragraph (+First) and (ii) the full text (+Full). For the path length based measures, we give the values for averaging over all category pairs (+Avg), or taking the best SR value computed among the pairs (+Best). For each dataset, we report Pearson's correlation r with human judgments on pairs that are found in both resources (BOTH). Otherwise, the results would not be comparable. We additionally use a subset containing only noun-noun pairs (BOTH NN). This comparison is fairer, because article titles in Wikipedia are usually nouns. Table 2 also gives the inter annotator agreement for each subset.",
  "y": "uses"
 },
 {
  "id": "a6b450d1113e0e6d3d2813c09d12a8_3",
  "x": "Otherwise, the results would not be comparable. We additionally use a subset containing only noun-noun pairs (BOTH NN). This comparison is fairer, because article titles in Wikipedia are usually nouns. Table 2 also gives the inter annotator agreement for each subset. It constitutes an upper bound of a measure's performance. ---------------------------------- **EXPERIMENTS & RESULTS** Our results on Gur65 using GermaNet are very close to those published by <cite>Gurevych (2005)</cite> , ranging from 0.69-0.75. For Gur350, the performance drops to 0.38-0.50, due to the lower upper bound, and because GermaNet does not model SR well.",
  "y": "similarities"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_0",
  "x": "Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways. ---------------------------------- **INTRODUCTION** Neural machine translation (NMT) (Bahdanau et al., 2015; Wu et al., 2016;<cite> Vaswani et al., 2017)</cite> trains an encoder-decoder network on sentence pairs to maximize the likelihood of predicting a target-language sentence given the corresponding source-language sentence, without considering the document context. By ignoring discourse connections between sentences and other valuable contextual information, this simplification potentially degrades the coherence and cohesion of a translated document (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017) . Recent studies (Tiedemann and Scherrer, 2017; Wang et al., 2017; have demonstrated that adding contextual information to the NMT model improves the general translation performance, and more importantly, improves the coherence and cohesion of the translated text (Bawden et al., 2018; Lapshinova-Koltunski and Hardmeier, 2017) . Most of these methods use an additional encoder Wang et al., 2017) to extract contextual information from previous source-side sentences. However, this requires additional parameters and it does not exploit the representations already learned by the NMT encoder. More recently, have shown that a cache-based memory network performs better than the above encoder-based methods.",
  "y": "background"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_1",
  "x": "In addition, we integrate two HANs in the NMT model to account for target and source context. The HAN encoder helps in the disambiguation of source-word representations, while the HAN decoder improves the target-side lexical cohesion and coherence. The integration is done by (i) re-using the hidden representations from both the encoder and decoder of previous sentence translations and (ii) providing input to both the encoder and decoder for the current translation. This integration method enables it to jointly optimize for multiple-sentences. Furthermore, we extend the original HAN with a multi-head attention<cite> (Vaswani et al., 2017)</cite> to capture different types of discourse phenomena. Our main contributions are the following: (i) We propose a HAN framework for translation to capture context and inter-sentence connections in a structured and dynamic manner. (ii) We integrate the HAN in a very competitive NMT ar-chitecture<cite> (Vaswani et al., 2017)</cite> and show significant improvement over two strong baselines on multiple data sets. (iii) We perform an ablation study of the contribution of each HAN configuration, showing that contextual information obtained from source and target sides are complementary. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_2",
  "x": "Furthermore, we extend the original HAN with a multi-head attention<cite> (Vaswani et al., 2017)</cite> to capture different types of discourse phenomena. Our main contributions are the following: (i) We propose a HAN framework for translation to capture context and inter-sentence connections in a structured and dynamic manner. (ii) We integrate the HAN in a very competitive NMT ar-chitecture<cite> (Vaswani et al., 2017)</cite> and show significant improvement over two strong baselines on multiple data sets. (iii) We perform an ablation study of the contribution of each HAN configuration, showing that contextual information obtained from source and target sides are complementary. ---------------------------------- **THE PROPOSED APPROACH** The goal of NMT is to maximize the likelihood of a set of sentences in a target language represented as sequences of words y = (y 1 , ..., y t ) given a set of input sentences in a source language x = (x 1 , ..., x m ) as: so, the translation of a document D is made by translating each of its sentences independently. In this study, we introduce dependencies on the previous sentences from the source and target sides:",
  "y": "uses"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_3",
  "x": "We used the MultiHead attention function proposed by<cite> (Vaswani et al., 2017)</cite> to capture different types of relations among words. It matches the query against each of the hidden representations h j i (used as value and key for the attention). The sentence-level abstraction summarizes the contextual information required at time t in d t as: Figure 1 : Integration of HAN during encoding at time step t,h t is the context-aware hidden state of the word x t . Similar architecture is used during decoding. where f s is a linear transformation, q s is the query for the attention function, FFN is a position-wise feed-forward layer<cite> (Vaswani et al., 2017</cite> ). Each layer is followed by a normalization layer (Lei Ba et al., 2016) . ---------------------------------- **CONTEXT GATING** We use a gate to regulate the information at sentence-level h t and the contextual information at document-level d t .",
  "y": "uses"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_4",
  "x": "In particular, h t is the last hidden state of the word to be encoded, or decoded at time step t, and h j i is the last hidden state of the i-th word of the j-th sentence of the context. The function f w is a linear transformation to obtain the query q w . We used the MultiHead attention function proposed by<cite> (Vaswani et al., 2017)</cite> to capture different types of relations among words. It matches the query against each of the hidden representations h j i (used as value and key for the attention). The sentence-level abstraction summarizes the contextual information required at time t in d t as: Figure 1 : Integration of HAN during encoding at time step t,h t is the context-aware hidden state of the word x t . Similar architecture is used during decoding. where f s is a linear transformation, q s is the query for the attention function, FFN is a position-wise feed-forward layer<cite> (Vaswani et al., 2017</cite> ). Each layer is followed by a normalization layer (Lei Ba et al., 2016) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_5",
  "x": "Table 2 shows the corpus statistics. For evaluation, we use BLEU score (Papineni et al., 2002 ) (multi-blue) on tokenized text, and we measure significance with the paired bootstrap resampling method proposed by Koehn (2004) (implementations by Koehn et al. (2007) ). ---------------------------------- **MODEL CONFIGURATION AND TRAINING** As baselines, we use a NMT transformer, and a context-aware NMT transformer with cache memory which we implemented for comparison following the best model described by , with memory size of 25 words. We used the OpenNMT (Klein et al., 2017) implementation of the transformer network. The configuration is the same as the model called \"base model\" in the original paper<cite> (Vaswani et al., 2017)</cite> . The encoder and decoder are composed of 6 hidden layers each. All hidden states have dimension of 512, dropout of 0.1, and 8 heads for the multi-head attention.",
  "y": "similarities uses"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_6",
  "x": "The configuration is the same as the model called \"base model\" in the original paper<cite> (Vaswani et al., 2017)</cite> . The encoder and decoder are composed of 6 hidden layers each. All hidden states have dimension of 512, dropout of 0.1, and 8 heads for the multi-head attention. The target and source vocabulary size is 30K. The optimization and regularization methods were the same as proposed by<cite> Vaswani et al. (2017)</cite> . Inspired by we trained the models in two stages. First we optimize the parameters for the NMT without the HAN, then we proceed to optimize the parameters of the whole network. We use k = 3 previous sentences, which gave the best performance on the development set. Table 1 shows the BLEU scores for different models.",
  "y": "similarities uses"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_0",
  "x": "Our results show that it is possible to use and train an 8-bit fixed-point value for word embedding without loss of performance in word/phrase similarity and dependency parsing tasks. ---------------------------------- **INTRODUCTION** There is an accumulation of evidence that the use of dense distributional lexical representations, known as word embeddings, often supports better performance on a range of NLP tasks (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; <cite>Mikolov et al., 2013a</cite>; Mikolov et al., 2013b; Levy et al., 2015) . Consequently, word embeddings have been commonly used in the last few years for lexical similarity tasks and as features in multiple, syntactic and semantic, NLP applications. However, keeping embedding vectors for hundreds of thousands of words for repeated use could take its toll both on storing the word vectors on disk and, even more so, on loading them into memory. For example, for 1 million words, loading 200 dimensional vectors takes up to 1.6 GB memory on a 64-bit system. Considering applications that make use of billions of tokens and multiple languages, size issues impose significant limitations on the practical use of word embeddings. This paper presents the question of whether it is possible to significantly reduce the memory needs for the use and training of word embeddings.",
  "y": "background"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_1",
  "x": "For example, for 1 million words, loading 200 dimensional vectors takes up to 1.6 GB memory on a 64-bit system. Considering applications that make use of billions of tokens and multiple languages, size issues impose significant limitations on the practical use of word embeddings. This paper presents the question of whether it is possible to significantly reduce the memory needs for the use and training of word embeddings. Specifically, we ask \"what is the impact of representing each dimension of a dense representation with significantly fewer bits than the standard 64 bits?\" Moreover, we investigate the possibility of directly training dense embedding vectors using significantly fewer bits than typically used. The results we present are quite surprising. We show that it is possible to reduce the memory consumption by an order of magnitude both when word embeddings are being used and in training. In the first case, as we show, simply truncating the resulting representations after training and using a smaller number of bits (as low as 4 bits per dimension) results in comparable performance to the use of 64 bits. Moreover, we provide two ways to train existing algorithms<cite> (Mikolov et al., 2013a</cite>; Mikolov et al., 2013b ) when the memory is limited during training and show that, here, too, an order of magnitude saving in memory is possible without degrading performance. We conduct comprehensive experiments on existing word and phrase similarity and relatedness datasets as well as on dependency parsing, to evaluate these results.",
  "y": "extends"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_2",
  "x": "edu/page/publication_view/790. ---------------------------------- **RELATED WORK** If we consider traditional cluster encoded word representation, e.g., Brown clusters (Brown et al., 1992) , it only uses a small number of bits to track the path on a hierarchical tree of word clusters to represent each word. In fact, word embedding generalized the idea of discrete clustering representation to continuous vector representation in language models, with the goal of improving the continuous word analogy prediction and generalization ability (Bengio et al., 2003; <cite>Mikolov et al., 2013a</cite>; Mikolov et al., 2013b) . However, it has been proven that Brown clusters as discrete features are even better than continuous word embedding as features for named entity recognition tasks (Ratinov and Roth, 2009 ). Guo et al. (Guo et al., 2014) further tried to binarize embeddings using a threshold tuned for each dimension, and essentially used less than two bits to represent each dimension. They have shown that binarization can be comparable to or even better than the original word embeddings when used as features for named entity recognition tasks. Moreover, Faruqui et al. (Faruqui et al., 2015) showed that imposing sparsity constraints over the embedding vectors can further improve the representation interpretability and performance on several word similarity and text classification benchmark datasets.",
  "y": "motivation background"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_4",
  "x": "When the cumulated values in the auxiliary update vectors are greater than the original numerical precision , e.g., = 2 \u22127 for 8 bits, we update the original vector and clear the value in the auxiliary vector. In this case, we can have final n-bit values in word embedding vectors as good as the method presented in Section 3.1. ---------------------------------- **EXPERIMENTS ON WORD/PHRASE SIMILARITY** In this section, we describe a comprehensive study on tasks that have been used for evaluating word embeddings. We train the word embedding algorithms, word2vec<cite> (Mikolov et al., 2013a</cite>; Mikolov et al., 2013b) , based on the Oct. 2013 Wikipedia dump. 1 We first compare levels of truncation of word2vec embeddings, and then evaluate the stochastic rounding and the auxiliary vectors based methods for training word2vec vectors. ---------------------------------- **DATASETS**",
  "y": "uses"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_5",
  "x": "Paraphrases (bigrams). We use the paraphrase (bigram) datasets used in (Wieting et al., 2015) , ppdb all, bigrams vn, bigrams nn, and bigrams jnn, to test whether the truncation affects phrase level embedding. Our phrase level embedding is based on the average of the words inside each phrase. Note that it is also easy to incorporate our truncation methods into existing phrase embedding algorithms. We follow (Wieting et al., 2015) in using cosine similarity to evaluate the correlation between the computed similarity and annotated similarity between paraphrases. ---------------------------------- **ANALYSIS OF BITS NEEDED** We ran both CBOW and skipgram with negative sampling<cite> (Mikolov et al., 2013a</cite>; Mikolov et al., 2013b) on the Wikipedia dump data, and set the window size of context to be five. Then we performed value truncation with 4 bits, 6 bits, and 8 bits.",
  "y": "uses"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_0",
  "x": "These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which n-grams characterize different topics. We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of<cite> Durrett and Klein (2014)</cite> and Nguyen et al. (2014) . ---------------------------------- **1** ---------------------------------- **INTRODUCTION** One of the major challenges of entity linking is resolving contextually polysemous mentions. For example, Germany may refer to a nation, to that nation's government, or even to a soccer team. Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013;<cite> Durrett and Klein, 2014)</cite> .",
  "y": "motivation"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_1",
  "x": "Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013;<cite> Durrett and Klein, 2014)</cite> . But an even simpler approach is to use context information from just the words in the source document itself to make sure the entity is being resolved sensibly in context. In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods. In this work, we model semantic similarity between a mention's source document context and its potential entity targets using convolutional neural networks (CNNs). CNNs have been shown to be effective for sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Iyyer et al., 2015) and for capturing similarity in models for entity linking (Sun et al., 2015) and other related tasks (Dong et al., 2015; Shen et al., 2014) , so we expect them to be effective at isolating the relevant topic semantics for entity linking. We show that convolutions over multiple granularities of the input document are useful for providing different notions of semantic context. Finally, we show how to integrate these networks with a preexisting entity linking system <cite>(Durrett and Klein, 2014)</cite> . Through a combination of these two distinct methods into a single system that leverages their complementary strengths, we achieve state-ofthe-art performance across several datasets. ----------------------------------",
  "y": "background"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_2",
  "x": "One of the major challenges of entity linking is resolving contextually polysemous mentions. For example, Germany may refer to a nation, to that nation's government, or even to a soccer team. Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013;<cite> Durrett and Klein, 2014)</cite> . But an even simpler approach is to use context information from just the words in the source document itself to make sure the entity is being resolved sensibly in context. In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods. In this work, we model semantic similarity between a mention's source document context and its potential entity targets using convolutional neural networks (CNNs). CNNs have been shown to be effective for sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Iyyer et al., 2015) and for capturing similarity in models for entity linking (Sun et al., 2015) and other related tasks (Dong et al., 2015; Shen et al., 2014) , so we expect them to be effective at isolating the relevant topic semantics for entity linking. We show that convolutions over multiple granularities of the input document are useful for providing different notions of semantic context. Finally, we show how to integrate these networks with a preexisting entity linking system <cite>(Durrett and Klein, 2014)</cite> .",
  "y": "motivation"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_3",
  "x": "This information can serve as a useful prior, but only if we can leverage it effectively by targeting the most salient part of a mention. For example, we may have never observed President Barack Obama as a linked string on Wikipedia, even though we have seen the substring Barack Obama and it unambiguously indicates the correct answer. Following<cite> Durrett and Klein (2014)</cite> , we introduce a latent variable q to capture which subset of a mention (known as a query) we resolve. Query generation includes potentially removing stop words, plural suffixes, punctuation, and leading or tailing words. This processes generates on average 9 queries for each mention. Conveniently, this set of queries also defines the set of candidate entities that we consider linking a mention to: each query generates a set of potential entities based on link counts, whose unions are then taken to give on the possible entity targets for each mention (including the null link). In the example shown in Figure 1 , the query phrases are Pink Floyd and Floyd, which generate Pink Floyd and Gavin Floyd as potential link targets (among other options that might be derived from the Floyd query). Our final model has the form P (t|x) = q P (t, q|x). We parameterize P (t, q|x) in a loglinear way with three separate components:",
  "y": "extends differences"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_4",
  "x": "This processes generates on average 9 queries for each mention. Conveniently, this set of queries also defines the set of candidate entities that we consider linking a mention to: each query generates a set of potential entities based on link counts, whose unions are then taken to give on the possible entity targets for each mention (including the null link). In the example shown in Figure 1 , the query phrases are Pink Floyd and Floyd, which generate Pink Floyd and Gavin Floyd as potential link targets (among other options that might be derived from the Floyd query). Our final model has the form P (t|x) = q P (t, q|x). We parameterize P (t, q|x) in a loglinear way with three separate components: f Q and f E are both sparse features vectors and are taken from previous work <cite>(Durrett and Klein, 2014)</cite> . f C is as discussed in Section 2.1. Note that f C has its own internal parameters \u03b8 because it relies on CNNs with learned filters; however, we can compute gradients for these parameters with standard backpropagation. The whole model is trained to maximize the log likelihood of a labeled training corpus using Adadelta (Zeiler, 2012) .",
  "y": "similarities extends"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_5",
  "x": "Our final model has the form P (t|x) = q P (t, q|x). We parameterize P (t, q|x) in a loglinear way with three separate components: f Q and f E are both sparse features vectors and are taken from previous work <cite>(Durrett and Klein, 2014)</cite> . f C is as discussed in Section 2.1. Note that f C has its own internal parameters \u03b8 because it relies on CNNs with learned filters; however, we can compute gradients for these parameters with standard backpropagation. The whole model is trained to maximize the log likelihood of a labeled training corpus using Adadelta (Zeiler, 2012) . The indicator features f Q and f E are described in more detail in<cite> Durrett and Klein (2014)</cite> . f Q only impacts which query is selected and not the disambiguation to a title. It is designed to roughly capture the basic shape of a query to measure its desirability, indicating whether suffixes were removed and whether the query captures the capitalized subsequence of a mention, as well as standard lexical, POS, and named entity type features.",
  "y": "background"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_6",
  "x": "f Q only impacts which query is selected and not the disambiguation to a title. It is designed to roughly capture the basic shape of a query to measure its desirability, indicating whether suffixes were removed and whether the query captures the capitalized subsequence of a mention, as well as standard lexical, POS, and named entity type features. f E mostly captures how likely the selected query is to correspond to a given entity based on factors like anchor text counts from Wikipedia, string match with proposed Wikipedia titles, and discretized cosine similarities of tf-idf vectors (Ratinov et al., 2011) . Adding tf-idf indicators is the only modification we made to the features of<cite> Durrett and Klein (2014)</cite> . ---------------------------------- **EXPERIMENTAL RESULTS** We performed experiments on 4 different entity linking datasets. \u2022 ACE (NIST, 2005; Bentivogli et al., 2010) : This corpus was used in Fahrni and Strube (2014) and<cite> Durrett and Klein (2014)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_7",
  "x": "f E mostly captures how likely the selected query is to correspond to a given entity based on factors like anchor text counts from Wikipedia, string match with proposed Wikipedia titles, and discretized cosine similarities of tf-idf vectors (Ratinov et al., 2011) . Adding tf-idf indicators is the only modification we made to the features of<cite> Durrett and Klein (2014)</cite> . ---------------------------------- **EXPERIMENTAL RESULTS** We performed experiments on 4 different entity linking datasets. \u2022 ACE (NIST, 2005; Bentivogli et al., 2010) : This corpus was used in Fahrni and Strube (2014) and<cite> Durrett and Klein (2014)</cite> . \u2022 CoNLL-YAGO (Hoffart et al., 2011) : This corpus is based on the CoNLL 2003 dataset; the test set consists of 231 news articles and contains a number of rarer entities. \u2022 WP (Heath and Bizer, 2011): This dataset consists of short snippets from Wikipedia.",
  "y": "similarities uses"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_8",
  "x": "\u2022 Table 2 : Performance of the system in this work (Full) compared to two baselines from prior work and two ablations. Our results outperform those of<cite> Durrett and Klein (2014)</cite> and Nguyen et al. (2014) . In general, we also see that the convolutional networks by themselves can outperform the system using only sparse features, and in all cases these stack to give substantial benefit. We use standard train-test splits for all datasets except for WP, where no standard split is available. In this case, we randomly sample a test set. For all experiments, we use word vectors computed by running word2vec (Mikolov et al., 2013) on all Wikipedia, as described in Section 3.2. Table 2 shows results for two baselines and three variants of our system. Our main contribution is the combination of indicator features and CNN features (Full). We see that this system outperforms the results of<cite> Durrett and Klein (2014)</cite> and the AIDA-LIGHT system of Nguyen et al. (2014) .",
  "y": "differences"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_9",
  "x": "For all experiments, we use word vectors computed by running word2vec (Mikolov et al., 2013) on all Wikipedia, as described in Section 3.2. Table 2 shows results for two baselines and three variants of our system. Our main contribution is the combination of indicator features and CNN features (Full). We see that this system outperforms the results of<cite> Durrett and Klein (2014)</cite> and the AIDA-LIGHT system of Nguyen et al. (2014) . We can also compare to two ablations: using just the sparse features (a system which is a direct extension of<cite> Durrett and Klein (2014)</cite> ) or using just the CNNderived features. 5 Our CNN features generally outperform the sparse features and improve even further when stacked with them. This reflects that they capture orthogonal sources of information: for example, the sparse features can capture how frequently the target document was linked to, whereas the CNNs can capture document context in a more nuanced way. These CNN features also clearly supersede the sparse features based on tf-idf (taken from (Ratinov et al., 2011)) , showing that indeed that CNNs are better at learning semantic topic similarity than heuristics like tf-idf. Table 3 : Comparison of using only topic information derived from the document and target article, only information derived from the mention itself and the target entity title, and the full set of information (six features, as shown in Figure 1 ).",
  "y": "extends differences"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_10",
  "x": "**ACE** In the sparse feature system, the highest weighted features are typically those indicating the frequency that a page was linked to and those indicating specific lexical items in the choice of the latent query variable q. This suggests that the system of<cite> Durrett and Klein (2014)</cite> has the power to pick the right span of a mention to resolve, but then is left to generally pick the most common link target in Wikipedia, which is not always correct. By contrast, the full system has a greater ability to pick less common link targets if the topic indicators distilled from the CNNs indicate that it should do so. ---------------------------------- **MULTIPLE GRANULARITIES OF CONVOLUTION** One question we might ask is how much we gain by having multiple convolutions on the source and target side. Table 3 compares our full suite of CNN features, i.e. the six features specified in Figure 1 , with two specific convolutional features in isolation. Using convolutions over just the source document (s doc ) and target article text (t doc ) gives a system 6 that performs, in aggregate, comparably to using convolutions over just the mention (s ment ) and the entity title (t title ). These represent two extremes of the system: consuming the maximum amount of context, which might give the most robust representation of topic semantics, and consuming the minimum amount of context, which gives the most focused representation of topics semantics (and which more generally might allow the system to directly memorize train-test pairs observed in training).",
  "y": "background motivation"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_0",
  "x": "Morphological segmentation is the task of dividing words into morphemes, the smallest meaning-bearing units in the word (Goldsmith, 2001) . For example the morpheme over occurs in words like hold+over, lay+over, and skip+over. 1 Roots combine with derivational (e.g. refut+able) and inflectional affixes (e.g. hold+ing). Computational segmentation approaches can be divided into rule-based (Porter, 1980) , supervised (Ruokolainen et al., 2013) , semi-supervised (Gr\u00f6nroos et al., 2014) , and unsupervised (Creutz and Lagus, 2002) . <cite>Bartlett et al. (2008)</cite> observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification. In this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology. We augment the syllabification approach of <cite>Bartlett et al. (2008)</cite> , with features encoding morphological segmentation of words. We investigate the degree of overlap between the morphological and syllable boundaries. The results of our experiments on English and German show that the incorporation of expert-annotated (gold) morphological boundaries extracted from lexical databases substantially reduces the syllabification error rate, particularly in low-resource settings.",
  "y": "background"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_1",
  "x": "<cite>Bartlett et al. (2008)</cite> observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification. In this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology. We augment the syllabification approach of <cite>Bartlett et al. (2008)</cite> , with features encoding morphological segmentation of words. We investigate the degree of overlap between the morphological and syllable boundaries. The results of our experiments on English and German show that the incorporation of expert-annotated (gold) morphological boundaries extracted from lexical databases substantially reduces the syllabification error rate, particularly in low-resource settings. We find that the accuracy gains tend to be preserved when unsupervised segmentation is used instead. On the other hand, relying on a fully-supervised system appears to be much less robust, even though it generates more accurate morphological segmentations than the unsupervised systems. We propose an explanation for this surprising result. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_2",
  "x": "We augment the syllabification approach of <cite>Bartlett et al. (2008)</cite> , with features encoding morphological segmentation of words. We investigate the degree of overlap between the morphological and syllable boundaries. The results of our experiments on English and German show that the incorporation of expert-annotated (gold) morphological boundaries extracted from lexical databases substantially reduces the syllabification error rate, particularly in low-resource settings. We find that the accuracy gains tend to be preserved when unsupervised segmentation is used instead. On the other hand, relying on a fully-supervised system appears to be much less robust, even though it generates more accurate morphological segmentations than the unsupervised systems. We propose an explanation for this surprising result. ---------------------------------- **METHODS** In this section, we describe the original syllabification method of <cite>Bartlett et al. (2008)</cite> , which serves as our baseline system, and discuss various approaches to incorporating morphological information.",
  "y": "uses"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_3",
  "x": "The results of our experiments on English and German show that the incorporation of expert-annotated (gold) morphological boundaries extracted from lexical databases substantially reduces the syllabification error rate, particularly in low-resource settings. We find that the accuracy gains tend to be preserved when unsupervised segmentation is used instead. On the other hand, relying on a fully-supervised system appears to be much less robust, even though it generates more accurate morphological segmentations than the unsupervised systems. We propose an explanation for this surprising result. ---------------------------------- **METHODS** In this section, we describe the original syllabification method of <cite>Bartlett et al. (2008)</cite> , which serves as our baseline system, and discuss various approaches to incorporating morphological information. <cite>Bartlett et al. (2008)</cite> present a discriminative approach to automatic syllabification. They formulate syllabification as a tagging problem, and learn a Structured SVM tagger from labeled data (Tsochantaridis et al., 2005) .",
  "y": "background"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_5",
  "x": "We attribute this to the tendency of the unsupervised methods to oversegment. ---------------------------------- **QUALITY OF MORPHOLOGICAL SEGMENTATION** ---------------------------------- **BASELINE SYLLABIFICATION** As a baseline, we replicate the experiments of <cite>Bartlett et al. (2008)</cite> , and extend them to lowresource settings. Since the training sets are of slightly different sizes, we label each training size point as specified in Table 3 . We see that correct syllabification of approximately half of the words is achieved with as few as 100 English and 50 German training examples. ----------------------------------",
  "y": "uses extends"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_0",
  "x": "Typically, a simplified sentence differs from a complex one in that it involves simpler, more usual and often shorter, words (e.g., use instead of exploit); simpler syntactic constructions (e.g., no relative clauses or apposition); and fewer modifiers (e.g., He slept vs. He also slept). In practice, simplification is thus often modeled using four main operations: splitting a complex sentence into several simpler sentences; dropping and reordering phrases or constituents; substituting words/phrases with simpler ones. As has been argued in previous work, sentence simplification has many potential applications. It is useful as a preprocessing step for a variety of NLP systems such as parsers and machine translation systems (Chandrasekar et al., 1996) , summarisation (Knight and Marcu, 2000) , sentence fusion (Filippova and Strube, 2008 ) and semantic role labelling (Vickrey and Koller, 2008) . It also has wide ranging potential societal application as a reading aid for people with aphasis (Carroll et al., 1999) , for low literacy readers (Watanabe et al., 2009 ) and for non native speakers (Siddharthan, 2002) . There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011) . Machine Translation systems have been adapted to translate complex sentences into simple ones <cite>(Zhu et al., 2010</cite>; Wubben et al., 2012; Coster and Kauchak, 2011) . And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996) .",
  "y": "background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_1",
  "x": "While previous simplification approaches starts from either the input sentence or its parse tree, our model takes as input a deep semantic representation namely, the Discourse Representation Structure (DRS, (Kamp, 1981) ) assigned by Boxer (Curran et al., 2007) to the input complex sentence. As we shall see in Section 4, this permits a linguistically principled account of the splitting operation in that semantically shared elements are taken to be the basis for splitting a complex sentence into several simpler ones; this facilitates completion (the re-creation of the shared element in the split sentences); and this provide a natural means to avoid deleting obligatory arguments. When compared against current state of the art methods <cite>(Zhu et al., 2010</cite>; Woodsend and Lapata, 2011; Wubben et al., 2012) , our model yields significantly simpler output that is both grammatical and meaning preserving. ---------------------------------- **RELATED WORK** Earlier work on sentence simplification relied on handcrafted rules to capture syntactic simplification e.g., to split coordinated and subordinated sentences into several, simpler clauses or to model active/passive transformations (Siddharthan, 2002; Chandrasekar and Srinivas, 1997; Bott et al., 2012; Canning, 2002; Siddharthan, 2011; Siddharthan, 2010) . While these handcrafted approaches can encode precise and linguistically well-informed syntactic transformation (using e.g., detailed morphological and syntactic information), they are limited in scope to purely syntactic rules and do not account for lexical simplifications and their interaction with the sentential context. Using the parallel dataset formed by Simple English Wikipedia (SWKP) 1 and traditional English Wikipedia (EWKP) 2 , more recent work has focused on developing machine learning approaches to sentence simplification. Zhu et al. (2010) constructed a parallel corpus (PWKP) of 108,016/114,924 complex/simple sentences by aligning sentences from EWKP and SWKP and used the resulting bitext to train a simplification model inspired by syntax-based machine translation (Yamada and Knight, 2001 ).",
  "y": "differences"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_2",
  "x": "Machine Translation systems have been adapted to translate complex sentences into simple ones <cite>(Zhu et al., 2010</cite>; Wubben et al., 2012; Coster and Kauchak, 2011) . And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996) . In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways. First, it combines a model encoding probabilities for splitting and deletion with a monolingual machine translation module which handles reordering and substitution. In this way, we exploit the ability of statistical machine translation (SMT) systems to capture phrasal/lexical substitution and reordering while relying on a dedicated probabilistic module to capture the splitting and deletion operations which are less well (deletion) or not at all (splitting) captured by SMT approaches. Second, our approach is semantic based. While previous simplification approaches starts from either the input sentence or its parse tree, our model takes as input a deep semantic representation namely, the Discourse Representation Structure (DRS, (Kamp, 1981) ) assigned by Boxer (Curran et al., 2007) to the input complex sentence. As we shall see in Section 4, this permits a linguistically principled account of the splitting operation in that semantically shared elements are taken to be the basis for splitting a complex sentence into several simpler ones; this facilitates completion (the re-creation of the shared element in the split sentences); and this provide a natural means to avoid deleting obligatory arguments. When compared against current state of the art methods <cite>(Zhu et al., 2010</cite>; Woodsend and Lapata, 2011; Wubben et al., 2012) , our model yields significantly simpler output that is both grammatical and meaning preserving.",
  "y": "extends"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_3",
  "x": "While these handcrafted approaches can encode precise and linguistically well-informed syntactic transformation (using e.g., detailed morphological and syntactic information), they are limited in scope to purely syntactic rules and do not account for lexical simplifications and their interaction with the sentential context. Using the parallel dataset formed by Simple English Wikipedia (SWKP) 1 and traditional English Wikipedia (EWKP) 2 , more recent work has focused on developing machine learning approaches to sentence simplification. Zhu et al. (2010) constructed a parallel corpus (PWKP) of 108,016/114,924 complex/simple sentences by aligning sentences from EWKP and SWKP and used the resulting bitext to train a simplification model inspired by syntax-based machine translation (Yamada and Knight, 2001 ). Their simplification model encodes the probabilities for four rewriting operations on the parse tree of an input sentences namely, substitution, reordering, splitting and deletion. It is combined with a language model to improve grammaticality and the decoder translates sentences into sim-pler ones by greedily selecting the output sentence with highest probability. Using both the PWKP corpus developed by<cite> Zhu et al. (2010)</cite> and the edit history of Simple Wikipedia, Woodsend and Lapata (2011) learn a quasi synchronous grammar (Smith and Eisner, 2006) describing a loose alignment between parse trees of complex and of simple sentences. Following Dras (1999) , they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate simplification. They evaluate their model on the same dataset used by<cite> Zhu et al. (2010)</cite> namely, an aligned corpus of 100/131 EWKP/SWKP sentences and show that they achieve better BLEU score. They also conducted a human evaluation on 64 of the 100 test sentences and showed again a better performance in terms of simplicity, grammaticality and meaning preservation.",
  "y": "background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_4",
  "x": "Earlier work on sentence simplification relied on handcrafted rules to capture syntactic simplification e.g., to split coordinated and subordinated sentences into several, simpler clauses or to model active/passive transformations (Siddharthan, 2002; Chandrasekar and Srinivas, 1997; Bott et al., 2012; Canning, 2002; Siddharthan, 2011; Siddharthan, 2010) . While these handcrafted approaches can encode precise and linguistically well-informed syntactic transformation (using e.g., detailed morphological and syntactic information), they are limited in scope to purely syntactic rules and do not account for lexical simplifications and their interaction with the sentential context. Using the parallel dataset formed by Simple English Wikipedia (SWKP) 1 and traditional English Wikipedia (EWKP) 2 , more recent work has focused on developing machine learning approaches to sentence simplification. Zhu et al. (2010) constructed a parallel corpus (PWKP) of 108,016/114,924 complex/simple sentences by aligning sentences from EWKP and SWKP and used the resulting bitext to train a simplification model inspired by syntax-based machine translation (Yamada and Knight, 2001 ). Their simplification model encodes the probabilities for four rewriting operations on the parse tree of an input sentences namely, substitution, reordering, splitting and deletion. It is combined with a language model to improve grammaticality and the decoder translates sentences into sim-pler ones by greedily selecting the output sentence with highest probability. Using both the PWKP corpus developed by<cite> Zhu et al. (2010)</cite> and the edit history of Simple Wikipedia, Woodsend and Lapata (2011) learn a quasi synchronous grammar (Smith and Eisner, 2006) describing a loose alignment between parse trees of complex and of simple sentences. Following Dras (1999) , they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate simplification. They evaluate their model on the same dataset used by<cite> Zhu et al. (2010)</cite> namely, an aligned corpus of 100/131 EWKP/SWKP sentences and show that they achieve better BLEU score.",
  "y": "uses background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_5",
  "x": "They also conducted a human evaluation on 64 of the 100 test sentences and showed again a better performance in terms of simplicity, grammaticality and meaning preservation. In (Wubben et al., 2012; Coster and Kauchak, 2011) , simplification is viewed as a monolingual translation task where the complex sentence is the source and the simpler one is the target. To account for deletions, reordering and substitution, Coster and Kauchak (2011) trained a phrase based machine translation system on the PWKP corpus while modifying the word alignment output by GIZA++ in Moses to allow for null phrasal alignments. In this way, they allow for phrases to be deleted during translation. No human evaluation is provided but the approach is shown to result in statistically significant improvements over a traditional phrase based approach. Similarly, Wubben et al. (2012) use Moses and the PWKP data to train a phrase based machine translation system augmented with a post-hoc reranking procedure designed to rank the output based on their dissimilarity from the source. A human evaluation on 20 sentences randomly selected from the test data indicates that, in terms of fluency and adequacy, their system is judged to outperform both<cite> Zhu et al. (2010)</cite> and Woodsend and Lapata (2011) systems. ---------------------------------- **SIMPLIFICATION FRAMEWORK**",
  "y": "background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_6",
  "x": "Bricks enabled the construction of permanent buildings. While splitting opportunities have a clear counterpart in syntax (i.e., splitting often occurs whenever a relative, a subordinate or an appositive clause occurs in the complex sentence), completion i.e., the reconstruction of the shared element in the second simpler clause, is arguably semantically governed in that the reconstructed element corefers with its matching phrase in the first simpler clause. While our semantic based approach naturally accounts for this by copying the phrase corresponding to the shared entity in both phrases, syntax based approach such as<cite> Zhu et al. (2010)</cite> and Woodsend and Lapata (2011) will often fail to appropriately reconstruct the shared phrase and introduce agreement mismatches because the alignment or rules they learn are based on syntax alone. For instance, in example (2),<cite> Zhu et al. (2010)</cite> fails to copy the shared argument \"The judge\" to the second clause whereas Woodsend and Lapata (2011) learns a synchronous rule matching (VP and VP) to (VP. NP(It) VP) thereby failing to produce the correct subject pronoun (\"he\" or \"she\") for the antecedent \"The judge\". (2) C. The judge ordered that Chapman should receive psychiatric treatment in prison and sentenced him to twenty years to life. S1. The judge ordered that Chapman should get psychiatric treatment. In prison and sentenced him to twenty years to life.",
  "y": "differences"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_7",
  "x": "It sentenced him to twenty years to life. (Woodsend and Lapata, 2011) Deletion. By handling deletion using a probabilistic model trained on semantic representations, we can avoid deleting obligatory arguments. Thus in our approach, semantic subformulae which are related to a predicate by a core thematic roles (e.g., agent and patient) are never considered for deletion. By contrast, syntax based approaches <cite>(Zhu et al., 2010</cite>; Woodsend and Lapata, 2011) do not distinguish between optional and obligatory arguments. For instance<cite> Zhu et al. (2010)</cite> simplifies (3C) to (3S) thereby incorrectly deleting the obligatory theme (gifts) of the complex sentence and modifying its meaning to giving knights and warriors (instead of giving gifts to knights and warriors). (3) C. Women would also often give knights and warriors gifts that included thyme leaves as it was believed to bring courage to the bearer. S. Women also often give knights and warriors. Gifts included thyme leaves as it was thought to bring courage to the saint.",
  "y": "differences"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_8",
  "x": "Thus in our approach, semantic subformulae which are related to a predicate by a core thematic roles (e.g., agent and patient) are never considered for deletion. By contrast, syntax based approaches <cite>(Zhu et al., 2010</cite>; Woodsend and Lapata, 2011) do not distinguish between optional and obligatory arguments. For instance<cite> Zhu et al. (2010)</cite> simplifies (3C) to (3S) thereby incorrectly deleting the obligatory theme (gifts) of the complex sentence and modifying its meaning to giving knights and warriors (instead of giving gifts to knights and warriors). (3) C. Women would also often give knights and warriors gifts that included thyme leaves as it was believed to bring courage to the bearer. S. Women also often give knights and warriors. Gifts included thyme leaves as it was thought to bring courage to the saint. (Zhu et al., 2010) We also depart from Coster and Kauchak (2011) who rely on null phrasal alignments for deletion during phrase based machine translation. In their approach, deletion is constrained by the training data and the possible alignments, independent of any linguistic knowledge. Substitution and Reordering SMT based approaches to paraphrasing (Barzilay and Elhadad, 2003; Bannard and Callison-Burch, 2005) and to sentence simplification (Wubben et al., 2012) have shown that by utilising knowledge about alignment and translation probabilities, SMT systems can account for the substitutions and the reorderings occurring in sentence simplification.",
  "y": "differences"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_9",
  "x": "All three deletion models use the associated word itself as a feature. In addition, the model for relations uses the PP length-range as a feature while the model for orphan words relies on boundary information i.e., whether or not, the OW occurs at the associated sentence boundary. ---------------------------------- **ESTIMATING THE PARAMETERS** We use the EM algorithm (Dempster et al., 1977) to estimate our split and deletion model parameters. For an efficient implementation of EM algorithm, we follow the work of Yamada and Knight (2001) and<cite> Zhu et al. (2010)</cite> ; and build training graphs (Figure 2 ) from the pair of complex and simple sentence pairs in the training data. Each training graph represents a complexsimple sentence pair and consists of two types of nodes: major nodes (M-nodes) and operation nodes (O-nodes). Each deletion candidate creates a deletion O-node marking successful or failed deletion of the candidate and a result M-node. The deletion process continues on the result M-node until there is no deletion candidate left to process.",
  "y": "uses"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_10",
  "x": "To evaluate performance, we compare our approach with three other state of the art systems using the test set provided by<cite> Zhu et al. (2010)</cite> and relying both on automatic metrics and on human judgments. ---------------------------------- **TRAINING AND TEST DATA** The DRS-Based simplification model is trained on PWKP, a bi-text of complex and simple sentences provided by<cite> Zhu et al. (2010)</cite> . To construct this bi-text,<cite> Zhu et al. (2010)</cite> extracted complex and simple sentences from EWKP and SWKP respectively and automatically aligned them using TF*IDF as a similarity measure. PWKP contains 108016/114924 complex/simple sentence pairs. We tokenize PWKP using Stanford CoreNLP toolkit 8 . We then parse all complex sentences in PWKP using Boxer 9 to produce their DRSs. Finally, our DRS-Based simplification model is trained on 97.75% of PWKP; we drop out 2.25% of the complex sentences in PWKP which are repeated in the test set or for which Boxer fails to produce DRSs.",
  "y": "uses"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_11",
  "x": "---------------------------------- **TRAINING AND TEST DATA** The DRS-Based simplification model is trained on PWKP, a bi-text of complex and simple sentences provided by<cite> Zhu et al. (2010)</cite> . To construct this bi-text,<cite> Zhu et al. (2010)</cite> extracted complex and simple sentences from EWKP and SWKP respectively and automatically aligned them using TF*IDF as a similarity measure. PWKP contains 108016/114924 complex/simple sentence pairs. We tokenize PWKP using Stanford CoreNLP toolkit 8 . We then parse all complex sentences in PWKP using Boxer 9 to produce their DRSs. Finally, our DRS-Based simplification model is trained on 97.75% of PWKP; we drop out 2.25% of the complex sentences in PWKP which are repeated in the test set or for which Boxer fails to produce DRSs. We evaluate our model on the test set used by<cite> Zhu et al. (2010)</cite> namely, an aligned corpus of 100/131 EWKP/SWKP sentences.",
  "y": "uses"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_12",
  "x": "---------------------------------- **EXPERIMENTS** We trained our simplification and translation models on the PWKP corpus. To evaluate performance, we compare our approach with three other state of the art systems using the test set provided by<cite> Zhu et al. (2010)</cite> and relying both on automatic metrics and on human judgments. ---------------------------------- **TRAINING AND TEST DATA** The DRS-Based simplification model is trained on PWKP, a bi-text of complex and simple sentences provided by<cite> Zhu et al. (2010)</cite> . To construct this bi-text,<cite> Zhu et al. (2010)</cite> extracted complex and simple sentences from EWKP and SWKP respectively and automatically aligned them using TF*IDF as a similarity measure. PWKP contains 108016/114924 complex/simple sentence pairs.",
  "y": "background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_13",
  "x": "PWKP contains 108016/114924 complex/simple sentence pairs. We tokenize PWKP using Stanford CoreNLP toolkit 8 . We then parse all complex sentences in PWKP using Boxer 9 to produce their DRSs. Finally, our DRS-Based simplification model is trained on 97.75% of PWKP; we drop out 2.25% of the complex sentences in PWKP which are repeated in the test set or for which Boxer fails to produce DRSs. We evaluate our model on the test set used by<cite> Zhu et al. (2010)</cite> namely, an aligned corpus of 100/131 EWKP/SWKP sentences. Boxer produces a DRS for 96 of the 100 input sentences. These input are simplified using our simplification system namely, the DRS-SM model and the phrase-based machine translation system (Section 3.2). For the remaining four complex sentences, Boxer fails to produce DRSs. These four sentences are directly sent to the phrase-based machine translation system to produce simplified sentences.",
  "y": "uses"
 },
 {
  "id": "a9897f66e05a0354c36daba0db9afe_0",
  "x": "We show that retraining this parser with a combination of one million BNC parse trees (produced by the same parser) and the original WSJ training data yields improvements of 0.4% on WSJ Section 23 and 1.7% on the new BNC gold standard set. ---------------------------------- **INTRODUCTION** Given the success of statistical parsing models on the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (Charniak, 2000; Collins, 2003, for example) , there has been a change in focus in recent years towards the problem of replicating this success on genres other than American financial news stories. The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples. A breakthrough has come in the form of research by <cite>McClosky et al. (2006a</cite>; 2006b ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005) . Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al., 2003) . <cite>McClosky et al. (2006a</cite>; 2006b ) proceed as follows: sentences * Now affiliated to Lalic, Universit\u00e9 Paris 4 La Sorbonne. from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker.",
  "y": "background"
 },
 {
  "id": "a9897f66e05a0354c36daba0db9afe_2",
  "x": "A breakthrough has come in the form of research by <cite>McClosky et al. (2006a</cite>; 2006b ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005) . Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al., 2003) . <cite>McClosky et al. (2006a</cite>; 2006b ) proceed as follows: sentences * Now affiliated to Lalic, Universit\u00e9 Paris 4 La Sorbonne. from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker. The highest ranked parse trees are added to the training set of the parser and the parser is retrained. This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Ku\u010dera, 1979 ) (an absolute fscore improvement of 2.6%). In the experiments of <cite>McClosky et al. (2006a</cite>; 2006b) , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees.",
  "y": "background"
 },
 {
  "id": "abe561b75389e026a9f140280f211c_0",
  "x": "**INTRODUCTION** Due to the vigorous development of social media in recent years, more and more user-generated sentiment data have been shared on the Web. It is a useful means to understand the opinion of the masses, which is a major issue for businesses. However, they exist in the forms of comments in a live webcast, opinion sites, or social media, and often contain considerable amount of noise. Such characteristics pose obstacles to those who intend to collect this type of information efficiently. It is the reason why opinion mining has recently become a topic of interest in both academia and business institutions. Sentiment analysis is a type of opinion mining where affective states are represented categorically or by multi-dimensional continuous values<cite> (Yu et al., 2015)</cite> . The categorical approach aims at classifying the sentiment into polarity classes (such as positive, neutral, and negative,) or Ekman's six basic emotions, i.e., anger, happiness, fear, sadness, disgust, and surprise (Ekman, 1992 ). This approach is extensively studied because it can provide a desirable outcome, which is an overall evaluation of the sentiment in the material that is being analyzed.",
  "y": "background"
 },
 {
  "id": "abe561b75389e026a9f140280f211c_1",
  "x": "For instance, a popular form of media in recent years is live webcasting. This kind of applications usually provide viewers with the ability to comment immediately while the stream is live. Categorical sentiment analysis can immediately classify each response as either positive or negative, thus helping the host to quickly summarize every period of their broadcast. On the other hand, the dimensional approach represents affective states as continuous numerical values in multiple dimensions, such as valencearousal space (Markus and Kitayama, 1991) , as shown in Fig. 1 gree of pleasant and unpleasant (i.e., positive and negative) feelings, while the arousal represents the degree of excitement. According to the twodimensional representation, any affective state can be represented as a point in the valence-arousal space by determining the degrees of valence and arousal of given words (Wei et al., 2011; <cite>Yu et al., 2015)</cite> or texts (Kim et al., 2010) . Dimen-sional sentiment analysis is an increasingly active research field with potential applications including antisocial behavior detection (Munezero et al., 2011) and mood analysis (De Choudhury et al., 2012) . In light of this, the objective of the Dimensional Sentiment Analysis for Chinese Words (DSAW) shared task at the 21th International Conference on Asian Language Processing is to automatically acquire the valence-arousal ratings of Chinese affective words and phrases for compiling Chinese valence-arousal lexicons. The expected output of this task is to predict a real-valued score from 1 to 9 for both valence and arousal dimensions of the given 750 test words and phrases. The score indicates the degree from most negative to most positive for valence, and from most calm to most excited for arousal.",
  "y": "background"
 },
 {
  "id": "abe561b75389e026a9f140280f211c_2",
  "x": "In order to cope with the problem of unknown words, we separate words in WVA into 4,184 characters with valence-arousal ratings, called CVA. The valence-arousal score of the unknown word can be obtained by averaging the matched CVA. Moreover, previous research suggested that it is possible to improve the performance by aggregating the results of a number of valence-arousal methods<cite> (Yu et al., 2015)</cite> . Thus, we use two sets of methods for the prediction of valence: (1) prediction based on WVA and CVA, and (2) a kNN valence prediction method. The results of these two methods are averaged as the final valence score. First, we describe the prediction of valence values. As shown in Fig. 3 , the \"\u5b8c\u6210\" of the test data exists in the WVA, so we can directly obtain its valence value of 7.0. However, another word \"\u901a\u77e5\" does not exist in the WVA, so we search in CVA and calculate a valence value of 5.6. Additionally, we propose another prediction method of the valence value, as shown in Fig. 4 , based on kNN.",
  "y": "motivation background"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_0",
  "x": "**ABSTRACT** We replicate the syntactic experiments of <cite>Mikolov et al. (2013b)</cite> on English, and expand them to include morphologically complex languages. We learn vector representations for Dutch, French, German, and Spanish with the WORD2VEC tool, and investigate to what extent inflectional information is preserved across vectors. We observe that the accuracy of vectors on a set of syntactic analogies is inversely correlated with the morphological complexity of the language. ---------------------------------- **** 1 Introduction <cite>Mikolov et al. (2013b)</cite> demonstrate that vector representations of words obtained from a neural network language model provide a way of capturing both semantic and syntactic regularities in language. They observe that by manipulating vector offsets between pairs of words, it is possible to derive an approximation of vectors representing other words, such as queen \u2248 king \u2212 man + woman. Similarly, an abstract relationship between the present and past tense may be computed by subtracting the base form eat from the past form ate; the result of composing such an offset with the base form cook may turn out to be similar to the vector for cooked (Figure 1 ).",
  "y": "uses extends"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_1",
  "x": "We learn vector representations for Dutch, French, German, and Spanish with the WORD2VEC tool, and investigate to what extent inflectional information is preserved across vectors. We observe that the accuracy of vectors on a set of syntactic analogies is inversely correlated with the morphological complexity of the language. ---------------------------------- **** 1 Introduction <cite>Mikolov et al. (2013b)</cite> demonstrate that vector representations of words obtained from a neural network language model provide a way of capturing both semantic and syntactic regularities in language. They observe that by manipulating vector offsets between pairs of words, it is possible to derive an approximation of vectors representing other words, such as queen \u2248 king \u2212 man + woman. Similarly, an abstract relationship between the present and past tense may be computed by subtracting the base form eat from the past form ate; the result of composing such an offset with the base form cook may turn out to be similar to the vector for cooked (Figure 1 ). They report state-of-the-art results on a set of analogy questions of the form \"a is to b as c is to \", where the variables represent various English word forms. Our work is motivated by two observations regarding Mikolov et al.'s experiments: first, the syntactic analogies that they test correspond to morphological inflections, and second, the tests only evaluate English, a language with little morphological complexity.",
  "y": "background"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_2",
  "x": "**REPLICATION EXPERIMENTS** In order to to validate our methodology, we first replicate the results of <cite>Mikolov et al. (2013b)</cite> on English syntactic analogies. ---------------------------------- **TRAINING CORPUS FOR WORD VECTORS** The vectors of <cite>Mikolov et al. (2013b)</cite> were trained on 320M tokens of broadcast news data, as described by Mikolov et al. (2011) . Since we have no access to this data, we instead train English vectors on a corpus from the Polyglot project (Al-Rfou et al., 2013) , which contains tokenized Wikipedia dumps intended for the training of vector-space models. For comparison with the results of <cite>Mikolov et al. (2013b)</cite> , we limit the data to the first 320M lowercased tokens of the corpus. <cite>Mikolov et al. (2013b)</cite> obtain their best results with vectors of size 1600 that combine several models, but do not elaborate how this composite model was constructed. Instead, we take as a point of reference their second-best model, which employs 640-dimensional vectors produced by a single recursive neural network (RNN) language model.",
  "y": "uses"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_3",
  "x": "In this paper, we replicate their syntactic experiments on four languages that are more morphologically complex than English: Dutch, French, German, and Spanish. ---------------------------------- **REPLICATION EXPERIMENTS** In order to to validate our methodology, we first replicate the results of <cite>Mikolov et al. (2013b)</cite> on English syntactic analogies. ---------------------------------- **TRAINING CORPUS FOR WORD VECTORS** The vectors of <cite>Mikolov et al. (2013b)</cite> were trained on 320M tokens of broadcast news data, as described by Mikolov et al. (2011) . Since we have no access to this data, we instead train English vectors on a corpus from the Polyglot project (Al-Rfou et al., 2013) , which contains tokenized Wikipedia dumps intended for the training of vector-space models. For comparison with the results of <cite>Mikolov et al. (2013b)</cite> , we limit the data to the first 320M lowercased tokens of the corpus.",
  "y": "background"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_4",
  "x": "In this paper, we replicate their syntactic experiments on four languages that are more morphologically complex than English: Dutch, French, German, and Spanish. ---------------------------------- **REPLICATION EXPERIMENTS** In order to to validate our methodology, we first replicate the results of <cite>Mikolov et al. (2013b)</cite> on English syntactic analogies. ---------------------------------- **TRAINING CORPUS FOR WORD VECTORS** The vectors of <cite>Mikolov et al. (2013b)</cite> were trained on 320M tokens of broadcast news data, as described by Mikolov et al. (2011) . Since we have no access to this data, we instead train English vectors on a corpus from the Polyglot project (Al-Rfou et al., 2013) , which contains tokenized Wikipedia dumps intended for the training of vector-space models. For comparison with the results of <cite>Mikolov et al. (2013b)</cite> , we limit the data to the first 320M lowercased tokens of the corpus.",
  "y": "motivation"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_5",
  "x": "<cite>Mikolov et al. (2013b)</cite> obtain their best results with vectors of size 1600 that combine several models, but do not elaborate how this composite model was constructed. Instead, we take as a point of reference their second-best model, which employs 640-dimensional vectors produced by a single recursive neural network (RNN) language model. 1 Rather than use an RNN model to learn our own vectors, we employ the far simpler skip-gram model. Mikolov et al. (2013a) show that higher accuracy can be obtained using vectors derived using this model, which is also far less expensive to train. The skip-gram model eschews a language modeling objective in favor of a logistic regression classifier that predicts surrounding words. The WORD2VEC package includes code for learning skip-gram models from very large corpora. 2 We train 640-dimensional vectors using the skip-gram model with a hierarchical softmax, a context window of 10, sub-sampling of 1e-3, and a minimum frequency threshold of 10. ---------------------------------- **TEST SET**",
  "y": "differences"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_7",
  "x": "The first, labeled as M13, is the result of applying the vectors of <cite>Mikolov et al. (2013b)</cite> to their test set. The results match the results reported in their paper, except for the nominal results, which reflect our modifications described in Section 2.2. The removal of the possessives improves the accuracy from 25.2% reported in the original paper to 40.1%. The second column, labeled as Ours, reports the results for our vectors, which were trained using WORD2VEC on the English data described in Section 2.1. Our verbal and adjectival vectors obtain slightly lower accuracies than the RNN trained vectors of <cite>Mikolov et al. (2013b)</cite> , but they are not far off. For nouns, however, we obtain higher accuracy than Mikolov et al. The tokenization method that removes possessives from consideration may produce better vectors for singular and plural forms, as it increases the frequency of these types. ---------------------------------- **MULTILINGUAL EXPERIMENTS** Our second set of experiments examine to what extent the syntactic regularities are captured by word vectors in four other languages: Dutch, French, German, and Spanish.",
  "y": "uses"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_8",
  "x": "**RESULTS** In Table 1 , we report two numbers for each part of speech. The first, labeled as M13, is the result of applying the vectors of <cite>Mikolov et al. (2013b)</cite> to their test set. The results match the results reported in their paper, except for the nominal results, which reflect our modifications described in Section 2.2. The removal of the possessives improves the accuracy from 25.2% reported in the original paper to 40.1%. The second column, labeled as Ours, reports the results for our vectors, which were trained using WORD2VEC on the English data described in Section 2.1. Our verbal and adjectival vectors obtain slightly lower accuracies than the RNN trained vectors of <cite>Mikolov et al. (2013b)</cite> , but they are not far off. For nouns, however, we obtain higher accuracy than Mikolov et al. The tokenization method that removes possessives from consideration may produce better vectors for singular and plural forms, as it increases the frequency of these types. ----------------------------------",
  "y": "similarities differences"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_9",
  "x": "In order to make results between multiple languages comparable, we made several changes to the construction of syntactic analogy questions. We follow the methodology of <cite>Mikolov et al. (2013b)</cite> in limiting analogy questions to the 100 most frequent verbs or nouns. The frequencies are obtained from corpora tagged by TREETAGGER (Schmid, 1994) . We identify inflections using manually constructed inflection tables from several sources. Spanish and German verbal inflections, as well as German nominal inflections, are from a Wiktionary data set introduced by Durrett and DeNero (2013) . 4 Dutch verbal inflections and English verbal and nominal inflections are from the CELEX database (Baayen et al., 1995) . French verbal inflections are from Verbiste, an online French conjugation dictionary. 5 Whereas Mikolov et al. create analogies from various inflectional forms, we require the analogies to always include the base dictionary form: the infinitive for verbs, and the nominative singular for nouns. In other words, all analogies are limited to 4 We exclude Finnish because of its high morphological complexity and the small size of the corresponding Polyglot corpus.",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_0",
  "x": "The N-gram-based SMT framework is based on tuples. Tuples are minimal translation units composed of source and target cepts 2 . N-gram-based models are Markov models over sequences of tuples or operations encapsulating tuples<cite> (Durrani et al., 2011)</cite> . This mechanism has several useful properties. Firstly, no phrasal independence assumption is made. The model has access to both source and target context outside of phrases. Secondly the model learns a unique derivation of a bilingual sentence given its alignment, thus avoiding the spurious segmentation problem. Using minimal translation units, however, results in a higher number of search errors because of i) 1 A phrase-pair in PBSMT is a sequence of source and target words that is translation of each other, and is not necessarily a linguistic constituent. Phrases are built by combining minimal translation units and ordering information.",
  "y": "background"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_1",
  "x": "Phrases are built by combining minimal translation units and ordering information. 2 A cept is a group of words in one language that is translated as a minimal unit in one specific context (Brown et al., 1993). poor translation selection, ii) inaccurate future-cost estimates and iii) incorrect early pruning of hypotheses that would produce better model scores if allowed to continue. In order to deal with these problems, search is carried out only on a graph of pre-calculated orderings, and ad-hoc reordering limits are imposed to constrain the search space (Crego et al., 2005; , or a higher beam size is used in decoding<cite> (Durrani et al., 2011)</cite> . The ability to memorize and produce larger translation chunks during decoding, on the other hand, gives a distinct advantage to the phrase-based system during search. Phrase-based systems i) have access to uncommon translations, ii) do not require higher beam sizes, iii) have more accurate future-cost estimates because of the availability of phrase-internal language model context before search is started. To illustrate this consider the German-English phrase-pair \"scho\u00df ein Torscored a goal\", composed from the tuples (ceptpairs) \"scho\u00df -scored\", \"ein -a\" and \"Tor -goal\". It is likely that the N-gram system does not have the tuple \"scho\u00df -scored\" in its n-best translation options because \"scored\" is an uncommon translation for \"scho\u00df\" outside the sports domain. Even if \"scho\u00df -scored\" is hypothesized, it will be ranked quite low in the stack until \"ein\" and \"Tor\" are generated in the next steps.",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_2",
  "x": "To illustrate this consider the German-English phrase-pair \"scho\u00df ein Torscored a goal\", composed from the tuples (ceptpairs) \"scho\u00df -scored\", \"ein -a\" and \"Tor -goal\". It is likely that the N-gram system does not have the tuple \"scho\u00df -scored\" in its n-best translation options because \"scored\" is an uncommon translation for \"scho\u00df\" outside the sports domain. Even if \"scho\u00df -scored\" is hypothesized, it will be ranked quite low in the stack until \"ein\" and \"Tor\" are generated in the next steps. A higher beam is required to prevent it from getting pruned. Phrase-based systems, on the other hand, are likely to have access to the phrasal unit \"scho\u00df ein Tor -scored a goal\" and can generate it in a single step. Moreover, a more accurate future-cost estimate can be computed because of the available context internal to the phrase. In this work, we extend the N-gram model, based on operation sequences<cite> (Durrani et al., 2011)</cite> , to use phrases during decoding. The main idea is to study whether a combination of modeling with minimal translation units and using phrasal information during decoding helps to solve the above-mentioned problems. The remainder of this paper is organized as follows.",
  "y": "extends"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_3",
  "x": "The POS-based rewrite rules serve to precompute the orderings that are hypothesized during decoding. Coupling reordering and search allows the N-gram model to arrange hypotheses in 2 m stacks (for an m word source sentence), each containing hypotheses that cover exactly the same foreign words. This removes the need for futurecost estimation 3 . Secondly, memorizing POS-based rules enables phrase-based like reordering, however without lexical selection. There are three drawbacks of this approach. Firstly, lexical generation and reordering are decoupled. Search is only performed on a small number of reorderings, pre-calculated using the source side and completely ignoring the targetside. And lastly, the POS-based rules face data sparsity problems especially in the case of long distance reorderings. <cite>Durrani et al. (2011)</cite> recently addressed these problems by proposing an operation sequence Ngram model which strongly couples translation and reordering, hypothesizes all possible reorderings and does not require POS-based rules.",
  "y": "background"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_4",
  "x": "where n indicates the amount of context used. The translation model is implemented as an N-gram model of operations using SRILM-Toolkit (Stolcke, 2002) with Kneser-Ney smoothing. A 9-gram model is used. Several count-based features such as gap and open gap penalties and distance-based features such as gap-width and reordering distance are added to the model, along with the lexical weighting and length penalty features in a standard log-linear framework<cite> (Durrani et al., 2011)</cite> . ---------------------------------- **SEARCH 4.1 OVERVIEW OF DECODING FRAMEWORK** The decoding framework used in the operation sequence model is based on Pharaoh (Koehn, 2004a) . The decoder uses beam search to build up the translation from left to right. The hypotheses are arranged in m stacks such that stack i maintains hypotheses that have already translated i many foreign words.",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_5",
  "x": "The reordering operations (gaps and jumps) are generated by looking at the position of the translator, the last foreign word generated etc. (Refer to Algorithm 1 in <cite>Durrani et al. (2011)</cite> ). The probability of an operation depends on the n \u2212 1 previous operations. The model backs-off to the smaller n-grams of operations if the full history is unknown. We use Kneser-Ney smoothing to handle back-off 5 . ---------------------------------- **DRAWBACKS OF CEPT-BASED DECODING** One of the main drawbacks of the operation sequence model is that it has a more difficult search problem than the phrase-based model. The operation model, although based on minimal translation units, can learn larger translation chunks by memorizing a sequence of operations.",
  "y": "background"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_6",
  "x": "The hypotheses are arranged in m stacks such that stack i maintains hypotheses that have already translated i many foreign words. The ultimate goal is to find the best scoring hypothesis, that has translated all the words in the foreign sentence. The overall process can be roughly divided into the following steps: i) extraction of translation units ii) future-cost estimation, iii) hypothesis extension iv) recombination and pruning. During the hypothesis extension each extracted phrase is translated into a sequence of operations. The reordering operations (gaps and jumps) are generated by looking at the position of the translator, the last foreign word generated etc. (Refer to Algorithm 1 in <cite>Durrani et al. (2011)</cite> ). The probability of an operation depends on the n \u2212 1 previous operations. The model backs-off to the smaller n-grams of operations if the full history is unknown. We use Kneser-Ney smoothing to handle back-off 5 .",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_7",
  "x": "The future-cost estimates computed in this manner are much more accurate because they not only consider context, but also take the reordering operations into account. ---------------------------------- **N-GRAM MODEL WITH PHRASE-BASED DECODING** In the last section we discussed the disadvantages of using cepts during search in a left-to-right decoding framework. We now define a method to empirically study the mentioned drawbacks and whether using information available in phrase-pairs during decoding can help improve search accuracy and translation quality. ---------------------------------- **TRAINING** We extended the training steps in <cite>Durrani et al. (2011)</cite> to extract a phrase lexicon from the parallel data. We extract all phrase pairs of length 6 and below, that are consistent (Och et al., 1999) with the word alignments.",
  "y": "extends"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_8",
  "x": "---------------------------------- **DECODING** We extended the decoder developed by <cite>Durrani et al. (2011)</cite> and tried three ideas. In our primary experiments we enabled the decoder to use phrases instead of cepts. This allows the decoder to i) use phraseinternal context when computing the future-cost estimates, ii) hypothesize translation options not available to the cept-based decoder iii) cover multiple source words in a single step subsequently improving translation coverage and search. Note that using phrases instead of cepts during decoding, does not reintroduce the spurious phrasal segmentation problem as is present in the phrase-based system, because the model is built on minimal units which avoids segmentation ambiguity. Different compositions of the same phrasal unit lead to exactly the same model score. We therefore do not create any alternative compositions of the same phrasal unit during decoding. This option is not available in phrase-based decoding, because an alternative composition may lead towards a better model score.",
  "y": "extends"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_9",
  "x": "Translation quality is measured through BLEU (Papineni et al., 2002) . ---------------------------------- **EXPERIMENTAL SETUP** We initially experimented with two language pairs: German-to-English (G-E) and French-to-English (F-E). We trained our system and the baseline systems on most of the data 6 made available for the translation task of the Fourth Workshop on Statistical Machine Translation. 7 We used 1M bilingual sentences, for the estimation of the translation model and 2M sentences from the monolingual corpus (news commentary) which also contains the English part of the bilingual corpus. Word alignments are obtained by running GIZA++ (Och and Ney, 2003) with the grow-diag-final-and (Koehn et al., 2005) symmetrization heuristic. We follow the training steps described in <cite>Durrani et al. (2011)</cite> , consisting of i) post-processing the alignments to remove discontinuous and unaligned target cepts, ii) conversion of bilingual alignments into operation sequences, iii) estimation of the n-gram language models. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_10",
  "x": "We found that using a stack size of 200 10 for the phrase-based decoder was comparable in speed to using a stacksize of 500 in the cept-based decoding. We first tuned the baseline on dev 11 to obtain an optimized weight vector. We then ran the baseline and our decoders as discussed in Section 5.2 on the dev-test. Then we repeated this experiment by tuning the weights with our phrase-based decoder (using a stack size of 100) and ran all the decoders again using the new weights. Table 1 shows the average search accuracies and BLEU scores of the two experiments. Using phrases during decoding in the G-E experiments resulted in a statistically significant 12 0.69 BLEU points gain comparing our best system phrase.200 with the baseline system cept.500. We mark a result as sig-8 Discontinuous source-side units did not lead to any improvements in<cite> (Durrani et al., 2011)</cite> and increased the decoding times by multiple folds. We also found these to be less useful. 9 Imposing a hard reordering limit significantly reduced the decoding time and also slightly increased the BLEU scores.",
  "y": "differences"
 },
 {
  "id": "b21dfcb9854b0b48af47f4f13899b0_0",
  "x": "While multiple intelligent writing assistants have been developed (Writelab, 2015; Draft, 2015; Turnitin, 2016) , they typically focus on the quality of the current essay instead of the revisions that have been made. For example, Turnitin identifies weak points of the essay and gives suggestions on how to improve them; it also assigns an overall score to the essay so students can get a coarse-grained feedback on whether they are making progress in their revisions. However, without explicit feedback on each change, students may inefficiently search for a way to optimize the automatic score rather than actively making the existing revisions \"better\". Moreover, because students are the target users of these systems, instructors typically can neither correct the errors made by the automatic analysis nor observe/assess the students' revision efforts. We argue that an intelligent writing assistant ought to be aware of the revision process; it should: 1) identify all significant changes made by a writer between the essay drafts, 2) automatically determine the purposes of these changes, 3) provide the writer the means to compare between drafts in an easy to understand visualization, and 4) support instructor monitoring and corrections in the revision process as well. In our previous work (Zhang and Litman, 2014; <cite>Zhang and Litman, 2015)</cite> , we focused on 1) and 2), the automatic extraction and classification of revisions for argumentative writings. In this work, we extend our framework to integrate the automatic analyzer with a web-based interface to support student argumentative writings. The purpose of each change between revisions is demonstrated to the writer as a kind of feedback. If the author's revision purpose is not correctly recognized, it indicates that the effect of the writer's change might have not met the writer's expectation, which suggests that the writer should revise their revisions.",
  "y": "background"
 },
 {
  "id": "b21dfcb9854b0b48af47f4f13899b0_1",
  "x": "Making revisions is central to improving a student's writings, especially when there is a helpful instructor to offer detailed feedback between drafts. However, it is not practical for instructors to provide feedback on every change every time. While multiple intelligent writing assistants have been developed (Writelab, 2015; Draft, 2015; Turnitin, 2016) , they typically focus on the quality of the current essay instead of the revisions that have been made. For example, Turnitin identifies weak points of the essay and gives suggestions on how to improve them; it also assigns an overall score to the essay so students can get a coarse-grained feedback on whether they are making progress in their revisions. However, without explicit feedback on each change, students may inefficiently search for a way to optimize the automatic score rather than actively making the existing revisions \"better\". Moreover, because students are the target users of these systems, instructors typically can neither correct the errors made by the automatic analysis nor observe/assess the students' revision efforts. We argue that an intelligent writing assistant ought to be aware of the revision process; it should: 1) identify all significant changes made by a writer between the essay drafts, 2) automatically determine the purposes of these changes, 3) provide the writer the means to compare between drafts in an easy to understand visualization, and 4) support instructor monitoring and corrections in the revision process as well. In our previous work (Zhang and Litman, 2014; <cite>Zhang and Litman, 2015)</cite> , we focused on 1) and 2), the automatic extraction and classification of revisions for argumentative writings. In this work, we extend our framework to integrate the automatic analyzer with a web-based interface to support student argumentative writings.",
  "y": "extends background"
 },
 {
  "id": "b21dfcb9854b0b48af47f4f13899b0_2",
  "x": "aligned pairs where the sentences in the pair are not identical are extracted as revisions. We first use the Stanford Parser (Klein and Manning, 2003) to break the original text into sentences and then align the sentences using the algorithm in our prior work (Zhang and Litman, 2014) which considers both sentence similarity (calculated using TF*IDF score) and the global context of sentences. Revision classification. Following the argumentative revision definition in our prior work<cite> (Zhang and Litman, 2015)</cite> , revisions are first categorized to Content (Text-based) and Surface 3 according to whether the revision changed the meaning of the essay or not. The Text-based revisions include Thesis/Ideas (Claim), Rebuttal, Reasoning (Warrant), Evidence, and Other content changes (General Content). The Surface revisions include Fluency (Wordusage/Clarity), Reordering (Organization) and Errors (Conventions/Grammar/Spelling). On the basis of later work, the system includes the two new categories Precision 4 and Unknown 5 . Using the corpora and features defined in our prior work, a multiclass Random Forest classifier was trained to automatically predict the revision purpose type for each extracted revision. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "b21dfcb9854b0b48af47f4f13899b0_3",
  "x": "---------------------------------- **REWRITING ASSISTANT INTERFACE** Our rewriting assistant interface is designed with several principles in mind. 1) Because the revision classification taxonomy goes beyond the binary textual versus surface distinction, we want to make sure that users don't get lost distinguishing different categories; 2) We want to encourage users to think about their revisions holistically, not always just focusing on low-level details; 3) We want to encourage users to continuously re-evaluate whether they succeeded in making changes between drafts (rather than focusing on generating new contents). Thus, we have designed an interface that offers multiple views of the revision changes. As demonstrated in Figure 2 , the interface includes a revision overview interface for the overview of the authors' revisions and a revision detail interface that allows the author to access the details of their essays and revisions. Inspired by works on learning analytics (Liu et al., 2013; Verbert et al., 2013) , we design the revision overview interface which displays the statistics of the revisions. Following design principle #1, the revision purposes are color coded and each purpose corresponds to a specific color. Our prior work<cite> (Zhang and Litman, 2015)</cite> demonstrates that only Text-based revisions are significantly correlated with the writing improvement.",
  "y": "background"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_0",
  "x": "---------------------------------- **RELATED WORK** From a synchronic perspective, <cite>Reddy et al. (2011</cite> ), Schulte im Walde et al. (2013 and Schulte im Walde et al. (2016a) are closest to our approach, since they predict the compositionality of compounds using vector space representations. However, Schulte im Walde et al. (2013) use German data and do not investigate diachronic changes. They report a Spearman's \u03c1 of 0.65 for predicting the compositionality of compounds based on the features of their semantic space and find that the modifiers mainly influence the compositionality of the whole compound, contrary to their expectation that the head should be the main source of influence. This is true for both the human annotation and their vector space model. Schulte im Walde et al. (2016a) further investigate the role of heads and modifiers on the prediction of compositionality and report \u03c1 values between 0.35 and 0.61 for their models on German and English data. Reddy et al. (2011) also report Spearman's \u03c1 between their surveyed compositionality values and word vectors. They achieve \u03c1 values of around 0.68, depending on the model.",
  "y": "similarities background"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_1",
  "x": "Several studies have been conducted in order to measure compositionality for compounds in different languages (von der Heide and Borgwaldt, 2009;<cite> Reddy et al., 2011</cite>; Schulte im Walde et al., 2016b) . Some of these works have used large corpora to extract vector-based representations of compounds and their parts to automatically determine the compositionality of a given compound. The models were validated on the basis of their correlation with human compositionality ratings for a set of compounds. Because we are interested in the diachronic perspective on compounds, we use a time-stamped corpus: the Google Books Ngram corpus 2 (Michel et al., 2011) It contains books from the 1500s to the 2000s, from which we retrieve the contextual information of compounds and their constituents per year. We operate on 5-grams, which is the largest unit provided by Google Ngrams and use the words appearing in the 5-grams as both target words and context. We use the Part-of-Speech information already included in the Google Ngram corpus to extract noun-noun patterns. We then regard all other tokens in the 5-gram as context words and from this build up a semantic space rep-resentation of noun compounds for each year. We use a sliding window approach, wherein we capture the context of a compound based on its position in the 5-gram. That means that a bigram (say the compound gold mine) could occur in four different positions in the 5-grams (1-2, 2-3, 3-4 and finally 4-5).",
  "y": "background"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_2",
  "x": "Because we are interested in the diachronic perspective on compounds, we use a time-stamped corpus: the Google Books Ngram corpus 2 (Michel et al., 2011) It contains books from the 1500s to the 2000s, from which we retrieve the contextual information of compounds and their constituents per year. We operate on 5-grams, which is the largest unit provided by Google Ngrams and use the words appearing in the 5-grams as both target words and context. We use the Part-of-Speech information already included in the Google Ngram corpus to extract noun-noun patterns. We then regard all other tokens in the 5-gram as context words and from this build up a semantic space rep-resentation of noun compounds for each year. We use a sliding window approach, wherein we capture the context of a compound based on its position in the 5-gram. That means that a bigram (say the compound gold mine) could occur in four different positions in the 5-grams (1-2, 2-3, 3-4 and finally 4-5). We then capture the contexts for each of these positions, in order to enrich the representation of a compound and its constituents (which similarly have five such positions, as they are unigrams). Ideally, we would validate our diachronic model on diachronic test data. However, as it is not possible to survey compositionality rating for diachronic data, we instead use the synchronic data provided by<cite> Reddy et al. (2011)</cite> (henceforth referred to as REDDY) for evaluating the quality of the Google Books Ngram data as a source for investigating the compositionality of compounds in general.",
  "y": "uses"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_3",
  "x": "---------------------------------- **CORRELATION** We first carry out a quantitative experiment, to see if our features bolster the prediction of compositionality in noun-noun compounds. To do so, we calculate correlation scores between our proposed metrics and the annotated compositionality ratings of REDDY. Like<cite> Reddy et al. (2011)</cite> and Schulte im Walde et al. (2013), we opt for Spearman's \u03c1. To find the best configuration of a time span and cut-off for the regression models, we use the R 2 metric. Table 1 presents our findings; we will discuss them in the following Section 5. ---------------------------------- **PROGRESSION OF COMPOSITIONALITY OVER TIME**",
  "y": "similarities"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_4",
  "x": "We find the best predictors for the compositionality ratings of REDDY to be LMI and LLR (compare Table 2 ). The overall highest correlation occurs between compound-mean and LMI with \u03c1 of 0.54. We also see that sim-bw-constituents and sim-with-heads are generally good predictors as well. Contrary to Schulte im Walde et al. (2013) we do not find a strong correlation between modifiers and the REDDY ratings. Interestingly, PPMI is always weakly negatively correlated with the ratings. This could be due to PPMI's property of inflating scores for rare events. As can also be seen from Table 2 , our correlation values are considerably lower than that of<cite> Reddy et al. (2011)</cite> , but on par with a replication study by Schulte im Walde et al. (2016a) for compound-mean. We speculate that these differences are potentially due to the use of different data sets, the fact that we use a considerably smaller context window for constructing the word vectors (5 due to the restrictions of Google Ngram corpus vs. 100 in<cite> Reddy et al. (2011)</cite> and 40 in Schulte im Walde et al. (2016b) ) and the use of a compound-centric setting (as described in 4.1). From Table 1 we see that our best reported R 2 value occurs when observing time in stretches of 20 years (scores) and compounds having a frequency cut-off of at least 100.",
  "y": "differences"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_5",
  "x": "The overall highest correlation occurs between compound-mean and LMI with \u03c1 of 0.54. We also see that sim-bw-constituents and sim-with-heads are generally good predictors as well. Contrary to Schulte im Walde et al. (2013) we do not find a strong correlation between modifiers and the REDDY ratings. Interestingly, PPMI is always weakly negatively correlated with the ratings. This could be due to PPMI's property of inflating scores for rare events. As can also be seen from Table 2 , our correlation values are considerably lower than that of<cite> Reddy et al. (2011)</cite> , but on par with a replication study by Schulte im Walde et al. (2016a) for compound-mean. We speculate that these differences are potentially due to the use of different data sets, the fact that we use a considerably smaller context window for constructing the word vectors (5 due to the restrictions of Google Ngram corpus vs. 100 in<cite> Reddy et al. (2011)</cite> and 40 in Schulte im Walde et al. (2016b) ) and the use of a compound-centric setting (as described in 4.1). From Table 1 we see that our best reported R 2 value occurs when observing time in stretches of 20 years (scores) and compounds having a frequency cut-off of at least 100. A few other observations could be made: In general the cut-off seems to improve the R 2 metric and the time spans of 10 and 20 years appear to be the most informative and stable for the cut-off values.",
  "y": "differences"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_6",
  "x": "Each group contains around 30 compounds. We then plot the LMI values of these three groups with their confidence interval across the time step of 20 years, shown in Figure 1 . We can observe that there is a separation between the groups towards the later years, and that the period between 1940s and 1960s caused a noticeable change in the compositionality of the REDDY compounds. We find the same trends for all three information theory based features. Although care should be taken given the small data sets (especially for the earlier decades) on which the models were build and tested, the slope of the lines for the three groups of compounds seems to suggest that less compositional compounds go through a more pronounced change in compositionality than compositional compounds, as expected. We also show the graphs for sim-with-head and sim-with-mod (Figures 2 and 3) for the different groups of compounds across time, as these underperformed in our previous experiment. Both figures based on cosine based features largely confound the three groups of compounds across time, reinforcing our previous findings. 6 Future Work Our current work was limited to English compounds from<cite> Reddy et al. (2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_0",
  "x": "****AN IMPROVED TAG DICTIONARY FOR FASTER PART-OF-SPEECH TAGGING**** **ABSTRACT** Abstract Ratnaparkhi (1996) introduced a method of inferring a tag dictionary from annotated data to speed up part-of-speech tagging by limiting the set of possible tags for each word. While Ratnaparkhi's tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed<cite> (Moore, 2014)</cite> makes tagging as fast as with Ratnaparkhi's tag dictionary, but with no decrease in accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging-more than 100,000 tokens per second in Perl. In this paper, we present a new method of constructing tag dictionaries for part-of-speech (POS) tagging. A tag dictionary is simply a list of words 1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed. Tag dictionaries are commonly used to speed up POS-tag inference by restricting the tags considered for a particular word to those specified by the dictionary. Early work on POS tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus (Leech et al., 1983; Church, 1988; Cutting et al., 1992) .",
  "y": "motivation background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_1",
  "x": "2 Ratnaparkhi (1996) seems to have been the first to use a tag dictionary automatically extracted only from training data. Ratnaparkhi's method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state-of-the-art tagging model. We recently presented<cite> (Moore, 2014)</cite> a new method of constructing a tag dictionary that produces a tagging speed-up comparable to Ratnaparkhi's, but with no decrease in tagging accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as we previously obtained, while allowing much faster tagging-more than 100,000 tokens per second even in a Perl implementation. ---------------------------------- **** introduced a method of inferring a tag dictionary from annotated data to speed up part-of-speech tagging by limiting the set of possible tags for each word. While Ratnaparkhi's tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed <cite>(Moore, 2014</cite> ) makes tagging as fast as with Ratnaparkhi's tag dictionary, but with no decrease in accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging-more than 100,000 tokens per second in Perl.",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_2",
  "x": "tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was not really fair. 2 Ratnaparkhi (1996) seems to have been the first to use a tag dictionary automatically extracted only from training data. Ratnaparkhi's method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state-of-the-art tagging model. We recently presented<cite> (Moore, 2014)</cite> a new method of constructing a tag dictionary that produces a tagging speed-up comparable to Ratnaparkhi's, but with no decrease in tagging accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as we previously obtained, while allowing much faster tagging-more than 100,000 tokens per second even in a Perl implementation. ---------------------------------- **** introduced a method of inferring a tag dictionary from annotated data to speed up part-of-speech tagging by limiting the set of possible tags for each word. While Ratnaparkhi's tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed <cite>(Moore, 2014</cite> ) makes tagging as fast as with Ratnaparkhi's tag dictionary, but with no decrease in accuracy.",
  "y": "differences motivation"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_3",
  "x": "**OVERVIEW** In this paper, we present a new method of constructing tag dictionaries for part-of-speech (POS) tagging. A tag dictionary is simply a list of words 1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed. Tag dictionaries are commonly used to speed up POS-tag inference by restricting the tags considered for a particular word to those specified by the dictionary. Early work on POS tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus (Leech et al., 1983; Church, 1988; Cutting et al., 1992) . Merialdo (1994) relied only on a tag dictionary extracted from annotated data, but he used the annotated tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was not really fair. 2 Ratnaparkhi (1996) seems to have been the first to use a tag dictionary automatically extracted only from training data. Ratnaparkhi's method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state-of-the-art tagging model. We recently presented<cite> (Moore, 2014)</cite> a new method of constructing a tag dictionary that produces a tagging speed-up comparable to Ratnaparkhi's, but with no decrease in tagging accuracy.",
  "y": "differences motivation"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_4",
  "x": "---------------------------------- **RATNAPARKHI'S METHOD** For each word observed in an annotated training set, Ratnaparkhi's tag dictionary includes all tags observed with that word in the training set, with all possible tags allowed for all other words. Ratnaparkhi reported that using this tag dictionary improved per-tag accuracy from 96.31% to 96.43% on his Penn Treebank (Marcus et al., 1993) Wall Street Journal (WSJ) development set, compared to considering all tags for all words. With a more accurate model, however, we found <cite>(Moore, 2014</cite> ) that while Ratnaparkhi's tag dictionary decreased the average number of tags per token from 45 to 3.7 on the current standard WSJ development set, it also decreased per-tag accuracy from 97.31% to 97.19%. This loss of accuracy can be explained by the fact that 0.5% of the development set tokens are known words with a tag not seen in the training set, for which our model achieved 44.5% accuracy with all word/tag pairs permitted. With Ratnaparkhi's dictionary, accuracy for these tokens is necessarily 0%. ---------------------------------- **OUR PREVIOUS METHOD**",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_5",
  "x": "With Ratnaparkhi's dictionary, accuracy for these tokens is necessarily 0%. ---------------------------------- **OUR PREVIOUS METHOD** We previously presented<cite> (Moore, 2014)</cite> a tag dictionary constructed by using the annotated training set to compute a smoothed probability estimate for any possible tag given any possible word, and for each word in the training set, including in the dictionary the tags having an estimated probability greater than a fixed threshold T . In this approach, the probability p(t|w) of tag t given word w is computed by interpolating a discounted relative frequency estimate of p(t|w) with an estimate of p(t) based on \"diversity counts\", taking the count of a tag t to be the number of distinct words ever observed with that tag. The distribution p(t) is also used to estimate tag probabilities for unknown words, so the set of possible tags for any word not explicitly listed is {t|p(t) > T }. If we think of w followed by t as a word bigram, this is exactly like a bigram language model estimated by the interpolated Kneser-Ney (KN) method described by Chen and Goodman (1999) . The way tag diversity counts are used has the desirable property that closed-class tags receive a very low estimated probability of being assigned to a rare or unknown word, even though they occur very often with a small number of frequent words. A single value for discounting the count of all observed word/tag pairs is set to maximize the estimated probability of the reference tagging of the development set.",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_6",
  "x": "Word-class-sequence features obtained by supervised clustering of the annotated training set replace the hidden tag-sequence features frequently used for POS tagging, and additional word-class features obtained by unsupervised clustering of a very large unannotated corpus provide information about words not occurring in the training set. For full details of the feature set, see our previous paper<cite> (Moore, 2014)</cite> . The model is trained by optimizing the multiclass SVM hinge loss objective (Crammer and Singer, 2001 ), using stochastic subgradient descent as described by Zhang (2004) , with early stopping and averaging. The only difference from our previous training procedure is that we now use a tag dictionary to speed up training, while we previously used tag dictionaries only at test time. Our training procedure makes multiple passes through the training data considering each training example in turn, comparing the current model score of the correct tag for the example to that of the highest scoring incorrect tag and updating the model if the score of the correct tag does not exceed the score of the highest scoring incorrect tag by a specified margin. In our new version of this procedure, we use the KN-smoothed tag dictionary described in Section 1.3. to speed up finding the highest scoring incorrect tag. Recall that the KN-smoothed tag dictionary estimates a non-zero probability p(t|w) for every possible word/tag pair, and that the possible tags for a given word are determinted by setting a threshold T on this probability. In each pass through the training set, we use the same probability distribution p(t|w) determined from the statistics of the annotated training data, but we employ an adaptive method to determine what threshold T to use in each pass. For the first pass through the training set, we set an initial threshold T 0 to the highest value such that for every token in the development set, p(t|w) \u2265 T 0 , where t is the correct tag for the token and w is the word for the token.",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_7",
  "x": "This sped up training by a factor of 3.7 compared to considering all tags for all tokens, with no loss of tagging accuracy when a development-set-optimized KN-smoothed tag dictionary is also used at test time. ---------------------------------- **TAGGING THE GIGAWORD CORPUS** To construct our new tag dictionary, we need an automatically-tagged corpus several orders of magnitude larger than the hand-tagged WSJ training set. To obtain this corpus we ran a POS tagger on the LDC English Gigaword Fifth Edition 3 corpus, which consists of more than 4 billion words of English text from seven newswire sources. We first removed all SGML mark-up, and performed sentence-breaking and tokenization using the Stanford CoreNLP toolkit (Manning et al, 2014) . This produced 4,471,025,373 tokens of Table 1 : WSJ development set token accuracy and tagging speed for different tag dictionaries 6,616,812 unique words. We tagged this corpus using the model described in Section 2.1 and a KN-smoothed tag dictionary as described in Section 1.3, with a threshold T = 0.0005. The tagger we used is based on the fastest of the methods described in our previous work <cite>(Moore, 2014</cite>, Section 3.1) .",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_8",
  "x": "**EXPERIMENTAL RESULTS** Tagging the WSJ development set with an unpruned semi-supervised tag dictionary obtained from the automatic tagging of the English Gigaword corpus produced the same tagging accuracy as allowing all tags for all tokens or using the pruned KN-smoothed tag dictionary used in tagging the Gigaword corpus. Additional experiments showed that we could prune this dictionary with a threshold on p(t|w) as high as T = 0.0024 without decreasing development set accuracy. In addition to applying this threshold to the tag probabilities for all listed words, we also applied it to the tag probabilities for unknown words, leaving 13 possible tags 5 for those. Tagging the WSJ development set with these two dictionaries is compared in Table 1 to tagging with our previous pruned KN-smoothed dictionary. The second column shows the accuracy per tag, which is 97.31% for all three dictionaries. The third column shows the mean number of tags per token allowed by each dictionary. The fourth column shows the percentage of tokens with only one tag allowed, which is significant since the tagger need not apply the model for such tokens-it can simply output the single possible tag. The last column shows the tagging speed in tokens per second for each of the three tag dictionaries, using the fast tagging method we previously described<cite> (Moore, 2014)</cite> , in a singlethreaded implementation in Perl on a Linux workstation equipped with Intel Xeon X5550 2.67 GHz processors.",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_9",
  "x": "For the pruned KN-smoothed dictionary, we previously reported a speed of 49,000 tokens per second under similar conditions. Our current faster speed of 69,000 tokens per second is due to an improved low-level implementation for computing the model scores for permitted tags, and a slightly faster version of Perl (v5.18.2) . The most restrictive tag dictionary, the pruned semi-supervised dictionary, allows only 1.51 tags per token, and our implementation runs at 103,000 tokens per second on the WSJ development set. For our final experiments, we tested our tagger with this dictionary on the standard Penn Treebank WSJ test set and on the Penn Treebank-3 parsed Brown corpus subset, as an out-of-domain evaluation. For comparison, we tested our previous tagger and the fast version (english-left3words-distsim) of the Stanford tagger (Toutanova et al., 2003; Manning, 2011) recommended for practical use on the Stanford tagger website, which we found to be by far the fastest of the six publicly available taggers tested in our previous work<cite> (Moore, 2014)</cite> . The results of these tests are shown in Table 2 Table 2 : WSJ test set and Brown corpus tagging speeds and token accuracies For our previous tagger, we give three speeds: the speed we reported earlier, a speed for a duplicate of the earlier experiment using the faster version of Perl that we use here, and a third measurement including both the faster version of Perl and our improved low-level tagger implementation. With the pruned semi-supervised dictionary, our new tagger has slightly higher all-token accuracy than our previous tagger on both the WSJ test set and Brown corpus set, and it is much more accurate than the fast Stanford tagger. The accuracy on the standard WSJ test set is 97.36%, one of the highest ever reported.",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_10",
  "x": "The results of these tests are shown in Table 2 Table 2 : WSJ test set and Brown corpus tagging speeds and token accuracies For our previous tagger, we give three speeds: the speed we reported earlier, a speed for a duplicate of the earlier experiment using the faster version of Perl that we use here, and a third measurement including both the faster version of Perl and our improved low-level tagger implementation. With the pruned semi-supervised dictionary, our new tagger has slightly higher all-token accuracy than our previous tagger on both the WSJ test set and Brown corpus set, and it is much more accurate than the fast Stanford tagger. The accuracy on the standard WSJ test set is 97.36%, one of the highest ever reported. The new tagger is also much faster than either of the other taggers, achieving a speed of more than 100,000 tokens per second on the WSJ test set, and almost 100,000 tokens per second on the out-of-domain Brown corpus data. ---------------------------------- **CONCLUSIONS** Our method of constructing a tag dictionary is technically very simple, but remarkably effective. It reduces the mean number of possible tags per token by 57% and increases the number of unambiguous tokens by by 47%, compared to the previous state of the art<cite> (Moore, 2014)</cite> for a tag dictionary that does not degrade tagging accuracy.",
  "y": "differences"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_0",
  "x": "We consider PropBank-style (Palmer et al., 2005) semantic role structures, or more specifi- cally their dependency versions (Surdeanu et al., 2008) . The semantic-role representations mark semantic arguments of predicates in a sentence and categorize them according to their semantic roles. Consider Figure 1 , the predicate gave has three arguments: 1 John (semantic role A0, 'the giver'), wife (A2, 'an entity given to') and present (A1, 'the thing given'). Semantic roles capture commonalities between different realizations of the same underlying predicate-argument structures. For example, present will still be A1 in sentence \"John gave a nice present to his wonderful wife\", despite different surface forms of the two sentences. We hypothesize that semantic roles can be especially beneficial in NMT, as 'argument switching' (flipping arguments corresponding to different roles) is one of frequent and severe mistakes made by NMT systems (Isabelle et al., 2017) . There is a limited amount of work on incorporating graph structures into neural sequence models. Though, unlike semantics in NMT, syntactically-aware NMT has been a relatively hot topic recently, with a number of approaches claiming improvements from using treebank syntax Eriguchi et al., 2016; Nadejde et al., 2017;<cite> Bastings et al., 2017</cite>; Aharoni and Goldberg, 2017) , our graphs are different from syntactic structures. Unlike syntactic dependency graphs, they are not trees and thus cannot be processed in a bottom-up fashion as in Eriguchi et al. (2016) or easily linearized as in Aharoni and Goldberg (2017) .",
  "y": "differences background"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_1",
  "x": "We hypothesize that semantic roles can be especially beneficial in NMT, as 'argument switching' (flipping arguments corresponding to different roles) is one of frequent and severe mistakes made by NMT systems (Isabelle et al., 2017) . There is a limited amount of work on incorporating graph structures into neural sequence models. Though, unlike semantics in NMT, syntactically-aware NMT has been a relatively hot topic recently, with a number of approaches claiming improvements from using treebank syntax Eriguchi et al., 2016; Nadejde et al., 2017;<cite> Bastings et al., 2017</cite>; Aharoni and Goldberg, 2017) , our graphs are different from syntactic structures. Unlike syntactic dependency graphs, they are not trees and thus cannot be processed in a bottom-up fashion as in Eriguchi et al. (2016) or easily linearized as in Aharoni and Goldberg (2017) . Luckily, the modeling approach of <cite>Bastings et al. (2017)</cite> does not make any assumptions about the graph structure, and thus we build on their method. <cite>Bastings et al. (2017)</cite> used Graph Convolutional Networks (GCNs) to encode syntactic structure. GCNs were originally proposed by Kipf and Welling (2016) and modified to handle labeled and automatically predicted (hence noisy) syntactic dependency graphs by . Representations of nodes (i.e. words in a sentence) in GCNs are directly influenced by representations of their neighbors in the graph. The form of influence (e.g., transition matrices and parameters of gates) are learned in such a way as to benefit the end task (i.e. translation).",
  "y": "similarities background"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_2",
  "x": "There is a limited amount of work on incorporating graph structures into neural sequence models. Though, unlike semantics in NMT, syntactically-aware NMT has been a relatively hot topic recently, with a number of approaches claiming improvements from using treebank syntax Eriguchi et al., 2016; Nadejde et al., 2017;<cite> Bastings et al., 2017</cite>; Aharoni and Goldberg, 2017) , our graphs are different from syntactic structures. Unlike syntactic dependency graphs, they are not trees and thus cannot be processed in a bottom-up fashion as in Eriguchi et al. (2016) or easily linearized as in Aharoni and Goldberg (2017) . Luckily, the modeling approach of <cite>Bastings et al. (2017)</cite> does not make any assumptions about the graph structure, and thus we build on their method. <cite>Bastings et al. (2017)</cite> used Graph Convolutional Networks (GCNs) to encode syntactic structure. GCNs were originally proposed by Kipf and Welling (2016) and modified to handle labeled and automatically predicted (hence noisy) syntactic dependency graphs by . Representations of nodes (i.e. words in a sentence) in GCNs are directly influenced by representations of their neighbors in the graph. The form of influence (e.g., transition matrices and parameters of gates) are learned in such a way as to benefit the end task (i.e. translation). These linguistically-aware word representations are used within a neural encoder.",
  "y": "background"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_3",
  "x": "Although recent research has shown that neural architectures are able to learn some linguistic phenomena without explicit linguistic supervision (Linzen et al., 2016; Vaswani et al., 2017) , informing word representations with linguistic structures can provide a useful inductive bias. We apply GCNs to the semantic dependency graphs and experiment on the English-German language pair (WMT16). We observe an improvement over the semantics-agnostic baseline (a BiRNN encoder; 23.3 vs 24.5 BLEU). As we use exactly the same modeling approach as in the syntactic method of <cite>Bastings et al. (2017)</cite> , we can easily compare the influence of the types of linguistic structures (i.e., syntax vs. semantics). We observe that when using full WMT data we obtain better results with semantics than with syntax (23.9 BLEU for syntactic GCN). Using syntactic and semantic GCN together, we obtain a further gain (24.9 BLEU) that suggests the complementarity of syntax and semantics. ---------------------------------- **MODEL** ----------------------------------",
  "y": "similarities uses motivation"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_4",
  "x": "At each step, the decoder calculates the probability of generating a word y t conditioning on a context vector c t and the previous state of the RNN decoder. The context vector c t is calculated based on the representation of the source sentence computed by the encoder, using an attention mechanism (Bahdanau et al., 2015) . Such a model is trained end-to-end on a parallel corpus to maximize the conditional likelihood of the target sentences. ---------------------------------- **GRAPH CONVOLUTIONAL NETWORKS** Graph neural networks are a family of neural architectures (Scarselli et al., 2009; Gilmer et al., 2017) specifically devised to induce representation of nodes in a graph relying on its graph structure. Graph convolutional networks (GCNs) belong to this family. While GCNs were introduced BiRNN CNN Baseline<cite> (Bastings et al., 2017)</cite> 14.9 12.6 +Sem 15.6 13.4 +Syn<cite> (Bastings et al., 2017)</cite> 16.1 13.7 +Syn + Sem 15.8 14.3 for modeling undirected unlabeled graphs (Kipf and Welling, 2016) , in this paper we use a formulation of GCNs for labeled directed graphs, where the direction and the label of an edge are incorporated. In particular, we follow the formulation of and <cite>Bastings et al. (2017)</cite> for syntactic graphs and apply it to dependency-based semantic-role structures (Hajic et al., 2009 ) (as in Figure 1 ).",
  "y": "differences background"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_5",
  "x": "Such a model is trained end-to-end on a parallel corpus to maximize the conditional likelihood of the target sentences. ---------------------------------- **GRAPH CONVOLUTIONAL NETWORKS** Graph neural networks are a family of neural architectures (Scarselli et al., 2009; Gilmer et al., 2017) specifically devised to induce representation of nodes in a graph relying on its graph structure. Graph convolutional networks (GCNs) belong to this family. While GCNs were introduced BiRNN CNN Baseline<cite> (Bastings et al., 2017)</cite> 14.9 12.6 +Sem 15.6 13.4 +Syn<cite> (Bastings et al., 2017)</cite> 16.1 13.7 +Syn + Sem 15.8 14.3 for modeling undirected unlabeled graphs (Kipf and Welling, 2016) , in this paper we use a formulation of GCNs for labeled directed graphs, where the direction and the label of an edge are incorporated. In particular, we follow the formulation of and <cite>Bastings et al. (2017)</cite> for syntactic graphs and apply it to dependency-based semantic-role structures (Hajic et al., 2009 ) (as in Figure 1 ). More formally, consider a directed graph G = (V, E), where V is a set of nodes, and E is a set of edges. Each node v \u2208 V is represented by a feature vector x v \u2208 R d , where d is the latent space dimensionality.",
  "y": "uses"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_6",
  "x": "In particular, we follow the formulation of and <cite>Bastings et al. (2017)</cite> for syntactic graphs and apply it to dependency-based semantic-role structures (Hajic et al., 2009 ) (as in Figure 1 ). More formally, consider a directed graph G = (V, E), where V is a set of nodes, and E is a set of edges. Each node v \u2208 V is represented by a feature vector x v \u2208 R d , where d is the latent space dimensionality. The GCN induces a new representation h v \u2208 R d of a node v while relying on representations h u of its neighbors: where N (v) is the set of neighbors of v, W dir(u,v) \u2208 R d\u00d7d is a direction-specific parameter matrix. There are three possible directions (dir(u, v) \u2208 {in, out, loop}): self-loop edges were added in order to ensure that the initial representation of node h v directly affects its new representation h v . The vector b lab(u,v) \u2208 R d is an embedding of a semantic role label of the edge (u, v) (e.g., A0). The functions g u,v are scalar gates which weight the importance of each edge. Gates are particularly useful when the graph is predicted BiRNN Baseline<cite> (Bastings et al., 2017)</cite> 23.3 +Sem 24.5 +Syn<cite> (Bastings et al., 2017)</cite> 23.9 +Syn + Sem 24.9 and thus may contain errors, i.e., wrong edges.",
  "y": "background"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_7",
  "x": "We experimented with the English-to-German WMT16 dataset (\u223c4.5 million sentence pairs for training). We use its subset, News Commentary v11, for development and additional experiments (\u223c226.000 sentence pairs). For all these experiments, we use newstest2015 and newstest2016 as a validation and test set, respectively. We parsed the English partitions of these datasets with a syntactic dependency parser (Andor et al., 2016) and dependency-based semantic role labeler . We constructed the English vocabulary by taking all words with frequency higher than three, while for German we used byte-pair encodings (BPE) (Sennrich et al., 2016). All hyperparameter selection was performed on the validation set (see Appendix A). We measured the performance of the models with (cased) BLEU scores (Papineni et al., 2002) . The settings and the framework (Neural Monkey (Helcl and Libovick\u00fd, 2017) ) used for experiments are the ones used in <cite>Bastings et al. (2017)</cite> , which we use as baselines. As RNNs, we use GRUs (Cho et al., 2014) .",
  "y": "uses"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_8",
  "x": "The settings and the framework (Neural Monkey (Helcl and Libovick\u00fd, 2017) ) used for experiments are the ones used in <cite>Bastings et al. (2017)</cite> , which we use as baselines. As RNNs, we use GRUs (Cho et al., 2014) . We now discuss the impact that different architectures and linguistic information have on the translation quality. ---------------------------------- **RESULTS AND DISCUSSION** First, we start with experiments with the smaller News Commentary training set (See Table 1 ). As in <cite>Bastings et al. (2017)</cite> , we used the standard attention-based encoder-decoder model as a baseline. We tested the impact of semantic GCNs when used on top of CNN and BiRNN encoders. As expected, BiRNN results are stronger than CNN ones.",
  "y": "uses"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_0",
  "x": "org/slowly-growing-offspring-zigglebottom-anno-2017-guest-post/ are evaluated on the SemEval dataset (Pontiki et al., 2014) but not all. Datasets can vary by domain (e.g. product), type (social media, review), or medium (written or spoken), and to date there has been no comparative evaluation of methods from these multiple classes. Our primary and secondary contributions therefore, are to carry out the first study that reports results across all three different dataset classes, and to release a open source code framework implementing three complementary groups of TDSA methods. In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (Mitchell et al., 2013; Zhang et al., 2016; Liu and Zhang, 2017; Wang et al., 2017) but other papers have not released code (Wang et al., 2016; Tay et al., 2017) . In some cases, the code was initially made available, then removed, and is now back online <cite>(Tang et al., 2016a)</cite> . Unfortunately, in some cases even when code has been published, different results have been obtained relative to the original paper. This can be seen when Chen et al. (2017) used the code and embeddings in Tang et al. (2016b) they observe different results. Similarly, when others (Tay et al., 2017; Chen et al., 2017) attempt to replicate the experiments of<cite> Tang et al. (2016a)</cite> they also produce different results to the original authors. Our observations within this one sub-field motivates the need to investigate further and understand how such problems can be avoided in the future.",
  "y": "background"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_1",
  "x": "Similarly, when others (Tay et al., 2017; Chen et al., 2017) attempt to replicate the experiments of<cite> Tang et al. (2016a)</cite> they also produce different results to the original authors. Our observations within this one sub-field motivates the need to investigate further and understand how such problems can be avoided in the future. In some cases, when code has been released, it is difficult to use which could explain why the results were not reproduced. Of course, we would not expect researchers to produce industrial strength code, or provide continuing free ongoing support for multiple years after publication, but the situation is clearly problematic for the development of the new field in general. In this paper, we therefore reproduce three papers chosen as they employ widely differing methods: Neural Pooling (NP) , NP with dependency parsing (Wang et al., 2017) , and RNN <cite>(Tang et al., 2016a)</cite> , as well as having been applied largely to different datasets. At the end of the paper, we reflect on bringing together elements of repeatability and generalisability which we find are crucial to NLP and data science based disciplines more widely to enable others to make use of the science created. ---------------------------------- **RELATED WORK** Reproducibility and replicability have long been key elements of the scientific method, but have been gaining renewed prominence recently across a number of disciplines with attention being given to a 'reproducibility crisis'.",
  "y": "motivation"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_2",
  "x": "Since its inception, papers have applied different methods such as feature based (Kiritchenko et al., 2014) , Recursive Neural Networks (RecNN) (Dong et al., 2014) , Recurrent Neural Networks (RNN) <cite>(Tang et al., 2016a)</cite> , attention applied to RNN (Wang et al., 2016; Chen et al., 2017; Tay et al., 2017) , Neural Pooling (NP) Wang et al., 2017) , RNN combined with NP (Zhang et al., 2016) , and attention based neural networks (Tang et al., 2016b) . Others have tackled TDSA as a joint task with target extraction, thus treating it as a sequence labelling problem. Mitchell et al. (2013) carried out this task using Conditional Random Fields (CRF), and this work was then extended using a neural CRF . Both approaches found that combining the two tasks did not improve results compared to treating the two tasks separately, apart from when considering POS and NEG when the joint task performs better. Finally, created an attention RNN for this task which was evaluated on two very different datasets containing written and spoken (video-based) reviews where the domain adaptation between the two shows some promise. Overall, within the field of sentiment analysis there are other granularities such as sentence level (Socher et al., 2013) , topic (Augenstein et al., 2018) , and aspect (Wang et al., 2016; Tay et al., 2017) . Aspect-level sentiment analysis relates to identifying the sentiment of (potentially multiple) topics in the same text although this can be seen as a similar task to TDSA. However the clear distinction between aspect and TDSA is that TDSA requires the target to be mentioned in the text itself while aspect-level employs a conceptual category with potentially multiple related instantiations in the text. Tang et al. (2016a) created a Target Dependent LSTM (TDLSTM) which encompassed two LSTMs either side of the target word, then improved the model by concatenating the target vector to the input embeddings to create a Target Connected LSTM (TCLSTM).",
  "y": "background"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_3",
  "x": "Other studies have adopted more linguistic approaches. Wang et al. (2017) extended the work of by using the dependency linked words from the target. Dong et al. (2014) used the dependency tree to create a Recursive Neural Network (RecNN) inspired by Socher et al. (2013) but compared to Socher et al. (2013) they also utilised the dependency tags to create an Adaptive RecNN (ARecNN). Critically, the methods reported above have not been applied to the same datasets, therefore a true comparative evaluation between the different methods is somewhat difficult. This has serious implications for generalisability of methods. We correct that limitation in our study. There are two papers taking a similar approach to our work in terms of generalisability although they do not combine them with the reproduction issues that we highlight. First, Chen et al. (2017) compared results across SemEval's laptop and restaurant reviews in English (Pontiki et al., 2014) , a Twitter dataset (Dong et al., 2014) and their own Chinese news comments dataset. They did perform a comparison across different languages, domains, corpora types, and different methods; SVM with features (Kiritchenko et al., 2014) , Rec-NN (Dong et al., 2014) , TDLSTM <cite>(Tang et al., 2016a)</cite> , Memory Neural Network (MNet) (Tang et al., 2016b) and their own attention method.",
  "y": "background"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_4",
  "x": "All of the methods outputs are fed into a softmax activation function. The experiments are performed on the Dong et al. (2014) dataset where we train and test on the specified splits. For the LSTMs we initialised the weights using uniform distribution U(0.003, 0.003), used Stochastic Gradient Descent (SGD) a learning rate of 0.01, cross entropy loss, padded and truncated sequence to the length of the maximum sequence in the training dataset as stated in the original paper, and we did not \"set the clipping threshold of softmax layer as 200\" <cite>(Tang et al., 2016a)</cite> as we were unsure what this meant. With regards to the number of epochs trained, we used early stopping with a patience of 10 and allowed 300 epochs. Within their experiments they used SSWE and Glove Twitter vectors 11 (Pennington et al., 2014) . As the paper being reproduced does not define the number of epochs they trained for, we use early stopping. Thus for early stopping we require to split the training data into train and validation sets to know when to stop. As it has been shown by Reimers and Gurevych (2017) that the random seed statistically significantly changes the results of experiments we ran each model over each word embedding thirty times, using a different seed value but keeping the same stratified train and validation split, and reported the results on the same test data as the original paper. As can be seen in Figure 4 , the initial seed value makes a large difference more so for the smaller embeddings.",
  "y": "extends differences"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_5",
  "x": "Overall, we were able to reproduce the results of all three papers. However for the neural network/deep learning approach of<cite> Tang et al. (2016a)</cite> we agree with Reimers and Gurevych (2017) that reporting multiple runs of the system over different seed values is required as the single performance scores can be misleading, which could explain why previous papers obtained different results to the original for the TDLSTM method (Chen et al., 2017; Tay et al., 2017) . ---------------------------------- **MASS EVALUATION** For all of the methods we pre-processed the text by lower casing and tokenising using Twokenizer (Gimpel et al., 2011) , and we used all three sentiment lexicons where applicable. We found the best word vectors from SSWE and the common crawl 42B 300 dimension Glove vectors by five fold stratified cross validation for the NP methods and the highest accuracy on the validation set for the LSTM methods. We chose these word vectors as they have very different sizes (50 and 300), also they have been shown to perform well in different text types; SSWE for social media <cite>(Tang et al., 2016a)</cite> and Glove for reviews (Chen et al., 2017) . To make the experiments quicker and computationally less expensive, we filtered out all words from the word vectors that did not appear in the train and test datasets, and this is equivalent with respect to word coverage as using all words. Finally we only reported results for the LSTM methods with one seed value and not multiple due to time constraints.",
  "y": "differences extends"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_6",
  "x": "However for the neural network/deep learning approach of<cite> Tang et al. (2016a)</cite> we agree with Reimers and Gurevych (2017) that reporting multiple runs of the system over different seed values is required as the single performance scores can be misleading, which could explain why previous papers obtained different results to the original for the TDLSTM method (Chen et al., 2017; Tay et al., 2017) . ---------------------------------- **MASS EVALUATION** For all of the methods we pre-processed the text by lower casing and tokenising using Twokenizer (Gimpel et al., 2011) , and we used all three sentiment lexicons where applicable. We found the best word vectors from SSWE and the common crawl 42B 300 dimension Glove vectors by five fold stratified cross validation for the NP methods and the highest accuracy on the validation set for the LSTM methods. We chose these word vectors as they have very different sizes (50 and 300), also they have been shown to perform well in different text types; SSWE for social media <cite>(Tang et al., 2016a)</cite> and Glove for reviews (Chen et al., 2017) . To make the experiments quicker and computationally less expensive, we filtered out all words from the word vectors that did not appear in the train and test datasets, and this is equivalent with respect to word coverage as using all words. Finally we only reported results for the LSTM methods with one seed value and not multiple due to time constraints. The results of the methods using the best found word vectors on the test sets can be seen in table 6.",
  "y": "similarities uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_0",
  "x": "Knowledge graphs have become a very useful framework to organize and store knowledge. Their interconnected nature is not just a natural way to represent facts, but it has potential that the separate storage of facts does not have, such as: (i) we can use it as a relational model of meaning, and derive jointly representations for nodes (entities) and edges (relations); (ii) the structure can be explored to discover systematic patterns that reveal interesting and exploitable regularities, such as paths connecting nodes in direct relations, (iii) discovering and inducing new connections. Link prediction methods in knowledge graphs (see (Nickel et al., 2016) for an overview) predict additional edges in the graph, based on induced node and edge representations that encode the structure of the graph and thus capture regularities (such as homophily). Lao and Cohen (2010) introduced a new method that predicts direct links based on paths that connect the source and target nodes. Such paths are not only useful for link prediction (Lao et al., 2011; <cite>Gardner et al., 2014)</cite> , but also for finding explanations for direct links and help with targeted information extraction to fill in incomplete knowledge repositories (Yin et al., 2018; Zhou and Nastase, 2018) . These approaches rely on the structure of the knowledge graph, which is inherently incomplete. This incompleteness can affect the process in different ways, e.g. it leads to representations for nodes with few connection that are not very informative, it can miss relevant patterns/paths (or derive misleading patterns/paths). In this paper we investigate whether a higherlevel view of a graph -an abstract graph that captures an intensional view of the original extensional graph -can help derive more robust and informative patterns. Such patterns are paths (i.e. sequences of relations) that could be used not only for link prediction, but also for targeted information extraction for completing the graph with external information.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_1",
  "x": "Such patterns are paths (i.e. sequences of relations) that could be used not only for link prediction, but also for targeted information extraction for completing the graph with external information. This abstract graph will contain only one edge for each relation type, that will connect a node representing the relation's domain (or source) to its range (or target). Additional edges will link the nodes to capture set relations (intersection, subset, superset) information between the different relations' domains and ranges. This step drastically reduces the graph size, making many different graph processing approaches more tractable. We investigate whether in this graph that represents a more general version of the information in the original KG, good patterns/paths are stronger and easier to find, because the aggregated view compensates for individual missing edges throughout the graph. We test the extracted paths through the link prediction task on Freebase (Bollacker et al., 2008) and NELL (Carlson et al., 2010a) , using<cite> Gardner et al. (2014)</cite> 's experimental set-up: pairs of nodes are represented using their connected paths as fea-tures, and a model for predicting the direct relations is learned and tested on training and test sets for 24 relations in Freebase and 10 relations in NELL. Our analysis shows that we find different and much fewer paths than the PRA method does (mostly because the abstract paths do not contain back-and-forth sequences of generalizing or type relations). The paths found in the abstract graphs lead to better performance on NELL than the PRA paths, which could be explained by the fact that NELL's relation inventory was designed to capture interdependencies (Carlson et al., 2010a) . On Freebase the results we obtain are lower, but this could be due to a different negative sampling process.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_2",
  "x": "Embedding models aim to map entities, relations and triples to vector space such that additional facts can be inferred from known facts using notions of vector similarity. A class of embedding models that aim to factorize the graph are termed as latent factor models. Neural network based models such as ER-MLP (Dong et al., 2014) , NTN (Socher et al., 2013) , RNNs (Neelakantan et al., 2015; Das et al., 2016) and Graph CNNs (Schlichtkrull et al., 2018) are examples of embedding models while RESCAL (Nickel et al., 2012) , DistMult (Yang et al., 2015) , TransE (Bordes et al., 2013) , ComplEx (Trouillon et al., 2017) are examples of latent factor models. Lao and Cohen (2010) introduced a novel way to exploit information in knowledge graphs: using weighted extracted paths as features in four different recommendation tasks, which can be modeled as typed proximity queries. The idea of using paths in the graph has then been applied to the task of link prediction (Lao et al., 2011) , and extended to incorporate textual information<cite> (Gardner et al., 2014)</cite> . Lao et al. (2011) obtain paths for given node pairs using random walks over the knowledge graph. To be used as features shared by multiple instances, the information about nodes on the paths is removed, transforming the actual paths into \"meta-paths\". The paths themselves can be incorporated in different ways in a model -as features (Lao et al., 2011; <cite>Gardner et al., 2014)</cite> , as Horn clauses to provide rules for inference in KGs whether directly or through scores that represent the strength of the path as a direct relation (Neelakantan et al., 2015; Guu et al., 2015) , also taking into account information about intermediary nodes (Das et al., 2017; Yin et al., 2018) . Gardner and Mitchell (2015) perform link prediction using random walks but do not attempt to connect a source and target node, but rather to characterize the local structure around a (source or target) node using such localized paths.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_3",
  "x": "Lao and Cohen (2010) introduced a novel way to exploit information in knowledge graphs: using weighted extracted paths as features in four different recommendation tasks, which can be modeled as typed proximity queries. The idea of using paths in the graph has then been applied to the task of link prediction (Lao et al., 2011) , and extended to incorporate textual information<cite> (Gardner et al., 2014)</cite> . Lao et al. (2011) obtain paths for given node pairs using random walks over the knowledge graph. To be used as features shared by multiple instances, the information about nodes on the paths is removed, transforming the actual paths into \"meta-paths\". The paths themselves can be incorporated in different ways in a model -as features (Lao et al., 2011; <cite>Gardner et al., 2014)</cite> , as Horn clauses to provide rules for inference in KGs whether directly or through scores that represent the strength of the path as a direct relation (Neelakantan et al., 2015; Guu et al., 2015) , also taking into account information about intermediary nodes (Das et al., 2017; Yin et al., 2018) . Gardner and Mitchell (2015) perform link prediction using random walks but do not attempt to connect a source and target node, but rather to characterize the local structure around a (source or target) node using such localized paths. Using these subgraph features leads to better results for the knowledge graph completion task. We focus here on discovering useful and explanatory paths, not on optimizing or further improving the KGC task. Using paths can lead to interpretable models because the paths can help explain the predicted fact.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_4",
  "x": "Meng et al. (2015) present a method to automate the induction of metapaths in large heterogeneous information networks (a.k.a. knowledge graphs) for given node pairs, even if the given node pairs are not connected by a direct relation. Path information is also found to improve performance since paths help the model learn logical rules. However, mining paths from a large knowledge graph is often computationally expensive since it involves performing a traversal through the graph. To overcome this limitation (Das et al., 2017) proposed deep reinforcement learning and (Chen et al., 2018) proposed RNNS for generating paths. However, many datasets suffer from paths sparsity, lack of enough paths connecting source target pairs, resulting in poor performance for many relations. Wang et al. (2013) have a different approachthey start with patterns in the form of first-order probabilistic rules, which they then ground in a small subgraph of a large knowledge graph. The approach we present here combines different elements of these previous approaches in a novel way: we build an abstract graph to find pat-terns that would be similar to those used by (Wang et al., 2013) . To test the quality of these paths we ground them using the original KG and use these grounded paths in a learning framework similar to<cite> (Gardner et al., 2014)</cite> . ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_5",
  "x": "Figure 1: Knowledge graphs statistics on a logarithmic scale: relation and nodes frequencies for Freebase and NELL (the version used by<cite> (Gardner et al., 2014)</cite> and in this paper). Every data point is the degree of a node (top plots), or frequency of a relation (bottom plots). The data points are ordered monotonically, the x axis is just an index. Building such a graph makes sense only for knowledge repositories that have strongly typed relations -like Freebase and NELL -but we do not require knowledge of the types of the relations' domains and ranges. Such information is not finegrained enough: for example, the relation capital has a type City as a domain, but capital cities are a very small subset of the set of all cities. Using an \"atomic\" node to represent the domain/range of a relation would not allow us to make finer grained connections and distinctions between the domains and ranges of the existing relations. Figure 2 shows a subset of the abstract graph built from the Freebase dataset. The blue edges are set relations -intersection, superset, subsetbetween the domains and ranges of a subset of the relations in the dataset. The black edges correspond to the actual relations in the dataset.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_6",
  "x": "---------------------------------- **ABSTRACT PATHS** The Path Ranking Algorithm formalism originally proposed by (Lao and Cohen, 2010) performs two main steps to represent of a pair of nodes in a graph: (i) feature selection -adding paths that connect the node pair; (ii) feature computation - Table 1 : Graph statistics on the datasets used by<cite> (Gardner et al., 2014)</cite> , and their abstract versions associating a value for each added path. Obtaining paths from a large graph is a computationally intensive problem, particularly in graphs that have numerous nodes with high degrees. Figure 1a shows that about 60% of Freebase nodes have degree higher than 10, which leads to an exponential growth in the number of paths starting in a node. Algorithms that harness path information often mine paths either by performing costly random walks (Guu et al., 2015) , traversals<cite> (Gardner et al., 2014</cite>; Neelakantan et al., 2015; Das et al., 2016) or by constructing paths through generative models (Das et al., 2017; Ding et al., 2018) . Here, we adopt a different approach, by abstracting the graph first, then finding paths in this graph through traversal algorithms. For a relation r i , we start at its domain (source) node V i,s and search for a path to its range (target) node V i,t using breadth first search. We constrain this path to contain at most k \"proper\" relations 2 , and we do not allow consecutive set relations, thus forcing the algorithm to move from one \"proper\" relation to another through a set relation that connects the range of one with the domain of the next.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_7",
  "x": "Obtaining paths from a large graph is a computationally intensive problem, particularly in graphs that have numerous nodes with high degrees. Figure 1a shows that about 60% of Freebase nodes have degree higher than 10, which leads to an exponential growth in the number of paths starting in a node. Algorithms that harness path information often mine paths either by performing costly random walks (Guu et al., 2015) , traversals<cite> (Gardner et al., 2014</cite>; Neelakantan et al., 2015; Das et al., 2016) or by constructing paths through generative models (Das et al., 2017; Ding et al., 2018) . Here, we adopt a different approach, by abstracting the graph first, then finding paths in this graph through traversal algorithms. For a relation r i , we start at its domain (source) node V i,s and search for a path to its range (target) node V i,t using breadth first search. We constrain this path to contain at most k \"proper\" relations 2 , and we do not allow consecutive set relations, thus forcing the algorithm to move from one \"proper\" relation to another through a set relation that connects the range of one with the domain of the next. An abstract path, just like a meta-path extracted by previous work, is a sequence of relation types: \u03c0 j =< r j,1 , r j,2 , ...r j,m >, some of which are \"proper\" relations, some are set relations. Because of the more general view of the graph, we lose information about individual paths (i.e. instances of a path in the original graph). Because of this, the paths we extract are hypothetical, but will have associated a confidence score based on the frequency of occurrence of relations in the original KG, and the strength of the connection of the range of one relation on the path with the domain of the next one.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_8",
  "x": "The black edges correspond to the actual relations in the dataset. ---------------------------------- **ABSTRACT PATHS** The Path Ranking Algorithm formalism originally proposed by (Lao and Cohen, 2010) performs two main steps to represent of a pair of nodes in a graph: (i) feature selection -adding paths that connect the node pair; (ii) feature computation - Table 1 : Graph statistics on the datasets used by<cite> (Gardner et al., 2014)</cite> , and their abstract versions associating a value for each added path. Obtaining paths from a large graph is a computationally intensive problem, particularly in graphs that have numerous nodes with high degrees. Figure 1a shows that about 60% of Freebase nodes have degree higher than 10, which leads to an exponential growth in the number of paths starting in a node. Algorithms that harness path information often mine paths either by performing costly random walks (Guu et al., 2015) , traversals<cite> (Gardner et al., 2014</cite>; Neelakantan et al., 2015; Das et al., 2016) or by constructing paths through generative models (Das et al., 2017; Ding et al., 2018) . Here, we adopt a different approach, by abstracting the graph first, then finding paths in this graph through traversal algorithms. For a relation r i , we start at its domain (source) node V i,s and search for a path to its range (target) node V i,t using breadth first search.",
  "y": "background differences"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_9",
  "x": "They can be used in different ways, e.g. (i) as features in a link prediction system (e.g.<cite> (Gardner et al., 2014)</cite> ), (ii) to fill in larger portions of the graph by producing, rather than finding, groundings of the path for specific instances. In the work presented here we test the abstract paths through the link prediction task, so we will try to ground abstract paths for relation instances in the training and test data. After finding the set of abstract paths {\u03c0 i,r } associated with a relation r, for a given instance of the relation r -< s, r, t > -we can (try to) ground the paths as follows: (i) we first eliminate set relations from the abstract paths: at this point set relations between relation types domain and ranges are not useful (they were necessary only for the connectivity and search process in the abstract graph). Set relations have no counterpart in the extensional graph, as at this level nodes themselves make the connection between successive relations (ii) starting at the source node, we follow again a breadth first traversal, constraining at each step the type of relation to follow based on the \"cleaned up\" abstract path. We compute the weight of a grounded path gp =< v 0 , r x 1 , v 1 , ..., v l\u22121 , r x l , v l > (where v 0 = s and v l = t) as a combination of the weight of the corresponding abstract path \u03c0 =< r 1 , ..., r m > (r x i \u2208 \u03c0) and specific information for the current node pair (s, t): where the weights of the relations on the grounded path reflect the specificity of the relation to its source node: ---------------------------------- **EXPERIMENTS** Because we want to compare the abstract paths found using the abstract graph with paths found using PRA, we use the experimental set-up of<cite> (Gardner et al., 2014)</cite> , where we replace the feature selection and feature computation steps with the approach presented here.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_10",
  "x": "We use this weight to rank abstract relations for potential filtering, and to compute the weight of its grounding for specific node pairs. ---------------------------------- **GROUNDED PATHS** The abstract paths are hypothetical paths that could connect the source s and target t of a < s, r, t > tuple. They can be used in different ways, e.g. (i) as features in a link prediction system (e.g.<cite> (Gardner et al., 2014)</cite> ), (ii) to fill in larger portions of the graph by producing, rather than finding, groundings of the path for specific instances. In the work presented here we test the abstract paths through the link prediction task, so we will try to ground abstract paths for relation instances in the training and test data. After finding the set of abstract paths {\u03c0 i,r } associated with a relation r, for a given instance of the relation r -< s, r, t > -we can (try to) ground the paths as follows: (i) we first eliminate set relations from the abstract paths: at this point set relations between relation types domain and ranges are not useful (they were necessary only for the connectivity and search process in the abstract graph). Set relations have no counterpart in the extensional graph, as at this level nodes themselves make the connection between successive relations (ii) starting at the source node, we follow again a breadth first traversal, constraining at each step the type of relation to follow based on the \"cleaned up\" abstract path. We compute the weight of a grounded path gp =< v 0 , r x 1 , v 1 , ..., v l\u22121 , r x l , v l > (where v 0 = s and v l = t) as a combination of the weight of the corresponding abstract path \u03c0 =< r 1 , ..., r m > (r x i \u2208 \u03c0) and specific information for the current node pair (s, t):",
  "y": "uses background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_11",
  "x": "We compute the weight of a grounded path gp =< v 0 , r x 1 , v 1 , ..., v l\u22121 , r x l , v l > (where v 0 = s and v l = t) as a combination of the weight of the corresponding abstract path \u03c0 =< r 1 , ..., r m > (r x i \u2208 \u03c0) and specific information for the current node pair (s, t): where the weights of the relations on the grounded path reflect the specificity of the relation to its source node: ---------------------------------- **EXPERIMENTS** Because we want to compare the abstract paths found using the abstract graph with paths found using PRA, we use the experimental set-up of<cite> (Gardner et al., 2014)</cite> , where we replace the feature selection and feature computation steps with the approach presented here. A big difference will be caused by the negative sampling, which also makes the results not directly comparable. The issues are explained in the negative sampling paragraph below. The data thus obtained is used for training a linear regression model (similarly to<cite> (Gardner et al., 2014)</cite> ), and tested on the provided test sets and evaluated using mean average precision (MAP). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_12",
  "x": "Because we want to compare the abstract paths found using the abstract graph with paths found using PRA, we use the experimental set-up of<cite> (Gardner et al., 2014)</cite> , where we replace the feature selection and feature computation steps with the approach presented here. A big difference will be caused by the negative sampling, which also makes the results not directly comparable. The issues are explained in the negative sampling paragraph below. The data thus obtained is used for training a linear regression model (similarly to<cite> (Gardner et al., 2014)</cite> ), and tested on the provided test sets and evaluated using mean average precision (MAP). ---------------------------------- **DATA** We build abstract graphs and paths from the Freebase and NELL data described in<cite> (Gardner et al., 2014)</cite> . We then use the extracted paths for link prediction. The graphs built by<cite> Gardner et al. (2014)</cite> cover several variations, where the KGs were enhanced with < subject, verb, object > triples extracted from dependency parses of ClueWeb documents.",
  "y": "similarities"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_13",
  "x": "---------------------------------- **DATA** We build abstract graphs and paths from the Freebase and NELL data described in<cite> (Gardner et al., 2014)</cite> . We then use the extracted paths for link prediction. The graphs built by<cite> Gardner et al. (2014)</cite> cover several variations, where the KGs were enhanced with < subject, verb, object > triples extracted from dependency parses of ClueWeb documents. Table 1 shows the statistics for each original and abstract graph. The generated abstract graph is several degrees of magnitude smaller compared to the original KG. The abstract graph approach we present here does not fit well the combination of the knowledge base (Freebase or NELL) with unstructured SVO triples, because we rely on strongly typed relations to build node sets. The SVO triples bring in numerous low frequency relations, that without additional processing are not beneficial.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_14",
  "x": "A big difference will be caused by the negative sampling, which also makes the results not directly comparable. The issues are explained in the negative sampling paragraph below. The data thus obtained is used for training a linear regression model (similarly to<cite> (Gardner et al., 2014)</cite> ), and tested on the provided test sets and evaluated using mean average precision (MAP). ---------------------------------- **DATA** We build abstract graphs and paths from the Freebase and NELL data described in<cite> (Gardner et al., 2014)</cite> . We then use the extracted paths for link prediction. The graphs built by<cite> Gardner et al. (2014)</cite> cover several variations, where the KGs were enhanced with < subject, verb, object > triples extracted from dependency parses of ClueWeb documents. Table 1 shows the statistics for each original and abstract graph.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_15",
  "x": "We build abstract graphs and paths from the Freebase and NELL data described in<cite> (Gardner et al., 2014)</cite> . We then use the extracted paths for link prediction. The graphs built by<cite> Gardner et al. (2014)</cite> cover several variations, where the KGs were enhanced with < subject, verb, object > triples extracted from dependency parses of ClueWeb documents. Table 1 shows the statistics for each original and abstract graph. The generated abstract graph is several degrees of magnitude smaller compared to the original KG. The abstract graph approach we present here does not fit well the combination of the knowledge base (Freebase or NELL) with unstructured SVO triples, because we rely on strongly typed relations to build node sets. The SVO triples bring in numerous low frequency relations, that without additional processing are not beneficial. The results presented by<cite> Gardner et al. (2014)</cite> show that this configuration very rarely (and never overall) leads to better results than the other graph variations. The numerous relation types brought in by the SVO triples also lead to high computation time for the abstract graph: its shortcoming is the computation of set relations between the different relations' domains and ranges, which grows quadratically with the number of relation types.",
  "y": "uses background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_16",
  "x": "The SVO triples bring in numerous low frequency relations, that without additional processing are not beneficial. The results presented by<cite> Gardner et al. (2014)</cite> show that this configuration very rarely (and never overall) leads to better results than the other graph variations. The numerous relation types brought in by the SVO triples also lead to high computation time for the abstract graph: its shortcoming is the computation of set relations between the different relations' domains and ranges, which grows quadratically with the number of relation types. We will skip this graph variation in the rest of the experiments presented here. Gardner et al. (2014) use these graphs to generate paths for augmenting the representation of node pairs, for link prediction, for a subset of 24 relation types from Freebase's inventory, and 10 relations from NELL. Each relation has a training and test set, whose numbers vary quite a bit, as shown through the statistics in Table 2 . Negative sampling The number of negative instances used in<cite> (Gardner et al., 2014)</cite> is not clearly stated. Both the number and methods of generating the negative samples can impact the results (Kotnis and Nastase, 2018) . We use (up to) 200 negative samples for each positive pair: for a pair (s, t) in the provided training or test sets for each relation r, we make 100 negative samples by corrupting the source s, and 100 negative samples by corrupting the target t. The corrupted s and t are chosen from r's domain V r,s and range V r,t respectively, such that these corrupted triples are not part of the training, test or graph.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_17",
  "x": "The generated abstract graph is several degrees of magnitude smaller compared to the original KG. The abstract graph approach we present here does not fit well the combination of the knowledge base (Freebase or NELL) with unstructured SVO triples, because we rely on strongly typed relations to build node sets. The SVO triples bring in numerous low frequency relations, that without additional processing are not beneficial. The results presented by<cite> Gardner et al. (2014)</cite> show that this configuration very rarely (and never overall) leads to better results than the other graph variations. The numerous relation types brought in by the SVO triples also lead to high computation time for the abstract graph: its shortcoming is the computation of set relations between the different relations' domains and ranges, which grows quadratically with the number of relation types. We will skip this graph variation in the rest of the experiments presented here. Gardner et al. (2014) use these graphs to generate paths for augmenting the representation of node pairs, for link prediction, for a subset of 24 relation types from Freebase's inventory, and 10 relations from NELL. Each relation has a training and test set, whose numbers vary quite a bit, as shown through the statistics in Table 2 . Negative sampling The number of negative instances used in<cite> (Gardner et al., 2014)</cite> is not clearly stated.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_18",
  "x": "For each relation type the set of features representing the corresponding data will be twice the number of abstract paths. We produce two features for each abstract path: one that is the weight of this path, and one that is the weight of its grounding for a given relation instance. If a relation instance does not have a grounding for an abstract path, the values of these features will be 0. ---------------------------------- **RESULTS AND DISCUSSION** The overall results of the experiments are presented in Table 3, and the relation-level results are  in Tables 4 for NELL, and 5 Table 3 : Results on the three graph variations of Freebase and NELL as reported by<cite> (Gardner et al., 2014)</cite> (G) and using abstract graphs (KG A ). Overall, the results indicate that enhancing Freebase and NELL with additional facts from textual sources leads to better results, particularly when these additional facts (< subject, verb, object > triples) are processed and clustered using low dimensional dense representations<cite> Gardner et al. (2014</cite>; use embeddings obtained by running PCA on the matrix of SVO triples). Freebase has 4200+ relation types, and NELL 500+. More than 500 relation types in Freebase have less than 10 instances, wheres NELL does not have this issue (see Figures 1a and 1b) .",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_19",
  "x": "The overall results of the experiments are presented in Table 3, and the relation-level results are  in Tables 4 for NELL, and 5 Table 3 : Results on the three graph variations of Freebase and NELL as reported by<cite> (Gardner et al., 2014)</cite> (G) and using abstract graphs (KG A ). Overall, the results indicate that enhancing Freebase and NELL with additional facts from textual sources leads to better results, particularly when these additional facts (< subject, verb, object > triples) are processed and clustered using low dimensional dense representations<cite> Gardner et al. (2014</cite>; use embeddings obtained by running PCA on the matrix of SVO triples). Freebase has 4200+ relation types, and NELL 500+. More than 500 relation types in Freebase have less than 10 instances, wheres NELL does not have this issue (see Figures 1a and 1b) . Because we test the approach for knowledge graph completion using classification based on the patterns as features, having features that appear too Table 4 : Relation results for the NELL KB. The second column is the best result for each relation reported by<cite> (Gardner et al., 2014)</cite> . few times will not help the system find a robust model. For the purpose of the presented experiments we filter the Freebase abstract graph to use only relation types that have at least 10 instances (Table 1 shows the statistics for this configuration). It is not surprising that overall the results for NELL are higher -NELL has been designed on the principle of coupled learning, where connections between different relations are the basis of the resource and its continuous growth (Carlson et al., 2010b) .",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_20",
  "x": "The second column is the best result for each relation reported by<cite> (Gardner et al., 2014)</cite> . few times will not help the system find a robust model. For the purpose of the presented experiments we filter the Freebase abstract graph to use only relation types that have at least 10 instances (Table 1 shows the statistics for this configuration). It is not surprising that overall the results for NELL are higher -NELL has been designed on the principle of coupled learning, where connections between different relations are the basis of the resource and its continuous growth (Carlson et al., 2010b) . It also has more training data for each relation (see table in Section 4.1). There is no consistent trend -for some relations using the paths extracted with this approach leads to better results, for others it does not (although, as we frequently mentioned, the fact that we used different negative sampling methods, the results are not directly comparable). A more complete picture emerges when we look at the paths found, and compare them with the paths obtained with the PRA approach 3 . For all Freebase KG configurations,<cite> Gardner et al. (2014)</cite> have 1000 paths for most relations (approx. 6 of the relations have between 230 and 973). For NELL the number varies more, between 58 and 5509, 6 of the relations have more than 1000 metapaths.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_21",
  "x": "few times will not help the system find a robust model. For the purpose of the presented experiments we filter the Freebase abstract graph to use only relation types that have at least 10 instances (Table 1 shows the statistics for this configuration). It is not surprising that overall the results for NELL are higher -NELL has been designed on the principle of coupled learning, where connections between different relations are the basis of the resource and its continuous growth (Carlson et al., 2010b) . It also has more training data for each relation (see table in Section 4.1). There is no consistent trend -for some relations using the paths extracted with this approach leads to better results, for others it does not (although, as we frequently mentioned, the fact that we used different negative sampling methods, the results are not directly comparable). A more complete picture emerges when we look at the paths found, and compare them with the paths obtained with the PRA approach 3 . For all Freebase KG configurations,<cite> Gardner et al. (2014)</cite> have 1000 paths for most relations (approx. 6 of the relations have between 230 and 973). For NELL the number varies more, between 58 and 5509, 6 of the relations have more than 1000 metapaths. With the abstract graphs the numbers are much lower.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_22",
  "x": "For Freebase we find between 1 and 258 abstract paths, most of the relations (21) having fewer than 30 abstract paths for all KG configurations. For NELL we find between 1 and 157 paths, 5 of the relations having more than 100 ab- Table 5 : Statistics of the number of instances in the training and testing sets for the relations analyzed, and the number of paths extracted for each set (in parentheses the number of abstract paths for each graph). stract paths. The overlap between the sets of paths discovered with the two methods is very small: for Freebase the average overlap with respect to PRA is around 0.004 (for the different graph configurations), and with respect to the abstract paths around 0.2; for NELL around 0.003 relative to PRA and 0.27 relative to the abstract paths. We note that overall, the system found more paths than what could be grounded for the given training instances for both Freebase and NELL. Another general observation is that relations for which we found the most patterns (AthletePlaysForTeam and StateHasLake for NELL, /medicine/disease/symptoms and /film/film/rating for Freebase) do not necessarily perform the best. NELL The results for each relation in terms of average precision are presented in Table 4 . We include the best result on PRA (on any variation of the graph), as reported by<cite> (Gardner et al., 2014)</cite> , although since we used different negative instances the results are not directly comparable. Several of the NELL target relations have interesting patterns in the abstract graph, in particular StadiumLocatedInCity, TeamPlaysInLeague.",
  "y": "uses differences"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_0",
  "x": "Muslea (1999) reviewed the approaches which were used at the time and found that the most common techniques relied on lexicosyntactic patterns being applied to text which has undergone relatively shallow linguistic processing. For example, the extraction rules used by Soderland (1999) and Riloff (1996) match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Sudo et al., 2001;<cite> Sudo et al., 2003</cite>; Yangarber, 2003) . In these approaches extraction patterns are essentially parts of the dependency tree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) while <cite>Sudo et al. (2003)</cite> allow any subtree within the dependency parse to act as an extraction pattern. Stevenson and Greenwood (2006) showed that the choice of pattern model has important implications for IE algorithms including significant differences between the various models in terms of their ability to identify information of interest in text.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_1",
  "x": "Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) while <cite>Sudo et al. (2003)</cite> allow any subtree within the dependency parse to act as an extraction pattern. Stevenson and Greenwood (2006) showed that the choice of pattern model has important implications for IE algorithms including significant differences between the various models in terms of their ability to identify information of interest in text. However, there has been little comparison between the various pattern models. Those which have been carried out have been limited by the fact that they used indirect tasks to evaluate the various models and did not compare them in an IE scenario. We address this limitation here by presenting a direct comparison of four previously described pattern models using an unsupervised learning method applied to a commonly used IE scenario. The remainder of the paper is organised as follows. The next section presents four pattern models which have been previously introduced in the litera-ture.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_2",
  "x": "**IE PATTERN MODELS** In dependency analysis (Mel'\u010duk, 1987 ) the syntax of a sentence is represented by a set of directed binary links between a word (the head) and one of its modifiers. These links may be labelled to indicate the relation between the head and modifier (e.g. subject, object). An example dependency analysis for the sentence \"Acme hired Smith as their new CEO, replacing Bloggs.\" is shown The remainder of this section outlines four models for representing extraction patterns which can be derived from dependency trees. Predicate-Argument Model (SVO): A simple approach, used by Yangarber et al. (2000) , Yangarber (2003) and , is to use subject-verb-object tuples from the dependency parse as extraction patterns. These consist of a verb and its subject and/or direct object. Figure  2 shows the two SVO patterns 1 which are produced for the dependency tree shown in Figure 1 . This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1 The formalism used for representing dependency patterns is similar to the one introduced by <cite>Sudo et al. (2003)</cite> . Each node in the tree is represented in the format a[b/c] (e.g. subj[N/Acme]) where c is the lexical item (Acme), b its grammatical tag (N) and a the dependency relation between this node and its parent (subj).",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_3",
  "x": "**IE PATTERN MODELS** In dependency analysis (Mel'\u010duk, 1987 ) the syntax of a sentence is represented by a set of directed binary links between a word (the head) and one of its modifiers. These links may be labelled to indicate the relation between the head and modifier (e.g. subject, object). An example dependency analysis for the sentence \"Acme hired Smith as their new CEO, replacing Bloggs.\" is shown The remainder of this section outlines four models for representing extraction patterns which can be derived from dependency trees. Predicate-Argument Model (SVO): A simple approach, used by Yangarber et al. (2000) , Yangarber (2003) and , is to use subject-verb-object tuples from the dependency parse as extraction patterns. These consist of a verb and its subject and/or direct object. Figure  2 shows the two SVO patterns 1 which are produced for the dependency tree shown in Figure 1 . This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1 The formalism used for representing dependency patterns is similar to the one introduced by <cite>Sudo et al. (2003)</cite> . Each node in the tree is represented in the format a[b/c] (e.g. subj[N/Acme]) where c is the lexical item (Acme), b its grammatical tag (N) and a the dependency relation between this node and its parent (subj).",
  "y": "uses"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_4",
  "x": "Figure 2 shows examples of the chains which can be extracted from the tree in Figure  1 . Chains provide a mechanism for encoding information beyond the direct arguments of predicates and includes areas of the dependency tree ignored by the SVO model. For example, they can represent information expressed as a nominalisation or within a prepositional phrase, e.g. \"The resignation of Smith from the board of Acme ...\" However, a potential shortcoming of this model is that it cannot represent the link between arguments of a verb. Patterns in the chain model format are unable to represent even the simplest of sentences containing a transitive verb, e.g. \"Smith left Acme\". Linked Chains: The linked chains model represents extraction patterns as a pair of chains which share the same verb but no direct descendants. Example linked chains are shown in Figure 2 . This pattern representation encodes most of the information in the sentence with the advantage of being able to link together event participants which neither of the SVO or chain model can, for example the relation between \"Smith\" and \"Bloggs\" in Figure 1 . Subtrees: The final model to be considered is the subtree model<cite> (Sudo et al., 2003)</cite> . In this model any subtree of a dependency tree can be used as an extraction pattern, where a subtree is any set of nodes in the tree which are connected to one another.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_5",
  "x": "Single nodes are not considered to be subtrees. The subtree model is a richer representation than those discussed so far and can represent any part of a dependency tree. Each of the previous models form a proper subset of the subtrees. By choosing an appropriate subtree it is possible to link together any pair of nodes in a tree and consequently this model can ---------------------------------- **PREVIOUS COMPARISONS** There have been few direct comparisons of the various pattern models. <cite>Sudo et al. (2003)</cite> compared three models (SVO, chains and subtrees) on two IE scenarios using a entity extraction task. Models were evaluated in terms of their ability to identify entities taking part in events and distinguish them from those which did not.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_6",
  "x": "They concluded that the linked chain model was optional since it is expressive enough to represent the information of interest without introducing a potentially unwieldy number of patterns. There is some agreement between these two studies, for example that the SVO model performs poorly in comparison with other models. However, Stevenson and Greenwood (2006) also found that the coverage of the chain model was significantly worse than the subtree model, although <cite>Sudo et al. (2003)</cite> found that in some cases their performance could not be distinguished. In addition to these disagreements, these studies are also limited by the fact that they are indirect; they do not evaluate the various pattern models on an IE task. ---------------------------------- **EXPERIMENTS** We compared each of the patterns models described in Section 2 using an unsupervised IE experiment similar to one described by <cite>Sudo et al. (2003)</cite> . Let D be a corpus of documents and R a set of documents which are relevant to a particular extraction task. In this context \"relevant\" means that the document contains the information we are interested in identifying.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_7",
  "x": "There is some agreement between these two studies, for example that the SVO model performs poorly in comparison with other models. However, Stevenson and Greenwood (2006) also found that the coverage of the chain model was significantly worse than the subtree model, although <cite>Sudo et al. (2003)</cite> found that in some cases their performance could not be distinguished. In addition to these disagreements, these studies are also limited by the fact that they are indirect; they do not evaluate the various pattern models on an IE task. ---------------------------------- **EXPERIMENTS** We compared each of the patterns models described in Section 2 using an unsupervised IE experiment similar to one described by <cite>Sudo et al. (2003)</cite> . Let D be a corpus of documents and R a set of documents which are relevant to a particular extraction task. In this context \"relevant\" means that the document contains the information we are interested in identifying. D and R are such that D = R \u222aR and R\u2229R = \u2205. As assumption behind this approach is that useful patterns will be far more likely to occur in R than D overall.",
  "y": "uses"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_8",
  "x": "Equation 1 combines two factors: the term frequency (in relevant documents) and inverse document frequency (across the corpus). Patterns which occur frequently in relevant documents without being too prevalent in the corpus are preferred. <cite>Sudo et al. (2003)</cite> found that it was important to find the appropriate balance between these two factors. They introduced the \u03b2 parameter as a way of controlling the relative contribution of the inverse document frequency. \u03b2 is tuned for each extraction task and pattern model combination. Although simple, this approach has the advantage that it can be applied to each of the four pattern models to provide a direct comparison. ---------------------------------- **EXTRACTION SCENARIO** The ranking process was applied to the IE scenario used for the sixth Message Understanding conference (MUC-6).",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_9",
  "x": "**EXTRACTION SCENARIO** The ranking process was applied to the IE scenario used for the sixth Message Understanding conference (MUC-6). The aim of this task was to identify management succession events from a corpus of newswire texts. Relevant information describes an executive entering or leaving a position within a company, for example \"Last month Smith resigned as CEO of Rooter Ltd.\". This sentence described as event involving three items: a person (Smith), position (CEO) and company (Rooter Ltd). We made use of a version of the MUC-6 corpus described by Soderland (1999) which consists of 598 documents. For these experiments relevant documents were identified using annotations in the corpus. However, this is not necessary since <cite>Sudo et al. (2003)</cite> showed that adequate knowledge about document relevance could be obtained automatically using an IR system. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_10",
  "x": "In order to avoid duplication the extension is restricted to allowing nodes only to be added to the nodes on the rightmost path of the tree. Applying the process recursively creates a search space in which all subtrees are enumerated with minimal duplication. The rightmost extension algorithm is most suited to finding subtrees which occur multiple times and, even using this efficient approach, we were unable to generate subtrees which occurred fewer than four times in the MUC-6 texts in a reasonable time. Similar restrictions have been encountered within other approaches which have relied on the generation of a comprehensive set of subtrees from a parse forest. For example, Kudo et al. (2005) used subtrees for parse ranking but could only generate subtrees which appear at least ten times in a 40,000 sentence corpus. They comment that the size of their data set meant that it would have been difficult to complete the experiments with less restrictive parameters. In addition, <cite>Sudo et al. (2003)</cite> only generated subtrees which appeared in at least three documents. Kudo et al. (2005) and <cite>Sudo et al. (2003)</cite> both used the rightmost extension algorithm to generate subtrees. To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_11",
  "x": "For example, Kudo et al. (2005) used subtrees for parse ranking but could only generate subtrees which appear at least ten times in a 40,000 sentence corpus. They comment that the size of their data set meant that it would have been difficult to complete the experiments with less restrictive parameters. In addition, <cite>Sudo et al. (2003)</cite> only generated subtrees which appeared in at least three documents. Kudo et al. (2005) and <cite>Sudo et al. (2003)</cite> both used the rightmost extension algorithm to generate subtrees. To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed. Table 1 shows the number of patterns generated for each of the four models when the patterns are both filtered and unfiltered. (Although the set of unfiltered subtree patterns were not generated it is possible to determine the number of patterns which would be generated using a process described by Stevenson and Greenwood (2006 Table 1 : Number of patterns generated by each model It can be seen that the various pattern models generate vastly different numbers of patterns and that the number of subtrees is significantly greater than the other three models. Previous analysis (see Section 3) suggested that the number of subtrees which would be generated from a corpus could be difficult to process computationally and this is supported by our findings here. ---------------------------------- **PARAMETER TUNING**",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_12",
  "x": "Kudo et al. (2005) and <cite>Sudo et al. (2003)</cite> both used the rightmost extension algorithm to generate subtrees. To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed. Table 1 shows the number of patterns generated for each of the four models when the patterns are both filtered and unfiltered. (Although the set of unfiltered subtree patterns were not generated it is possible to determine the number of patterns which would be generated using a process described by Stevenson and Greenwood (2006 Table 1 : Number of patterns generated by each model It can be seen that the various pattern models generate vastly different numbers of patterns and that the number of subtrees is significantly greater than the other three models. Previous analysis (see Section 3) suggested that the number of subtrees which would be generated from a corpus could be difficult to process computationally and this is supported by our findings here. ---------------------------------- **PARAMETER TUNING** The value of \u03b2 in equation 1 was set using a separate corpus from which the patterns were generated, a methodology suggested by <cite>Sudo et al. (2003)</cite> . To generate this additional text we used the Reuters Corpus (Rose et al., 2002 ) which consists of a year's worth of newswire output. Each document in the Reuters corpus has been manually annotated with topic codes indicating its general subject area(s).",
  "y": "uses"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_0",
  "x": "**INTRODUCTION** Neural machine translation has achieved great success in the last few years (Bahdanau et al., 2014; Gehring et al., 2017;<cite> Vaswani et al., 2017)</cite> . The Transformer <cite>(Vaswani et al., 2017)</cite> , which has outperformed previous RNN/CNN based translation models (Bahdanau et al., 2014; Gehring et al., 2017) , is based on multi-layer self-attention networks and can be trained very efficiently. The multi-layer structure allows the Transformer to model complicated functions. Increasing the depth of models can increase their capacity but may also cause optimization difficulties (Mhaskar et al., 2017; Telgarsky, 2016; Eldan and Shamir, 2016; He et al., 2016; . In order to ease optimization, the Transformer employs residual connection and layer normalization techniques which have been proven useful in reducing optimization difficulties of deep neural networks for various tasks (He et al., 2016; Ba et al., 2016) . However, even with residual connections and layer normalization, deep Transformers are still hard to train: the original Transformer <cite>(Vaswani et al., 2017)</cite> only contains 6 encoder/decoder layers. show that Transformer models with more than 12 encoder layers fail to converge, and propose the Transparent Attention (TA) mechanism which weighted combines outputs of all encoder layers as encoded representation. However, the TA mechanism has to value outputs of shallow encoder layers to feedback sufficient gradients during back-propagation to ensure their convergence, which implies that weights of deep layers are likely to be hampered and against the motivation when go very deep, and as a result cannot get further improvements with more than 16 layers. reveal that deep Transformers with proper use of layer normalization is able to converge and propose to aggregate previous layers' outputs for each layer instead of at the end of encoding.",
  "y": "motivation"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_1",
  "x": "Neural machine translation has achieved great success in the last few years (Bahdanau et al., 2014; Gehring et al., 2017;<cite> Vaswani et al., 2017)</cite> . The Transformer <cite>(Vaswani et al., 2017)</cite> , which has outperformed previous RNN/CNN based translation models (Bahdanau et al., 2014; Gehring et al., 2017) , is based on multi-layer self-attention networks and can be trained very efficiently. The multi-layer structure allows the Transformer to model complicated functions. Increasing the depth of models can increase their capacity but may also cause optimization difficulties (Mhaskar et al., 2017; Telgarsky, 2016; Eldan and Shamir, 2016; He et al., 2016; . In order to ease optimization, the Transformer employs residual connection and layer normalization techniques which have been proven useful in reducing optimization difficulties of deep neural networks for various tasks (He et al., 2016; Ba et al., 2016) . However, even with residual connections and layer normalization, deep Transformers are still hard to train: the original Transformer <cite>(Vaswani et al., 2017)</cite> only contains 6 encoder/decoder layers. show that Transformer models with more than 12 encoder layers fail to converge, and propose the Transparent Attention (TA) mechanism which weighted combines outputs of all encoder layers as encoded representation. However, the TA mechanism has to value outputs of shallow encoder layers to feedback sufficient gradients during back-propagation to ensure their convergence, which implies that weights of deep layers are likely to be hampered and against the motivation when go very deep, and as a result cannot get further improvements with more than 16 layers. reveal that deep Transformers with proper use of layer normalization is able to converge and propose to aggregate previous layers' outputs for each layer instead of at the end of encoding. Wu et al. (2019) research on incremental increasing the depth of the Transformer Big by freezing pre-trained shallow layers.",
  "y": "motivation"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_2",
  "x": "Our proposed approach additionally enables to benefit from deep decoders compared to previous works which focus on deep encoders. ---------------------------------- **INTRODUCTION** Neural machine translation has achieved great success in the last few years (Bahdanau et al., 2014; Gehring et al., 2017;<cite> Vaswani et al., 2017)</cite> . The Transformer <cite>(Vaswani et al., 2017)</cite> , which has outperformed previous RNN/CNN based translation models (Bahdanau et al., 2014; Gehring et al., 2017) , is based on multi-layer self-attention networks and can be trained very efficiently. The multi-layer structure allows the Transformer to model complicated functions. Increasing the depth of models can increase their capacity but may also cause optimization difficulties (Mhaskar et al., 2017; Telgarsky, 2016; Eldan and Shamir, 2016; He et al., 2016; . In order to ease optimization, the Transformer employs residual connection and layer normalization techniques which have been proven useful in reducing optimization difficulties of deep neural networks for various tasks (He et al., 2016; Ba et al., 2016) . However, even with residual connections and layer normalization, deep Transformers are still hard to train: the original Transformer <cite>(Vaswani et al., 2017)</cite> only contains 6 encoder/decoder layers.",
  "y": "background"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_3",
  "x": "In our research we focus on training problems of deep Transformers which prevent them from convergence (as opposed to other important issues such as over-fitting on the training set). To alleviate the training problem for the standard Transformer model, Layer Normalization (Ba et al., 2016) and Residual Connection (He et al., 2016 ) are adopted. The official implementation of the Transformer uses a different computation sequence (Figure 1 b) compared to the published version <cite>(Vaswani et al., 2017)</cite> (Figure 1 a), since it seems better for harder-to-learn models 1 . Though several papers Domhan, 2018) mentioned this change, how this modification impacts on the performance of the Transformer, especially for deep Transformers, has never been deeply studied before with empirical results to the best of our knowledge, except analyzed the difference between two computation orders during back-propagation, and Zhang et al. (2019) point out the same effects of normalization in concurrent work. In order to compare with , we used the datasets from the WMT 14 English to German task and the WMT 15 Czech to English task for experiments. We applied joint Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 32k merge operations. We used the same setting as the Transformer base <cite>(Vaswani et al., 2017)</cite> except the number of warm-up steps was set to 8k. We conducted our experiments based on the Neutron implementation (Xu and Liu, 2019) of the Transformer. Parameters were initialized with Glorot Initialization 2 (Glorot and Bengio, 2010) like in many other Transformer implementation (Klein et al., 2017; Hieber et al., 2017; .",
  "y": "background"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_4",
  "x": "The official implementation of the Transformer uses a different computation sequence (Figure 1 b) compared to the published version <cite>(Vaswani et al., 2017)</cite> (Figure 1 a), since it seems better for harder-to-learn models 1 . Though several papers Domhan, 2018) mentioned this change, how this modification impacts on the performance of the Transformer, especially for deep Transformers, has never been deeply studied before with empirical results to the best of our knowledge, except analyzed the difference between two computation orders during back-propagation, and Zhang et al. (2019) point out the same effects of normalization in concurrent work. In order to compare with , we used the datasets from the WMT 14 English to German task and the WMT 15 Czech to English task for experiments. We applied joint Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 32k merge operations. We used the same setting as the Transformer base <cite>(Vaswani et al., 2017)</cite> except the number of warm-up steps was set to 8k. We conducted our experiments based on the Neutron implementation (Xu and Liu, 2019) of the Transformer. Parameters were initialized with Glorot Initialization 2 (Glorot and Bengio, 2010) like in many other Transformer implementation (Klein et al., 2017; Hieber et al., 2017; . Our experiments run on 2 GTX 1080 Ti GPUs, and a batch size of at least 25k target tokens is achieved through gradient accumulation of small batches. We used a beam size of 4 for decoding, and evaluated tokenized case-sensitive BLEU with the averaged model of the last 5 checkpoints saved with an interval of 1,500 training steps <cite>(Vaswani et al., 2017)</cite> .",
  "y": "differences extends"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_5",
  "x": "Parameters were initialized with Glorot Initialization 2 (Glorot and Bengio, 2010) like in many other Transformer implementation (Klein et al., 2017; Hieber et al., 2017; . Our experiments run on 2 GTX 1080 Ti GPUs, and a batch size of at least 25k target tokens is achieved through gradient accumulation of small batches. We used a beam size of 4 for decoding, and evaluated tokenized case-sensitive BLEU with the averaged model of the last 5 checkpoints saved with an interval of 1,500 training steps <cite>(Vaswani et al., 2017)</cite> . Results of two different computation order are shown in Table 1 . v1 and v2 stand for the computation order of the proposed Transformer <cite>(Vaswani et al., 2017)</cite> and that of the official implementation respectively. \"\u00ac\" means fail to converge, \"None\" means not reported in original works, \"*\" indicates our implementation of their approach. \u2020 and \u2021 mean p < 0.01 and p < 0.05 while comparing between v1 and v2 of the same number of layers in significance test. ---------------------------------- **ANALYSIS AND LIPSCHITZ RESTRICTED PARAMETER INITIALIZATION** Since the subtle change of computation order results in huge differences in convergence, we analyze the differences between the computation orders to figure out how they affect convergence.",
  "y": "similarities"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_6",
  "x": "We used a beam size of 4 for decoding, and evaluated tokenized case-sensitive BLEU with the averaged model of the last 5 checkpoints saved with an interval of 1,500 training steps <cite>(Vaswani et al., 2017)</cite> . Results of two different computation order are shown in Table 1 . v1 and v2 stand for the computation order of the proposed Transformer <cite>(Vaswani et al., 2017)</cite> and that of the official implementation respectively. \"\u00ac\" means fail to converge, \"None\" means not reported in original works, \"*\" indicates our implementation of their approach. \u2020 and \u2021 mean p < 0.01 and p < 0.05 while comparing between v1 and v2 of the same number of layers in significance test. ---------------------------------- **ANALYSIS AND LIPSCHITZ RESTRICTED PARAMETER INITIALIZATION** Since the subtle change of computation order results in huge differences in convergence, we analyze the differences between the computation orders to figure out how they affect convergence. ---------------------------------- **COMPARISON BETWEEN COMPUTATION ORDERS**",
  "y": "background"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_7",
  "x": "We analyzed the influence of deep encoders and decoders separately and results are shown in Table 4 . Table 4 shows that the deep decoder can benefit the performance in addition to the deep encoder, especially on the Czech to English task. ---------------------------------- **CONCLUSION** In contrast to all previous works Wu et al., 2019) which show that deep Transformers with the computation order as in<cite> Vaswani et al. (2017)</cite> have difficulty in convergence. We empirically show that deep Transformers with the original computation order can converge as long as with proper parameter initialization. In this paper, we first investigate convergence differences between the published Transformer <cite>(Vaswani et al., 2017)</cite> and the official implementation of the Transformer , and compare the differences of computation orders between them. Then we conjecture the training problem of deep Transformers is because layer normalization sometimes shrinks residual connections, and propose this can be tackled simply with Lipschitz restricted parameter initialization. Our experiments demonstrate the effectiveness of our simple approach on the convergence of deep Transformers, and brings significant improvements on the WMT 14 English to German and the WMT 15 Czech to English news translation tasks.",
  "y": "background"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_8",
  "x": "We analyzed the influence of deep encoders and decoders separately and results are shown in Table 4 . Table 4 shows that the deep decoder can benefit the performance in addition to the deep encoder, especially on the Czech to English task. ---------------------------------- **CONCLUSION** In contrast to all previous works Wu et al., 2019) which show that deep Transformers with the computation order as in<cite> Vaswani et al. (2017)</cite> have difficulty in convergence. We empirically show that deep Transformers with the original computation order can converge as long as with proper parameter initialization. In this paper, we first investigate convergence differences between the published Transformer <cite>(Vaswani et al., 2017)</cite> and the official implementation of the Transformer , and compare the differences of computation orders between them. Then we conjecture the training problem of deep Transformers is because layer normalization sometimes shrinks residual connections, and propose this can be tackled simply with Lipschitz restricted parameter initialization. Our experiments demonstrate the effectiveness of our simple approach on the convergence of deep Transformers, and brings significant improvements on the WMT 14 English to German and the WMT 15 Czech to English news translation tasks.",
  "y": "differences"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_0",
  "x": "**ABSTRACT** ROOT9 is a supervised system for the classification of hypernyms, co-hyponyms and random words that is derived from the already introduced ROOT13 (Santus et al., 2016). It relies on a Random Forest algorithm and nine unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are present, ROOT9 achieves an F1 score of 90.7%, against a baseline of 57.2% (vector cosine). When the classification is binary, ROOT9 achieves the following results against the baseline: hypernyms-co-hyponyms 95.7% vs. 69.8%, hypernyms-random 91.8% vs. 64.1% and co-hyponyms-random 97.8% vs. 79.4%. In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the<cite> Weeds et al. (2014)</cite> datasets, proving that it is in fact competitive. Finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by Levy et al. (2015) . The second possibility seems to be the most likely, even though ROOT9 can be trained on negative examples (i.e., switched hypernyms) to drastically reduce this bias.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_1",
  "x": "**INTRODUCTION** Distinguishing hypernyms from co-hyponyms and, in turn, discriminating them from semantically unrelated words (henceforth randoms) is a fundamental task in Natural Language Processing (NLP). Hypernymy in fact represents a key organization principle of semantic memory (Murphy, 2002) , the backbone of taxonomies and ontologies, and one of the crucial semantic relations supporting lexical entailment (Geffet and Dagan, 2005) . Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar<cite> (Weeds et al., 2014)</cite> . The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on<cite> (Weeds et al., 2014</cite>; Tungthamthiti et al. 2015) . For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014<cite> , Weeds et al., 2014</cite> Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003) . Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in<cite> Weeds et al. (2014)</cite> , even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b) , a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011) .",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_2",
  "x": "The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on<cite> (Weeds et al., 2014</cite>; Tungthamthiti et al. 2015) . For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014<cite> , Weeds et al., 2014</cite> Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003) . Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in<cite> Weeds et al. (2014)</cite> , even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b) , a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011) . The ablation test has shown that four out of thirteen features were actually not contributing to the system's performance, and they were therefore removed, turning ROOT13 into ROOT9. On the 9,600 pairs, ROOT9 achieved an F1 score of 90.7% when the three classes were present, 95.7% when we had to discriminate hypernyms and co-hyponyms, 91.8% for hypernyms and randoms, and 97.8% for co-hyponyms and randoms. In order to compare ROOT9 with the state-of-the-art, we have also evaluated it in the<cite> Weeds et al. (2014)</cite> datasets. Unfortunately, ROOT9 was not able to cover the full datasets, as several words in their pairs were missing from our Distributional Semantic Model (DSM) because of their low frequency.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_3",
  "x": "Distinguishing hypernyms from co-hyponyms and, in turn, discriminating them from semantically unrelated words (henceforth randoms) is a fundamental task in Natural Language Processing (NLP). Hypernymy in fact represents a key organization principle of semantic memory (Murphy, 2002) , the backbone of taxonomies and ontologies, and one of the crucial semantic relations supporting lexical entailment (Geffet and Dagan, 2005) . Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar<cite> (Weeds et al., 2014)</cite> . The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on<cite> (Weeds et al., 2014</cite>; Tungthamthiti et al. 2015) . For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014<cite> , Weeds et al., 2014</cite> Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003) . Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in<cite> Weeds et al. (2014)</cite> , even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b) , a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011) . The ablation test has shown that four out of thirteen features were actually not contributing to the system's performance, and they were therefore removed, turning ROOT13 into ROOT9.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_4",
  "x": "---------------------------------- **INTRODUCTION** Distinguishing hypernyms from co-hyponyms and, in turn, discriminating them from semantically unrelated words (henceforth randoms) is a fundamental task in Natural Language Processing (NLP). Hypernymy in fact represents a key organization principle of semantic memory (Murphy, 2002) , the backbone of taxonomies and ontologies, and one of the crucial semantic relations supporting lexical entailment (Geffet and Dagan, 2005) . Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar<cite> (Weeds et al., 2014)</cite> . The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on<cite> (Weeds et al., 2014</cite>; Tungthamthiti et al. 2015) . For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014<cite> , Weeds et al., 2014</cite> Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003) . Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in<cite> Weeds et al. (2014)</cite> , even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b) , a supervised method based on a Random Forest algorithm and thirteen corpus-based features.",
  "y": "extends background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_5",
  "x": "On the 9,600 pairs, ROOT9 achieved an F1 score of 90.7% when the three classes were present, 95.7% when we had to discriminate hypernyms and co-hyponyms, 91.8% for hypernyms and randoms, and 97.8% for co-hyponyms and randoms. In order to compare ROOT9 with the state-of-the-art, we have also evaluated it in the<cite> Weeds et al. (2014)</cite> datasets. Unfortunately, ROOT9 was not able to cover the full datasets, as several words in their pairs were missing from our Distributional Semantic Model (DSM) because of their low frequency. Nevertheless, the authors kindly provided the results of their models on our subsets, so that the comparison can be considered reliable. Also in 1 The 9,600 pairs are available at https://github.com/esantus/ROOT9 relation to the state of the art, ROOT9 is proved to be competitive, being slightly outperformed in all the datasets only by the svmCAT model<cite> (Weeds et al., 2014)</cite> , which is a Support Vector Machine (SVM) classifier run on the concatenation of the distributional vectors of the words in the pairs. Finally, we carried out an extra test to verify whether the system was actually learning the semantic relation between two word pairs, or simply identifying prototypical hypernyms (Levy et al. 2015) . The test consisted in providing to the trained model switched hypernyms (e.g. from \"dog HYPER animal\" to \"dog RANDOM fruit\"), and verify how they were classified. Our results show that most of the switched hypernyms were in fact misclassified as hypernyms (especially when the words in the switched hypernyms were the same used to train the model on the real hypernyms), and that the only way to overcome such problem is to explicitly provide the model with bad examples (i.e., switched hypernyms tagged as randoms) during the training. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_6",
  "x": "In order to compare ROOT9 with the state-of-the-art, we have also evaluated it in the<cite> Weeds et al. (2014)</cite> datasets. Unfortunately, ROOT9 was not able to cover the full datasets, as several words in their pairs were missing from our Distributional Semantic Model (DSM) because of their low frequency. Nevertheless, the authors kindly provided the results of their models on our subsets, so that the comparison can be considered reliable. Also in 1 The 9,600 pairs are available at https://github.com/esantus/ROOT9 relation to the state of the art, ROOT9 is proved to be competitive, being slightly outperformed in all the datasets only by the svmCAT model<cite> (Weeds et al., 2014)</cite> , which is a Support Vector Machine (SVM) classifier run on the concatenation of the distributional vectors of the words in the pairs. Finally, we carried out an extra test to verify whether the system was actually learning the semantic relation between two word pairs, or simply identifying prototypical hypernyms (Levy et al. 2015) . The test consisted in providing to the trained model switched hypernyms (e.g. from \"dog HYPER animal\" to \"dog RANDOM fruit\"), and verify how they were classified. Our results show that most of the switched hypernyms were in fact misclassified as hypernyms (especially when the words in the switched hypernyms were the same used to train the model on the real hypernyms), and that the only way to overcome such problem is to explicitly provide the model with bad examples (i.e., switched hypernyms tagged as randoms) during the training. ---------------------------------- **RELATED WORK**",
  "y": "differences"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_7",
  "x": "Clarke (2009) extended the DIH, suggesting that generality difference can be calculated as the degree to which the narrower term's dimensions have lower values than the broader ones, across all the intersected dimensions. Lenci and Benotto (2012) adapted this measure to check not only to which extent the features of the narrower term are included in the features of the broader, but also how the features of the broader are not included in the features of the narrower. Kotlerman et al. (2010) combined Average Precision (AP) with the balancing approach of Szpektor and Dagan (2008) , outperforming the above mentioned methods. Herbelot and Ganesalingam (2013) measured the Kullback-Leibler (KL) divergence between the probability distribution over context words for a term, and the background probability distribution, based on the idea that the smaller such KL was, the less informative the word was (and therefore more likely to be a hypernym). Rimmel (2014) considered the top features in a context vector as topics and used a Topic Coherence (TC) measure. Santus et al. (2014a) formulated the Distributional Informativeness Hypothesis (DInH), according to which the generality of a term can be inferred from the informativeness of its most typical linguistic contexts. In their evaluation, the authors have shown that hypernyms' most typical contexts are in fact less informative than hyponyms' ones. Among the supervised methods, Baroni et al. (2012) proposed to use an SVM classifier on the concatenation (after having tried also subtraction and division) of the vectors. Roller et al. (2014) used the vectors' difference, while<cite> Weeds et al. (2014)</cite> implemented numerous combinations (difference, multiplication, sum, concatenation, etc.), comparing them against the most common unsupervised methods.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_8",
  "x": "---------------------------------- **TASKS** We have performed three tasks: i) an ablation test to evaluate the contribution of the features on our dataset (henceforth, ROOT9 Dataset; see Section 4.2); ii) an evaluation against the state of the art, and -in particularagainst the best performant models in<cite> Weeds et al. (2014)</cite> ; iii) an evaluation on switched pairs to verify whether the actual semantic relations or the prototypical hypernyms (Levy et al., 2015) were learnt. For what concerns the ablation test, we performed it on a tree-classes classification task (hypernyms, co-hyponyms and randoms), removing each feature at a time and measuring the loss/gain (F1 score is used for the evaluation on a 10-fold cross validation). Thanks to this task, we have found that four of our features were in fact redundant, and we have therefore removed them from the final model, turning ROOT13 into ROOT9. This is discussed in Section 5. Once the best model has been identified, we have performed three binary classification tasks, involving only two classes per time. F1 score on a 10-fold cross validation was chosen as accuracy measure. The second task, which is described in Section 6, consisted in binary classification tasks on the four datasets proposed by<cite> Weeds et al. (2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_9",
  "x": "Thanks to this task, we have found that four of our features were in fact redundant, and we have therefore removed them from the final model, turning ROOT13 into ROOT9. This is discussed in Section 5. Once the best model has been identified, we have performed three binary classification tasks, involving only two classes per time. F1 score on a 10-fold cross validation was chosen as accuracy measure. The second task, which is described in Section 6, consisted in binary classification tasks on the four datasets proposed by<cite> Weeds et al. (2014)</cite> . These datasets are described below, in Section 4.3. The task allowed us to compare ROOT9 against the state of the art models reported in<cite> Weeds et al. (2014)</cite> . The last task is described in Section 7. It was performed on an extended ROOT9 Dataset, including also 3,200 randomly switched hypernyms to verify whether they were classified as hypernyms or as randoms.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_10",
  "x": "Thanks to this task, we have found that four of our features were in fact redundant, and we have therefore removed them from the final model, turning ROOT13 into ROOT9. This is discussed in Section 5. Once the best model has been identified, we have performed three binary classification tasks, involving only two classes per time. F1 score on a 10-fold cross validation was chosen as accuracy measure. The second task, which is described in Section 6, consisted in binary classification tasks on the four datasets proposed by<cite> Weeds et al. (2014)</cite> . These datasets are described below, in Section 4.3. The task allowed us to compare ROOT9 against the state of the art models reported in<cite> Weeds et al. (2014)</cite> . The last task is described in Section 7. It was performed on an extended ROOT9 Dataset, including also 3,200 randomly switched hypernyms to verify whether they were classified as hypernyms or as randoms.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_11",
  "x": "The class of randoms contains 1,100 noun pairs, 1,050 verb pairs and 1,050 random pairs. The full dataset contains 4,263 terms (2,380 nouns, 958 verbs and 927 adjectives), so that every term occurs on average 4.5 times. Considering only the first word in the pairs, we have 1,265 different terms (987 nouns, 186 verbs and 92 adjectives). Considering instead only the second word, we have 3,665 terms (1,945 nouns, 860 verbs and 862 adjectives). In the third task, we have extended this dataset randomly switching the 3,200 hypernymy pairs (e.g. from \"car HYPER vehicle\" to \"car RANDOM mammal\") to verify whether ROOT9 was able to classify them as randoms. ---------------------------------- **WEEDS DATASET** In order to compare ROOT9 to the state-of-the-art, we have evaluated it with the datasets created by<cite> Weeds et al. (2014)</cite> . 2 These are four datasets, containing respectively: i) hypernyms versus other relations (extracted from WordNet; henceforth WN Hyper); ii) co-hyponyms versus other relations (extracted from WordNet; henceforth WN Co-Hyp); iii) hypernyms versus other relations (extracted from BLESS; henceforth Bless Hyper); iv) co-hyponyms versus other relations (extracted from BLESS; henceforth Bless Co-Hyp).",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_12",
  "x": "2 These are four datasets, containing respectively: i) hypernyms versus other relations (extracted from WordNet; henceforth WN Hyper); ii) co-hyponyms versus other relations (extracted from WordNet; henceforth WN Co-Hyp); iii) hypernyms versus other relations (extracted from BLESS; henceforth Bless Hyper); iv) co-hyponyms versus other relations (extracted from BLESS; henceforth Bless Co-Hyp). The WN dataset<cite> (Weeds et al., 2014</cite> ) -meaning both WN Hyper and WN Co-Hyp -in particular, was built after noticing that supervised systems tended to perform well also on random vectors. This happens because they are able to learn ontological information and re-use it whenever the words re-appear in other pairs. For this reason, the authors have constructed a dataset where words occurred at most twice (once on the left and once on the right of the relation). In this dataset, ontological information cannot be learnt and re-used, and indeed the random vectors cannot perform well. Unfortunately our DSM did not cover the whole datasets, because of the chosen frequency threshold (in Table 1 , we report the size of our subsets in comparison to the original datasets). However,<cite> Weeds et al. (2014)</cite> ---------------------------------- **BASELINES AND OTHER MODELS**",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_13",
  "x": "The full dataset contains 4,263 terms (2,380 nouns, 958 verbs and 927 adjectives), so that every term occurs on average 4.5 times. Considering only the first word in the pairs, we have 1,265 different terms (987 nouns, 186 verbs and 92 adjectives). Considering instead only the second word, we have 3,665 terms (1,945 nouns, 860 verbs and 862 adjectives). In the third task, we have extended this dataset randomly switching the 3,200 hypernymy pairs (e.g. from \"car HYPER vehicle\" to \"car RANDOM mammal\") to verify whether ROOT9 was able to classify them as randoms. ---------------------------------- **WEEDS DATASET** In order to compare ROOT9 to the state-of-the-art, we have evaluated it with the datasets created by<cite> Weeds et al. (2014)</cite> . 2 These are four datasets, containing respectively: i) hypernyms versus other relations (extracted from WordNet; henceforth WN Hyper); ii) co-hyponyms versus other relations (extracted from WordNet; henceforth WN Co-Hyp); iii) hypernyms versus other relations (extracted from BLESS; henceforth Bless Hyper); iv) co-hyponyms versus other relations (extracted from BLESS; henceforth Bless Co-Hyp). The WN dataset<cite> (Weeds et al., 2014</cite> ) -meaning both WN Hyper and WN Co-Hyp -in particular, was built after noticing that supervised systems tended to perform well also on random vectors.",
  "y": "uses background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_15",
  "x": "---------------------------------- **BASELINES AND OTHER MODELS** For our internal tests, we have implemented two baselines, which can be used as reference for evaluating the performance of ROOT9: COSINE and RANDOM13. The first baseline simply uses the vector cosine (COSINE) with a Random Forest classifier in the default settings (i.e. 100 trees, 1 seed, and maxDepth and numFeatures initialized to 0). This baseline is supposed to perform particularly well in discriminating similar words (i.e. hypernyms and co-hyponyms) from randoms. In fact, this measure has been extensively used to identify word similarity in vector spaces (Turney and Pantel, 2010) because it verifies the normalized correlation between the vectors of 1 and 2 : where is the i-th dimension in the vector x. The second baseline (RANDOM13) relies on a default Random Forest classifier, but uses thirteen randomly initialized features, with values between 0 and 1. While the vector cosine achieves a reasonable accuracy, which is anyway far below the results obtained by our model, the random baseline performs much worst. The discrepancy with what found by<cite> Weeds et al. (2014)</cite> namely that random vectors perform particularly well when words are re-used in the dataset -may depend on the small number of features, which does not allow the system to identify discriminative random dimensions.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_16",
  "x": "where is the i-th dimension in the vector x. The second baseline (RANDOM13) relies on a default Random Forest classifier, but uses thirteen randomly initialized features, with values between 0 and 1. While the vector cosine achieves a reasonable accuracy, which is anyway far below the results obtained by our model, the random baseline performs much worst. The discrepancy with what found by<cite> Weeds et al. (2014)</cite> namely that random vectors perform particularly well when words are re-used in the dataset -may depend on the small number of features, which does not allow the system to identify discriminative random dimensions. In the second task (see Section 6), we have used as baselines the most competitive models reported in<cite> Weeds et al. (2014)</cite> , namely the SVM classifiers trained on the PPMI vector of the second word (svmSINGLE), or on the concatenated (svmCAT), summed (svmADD), multiplied (svmMULT) and subtracted (svmDIFF) PPMI vectors of the words in the pair. Such vectors contain as features all major grammatical dependency relations involving open class Parts Of Speech. Also, the performance of three main unsupervised methods is reported as a reference: cosine (see above in this section), balAPinc (Kotlerman et al., 2010) and invCL (Lenci and Benotto, 2012) . A threshold p empirically found in a training set was used in these methods for the decision, Table 2 . Ablation test, F1 scores on a 10-fold cross validation and loss/gain values. Scores are in percent.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_17",
  "x": "For the sake of completeness, in Table 2 we also report the performance of ROOT9 using Logistic Regression (Cessie, 1992) and SMO (Keerthi et al., 2001) classifiers. As it can be seen, the Random Forest version largely outperforms the other classifiers in this dataset. However, it is worth noticing here that such difference disappears with the WN datasets proposed by<cite> Weeds et al. (2014)</cite> . See section 6, and -in particular - Table 3 . F1 scores on a 10-fold cross validation for binary classification tasks. Scores are in percent. Table 3 describes the results of ROOT9 and the baseline in the binary classification tasks. These results confirm the analysis suggested above. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_18",
  "x": "Scores are in percent. Table 3 describes the results of ROOT9 and the baseline in the binary classification tasks. These results confirm the analysis suggested above. ---------------------------------- **TASK 2: ROOT9 VS. STATE OF THE ART** In Table 4 , we show ROOT9's performance compared to the best systems reported by<cite> Weeds et al. (2014)</cite> . The scores are all calculated on subsets of<cite> Weeds et al. (2014)</cite> 's datasets, as reported in Section 4.3. Considering all the datasets, ROOT9 is the second best performing system, after svmCAT<cite> (Weeds et al., 2014)</cite> , which uses the SVM classifier on the concatenation of PPMI vectors, containing as features all major grammatical dependency relations involving open class Parts Of Speech. The SVM classifier on the sum (svmADD) and the multiplication (svmMULT) of the same PPMI vectors performs better in identifying co-hyponyms, but worst in identifying hypernyms.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_19",
  "x": "See section 6, and -in particular - Table 3 . F1 scores on a 10-fold cross validation for binary classification tasks. Scores are in percent. Table 3 describes the results of ROOT9 and the baseline in the binary classification tasks. These results confirm the analysis suggested above. ---------------------------------- **TASK 2: ROOT9 VS. STATE OF THE ART** In Table 4 , we show ROOT9's performance compared to the best systems reported by<cite> Weeds et al. (2014)</cite> . The scores are all calculated on subsets of<cite> Weeds et al. (2014)</cite> 's datasets, as reported in Section 4.3.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_20",
  "x": "In Table 4 , we show ROOT9's performance compared to the best systems reported by<cite> Weeds et al. (2014)</cite> . The scores are all calculated on subsets of<cite> Weeds et al. (2014)</cite> 's datasets, as reported in Section 4.3. Considering all the datasets, ROOT9 is the second best performing system, after svmCAT<cite> (Weeds et al., 2014)</cite> , which uses the SVM classifier on the concatenation of PPMI vectors, containing as features all major grammatical dependency relations involving open class Parts Of Speech. The SVM classifier on the sum (svmADD) and the multiplication (svmMULT) of the same PPMI vectors performs better in identifying co-hyponyms, but worst in identifying hypernyms. The SVM on the difference (svmDIFF) and on the second PPMI vector (svmSINGLE) is instead particularly good at identifying hypernyms, while it performs bad at identifying co-hyponyms. Among the unsupervised methods, we report the results for the cosine and the methods of Lenci and Benotto (2012; invCL) and Kotlerman et al. (2010; balAPinc Table 4 . F1 scores, in percent, on a 10-fold cross validation (state of the art models are evaluated on a 5-fold cross validation). {bold= best results vs. ROOT9; italics = other classifiers}. ----------------------------------",
  "y": "differences background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_21",
  "x": [
   "In this evaluation, we have noticed that a large number of the switched hypernyms were indeed misclassified as hypernyms (up to 100% of them, if the words in the testing switched pairs were exactly the same used as hypernyms in the training set). In the attempt of correcting the behavior of the classifier, we extended the original 9,600 pairs dataset with other 3,200 switched hypernyms pairs labeled as randoms. It is relevant to notice that the switched hypernyms (tagged as randoms) contain the same words used in for the real hypernyms, and that in this new dataset, the size of the random class is double the others, including a total of 6,400 pairs. The new 10-fold cross validation test on the three classes registered a significant loss, passing from 90.7% to 84%. However, only 576 out of 6,400 randoms (most of which are likely to be the switched pairs) were misclassified as hypernyms. ---------------------------------- **CONCLUSIONS** In this paper, we have described ROOT9, a classifier for hypernyms, co-hyponyms and random words that is derived from an optimization of ROOT13 (Santus et al., 2016b) . The classifier, based on the Random Forest algorithm, uses only nine unsupervised corpus-based features, which have been described, and their contribution assessed."
  ],
  "y": "uses future_work"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_0",
  "x": "Consider two debate-portal arguments for \"advancing the common good is better than personal pursuit\", taken from the corpora analyzed later in this paper: Argument A \"While striving to make advancements for the common good you can change the world forever. Allot of people have succeded in doing so. Our founding fathers, Thomas Edison, George Washington, Martin Luther King jr, and many more. These people made huge advances for the common good and they are honored for it.\" Argument B \"I think the common good is a better endeavor, because it's better to give then to receive. It's better to give other people you're hand out in help then you holding your own hand.\" In the study of Habernal and Gurevych (2016b) , annotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory<cite> (Wachsmuth et al., 2017a)</cite> and one with 17 reasons for quality differences phrased spontaneously in practice (Habernal and Gurevych, 2016a) .",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_2",
  "x": "A fallacy is a kind of error that undermines reasoning (Tindale, 2007) . Strength may mean cogency but also rhetorical effectiveness (Perelman and Olbrechts-Tyteca, 1969) . Rhetoric has been studied since Aristotle (2007) who developed the notion of the means of persuasion (logos, ethos, pathos) and their linguistic delivery in terms of arrangement and style. Dialectical quality dimensions resemble those of cogency, but arguments are judged specifically by their reasonableness for achieving agreement (van Eemeren and Grootendorst, 2004) . <cite>Wachsmuth et al. (2017a)</cite> point out that dialectical builds on rhetorical, and rhetorical builds on logical quality. They derive a unifying taxonomy from the major theories, decomposing quality hierarchically into cogency, effectiveness, reasonableness, and subdimensions. 5-1 B is attacking / abusive. 5-2 B has language/grammar issues, or uses humour or sarcasm. 5-3 B is unclear / hard to follow.",
  "y": "background"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_3",
  "x": "Persing and Ng (2015) annotated the argumentative strength of essays composing multiple arguments with notable agreement. For single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e.g., Cabrio and Villata (2012) find accepted arguments based on attack relations, Wei et al. (2016) rank arguments by their persuasiveness, and Wachsmuth et al. (2017b) rank them by their relevance. Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. Based on the idea that relative assessment is easier, Habernal and Gurevych (2016b) crowdsourced the UKPConvArg1 corpus. Argument pairs (A, B) from a debate portal were classified as to which argument is more convincing. Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a follow-up study (Habernal and Gurevych, 2016a) , these reasons were used to derive a hierarchical annotation scheme. 9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- <cite>Wachsmuth et al. (2017a)</cite> given for each of the 17+1 reason labels of Habernal and Gurevych (2016a) . Bold/gray: Highest/lowest value in each column. Bottom row: The number of labels for each dimension.",
  "y": "background"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_4",
  "x": "Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a follow-up study (Habernal and Gurevych, 2016a) , these reasons were used to derive a hierarchical annotation scheme. 9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- <cite>Wachsmuth et al. (2017a)</cite> given for each of the 17+1 reason labels of Habernal and Gurevych (2016a) . Bold/gray: Highest/lowest value in each column. Bottom row: The number of labels for each dimension. by crowd workers (UKPConvArg2). These pairs represent the practical view in our experiments. ---------------------------------- **MATCHING THEORY AND PRACTICE** We now report on experiments that we performed to examine to what extent the theory and practice of argumentation quality assessment match.",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_5",
  "x": "This raises the question of how absolute ratings of arguments based on theory relate to relative comparisons of argument pairs in practice. We informally state three hypotheses: The reasons for quality differences in practice are adequately represented in theory. Hypothesis 2 The perception of overall argumentation quality is the same in theory and practice. Hypothesis 3 Relative quality differences are reflected by differences in absolute quality ratings. As both corpora described in Section 2 are based on the UKPConvArg1 corpus and thus share many arguments, we can test the hypotheses empirically. ---------------------------------- **CORRELATIONS OF DIMENSIONS AND REASONS** For Hypotheses 1 and 2, we consider all 736 pairs of arguments from Habernal and Gurevych (2016a) where both have been annotated by <cite>Wachsmuth et al. (2017a)</cite> .",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_6",
  "x": "2  Table 3 presents all \u03c4 -values. The phrasing of a reason can be assumed to indicate a clear quality difference-this is underlined by the generally high correlations. Analyzing the single values, we find much evidence for Hypothesis 1: Most notably, label 5-1 perfectly correlates with global acceptability, fitting the intuition that abuse is not acceptable. The high \u03c4 's of 8-5 (more credible) for local acceptability (.73) and of 9-4 (well thought through) for cogency (.75) confirm the match assumed in Section 1. Also, the values of 5-3 (unclear) for clarity (.91) and of 7-2 (non-sense) for reasonableness (.94) as well as the weaker correlation of 8-4 (objective) for emotional appeal (.35) makes sense. Only the comparably low \u03c4 of 6-1 (no credible evidence) for local acceptability (.49) and credibility (.52) seem really unexpected. Besides, the descriptions of 6-2 and 6-3 sound like local but cor- Table 4 : The mean rating for each quality dimension of those arguments from <cite>Wachsmuth et al. (2017a)</cite> given for each reason label (Habernal and Gurevych, 2016a) . The bottom rows show that the minimum maximum mean ratings are consistently higher for the positive properties than for the negative properties. relate more with global relevance and sufficiency respectively. Similarly, 7-3 (off-topic) correlates strongly with local and global relevance (both .95).",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_7",
  "x": "In line with Hypothesis 2, the highest correlation of Conv is indeed given for overall quality (.64). Thus, argumentation quality assessment seems to match in theory and practice to a broad extent. ---------------------------------- **ABSOLUTE RATINGS FOR RELATIVE DIFFERENCES** The correlations found imply that the relative quality differences captured are reflected in absolute differences. For explicitness, we computed the mean rating for each quality dimension of all arguments from <cite>Wachsmuth et al. (2017a)</cite> with a particular reason label from Habernal and Gurevych (2016a) . As each reason refers to one argument of a pair, this reveals whether the labels, although meant to signal relative differences, indicate absolute ratings. Table 4 compares the mean ratings of \"negative labels\" (5-1 to 7-4) and \"positive\" ones (8-1 to 9-4). For all dimensions, the maximum and minimum value are higher for the positive than for the negative labels-a clear support of Hypothesis 3.",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_8",
  "x": "The results of Section 3 suggest that theory may guide the assessment of argumentation quality in practice. In this section, we evaluate the reliability of a crowd-based annotation process. ---------------------------------- **ABSOLUTE QUALITY RATINGS BY THE CROWD** We emulated the expert annotation process carried out by <cite>Wachsmuth et al. (2017a)</cite> on CrowdFlower in order to evaluate whether lay annotators suffice for a theory-based quality assessment. In particular, we asked the crowd to rate the same 304 arguments as the experts for all 15 given quality dimensions with scores from 1 to 3 (or choose \"cannot judge\"). Each argument was rated 10 times at an offered price of $0.10 for each rating (102 annotators in total). Given the crowd ratings, we then performed two comparisons as detailed in the following. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_9",
  "x": "We emulated the expert annotation process carried out by <cite>Wachsmuth et al. (2017a)</cite> on CrowdFlower in order to evaluate whether lay annotators suffice for a theory-based quality assessment. In particular, we asked the crowd to rate the same 304 arguments as the experts for all 15 given quality dimensions with scores from 1 to 3 (or choose \"cannot judge\"). Each argument was rated 10 times at an offered price of $0.10 for each rating (102 annotators in total). Given the crowd ratings, we then performed two comparisons as detailed in the following. ---------------------------------- **AGREEMENT OF THE CROWD WITH EXPERTS** First, we checked to what extent lay annotators and experts agree in terms of Krippendorff's \u03b1. On one hand, we compared the mean of all 10 crowd ratings to the mean of the three ratings of <cite>Wachsmuth et al. (2017a)</cite> . On the other hand, we estimated a reliable rating from the crowd ratings using MACE (Hovy et al., 2013) and compared it to the experts.",
  "y": "uses"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_0",
  "x": "Many weakly supervised and unsupervised information extraction techniques assess the correctness of extractions using the distributional hypothesis-the notion that words with similar meanings tend to occur in similar contexts (Harris, 1985) . A candidate extraction of a relation is deemed more likely to be correct when it appears in contexts similar to those of \"seed\" instances of the relation, where the seeds may be specified by hand (Pa\u015fca et al., 2006) , taken from an existing, incomplete knowledge base (Snow et al., 2006; Pantel et al., 2009) , or obtained in an unsupervised manner using a generic extractor (Banko et al., 2007) . We refer to this technique as Assessment by Distributional Similarity (ADS). Typically, distributional similarity is computed by comparing co-occurrence counts of extractions and seeds with various contexts found in the corpus. Statistical Language Models (SLMs) include methods for more accurately estimating co-occurrence probabilities via back-off, smoothing, and clustering techniques (e.g. (Chen and Goodman, 1996; Rabiner, 1989; Bell et al., 1990) ). Because SLMs can be trained from only unlabeled text, they can be applied for ADS even when the relations of interest are not specified in advance <cite>(Downey et al., 2007)</cite> . Unlabeled text is abundant in large corpora like the Web, making nearly-ceaseless automated optimization of SLMs possible. But how fruitful is such an effort likely to be-to what extent does optimizing a language model over a fixed corpus lead to improvements in assessment accuracy? In this paper, we show that an ADS technique based on SLMs is improved substantially when the language model it employs becomes more accurate. In a large-scale set of experiments, we quantify how language model perplexity correlates with ADS performance over multiple data sets and SLM techniques.",
  "y": "differences"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_1",
  "x": "For relations of arity greater than one, we consider the typechecking task, an important sub-task of extraction <cite>(Downey et al., 2007)</cite> . The typechecking task is to rank extractions with arguments that are of the proper type for a relation above type errors. As an example, the extraction Founded(Bill Gates, Oracle) is type correct, but is not correct for the extraction task. ---------------------------------- **STATISTICAL LANGUAGE MODELS** A Statistical Language Model (SLM) is a probability distribution P (w) over word sequences w = (w 1 , ..., w r ). The most common SLM techniques are n-gram models, which are Markov models in which the probability of a given word is dependent on only the previous n\u22121 words. The accuracy of an n-gram model of a corpus depends on two key factors: the choice of n, and the smoothing technique employed to assign probabilities to word sequences seen infrequently in training. We experiment with choices of n from 2 to 4, and two popular smoothing approaches, Modified Kneser-Ney (Chen and Goodman, 1996) and Witten-Bell (Bell et al., 1990) .",
  "y": "uses"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_2",
  "x": "The typechecking task is to rank extractions with arguments that are of the proper type for a relation above type errors. As an example, the extraction Founded(Bill Gates, Oracle) is type correct, but is not correct for the extraction task. ---------------------------------- **STATISTICAL LANGUAGE MODELS** A Statistical Language Model (SLM) is a probability distribution P (w) over word sequences w = (w 1 , ..., w r ). The most common SLM techniques are n-gram models, which are Markov models in which the probability of a given word is dependent on only the previous n\u22121 words. The accuracy of an n-gram model of a corpus depends on two key factors: the choice of n, and the smoothing technique employed to assign probabilities to word sequences seen infrequently in training. We experiment with choices of n from 2 to 4, and two popular smoothing approaches, Modified Kneser-Ney (Chen and Goodman, 1996) and Witten-Bell (Bell et al., 1990) . Unsupervised Hidden Markov Models (HMMs) are an alternative SLM approach previously shown to offer accuracy and scalability advantages over ngram models in ADS <cite>(Downey et al., 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_3",
  "x": "In our experiments, we utilize an ADS approach previously proposed for HMMs <cite>(Downey et al., 2007)</cite> and adapt it to also apply to n-gram models, as detailed below. Define a context of an extraction argument e i to be a string containing the m words preceding and m words following an occurrence of e i in the corpus. Let C i = {c 1 , c 2 , ..., c |C i | } be the union of all contexts of extraction arguments e i and seed arguments s i for a given relation R. We create a probabilistic context vector for each extraction e i where the j-th dimension of the vector is the probability of the context surrounding given the extraction, P (c j |e i ), computed from the language model. 1 We rank the extractions in U R according to how similar their arguments' contextual distributions, P (c|e i ), are to those of the seed arguments. Specifically, extractions are ranked according to: where KL represents KL Divergence, and the outer sum is taken over arguments e i of the extraction e. For HMMs, we alternatively rank extractions using the HMM state distributions P (t|e i ) in place of the probabilistic context vectors P (c|e i ). Our experiments show that state distributions are much more accurate for ADS than are HMM context vectors. ---------------------------------- **EXPERIMENTS**",
  "y": "extends"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_4",
  "x": "The first two data sets evaluate ADS for unsupervised information extraction, and were taken from <cite>(Downey et al., 2007)</cite> . The first, Unary, was an extraction task for unary relations (Company, Country, Language, Film) and the second, Binary, was a type-checking task for binary relations (Conquered, Founded, Headquartered, Merged). The 10 most frequent extractions served as bootstrapped seeds. The two test sets contained 361 and 265 extractions, respectively. The third data set, Wikipedia, evaluates ADS on weaklysupervised extraction, using seeds and extractions taken from Wikipedia 'List of' pages (Pantel et al., 2009) . Seed sets of various sizes (5, 10, 15 and 20) were randomly selected from each list, and we present results averaged over 10 random samplings. Other members of the seed list were added to a test set as correct extractions, and elements from other lists were added as errors. The data set included 2264 extractions across 36 unary relations, including Composers and US Internet Companies. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_5",
  "x": "We evaluate on the Unary and Binary data sets, since they have been employed in previous work on our corpora. Figure 2 shows that for HMMs, ADS performance increases as perplexity decreases across various model configurations (a similar relationship holds for n-gram models). A model selection technique that picks the HMM model with lowest perplexity (HMM 1-100) results in better ADS performance than previous results. As shown in Table 2, HMM 1-100 reduces error over the HMM-T model in <cite>(Downey et al., 2007)</cite> by 26%, on average. The experiments also reveal an important difference between the HMM and n-gram approaches. While KN3 is more accurate in SLM than our HMM models, it performs worse in ADS on average. For example, HMM 1-25 underperforms KN3 in perpexity, at 537.2 versus 227.1, but wins in ADS, 0.880 to 0.853. We hypothesize that this is because the latent state distributions in the HMMs provide a more informative distributional similarity measure. Indeed, when we compute distributional similarity for HMMs using probabilistic context vectors as opposed to state distributions, ADS performance for HMM 1-25 decreases to 5.8% below that of KN3.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_0",
  "x": "The added paths consisted of unlexicalized dependency labels obtained from a dependency parsed external corpus. To improve the expressivity of the added paths, instead of the unlexicalized labels, (Gardner et al., 2013) augmented the KB graph with verbs (surface relations) from a corpus containing over 600 million Subject-Verb-Object (SVO) triples. These verbs act as edges that connect previously unconnected entities thereby increasing the connectivity of the KB graph which can potentially improve PRA performance. However, na\u00efvely adding these edges increases the feature sparsity which degrades the discriminative ability of the logistic regression classifier used in PRA. This can be addressed by adding latent relations obtained by clustering the surface relations, instead of directly adding the surface relations. This reduces feature sparsity and has been shown to improve PRA inference (Gardner et al., 2013) , <cite>(Gardner et al., 2014)</cite> . In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus. We term these noun phrases as bridging entities since they bridge two KB relations to form a path. This is different from the scheme in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> , which adds edges between KB nodes by mining surface relations from an external corpus.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_1",
  "x": "This reduces feature sparsity and has been shown to improve PRA inference (Gardner et al., 2013) , <cite>(Gardner et al., 2014)</cite> . In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus. We term these noun phrases as bridging entities since they bridge two KB relations to form a path. This is different from the scheme in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> , which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_2",
  "x": "This can be addressed by adding latent relations obtained by clustering the surface relations, instead of directly adding the surface relations. This reduces feature sparsity and has been shown to improve PRA inference (Gardner et al., 2013) , <cite>(Gardner et al., 2014)</cite> . In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus. We term these noun phrases as bridging entities since they bridge two KB relations to form a path. This is different from the scheme in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> , which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> .",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_3",
  "x": "We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by <cite>(Gardner et al., 2014)</cite> , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. ---------------------------------- **RELATED WORK** Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992) , (Brin, 1999) , (Etzioni et al., 2004) .",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_4",
  "x": "We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by <cite>(Gardner et al., 2014)</cite> , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. ---------------------------------- **RELATED WORK**",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_5",
  "x": "This reduces feature sparsity and has been shown to improve PRA inference (Gardner et al., 2013) , <cite>(Gardner et al., 2014)</cite> . In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus. We term these noun phrases as bridging entities since they bridge two KB relations to form a path. This is different from the scheme in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> , which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_6",
  "x": "Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by <cite>(Gardner et al., 2014)</cite> , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. ---------------------------------- **RELATED WORK** Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992) , (Brin, 1999) , (Etzioni et al., 2004) . However, none of them make use of Knowledge Bases for improving information extraction. The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011) . It was extended by (Lao et al., 2012) , to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in (Gardner et al., 2013) .",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_7",
  "x": "However, none of them make use of Knowledge Bases for improving information extraction. The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011) . It was extended by (Lao et al., 2012) , to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in (Gardner et al., 2013) . Instead of hard mapping of surface relations to latent embeddings, <cite>(Gardner et al., 2014 )</cite> perform a 'soft' mapping using <cite>vector space random walks</cite>. This allows the random walker to traverse an edge semantically similar to the current edge type more frequently than other edges. Although, like others, we too use an external corpus to augment the KB, the crucial difference in our approach is that apart from adding surface relations, we also add bridging entities that enable us to create new paths in the KB. Furthermore, the procedure is targeted so that only paths that play a part in inferring the relations that are of interest are added. Thus, the number of paths added in this manner is much lower than the number of surface relations added using the procedure in (Gardner et al., 2013) .",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_8",
  "x": "Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992) , (Brin, 1999) , (Etzioni et al., 2004) . However, none of them make use of Knowledge Bases for improving information extraction. The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011) . It was extended by (Lao et al., 2012) , to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in (Gardner et al., 2013) . Instead of hard mapping of surface relations to latent embeddings, <cite>(Gardner et al., 2014 )</cite> perform a 'soft' mapping using <cite>vector space random walks</cite>. This allows the random walker to traverse an edge semantically similar to the current edge type more frequently than other edges. Although, like others, we too use an external corpus to augment the KB, the crucial difference in our approach is that apart from adding surface relations, we also add bridging entities that enable us to create new paths in the KB. Furthermore, the procedure is targeted so that only paths that play a part in inferring the relations that are of interest are added.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_10",
  "x": "The score is given by where \u03b8 r \u03c0 is the weight learned by the logistic regression classifier during training specially for relation r and path type \u03c0. During the test phase, since targets are not available, the PRA gathers candidate targets by performing a random walk and then computes feature vectors and the score. ---------------------------------- **PRA-SVO AND <cite>PRA-VS</cite>** PRA-SVO and <cite>PRA-VS</cite> are the systems proposed in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner. ----------------------------------",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_11",
  "x": "where \u03b8 r \u03c0 is the weight learned by the logistic regression classifier during training specially for relation r and path type \u03c0. During the test phase, since targets are not available, the PRA gathers candidate targets by performing a random walk and then computes feature vectors and the score. ---------------------------------- **PRA-SVO AND <cite>PRA-VS</cite>** PRA-SVO and <cite>PRA-VS</cite> are the systems proposed in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner. ---------------------------------- **PRA ON-DEMAND AUGMENTATION**",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_12",
  "x": "**PRA-SVO AND <cite>PRA-VS</cite>** PRA-SVO and <cite>PRA-VS</cite> are the systems proposed in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner. ---------------------------------- **PRA ON-DEMAND AUGMENTATION** (PRA-ODA) Training: Let s and t be any two KB entities and let s (n) and t (n) be their corresponding noun phrase representations or aliases. We search for bridging entities x 1 , x 2 , ..x n by performing limited depth first search (DFS) starting with s n such that we obtain a path s \u2212\u2192 t, where v i are verbs present in the corpus graph.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_13",
  "x": "Although the quality of bridging entities depend on the corpus, low quality bridging entities can be filtered out by adding negative training data. Low quality bridging entities connect source target pairs from both positive and negative training sets, and hence are eliminated by the sparse logistic regression classifier. The negative dataset is generated using the closed world assumption by performing a random walk. After augmenting the KB, we run the training phase of the PRA algorithm to obtain the feature (path) weights computed by the logistic regression Table 2 : Comparison of Mean Reciprocal Rank (MRR) metric for 10 relations from NELL (higher is better). PRA-SVO, <cite>PRA-VS</cite> are the systems proposed in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . PRA-ODA is the approach proposed in this paper. Improvements in PRA-ODA over PRA-SVO is statistically significant with p < 0.007, with PRA-SVO as null hypothesis. classifier. Query Time: The set of target entities corresponding to a source entity and the relation being predicted is not available during query (test) time.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_14",
  "x": "Any path (along with bridging entities) found during this search are added to the KB, and the PRA algorithm is now run over this augmented graph. ---------------------------------- **EXPERIMENTS** We used the implementation of PRA provided by the authors of <cite>(Gardner et al., 2014)</cite> . For our experiments, we used the same 10 NELL relation data as used in <cite>(Gardner et al., 2014)</cite> . The augmentation resulted in the addition of 1086 paths during training and 1430 paths during test time. We split the NELL data into 60% training data, 15 % development data and 25% test data. Values for d max , and K, the most frequent paths, were obtained by tuning on a development set for 4 relations (athleteplaysforsport,actorstarredinmovie,citylocatedincountry (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . PRA-ODA is the approach proposed in this paper.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_15",
  "x": "---------------------------------- **EXPERIMENTS** We used the implementation of PRA provided by the authors of <cite>(Gardner et al., 2014)</cite> . For our experiments, we used the same 10 NELL relation data as used in <cite>(Gardner et al., 2014)</cite> . The augmentation resulted in the addition of 1086 paths during training and 1430 paths during test time. We split the NELL data into 60% training data, 15 % development data and 25% test data. Values for d max , and K, the most frequent paths, were obtained by tuning on a development set for 4 relations (athleteplaysforsport,actorstarredinmovie,citylocatedincountry (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . PRA-ODA is the approach proposed in this paper. Between the two top performing systems, i.e., PRA-ODA and <cite>PRA-VS</cite>, PRA-ODA is faster by a factor of 1.8.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_16",
  "x": "Any path (along with bridging entities) found during this search are added to the KB, and the PRA algorithm is now run over this augmented graph. ---------------------------------- **EXPERIMENTS** We used the implementation of PRA provided by the authors of <cite>(Gardner et al., 2014)</cite> . For our experiments, we used the same 10 NELL relation data as used in <cite>(Gardner et al., 2014)</cite> . The augmentation resulted in the addition of 1086 paths during training and 1430 paths during test time. We split the NELL data into 60% training data, 15 % development data and 25% test data. Values for d max , and K, the most frequent paths, were obtained by tuning on a development set for 4 relations (athleteplaysforsport,actorstarredinmovie,citylocatedincountry (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . PRA-ODA is the approach proposed in this paper.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_17",
  "x": "For our experiments, we used the same 10 NELL relation data as used in <cite>(Gardner et al., 2014)</cite> . The augmentation resulted in the addition of 1086 paths during training and 1430 paths during test time. We split the NELL data into 60% training data, 15 % development data and 25% test data. Values for d max , and K, the most frequent paths, were obtained by tuning on a development set for 4 relations (athleteplaysforsport,actorstarredinmovie,citylocatedincountry (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . PRA-ODA is the approach proposed in this paper. Between the two top performing systems, i.e., PRA-ODA and <cite>PRA-VS</cite>, PRA-ODA is faster by a factor of 1.8. and journalistwritesforpublication). The hyperparameter values d max = 2, K = 10 reported the highest MRR and were used for the rest of the relations. For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> , viz., L 1 = 0.005, and L 2 = 1.0.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_18",
  "x": "For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> , viz., L 1 = 0.005, and L 2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> . The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_19",
  "x": "and journalistwritesforpublication). The hyperparameter values d max = 2, K = 10 reported the highest MRR and were used for the rest of the relations. For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> , viz., L 1 = 0.005, and L 2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> . The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_20",
  "x": "PRA-ODA is the approach proposed in this paper. Between the two top performing systems, i.e., PRA-ODA and <cite>PRA-VS</cite>, PRA-ODA is faster by a factor of 1.8. and journalistwritesforpublication). The hyperparameter values d max = 2, K = 10 reported the highest MRR and were used for the rest of the relations. For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> , viz., L 1 = 0.005, and L 2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> . The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_21",
  "x": "This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> . The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing. Runtime gains with PRA-ODA are likely to be even more pronounced in such settings.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_22",
  "x": "The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing. Runtime gains with PRA-ODA are likely to be even more pronounced in such settings. An additional advantage of the proposed algorithm is that it can also be run on the top of any PRA based algorithm such as the PRA-SVO and <cite>PRA-VS</cite>. ---------------------------------- **CONCLUSION**",
  "y": "similarities"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_23",
  "x": "The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing. Runtime gains with PRA-ODA are likely to be even more pronounced in such settings. An additional advantage of the proposed algorithm is that it can also be run on the top of any PRA based algorithm such as the PRA-SVO and <cite>PRA-VS</cite>. ----------------------------------",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_24",
  "x": "This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> . The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing. Runtime gains with PRA-ODA are likely to be even more pronounced in such settings.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_25",
  "x": "We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing. Runtime gains with PRA-ODA are likely to be even more pronounced in such settings. An additional advantage of the proposed algorithm is that it can also be run on the top of any PRA based algorithm such as the PRA-SVO and <cite>PRA-VS</cite>. ---------------------------------- **CONCLUSION** In this paper, we investigated the usefulness of adding paths to a Knowledge Base for improving its connectivity by mining bridging entities from an external corpus. While previous KB augmentation methods focused only on augmentation using mined surface verbs while keeping the node set fixed, we extended these approaches by also adding bridging entities in an online fashion. We used a large corpus of 500 million web text corpus to mine these additional edges and bridging entities.",
  "y": "uses"
 },
 {
  "id": "be26538a785f9ec9edc1ea031194cf_0",
  "x": "Specifically, in the Indian subcontinent, number of Internet users has crossed 500 mi 1 , and is rising rapidly due to inexpensive data 2 . With this rise, comes the problem of hate speech, offensive and abusive posts on social media. Although there are many previous works which deal with Hindi and English hate speech (the top two languages in India), but very few on the code-switched version (Hinglish) of the two (<cite>Mathur et al. 2018</cite>) . This is partially due to the following reasons: (i) Hinglish consists of no-fixed grammar and vocabulary. It derives a part of its semantics from Devnagari and another part from the Roman script. (ii) Hinglish speech and written text consists of a concoction of words spoken in Hindi as well as English, but written in the Roman script. This makes the spellings variable and dependent on the writer of the text. Hence code-switched languages present tough challenges in terms of parsing and getting the meaning out of the text. For instance, the sentence, \"Modiji foreign yatra par hai\", is in the Hinglish language.",
  "y": "motivation"
 },
 {
  "id": "be26538a785f9ec9edc1ea031194cf_1",
  "x": "Due to the various difficulties associated with the Hinglish language, it is challenging to automatically detect and ban such kind of speech. Thus, with this in mind, we build a transfer learning based model for the code-switched language Hinglish, which outperforms the baseline model of (<cite>Mathur et al. 2018</cite>) . We also release the embeddings and the model trained. ---------------------------------- **METHODOLOGY** Our methodology primarily consists of these steps: Preprocessing of the dataset, training of word embeddings, training of the classifier model and then using that on HEOT dataset. ---------------------------------- **PRE-PROCESSING** In this work, we use the datasets released by (Davidson et al. 2017 ) and HEOT dataset provided by (<cite>Mathur et al. 2018</cite>) .",
  "y": "differences"
 },
 {
  "id": "be26538a785f9ec9edc1ea031194cf_2",
  "x": "---------------------------------- **PRE-PROCESSING** In this work, we use the datasets released by (Davidson et al. 2017 ) and HEOT dataset provided by (<cite>Mathur et al. 2018</cite>) . The datasets obtained pass through these steps of processing: (i) Removal of punctuatios, stopwords, URLs, numbers, emoticons, etc. This was then followed by transliteration using the Xlit-Crowd conversion dictionary 3 and translation of each word to English using Hindi to English dictionary 4 . To deal with the spelling variations, we manually added some common variations of popular Hinglish words. Final dictionary comprised of 7200 word pairs. Additionally, to deal with profane words, which are not present in Xlit-Crowd, we had to make a profanity dictionary (with 209 profane words) as well. Table 1 gives some examples from the dictionary.",
  "y": "uses"
 },
 {
  "id": "be26538a785f9ec9edc1ea031194cf_3",
  "x": "As indicated by the Figure 1 , the model was initially trained on the dataset provided by (Davidson et al. 2017) , and then re-trained on the HEOT dataset so as to benefit from the transfer of learned features in the last stage. The model hyperparameters were experimentally selected by trying out a large number of combinations through grid search. Results Table 3 shows the performance of our model (after getting trained on (Davidson et al. 2017) ) with two types of embeddings in comparison to the models by (<cite>Mathur et al. 2018</cite>) and (Davidson et al. 2017 ) on the HEOT dataset averaged over three runs. We also compare results on pre-trained embeddings. As shown in the table, our model when given Glove embeddings performs better than all other models. For comparison purposes, in Table 4 we have also evaluated our results on the dataset by (Davidson et al. 2017 ). ---------------------------------- **CONCLUSION** In this paper, we presented a pipeline which given Hinglish text can classify it into three categories: offensive, abusive and benign.",
  "y": "differences"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_0",
  "x": "Since sentiment lexicons helped in improving the accuracy of sentiment classification models (Liu and Zhang, 2012; Al-Sallab et al., 2017; Badaro et al., 2014a Badaro et al., ,b, 2015 , several researchers are working on developing emotion lexicons for different languages such as English, French, Polish and Chinese (Mohammad, 2017; Bandhakavi et al., 2017; Yang et al., 2007; Mohammad and Turney, 2013; Abdaoui et al., 2017;<cite> Staiano and Guerini, 2014</cite>; Maziarz et al., 2016; Janz et al., 2017) . While sentiment is usually represented by three labels namely positive, negative or neutral, several representation models exist for emotions such as Ekman representation (Ekman, 1992) (happiness, sadness, fear, anger, surprise and disgust) or Plutchik model (Plutchik, 1994) that includes trust and anticipation in addition to Ekman's six emotions. Despite the efforts for creating large scale emotion lexicons for English, the size of existing emotion lexicons remain much smaller compared to sentiment lexicons. For example, DepecheMood<cite> (Staiano and Guerini, 2014)</cite> , one of the largest publicly available emotion lexicon for English, includes around 37K terms while SentiWordNet (SWN) (Esuli and Sebastiani, 2007; Baccianella et al., 2010) , a large scale English sentiment lexicon semi-automatically generated using English WordNet (EWN) (Fellbaum, 1998) , includes around 150K terms annotated with three sentiment scores: positive, negative and objective. In this paper, we focus on expanding coverage of existing emotion lexicon, namely DepecheMood, using the synonymy semantic relation available in English WordNet. We decide to expand DepecheMood since it is one of the largest emotion lexicon publicly available, and since its terms are aligned with EWN, thus allowing us to benefit from powerful semantic relations in EWN. The paper is organized as follows. In section 2, we conduct a brief literature survey on existing emotion lexicons. In section 3, we describe the expansion approach to build EmoWordNet.",
  "y": "background"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_1",
  "x": "While plenty of works exist for sentiment analysis for different languages including analysis of social media data for sentiment characteristics (Al Sallab et al., 2015; Baly et al., , 2017b , few works focused on emotion recognition from text. Since sentiment lexicons helped in improving the accuracy of sentiment classification models (Liu and Zhang, 2012; Al-Sallab et al., 2017; Badaro et al., 2014a Badaro et al., ,b, 2015 , several researchers are working on developing emotion lexicons for different languages such as English, French, Polish and Chinese (Mohammad, 2017; Bandhakavi et al., 2017; Yang et al., 2007; Mohammad and Turney, 2013; Abdaoui et al., 2017;<cite> Staiano and Guerini, 2014</cite>; Maziarz et al., 2016; Janz et al., 2017) . While sentiment is usually represented by three labels namely positive, negative or neutral, several representation models exist for emotions such as Ekman representation (Ekman, 1992) (happiness, sadness, fear, anger, surprise and disgust) or Plutchik model (Plutchik, 1994) that includes trust and anticipation in addition to Ekman's six emotions. Despite the efforts for creating large scale emotion lexicons for English, the size of existing emotion lexicons remain much smaller compared to sentiment lexicons. For example, DepecheMood<cite> (Staiano and Guerini, 2014)</cite> , one of the largest publicly available emotion lexicon for English, includes around 37K terms while SentiWordNet (SWN) (Esuli and Sebastiani, 2007; Baccianella et al., 2010) , a large scale English sentiment lexicon semi-automatically generated using English WordNet (EWN) (Fellbaum, 1998) , includes around 150K terms annotated with three sentiment scores: positive, negative and objective. In this paper, we focus on expanding coverage of existing emotion lexicon, namely DepecheMood, using the synonymy semantic relation available in English WordNet. We decide to expand DepecheMood since it is one of the largest emotion lexicon publicly available, and since its terms are aligned with EWN, thus allowing us to benefit from powerful semantic relations in EWN. The paper is organized as follows. In section 2, we conduct a brief literature survey on existing emotion lexicons.",
  "y": "background"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_2",
  "x": "To create EmoLex, the authors first identified target terms for annotation extracted from Macquarie Thesaurus (Bernard and Bernard, 1986) , WordNet Affect and the General Inquirer (Stone et al., 1966) . Then, they launched the annotation task on Amazon's Mechanical Turk. EmoLex has around 10K terms annotated for emotions as well as for sentiment polarities. They evaluated the annotation quality using different techniques such as computing inter-annotator agreement and comparing a subsample of EmoLex with existing gold data. AffectNet (Cambria et al., 2012) , part of the SenticNet project, includes also around 10K terms extracted from ConceptNet (Liu and Singh, 2004) and aligned with WordNet Affect. They extended WordNet Affect using the concepts in ConceptNet. While WordNet Affect, EmoLex and AffectNet include terms with emotion labels, Affect database (Neviarouskaya et al., 2007) and DepecheMood<cite> (Staiano and Guerini, 2014)</cite> include words that have emotion scores instead, which can be useful for compositional computations of emotion scores. Affect database extends SentiFul and covers around 2.5K words presented in their lemma form along with the corresponding part of speech (POS) tag. DepecheMood was automatically built by harvesting social media data that were implicitly annotated with emotions.",
  "y": "background"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_3",
  "x": "DepecheMood was automatically built by harvesting social media data that were implicitly annotated with emotions. <cite>Staiano and Guerini (2014)</cite> utilized news articles from rappler.com. The articles are accompanied by Rappler's Mood Meter, which allows readers to express their emotions about the article they are reading. DepecheMood includes around 37K lemmas along with their part of speech tags and the lemmas are aligned with EWN. Staiano and Guerini also evaluated DepecheMood in emotion regression and classification tasks in unsupervised settings. They claim that although they utilized a na\u00efve unsupervised model, they were able to outperform existing lexicons when applied on SemEval 2007 dataset (Strapparava and Mihalcea, 2007) . Since DepecheMood is aligned with EWN, is publicly available and has a better coverage and claimed performance compared to existing emotion lexicons, we decide to expand it using EWN semantic relations as described below in section 3. ---------------------------------- **LITERATURE REVIEW**",
  "y": "background"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_4",
  "x": "Three variations of score representations exist for DepecheMood. We select to expand the DepecheMood variation with normalized scores since this variation performed best according to the presented results in<cite> (Staiano and Guerini, 2014)</cite> . In Fig. 1 , we show an overview of the steps followed to expand DepecheMood. Step 1: EWN synsets that include lemmas of DepecheMood were retrieved. A score was then computed for each retrieved synset, s. Let S denotes the set of all such synsets. Two cases might appear: either the retrieved synset included only one lemma from DepecheMood, in this case the synset was assigned the same score of the lemma, or, the synset included multiple lemmas that exist in DepecheMood, in this case the synset's score was the average of the scores of its corresponding lemmas. Step 2: A synset, s, includes two set of terms: T, terms that are in DepecheMood, andT , terms not in DepecheMood. Using the synonymy semantic relation in EWN, and based on the concept that synonym words would likely share the same emotion scores, we assigned the synset's scores to its corresponding termsT . Again, a term t inT might appear in one or multiple synsets from S. Hence, the score assigned to t would be either the one of its corresponding synset or the average of the scores of its corresponding synsets that belong to S.",
  "y": "extends differences"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_5",
  "x": "We considered the same emotion mapping assumptions presented in the work of<cite> (Staiano and Guerini, 2014)</cite> : Fear \u2192 Afraid, Anger \u2192 Angry, Joy \u2192 Happy, Sadness \u2192 Sad and Surprise \u2192 Inspired. Disgust was not aligned with any emotion in EmoWordNet and hence was discarded as also assumed in<cite> (Staiano and Guerini, 2014)</cite> . One important aspect of the extrinsic evaluation was checking the coverage of EmoWordNet against SemEval dataset. In order to compute coverage, we performed lemmatization of the news headlines using WordNet lemmatizer available through Python NLTK package. We excluded all words with POS tags different than noun, verb, adjective and adverb. EmoWordNet achieved a coverage of 68.6% while DepecheMood had a coverage of 67.1%. An increase in coverage was expected but since the size of the dataset is relatively small, the increase was only around 1.5%. In terms of headline coverage, only one headline (\"Toshiba Portege R400\") was left without any emotion scores when using both EmoWordNet and DepecheMood since none of its terms were found in any of the two lexicons. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_6",
  "x": "---------------------------------- **DATASET & COVERAGE** We utilized the dataset provided publicly by SemEval 2007 task on Affective text (Strapparava and Mihalcea, 2007) . The dataset consists of one thousand news headlines annotated with six emotion scores: anger, disgust, fear, joy, sadness and surprise. For the regression task, a score between 0 and 1 is provided for each emotion. For the classification task, a threshold is applied on the emotion scores to get a binary representation of the emotions: if the score of a certain emotion is greater than 0.5, the corresponding emotion label is set to 1, otherwise it is 0. The emotion labels used in the dataset correspond to the six emotions of the Ekman model (Ekman, 1992) while those in EmoWordNet, as well as DepecheMood, follow the ones provided by Rappler Mood Meter. We considered the same emotion mapping assumptions presented in the work of<cite> (Staiano and Guerini, 2014)</cite> : Fear \u2192 Afraid, Anger \u2192 Angry, Joy \u2192 Happy, Sadness \u2192 Sad and Surprise \u2192 Inspired. Disgust was not aligned with any emotion in EmoWordNet and hence was discarded as also assumed in<cite> (Staiano and Guerini, 2014)</cite> .",
  "y": "similarities"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_7",
  "x": "As stated in<cite> (Staiano and Guerini, 2014)</cite> paper, 'Disgust' emotion was excluded since there was no corresponding mapping in EmoWordNet/DepecheMood. The first evaluation consisted of measuring Pearson Correlation between the scores computed using the lexicons and those provided in SemEval. The results are reported in Table 1. We could see that the results are relatively close to each other: EmoWordNet slightly outperformed DepecheMood for the five different emotions. It was expected to have close results given that the coverage of EmoWordNet is very close to DepecheMood. Given the slight improvement, we expect EmoWordNet to perform much better on larger datasets. For the classification task, we first transformed the numerical emotion scores of the headlines to a binary representation. We applied min-max normalization on the computed emotion scores per headline, and then assigned a '1' for the emotion label with score greater than '0.5', and a '0' otherwise. We used F1 measure for evaluation.",
  "y": "similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_0",
  "x": "An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur-ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; <cite>Ninomiya et al., 2006</cite>; , which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999) . Supertagging is a process where words in an input sentence are tagged with 'supertags,' which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (2004) also demonstrated the effects of supertagging with a statistical constraint dependency grammar (CDG) parser by showing accuracy as high as the state-of-the-art parsers, and and reported that accuracy was significantly improved by incorporating the supertagging probabilities into manually tuned Weighted CDG. <cite>Ninomiya et al. (2006)</cite> showed the parsing model using only supertagging probabilities could achieve accuracy as high as the probabilistic model for phrase structures. This means that syntactic structures are almost determined by supertags as is claimed by Bangalore and Joshi (1999) . However, supertaggers themselves were heuristically used as an external tagger. They filter out unlikely lexical entries just to help parsing (Clark and Curran, 2004a) , or the probabilistic models for phrase structures were trained independently of the supertagger's probabilistic models (Wang and Harper, 2004; <cite>Ninomiya et al., 2006)</cite> . In the case of supertagging of Weighted CDG , parameters for Weighted CDG are manually tuned, i.e., their model is not a well-defined probabilistic model.",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_1",
  "x": "The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Malouf and van Noord, 2004; Kaplan et al., 2004; . Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; . An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur-ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; <cite>Ninomiya et al., 2006</cite>; , which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999) . Supertagging is a process where words in an input sentence are tagged with 'supertags,' which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (2004) also demonstrated the effects of supertagging with a statistical constraint dependency grammar (CDG) parser by showing accuracy as high as the state-of-the-art parsers, and and reported that accuracy was significantly improved by incorporating the supertagging probabilities into manually tuned Weighted CDG. <cite>Ninomiya et al. (2006)</cite> showed the parsing model using only supertagging probabilities could achieve accuracy as high as the probabilistic model for phrase structures. This means that syntactic structures are almost determined by supertags as is claimed by Bangalore and Joshi (1999) .",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_2",
  "x": "Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; . An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur-ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; <cite>Ninomiya et al., 2006</cite>; , which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999) . Supertagging is a process where words in an input sentence are tagged with 'supertags,' which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (2004) also demonstrated the effects of supertagging with a statistical constraint dependency grammar (CDG) parser by showing accuracy as high as the state-of-the-art parsers, and and reported that accuracy was significantly improved by incorporating the supertagging probabilities into manually tuned Weighted CDG. <cite>Ninomiya et al. (2006)</cite> showed the parsing model using only supertagging probabilities could achieve accuracy as high as the probabilistic model for phrase structures. This means that syntactic structures are almost determined by supertags as is claimed by Bangalore and Joshi (1999) . However, supertaggers themselves were heuristically used as an external tagger. They filter out unlikely lexical entries just to help parsing (Clark and Curran, 2004a) , or the probabilistic models for phrase structures were trained independently of the supertagger's probabilistic models (Wang and Harper, 2004; <cite>Ninomiya et al., 2006)</cite> .",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_4",
  "x": "The reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and part-of-speech (POS) n-gram as defined in the CCG/HPSG/CDG supertagging. This is the first model which properly incorporates the supertagging probabilities into parse tree's probabilistic model. We compared our model with the probabilistic model for phrase structures . This model uses word and POS unigram for its reference distribution, i.e., the probabilities of unigram supertagging. Our model can be regarded as an extension of a unigram reference distribution to an n-gram reference distribution with features that are used in supertagging. We also compared with a probabilistic model in <cite>(Ninomiya et al., 2006)</cite> . The probabilities of <cite>their model</cite> are defined as the product of probabilities of supertagging and probabilities of the probabilistic model for phrase structures, but <cite>their model</cite> was trained independently of supertagging probabilities, i.e., the supertagging probabilities are not used for reference distributions. ---------------------------------- **HPSG AND PROBABILISTIC MODELS**",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_8",
  "x": "This is an extension of 's model by replacing the unigram reference distribution with an n-gram reference distribution. Our model is formally defined as follows: combinations , d, c, hw, hp, hl , r, d, c, hw, hp , r, d, c, hw, hl , r, d, c, sy, hw , r, c, sp, hw, hp, hl , r, c, sp, hw, hp , r, c, sp, hw, hl , r, c, sp, sy, hw , r, d, c, hp, hl , r, d, c, hp , r, d, c, hl , r, d, c, sy , r, c, sp, hp, hl , r, c, sp, hp , r, c, sp, hl , r, c, sp, sy combinations of feature templates for funary r, hw, hp, hl , r, hw, hp , r, hw, hl , r, sy, hw , r, hp, hl , r, hp , r, hl , r, sy combinations of feature templates for f root hw, hp, hl , hw, hp , hw, hl , sy, hw , hp, hl , hp , hl (Probabilistic HPSG with an n-gram reference distribution) In our model, <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite> is used as a reference distribution. The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures.",
  "y": "uses"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_9",
  "x": "In our model, <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite> is used as a reference distribution. The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures.",
  "y": "similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_10",
  "x": "The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures. In our model, given the probabilities for lexical entries, the parameters for phrase structures are estimated so as to maximize the entire probabilistic model (= the product of the probabilities for lexical entries and the probabilities for phrase structures) in the training corpus.",
  "y": "similarities differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_11",
  "x": "The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures. In our model, given the probabilities for lexical entries, the parameters for phrase structures are estimated so as to maximize the entire probabilistic model (= the product of the probabilities for lexical entries and the probabilities for phrase structures) in the training corpus. In <cite>their model</cite>, the parameters for phrase structures are trained without using the probabilities for lexical entries, i.e., the parameters for phrase structures are estimated so as to maximize the probabilities for phrase structures only.",
  "y": "similarities differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_12",
  "x": "Our model is formally defined as follows: combinations , d, c, hw, hp, hl , r, d, c, hw, hp , r, d, c, hw, hl , r, d, c, sy, hw , r, c, sp, hw, hp, hl , r, c, sp, hw, hp , r, c, sp, hw, hl , r, c, sp, sy, hw , r, d, c, hp, hl , r, d, c, hp , r, d, c, hl , r, d, c, sy , r, c, sp, hp, hl , r, c, sp, hp , r, c, sp, hl , r, c, sp, sy combinations of feature templates for funary r, hw, hp, hl , r, hw, hp , r, hw, hl , r, sy, hw , r, hp, hl , r, hp , r, hl , r, sy combinations of feature templates for f root hw, hp, hl , hw, hp , hw, hl , sy, hw , hp, hl , hp , hl (Probabilistic HPSG with an n-gram reference distribution) In our model, <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite> is used as a reference distribution. The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same.",
  "y": "similarities differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_13",
  "x": "This is an extension of 's model by replacing the unigram reference distribution with an n-gram reference distribution. Our model is formally defined as follows: combinations , d, c, hw, hp, hl , r, d, c, hw, hp , r, d, c, hw, hl , r, d, c, sy, hw , r, c, sp, hw, hp, hl , r, c, sp, hw, hp , r, c, sp, hw, hl , r, c, sp, sy, hw , r, d, c, hp, hl , r, d, c, hp , r, d, c, hl , r, d, c, sy , r, c, sp, hp, hl , r, c, sp, hp , r, c, sp, hl , r, c, sp, sy combinations of feature templates for funary r, hw, hp, hl , r, hw, hp , r, hw, hl , r, sy, hw , r, hp, hl , r, hp , r, hl , r, sy combinations of feature templates for f root hw, hp, hl , hw, hp , hw, hl , sy, hw , hp, hl , hp , hl (Probabilistic HPSG with an n-gram reference distribution) In our model, <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite> is used as a reference distribution. The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures.",
  "y": "similarities differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_14",
  "x": "The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures. In our model, given the probabilities for lexical entries, the parameters for phrase structures are estimated so as to maximize the entire probabilistic model (= the product of the probabilities for lexical entries and the probabilities for phrase structures) in the training corpus. In <cite>their model</cite>, the parameters for phrase structures are trained without using the probabilities for lexical entries, i.e., the parameters for phrase structures are estimated so as to maximize the probabilities for phrase structures only.",
  "y": "similarities differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_15",
  "x": "The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures. In our model, given the probabilities for lexical entries, the parameters for phrase structures are estimated so as to maximize the entire probabilistic model (= the product of the probabilities for lexical entries and the probabilities for phrase structures) in the training corpus. In <cite>their model</cite>, the parameters for phrase structures are trained without using the probabilities for lexical entries, i.e., the parameters for phrase structures are estimated so as to maximize the probabilities for phrase structures only.",
  "y": "similarities differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_16",
  "x": "The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures. In our model, given the probabilities for lexical entries, the parameters for phrase structures are estimated so as to maximize the entire probabilistic model (= the product of the probabilities for lexical entries and the probabilities for phrase structures) in the training corpus. In <cite>their model</cite>, the parameters for phrase structures are trained without using the probabilities for lexical entries, i.e., the parameters for phrase structures are estimated so as to maximize the probabilities for phrase structures only. That is, the parameters for lexical entries and the parameters for phrase structures are trained independently in <cite>their model</cite>.",
  "y": "similarities differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_17",
  "x": "Our models increased the parsing accuracy. 'our model 1' was around 2.6 times faster and had around 2.65 points higher F-score than 's model. 'our model 2' was around 2.3 times slower but had around 2.9 points higher F-score than 's model. We must admit that the difference between our models and <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite> was not as great as the difference from 's model, but 'our model 1' achieved 0.56 points higher F-score, and 'our model 2' achieved 0.8 points higher F-score. When the automatic POS tagger was introduced, Fscore dropped by around 2.4 points for all models. We also compared our model with Matsuzaki et al. (2007) 's model. Matsuzaki et al. (2007) ---------------------------------- **PRO-**",
  "y": "differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_18",
  "x": "Their results with the same grammar and servers are also listed in the lower half of Table 4 . They achieved drastic improvement in efficiency. Their parser ran around 6 times faster than <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>, 9 times faster than 'our model 1' and 60 times faster than 'our model 2.' Instead, our models achieved better accuracy. 'our model 1' had around 0.5 higher F-score, and 'our model 2' had around 0.8 points higher F-score. Their efficiency is mainly due to elimination of ungrammatical lexical entries by the CFG filtering. They first parse a sentence with a CFG grammar compiled from an HPSG grammar, and then eliminate lexical entries that are not in the parsed CFG trees. Obviously, this technique can also be applied to the HPSG parsing of our models. We think that efficiency of HPSG parsing with our models will be drastically improved by applying this technique. The average parsing time and labeled F-score curves of each probabilistic model for the sentences in Section 24 of \u2264 100 words are graphed in Figure 3.",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_19",
  "x": "The average parsing time and labeled F-score curves of each probabilistic model for the sentences in Section 24 of \u2264 100 words are graphed in Figure 3. The graph clearly shows the difference of our model and other models. As seen in the graph, our model achieved higher F-score than other model when beam threshold was widen. This implies that other models were probably difficult to reach the Fscore of 'our model 1' and 'our model 2' for Section 23 even if we changed the beam thresholding parameters. However, F-score of our model dropped eas-ily when we narrow down the beam threshold, compared to other models. We think that this is mainly due to its bad implementation of parser interface. The n-gram reference distribution is incorporated into the kernel of the parser, but the n-gram features and a maximum entropy estimator are defined in other modules; n-gram features are defined in a grammar module, and a maximum entropy estimator for the n-gram reference distribution is implemented with a general-purpose maximum entropy estimator module. Consequently, strings that represent the ngram information are very frequently changed into feature structures and vice versa when they go in and out of the kernel of the parser. On the other hand, <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite> uses the supertagger as an external module.",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_23",
  "x": "---------------------------------- **CONCLUSION** We proposed a probabilistic model in which supertagging is consistently integrated into the probabilistic model for HPSG. In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries with machine learning features of word and POS ngram as defined in the CCG/HPSG/CDG supertagging. We conducted experiments on the Penn Treebank with a wide-coverage HPSG parser. In the experiments, we compared our model with the probabilistic HPSG with a unigram reference distribution and the probabilistic HPSG with supertagging <cite>(Ninomiya et al., 2006 )</cite>. Though our model was not as fast as <cite>Ninomiya et al. (2006)</cite> <cite>'s models</cite>, it achieved the highest accuracy among them. Our model had around 2.65 points higher F-score than 's model and around 0.56 points higher F-score than the <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>. When we sacrifice parsing speed, our model achieved around 2.9 points higher F-score than 's model and around 0.8 points higher F-score than <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>.",
  "y": "differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_24",
  "x": "---------------------------------- **CONCLUSION** We proposed a probabilistic model in which supertagging is consistently integrated into the probabilistic model for HPSG. In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries with machine learning features of word and POS ngram as defined in the CCG/HPSG/CDG supertagging. We conducted experiments on the Penn Treebank with a wide-coverage HPSG parser. In the experiments, we compared our model with the probabilistic HPSG with a unigram reference distribution and the probabilistic HPSG with supertagging <cite>(Ninomiya et al., 2006 )</cite>. Though our model was not as fast as <cite>Ninomiya et al. (2006)</cite> <cite>'s models</cite>, it achieved the highest accuracy among them. Our model had around 2.65 points higher F-score than 's model and around 0.56 points higher F-score than the <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>. When we sacrifice parsing speed, our model achieved around 2.9 points higher F-score than 's model and around 0.8 points higher F-score than <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>.",
  "y": "differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_25",
  "x": "---------------------------------- **CONCLUSION** We proposed a probabilistic model in which supertagging is consistently integrated into the probabilistic model for HPSG. In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries with machine learning features of word and POS ngram as defined in the CCG/HPSG/CDG supertagging. We conducted experiments on the Penn Treebank with a wide-coverage HPSG parser. In the experiments, we compared our model with the probabilistic HPSG with a unigram reference distribution and the probabilistic HPSG with supertagging <cite>(Ninomiya et al., 2006 )</cite>. Though our model was not as fast as <cite>Ninomiya et al. (2006)</cite> <cite>'s models</cite>, it achieved the highest accuracy among them. Our model had around 2.65 points higher F-score than 's model and around 0.56 points higher F-score than the <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>. When we sacrifice parsing speed, our model achieved around 2.9 points higher F-score than 's model and around 0.8 points higher F-score than <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>.",
  "y": "differences"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_0",
  "x": "The key challenge in this paradigm is the ranking model that computes the relevance of each target entity candidate to the corresponding entity mention using the available context information in both the documents and the knowledge bases. The early approach for the ranking problem in EL has resolved the entity mentions in documents independently (the local approach), utilizing various discrete and hand-designed features/heuristics to measure the local mention-to-entity relatedness for ranking. These features are often specific to each entity mention and candidate entity, covering a wide range of linguistic and/or structured representations such as lexical and part-of-speech tags of context words, dependency paths, topical features, KB infoboxes (Bunescu and Pasca, 2006; Mendes et al., 2011; Cassidy et al., 2011; Ji and Grishman, 2011; Shen et al., 2014) etc. Although the local approach can exploit a rich set of discrete structures for EL, its limitation is twofold: (i) The independent ranking mechanism in the local approach overlooks the topical coherence among the target entities referred by the entity mentions within the same document. This is undesirable as the topical coherence has been shown to be effective for EL in the previous work (Han et al., This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 2011; Hoffart et al., 2011; He et al., 2013b; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015) . (ii) The local approach might suffer from the data sparseness issue of unseen words/features, the difficulty of calibrating, and the failure to induce the underlying similarity structures at high levels of abstraction for EL (due to the extensive reliance on the hand-designed coarse features) (Sun et al., 2015;<cite> Francis-Landau et al., 2016)</cite> . The first drawback of the local approach has been overcome by the global models in which all entity mentions (or a group of entity mentions) within a document are disambiguated simultaneously to obtain a coherent set of target entities.",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_1",
  "x": "Consequently, the global approach is still subject to the second limitation of data sparseness of the local approach due to their use of discrete features. Recently, the surge of neural network (NN) models has presented an effective mechanism to mitigate the second limitation of the local approach. In such models, words are represented by continuous representations (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013) and features for the entity mentions and candidate entities are automatically learnt from data. This essentially alleviates the data sparseness problem of unseen words/features and helps to extract more effective features for EL in a given dataset (Kalchbrenner et al., 2014; Nguyen et al., 2016a) . In practice, the features automatically induced by NN are combined with the discrete features in the local approach to extend their coverage for EL (Sun et al., 2015;<cite> Francis-Landau et al., 2016)</cite> . However, as the previous NN models for EL are local, they cannot capture the global interdependence among the target entities in the same document (the first limitation of the local approach). Guided by these analyses, in this paper, we propose to use neural networks to model both the local mention-to-entity similarities and the global relatedness among target entities in an unified architecture. This allows us to inherit all the benefits from the previous systems as well as overcome their inherent issues. Our work is an extension of<cite> (Francis-Landau et al., 2016)</cite> which only considers the local similarities.",
  "y": "motivation background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_2",
  "x": "This allows us to inherit all the benefits from the previous systems as well as overcome their inherent issues. Our work is an extension of<cite> (Francis-Landau et al., 2016)</cite> which only considers the local similarities. Given a document, we simultaneously perform linking for every entity mention from the beginning to the end of the document. For each entity mention, we utilize convolutional neural networks (CNN) to obtain the distributed representations for the entity mention as well as its target candidates. These distributed representations are then used for two purposes: (i) computing the local similarities for the entity mention and target candidates, and (ii) functioning as the input for the recurrent neural networks (RNN) that runs over the entity mentions in the documents. The role of the RNNs is to accumulate information about the previous entity mentions and target entities, and provide them as the global constraints for the linking process of the current entity mention. We systematically evaluate the proposed model on multiple datasets in both the general setting and the domain adaptation setting. The experiment results show that the proposed model outperforms the current state-of-the-art models on the evaluated datasets. To our knowledge, this is also the first work investigating the EL problem in the domain adaptation setting.",
  "y": "similarities background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_3",
  "x": "In order to obtain the distributed representation for x, we first transform each word x i \u2208 x into a real-valued, h-dimensional vector w i using the word embedding table E (Mikolov et al., 2013) : . This essentially converts the word sequence x into a sequence of vectors that is padded with zero vectors to form a fixed-length sequence of vectors w = (w 1 , w 2 , . . . , w n ) of length n. In the next step, we apply the convolution operation over w to generate the hidden vector sequence, that is then transformed by a non-linear function G and pooled by the sum function<cite> (Francis-Landau et al., 2016)</cite> . Following the previous work on CNN (Nguyen and Grishman, (2015a; 2015b) ), we utilize the set L of multiple window sizes to parameterize the convolution operation. Each window size l \u2208 L corresponds to a convolution matrix M l \u2208 R v\u00d7lh of dimensionality v. Eventually, the concatenation vectorx of the resulting vectors for each window size in L would be used as the distributed representation for where is the concatenation operation over the window set L and w i:(i+l\u22121) is the concatenation vector of the given word vectors. For convenience, lets i ,c i , i and b * i obtained by the convolution procedure above, respectively.",
  "y": "uses background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_4",
  "x": "where is the concatenation operation over the window set L and w i:(i+l\u22121) is the concatenation vector of the given word vectors. For convenience, lets i ,c i , i and b * i obtained by the convolution procedure above, respectively. Note that we apply the same set of convolution parameters for each type of text granularity in the source document D as well as in the target entity side. The vector representations of the context would then be fed into the next components to compute the features for EL. ---------------------------------- **LOCAL SIMILARITIES** We employ the local similarities \u03c6 local (m i , p ij ) from<cite> (Francis-Landau et al., 2016)</cite> , the state-of-the-art neural network model for EL. In particular:",
  "y": "uses background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_5",
  "x": "We employ the local similarities \u03c6 local (m i , p ij ) from<cite> (Francis-Landau et al., 2016)</cite> , the state-of-the-art neural network model for EL. In particular: In this formula, W sparse and W CN N are the weights for the feature vectors F sparse and W CN N respectively. F sparse (m i , p ij ) is the sparse feature vector obtained from (Durrett and Klein, 2014) . This vector captures various linguistic properties and statistics that have been discovered in the previous studies for EL. The representative features include the anchor text counts from Wikipedia, the string match indications with the title of the Wikipedia candidate pages, or the information about the shape of the queries for candidate generations<cite> (Francis-Landau et al., 2016)</cite> . , on the other hand, involves the cosine similarities between the representation vectors at multiple granularities of m i and p ij . In particular: The intuition for this computation is that the similarities at different levels of contexts might help to enforce the potential topic compatibility between the contexts of the entity mentions and target candidates for EL<cite> (Francis-Landau et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_6",
  "x": "In this formula, W sparse and W CN N are the weights for the feature vectors F sparse and W CN N respectively. F sparse (m i , p ij ) is the sparse feature vector obtained from (Durrett and Klein, 2014) . This vector captures various linguistic properties and statistics that have been discovered in the previous studies for EL. The representative features include the anchor text counts from Wikipedia, the string match indications with the title of the Wikipedia candidate pages, or the information about the shape of the queries for candidate generations<cite> (Francis-Landau et al., 2016)</cite> . , on the other hand, involves the cosine similarities between the representation vectors at multiple granularities of m i and p ij . In particular: The intuition for this computation is that the similarities at different levels of contexts might help to enforce the potential topic compatibility between the contexts of the entity mentions and target candidates for EL<cite> (Francis-Landau et al., 2016)</cite> . ---------------------------------- **GLOBAL SIMILARITIES**",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_7",
  "x": "**EXPERIMENTS** ---------------------------------- **DATASETS** Following<cite> (Francis-Landau et al., 2016)</cite>, we evaluate the models on 4 different entity linking datasets: i) ACE (Bentivogli et al., 2010 ): This corpus is from the 2005 evaluation of NIST. It is also used in (Fahrni and Strube, 2014) and (Durrett and Klein, 2014) . ii) CoNLL-YAGO (Hoffart et al., 2011 ): This corpus is originally from the CoNLL 2003 shared task of named entity recognition for English. iii) WP (Heath and Bizer, 2011) : This dataset consists of short snippets from Wikipedia. iv) WIKI : This dataset contains 10,000 randomly sampled Wikipedia articles. The task is to disambiguate the links in each article 4 .",
  "y": "uses"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_8",
  "x": "Following<cite> (Francis-Landau et al., 2016)</cite>, we evaluate the models on 4 different entity linking datasets: i) ACE (Bentivogli et al., 2010 ): This corpus is from the 2005 evaluation of NIST. It is also used in (Fahrni and Strube, 2014) and (Durrett and Klein, 2014) . ii) CoNLL-YAGO (Hoffart et al., 2011 ): This corpus is originally from the CoNLL 2003 shared task of named entity recognition for English. iii) WP (Heath and Bizer, 2011) : This dataset consists of short snippets from Wikipedia. iv) WIKI : This dataset contains 10,000 randomly sampled Wikipedia articles. The task is to disambiguate the links in each article 4 . For all the datasets, we use the standard data splits (for training data, test data and development data) as the previous works for comparable comparison<cite> (Francis-Landau et al., 2016)</cite>. ---------------------------------- **PARAMETERS AND RESOURCES**",
  "y": "uses"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_9",
  "x": "The training parameters are set to the default values in this toolkit. The dimensionality of the word embeddings is 300. Note that every parameter and resource in this work is either taken from the previous work (Nguyen and Grishman, 2016b;<cite> Francis-Landau et al., 2016)</cite> or selected by the development data. ---------------------------------- **EVALUATING THE GLOBAL FEATURES** In this section, we evaluate the effectiveness of the global features for EL. In particular, we differentiate two types of global features based on the side of information we expect to enforce the coherence. The first type of global features (global-mention) concerns the entity mention side and involves applying the global RNN models on the CNN-induced representation vectors of the entity mentions (i.e, the surface vectors (s 1 ,s 2 , . . . ,s k ) and the immediate context vectors (c 1 ,c 2 , . . . ,c k ) ). The second type of global features (global-entity), on the other hand, focuses on the target entity side and models the coherence with the representation vectors of the target entities (i.e, the page title vectors (t * 1 ,t * 2 , . . . ,t * k ) and the body content vectors (b * 1 ,b * 2 , . . . ,b * k )).",
  "y": "similarities background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_10",
  "x": "This is substantial on the ACE and CoNLL datasets when only one type of the global features (either global-mention or global-entity) is integrated into the model. The combination of global-mention and global-entity is not very effective as it is actually worse than the performance of the individual global feature types. This suggests that global-mention and global-entity might cover overlapping information and their combination would inject redundancy into the model. The best performance is achieved by the global-entity features that would be used in all the evaluations below. ---------------------------------- **COMPARING TO THE PREVIOUS WORK** This section compares the proposed system (called Global-RNN) with the state-of-the-art models on our four datasets. These systems include the neural network model in<cite> (Francis-Landau et al., 2016)</cite> , the joint model for entity analysis in (Durrett and Klein, 2014) and the AIDA-light system with two-stage mapping in (Nguyen et al., 2014b) 6 . Table 2 shows the performance of the systems on the test sets with the reference knowledge base of the June 2016 Wikipedia dump.",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_11",
  "x": "We also include the performance of the systems on the December 2014 Wikipedia dump that was used and provided by<cite> (Francis-Landau et al., 2016)</cite> for further and compatible comparison. ---------------------------------- **SYSTEMS** Wikipedia 2014 Wikipedia 2016 ACE CoNLL WP WIKI ACE CoNLL WP WIKI DK2014 (Durrett and Klein, 2014) 79 First, we see that the performance of the systems drop significantly when we switch from Wikipedia 2014 to Wikipedia 2016 (especially for the datasets ACE and CoNLL). This is can be partly explained by the inclusion of new entities (pages) into Wikipedia from 2014 to 2016 that has made the entity mentions in the datasets more ambiguous 7 . Second and more importantly, Global-RNN significantly outperforms the all the compared models (except for the ACE dataset on Wikipedia 2014 and the WIKI dataset on Wikipedia 2016), thereby demonstrating the benefits of the joint modeling for local and global features via neural networks for EL in this work. ---------------------------------- **DOMAIN ADAPTATION EXPERIMENTS** The purpose of this section is to further evaluate the models in the domain adaptation setting to investigate their cross-domain robustness for EL.",
  "y": "similarities background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_12",
  "x": "Such a performance loss originates from a variety of mismatches between the source and the target domains, including the differences in vocabulary, data distributions, styles etc. This has motivated the domain adaptation research that aims to improve the cross-domain performance of the models by adaptation techniques. One of the key strategies of the domain adaptation techniques is the search for the domain-independent features that are discriminative across different domains (Blitzer et al., 2006; Jiang, 2009; Plank and Moschitti, 2013; Nguyen and Grishman, 2014a) . These invariants serve as the connectors between different domains and help to transfer the knowledge from one domain to the others. For EL, we hypothesize that the global coherence is an effective domain-independent feature that would help to improve the crossdomain performance of the models. The intuition is that the entities mentioned in a document of any domains should be related to each other. Eventually, we expect that the proposed model with global coherence features would be more robust to domain shifts than the local approach<cite> (Francis-Landau et al., 2016)</cite> . ---------------------------------- **DATASET**",
  "y": "motivation background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_13",
  "x": "Table 3 compares Global-RNN with the neural network EL model in<cite> (Francis-Landau et al., 2016)</cite> , the best reported model on the ACE dataset in the literature 8 . In this table, the models are trained on the source domain news, and evaluated on news itself (in-domain performance) (via 5-fold cross validation) as well as on the 4 target domains bc, cts, wl, un (out-of-domain performance The first observation from the table is that the performance of all the compared systems on the target domains is much worse than the corresponding in-domain performance. In particular, the performance gap between the in-domain performance and the the worst out-of-domain performance (on the domain wl) is up to 10%, thus indicating the mismatches between the source and the target domains for EL. Second and most importantly, Global-RNN is consistently better than the model with only local features in<cite> (Francis-Landau et al., 2016)</cite> over all the target domains (although it is less pronounced in the cts domain). This demonstrates the cross-domain robustness of the proposed model and confirms our hypothesis about the domain-independence of the global coherence features for EL. ---------------------------------- **EVALUATION** ---------------------------------- **ANALYSIS**",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_14",
  "x": "In particular, the performance gap between the in-domain performance and the the worst out-of-domain performance (on the domain wl) is up to 10%, thus indicating the mismatches between the source and the target domains for EL. Second and most importantly, Global-RNN is consistently better than the model with only local features in<cite> (Francis-Landau et al., 2016)</cite> over all the target domains (although it is less pronounced in the cts domain). This demonstrates the cross-domain robustness of the proposed model and confirms our hypothesis about the domain-independence of the global coherence features for EL. ---------------------------------- **EVALUATION** ---------------------------------- **ANALYSIS** In order to better understand the performance gap in the domain adaptation experiments for EL, we visualize the representation vectors of the entity mentions in different domains. In particular, after Global-RNN is trained, we retrieve the representation vectorsc i for the immediate contexts of the entity mentions in the source and target domains, project them into the 2-dimension space via the t-SNE algorithm and plot them.",
  "y": "background differences"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_15",
  "x": "Entity linking or disambiguation has been studied extensively in NLP research, falling broadly into two major approaches: local and global disambiguation. Both approaches share the goal of measuring the similarities between the entity mentions and the target candidates in the reference KB. The local paradigm focuses on the internal structures of each separate mention-entity pair, covering the name string comparisons between the surfaces of the entity mentions and target candidates, entity popularity or entity type and so on (Bunescu and Pasca, 2006; Milne and Witten, 2008; Zheng et al., 2010; Ji and Grishman, 2011; Mendes et al., 2011; Cassidy et al., 2011; Shen et al., 2014) . In contrast, the global approach jointly maps all the entity mentions within documents to model the topical coherence. Various techniques have been exploited for capturing such semantic consistency, including Wikipedia category agreement (Cucerzan, 2007) , Wikipedia link-based measures (Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) , Point-wise Mutual Information measures , integer linear programming (Cheng and Roth, 2013) , PageRank (Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015) , stacked generalization (He et al., 2013a) , to name a few. The entity linking techniques and systems have been actively evaluated at the NIST-organized Text Analysis Conference (Ji et al., 2014) . Neural networks are applied to entity linking very recently. He et al. (2013b) learn enttiy representation via Stacked Denoising Auto-encoders. Sun et al. (2015) employ convolutional neural networks and neural tensor networks to model mentions, entities and contexts while <cite>Francis-Landau et al. (2016)</cite> combine CNN-based representations with sparse features to improve the performance.",
  "y": "differences background"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_0",
  "x": "Therefore, mapping documents in different languages into a common latent topic space can be of great benefit when detecting document translation pairs (Mimno et al., 2009;<cite> Platt et al., 2010)</cite> . Aside from the benefits that it offers in the task of detecting document translation pairs, topic models offer potential benefits to the task of creating translation lexica, aligning passages, etc. The process of discovering relationship between documents using topic models involves: (1) representing documents in the latent space by inferring their topic distributions and (2) comparing pairs of topic distributions to find close matches. Many widely used techniques do not scale efficiently, however, as the size of the document collection grows. Posterior inference by Gibbs sampling, for instance, may make thousands of passes through the data. For the task of comparing topic distributions, recent work has also resorted to comparing all pairs of documents (Talley et al., 2011) . This paper presents efficient methods for both of these steps and performs empirical evaluations on the task of detected translated document pairs embedded in a large multilingual corpus. Unlike some more exploratory applications of topic models, translation detection is easy to evaluate. The need for bilingual training data in many language pairs and domains also makes it attractive to mitigate the quadratic runtime of brute force translation detection.",
  "y": "motivation"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_1",
  "x": "Then, in \u00a73, we build on prior work on efficient approximations to the nearest neighbor problem by presenting theoretical and empirical evidence for applicability to topic distributions in the probability simplex and in \u00a74, we evaluate the combination of online variational Bayes and approximate nearest neighbor methods on the translation detection task. ---------------------------------- **ONLINE VARIATIONAL BAYES FOR POLYLINGUAL TOPIC MODELS** Hierarchical generative Bayesian models, such as topic models, have proven to be very effective for modeling document collections and discovering underlying latent semantic structures. Most current topic models are based on Latent Dirichlet Allocation (LDA) . In some early work on the subject, showed the usefulness of LDA on the task of automatic annotation of images. Hall et al. (2008) used LDA to analyze historical trends in the scientific literature; Wei and Croft (2006) showed improvements on an information retrieval task. More recently Eisenstein et al. (2010) modeled geographic linguistic variation using Twitter data. Aside from their widespread use on monolingual text, topic models have also been used to model multilingual data (Boyd-Graber and Blei, 2009;<cite> Platt et al., 2010</cite>; Jagarlamudi and Daum\u00e9, 2010; Fukumasu et al., 2012) , to name a few.",
  "y": "background"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_2",
  "x": "Mapping multilingual documents into a common, language-independent vector space for the purpose of improving machine translation (MT) and performing cross-language information retrieval (CLIR) tasks has been explored through various techniques. Mimno et al. (2009) introduced polylingual topic models (PLTM), an extension of latent Dirichlet allocation (LDA), and, more recently,<cite> Platt et al. (2010)</cite> proposed extensions of principal component analysis (PCA) and probabilistic latent semantic indexing (PLSI). Both the PLTM and PLSI represent bilingual documents in the probability simplex, and thus the task of finding document translation pairs is formulated as finding similar probability distributions. While the nature of both works was exploratory, results shown on fairly large collections of bilingual documents (less than 20k documents) offer convincing argument of their potential. Expanding these approaches to much large collections of multilingual documents would require utilizing fast NN search for computing similarity in the probability simplex. While there are many other proposed approaches to the task of finding document translation pairs that represent documents in metric space, such as Krstovski and Smith (2011) which utilizes LSH for cosine distance, there is no evidence that they yield good results on documents of small lengths such as paragraphs and even sen-tences. In this section, we empirically show how to utilize approaches that deal with representing documents in the probability simplex without a significant loss in accuracy while significantly improving the processing time. We use PLTM representations of bilingual documents. In addition, we show how the results as reported by<cite> Platt et al. (2010)</cite> can be obtained using the PLTM representation with a significant speed improvement.",
  "y": "background"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_3",
  "x": "In this section, we empirically show how to utilize approaches that deal with representing documents in the probability simplex without a significant loss in accuracy while significantly improving the processing time. We use PLTM representations of bilingual documents. In addition, we show how the results as reported by<cite> Platt et al. (2010)</cite> can be obtained using the PLTM representation with a significant speed improvement. As in <cite>(Platt et al., 2010)</cite> and (Mimno et al., 2009 ) the task is to find document translation pairs in a multilingual collection of documents by representing documents in the probability simplex and computing similarity between their probability distribution representation across all document pairs. For this experimental setup, accuracy is defined as the number of times (in percentage) that the target language document was discovered at rank 1 (i.e. % @Rank 1.) across the whole test collection. ---------------------------------- **EXPERIMENTAL SETUP** We use Mallet's (McCallum, 2002) implementation of the PLTM to train and infer topics on the same data set used in<cite> Platt et al. (2010)</cite> . That paper used the Europarl (Koehn, 2005) (Mimno et al., 2009) , these performance comparisons are not done on the same training and test sets-a gap that we fill below.",
  "y": "differences"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_4",
  "x": "While there are many other proposed approaches to the task of finding document translation pairs that represent documents in metric space, such as Krstovski and Smith (2011) which utilizes LSH for cosine distance, there is no evidence that they yield good results on documents of small lengths such as paragraphs and even sen-tences. In this section, we empirically show how to utilize approaches that deal with representing documents in the probability simplex without a significant loss in accuracy while significantly improving the processing time. We use PLTM representations of bilingual documents. In addition, we show how the results as reported by<cite> Platt et al. (2010)</cite> can be obtained using the PLTM representation with a significant speed improvement. As in <cite>(Platt et al., 2010)</cite> and (Mimno et al., 2009 ) the task is to find document translation pairs in a multilingual collection of documents by representing documents in the probability simplex and computing similarity between their probability distribution representation across all document pairs. For this experimental setup, accuracy is defined as the number of times (in percentage) that the target language document was discovered at rank 1 (i.e. % @Rank 1.) across the whole test collection. ---------------------------------- **EXPERIMENTAL SETUP** We use Mallet's (McCallum, 2002) implementation of the PLTM to train and infer topics on the same data set used in<cite> Platt et al. (2010)</cite> .",
  "y": "similarities"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_5",
  "x": "In this section, we empirically show how to utilize approaches that deal with representing documents in the probability simplex without a significant loss in accuracy while significantly improving the processing time. We use PLTM representations of bilingual documents. In addition, we show how the results as reported by<cite> Platt et al. (2010)</cite> can be obtained using the PLTM representation with a significant speed improvement. As in <cite>(Platt et al., 2010)</cite> and (Mimno et al., 2009 ) the task is to find document translation pairs in a multilingual collection of documents by representing documents in the probability simplex and computing similarity between their probability distribution representation across all document pairs. For this experimental setup, accuracy is defined as the number of times (in percentage) that the target language document was discovered at rank 1 (i.e. % @Rank 1.) across the whole test collection. ---------------------------------- **EXPERIMENTAL SETUP** We use Mallet's (McCallum, 2002) implementation of the PLTM to train and infer topics on the same data set used in<cite> Platt et al. (2010)</cite> . That paper used the Europarl (Koehn, 2005) (Mimno et al., 2009) , these performance comparisons are not done on the same training and test sets-a gap that we fill below.",
  "y": "similarities uses"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_6",
  "x": "---------------------------------- **EVALUATION TASK AND RESULTS** Performance of the four PLTM models and the performance across the four different similarity measurements was evaluated based on the percentage of document translation pairs (out of the whole test set) that were discovered at rank one. This same approach was used by <cite>(Platt et al., 2010)</cite> to show the absolute performance comparison. As in the case of the previous two tasks, in order to evaluate the approximate, LSH based, Hellinger distance we used values of R=0.4, R=0.6 and R=0.8. Since in <cite>(Platt et al., 2010)</cite> numbers were reported on the test speeches whose word length is greater or equal to 100, we used the same subset (total of 14150 speeches) of the original test collection. Shown in Table 1 are results across the four different measurements for all four PLTM models. When using regular JS divergence, our PLTM model with 200 topics performs the best with 99.42% of the top one ranked candidate translation documents being true translations. When using approximate, kd-trees based, Hellinger distance, we outperform regular JS and Hellinger divergence across all topics and for T=500 we achieve the best overall accuracy of 99.61%.",
  "y": "uses similarities"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_7",
  "x": "---------------------------------- **EVALUATION TASK AND RESULTS** Performance of the four PLTM models and the performance across the four different similarity measurements was evaluated based on the percentage of document translation pairs (out of the whole test set) that were discovered at rank one. This same approach was used by <cite>(Platt et al., 2010)</cite> to show the absolute performance comparison. As in the case of the previous two tasks, in order to evaluate the approximate, LSH based, Hellinger distance we used values of R=0.4, R=0.6 and R=0.8. Since in <cite>(Platt et al., 2010)</cite> numbers were reported on the test speeches whose word length is greater or equal to 100, we used the same subset (total of 14150 speeches) of the original test collection. Shown in Table 1 are results across the four different measurements for all four PLTM models. When using regular JS divergence, our PLTM model with 200 topics performs the best with 99.42% of the top one ranked candidate translation documents being true translations. When using approximate, kd-trees based, Hellinger distance, we outperform regular JS and Hellinger divergence across all topics and for T=500 we achieve the best overall accuracy of 99.61%.",
  "y": "similarities uses"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_8",
  "x": "On the same data set, <cite>(Platt et al., 2010)</cite> report accuracy of 98.9% using 50 topics, a slightly different prior distribution, and MAP instead of posterior inference. Shown in Table 2 are the relative differences in time between all pairs JS divergence, approximate kd-trees and LSH based Hellinger distance with different value of R. Rather than showing absolute speed numbers, which are often influenced by the processor configuration and available memory, we show relative speed improvements where we take the slowest running configuration as a referent value. In our case we assign the referent speed value of 1 to the configuration with T=500 and allpairs JS computation. Results shown are based on comparing running time of E2LSH and ANN against the all-pairs similarity comparison implementation that uses hash tables to store all documents in the bilingual collection which is significantly faster than the other code implementation. For the approximate, LSH based, Hellinger distance with T=100 we obtain a speed improvement of 24.2 times compared to regular all-pairs JS divergence while maintaining the same performance compared to Hellinger distance metric and insignificant loss over all-pairs JS divergence. From Table 2 it is evident that as we increase the radius R we reduce the relative speed of performance since the range of points that LSH considers for a given query point increases. Also, as the number of topics increases, the speed benefit is reduced for both the LSH and k-d tree techniques. ---------------------------------- **CONCLUSION**",
  "y": "differences extends"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_0",
  "x": "However, the DS assumption is too strong and may introduce noise such as false negative samples due to missing facts in the knowledge base. In this paper, we propose relation extraction models and a new dataset to improve RE. We define 'instance' as a sentence containing an entity-pair, and 'instance set' as a set of sentences containing the same entity-pair. It was observed by<cite> [Zeng et al., 2015]</cite> that 50% of the sentences in the Riedel2010 Distant Supervision dataset [Riedel et al., 2010] , a popular DS benchmark dataset, had 40 or more words in them. We note that not all the words in these long sentences contribute towards expressing the given relation. In this work, we formulate various word attention mechanisms to help the relation extraction model focus on the right context in a given sentence. The MIML assumption states that in an instance set corresponding to an entity pair, at least one sentence in that set should express the true relation assigned to the set. However, we observe that this is not always true in currently available benchmark datasets for RE in the distantly supervised setting. In particular, current datasets have noise in the test set, for example, a fact may be labelled false if it is missing in the knowledge base, leading to a false negative label in train and test set.",
  "y": "background"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_1",
  "x": "Given a set of sentences S = {s i }; i \u2208 [1 . . . N ], where each sentence s i contains both the entities, the task of relation extraction with distantly supervised dataset is to learn a function F r : F r (S, (e 1 , e 2 )) = 1 if relation r is true for pair(e 1 , e 2 ) 0 Otherwise PCNN:<cite> [Zeng et al., 2015]</cite> proposed the Piecewise Convolution Neural Network (PCNN), a successful model for distantly supervised relation extraction. The Success of the relation extraction task depends on extracting the right structural features from the sentence containing the entity-pair. Neural networks, such as Convolutional Neural Networks (CNNs), have been proposed to alleviate the need to manually design features for a given task [Zeng et al., 2014] . As the output of CNNs is dependent on the number of tokens in the sentence, max pooling operation is often applied to remove this dependence. However, the use of a single max-pool misses out on some of these structural features useful for relation extraction task. PCNN model divides a sentence s i 's convolution filter output c i containing two entities into three parts c i1 , c i2 , c i3 -sentence context to the left of first entity, between the two entities, and to right of the second entity respectively-and performs max-pooling on each of the three parts, shown in Figure 2 . Thereby, leveraging the entity location information to retain the structural features of a sentence after the max-pooling operation. The output of this operation is the concatenation of {pc i1 , pc i2 , pc i3 } yielding a fixed size output. The fixed size output is processed through a tanh non-linearity followed by a linear layer to produce relation probabilities.",
  "y": "background"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_2",
  "x": "This extra information helps in narrowing down the relation possibilities by looking only at the relations that occur between a person and a city. [Shen and Huang, 2016] proposed an entity attention model for supervised relation extraction with a single sentence as input to the model. We modify and adapt their model for the distant supervision setting and propose Entity Attention (EA) which works with a bag of sentences. For a given bag of sentences, learning is done using the setting proposed by<cite> [Zeng et al., 2015]</cite> , wherein the sentence with the highest probability of expressing a relation in a bag is selected to train the model in each iteration. The EA model has two components: 1) PCNN layer, and 2) Entity Attention Layer, as shown in Figure 4 . Consider an instance set S q with set of sentences, 1\u00d7d is a word embedding and {e emb q1 , e emb q2 } are the embeddings for the two entities. The PCNN layer is applied on the words in the sentence<cite> [Zeng et al., 2015]</cite> . The entity-specific attention u i,j,qk for j th word with respect to k th entity is calculated as follows: is the concatenation of a word and the entity embedding.",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_3",
  "x": "Let us once again consider the example sentence from Section 2.2 involving entity pair (Obama, Honolulu). In the sentence, for entity Obama, the word President helps in identifying that the entity is a person. This extra information helps in narrowing down the relation possibilities by looking only at the relations that occur between a person and a city. [Shen and Huang, 2016] proposed an entity attention model for supervised relation extraction with a single sentence as input to the model. We modify and adapt their model for the distant supervision setting and propose Entity Attention (EA) which works with a bag of sentences. For a given bag of sentences, learning is done using the setting proposed by<cite> [Zeng et al., 2015]</cite> , wherein the sentence with the highest probability of expressing a relation in a bag is selected to train the model in each iteration. The EA model has two components: 1) PCNN layer, and 2) Entity Attention Layer, as shown in Figure 4 . Consider an instance set S q with set of sentences, 1\u00d7d is a word embedding and {e emb q1 , e emb q2 } are the embeddings for the two entities. The PCNN layer is applied on the words in the sentence<cite> [Zeng et al., 2015]</cite> .",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_4",
  "x": "The u i,j,qk are normalized using a softmax function to generate a i,j,qk , the attention scores for a given word. Similar to the PCNN model in Section 2.1, the attention weighted word embeddings are pooled using piecewise pooling method to generate s ea \u2208 R 1\u00d73g dimensional sentence embeddings. The output from the PCNN layer and the entity attention layers are concatenated and then passed through a linear layer to obtain probabilities for each relation. The entity attention model (EA) we propose is adapted to the distantly supervised setting by using two important variations from the original [Shen and Huang, 2016] model (a) The EA processes a set of sentences. It uses PCNN<cite> [Zeng et al., 2015]</cite> assumption to select the sentence with highest probability of any relation. The selected sentence is used to estimate the relation probabilities for an entity-pair and for back-propagation of the error for the bag-of-sentences. (b) EA uses PCNN instead of CNN to preserve structural features in a sentence. We found the two variations to be crucial for the model to work in the distant supervision setting. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_5",
  "x": ", 2016], we use held-out evaluation scheme. The performance of each model is evaluated on a test set using Precision-Recall (PR) curve. Baselines: We compare proposed models with (a) Piecewise Convolution Neural Network (PCNN)<cite> [Zeng et al., 2015]</cite> and (b) Neural Relation Extraction with Selective Attention over Instances (NRE) [Lin et al., 2016] . Both NRE and PCNN baseline outperform traditional baselines like MIML-RE and hence we use them as a representative state-of-the-art baseline to compare with proposed models. Model Parameters: The parameters used for the various models are summarized in Table 4 . Word embeddings are initialized using the Word2Vec vectors from NYT dataset, similar to [Lin et al., 2016] . Word Position feature embeddings (with respect to each entity) are randomly initialized and learned during training. Concatenation of the word embedding and position embedding results in a 60-dimensional (d w + (2 * d p )) embedding x ij for each word. We implemented PCNN model baseline following<cite> [Zeng et al., 2015]</cite> and used author provided results and implementation for NRE baseline.",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_6",
  "x": "Both NRE and PCNN baseline outperform traditional baselines like MIML-RE and hence we use them as a representative state-of-the-art baseline to compare with proposed models. Model Parameters: The parameters used for the various models are summarized in Table 4 . Word embeddings are initialized using the Word2Vec vectors from NYT dataset, similar to [Lin et al., 2016] . Word Position feature embeddings (with respect to each entity) are randomly initialized and learned during training. Concatenation of the word embedding and position embedding results in a 60-dimensional (d w + (2 * d p )) embedding x ij for each word. We implemented PCNN model baseline following<cite> [Zeng et al., 2015]</cite> and used author provided results and implementation for NRE baseline. The EA and BGWA models were developed in PyTorch 2 . We use SGD algorithm with dropout [Srivastava et al., 2014] for model learning. The experiments were run on GeForce GTX 1080 Ti using NVIDIA-CUDA.",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_7",
  "x": "[Zeng et al., 2015] proposed a Piecewise Convolutional Neural Network (PCNN) model to preserve the structural features of a sentence using piecewise max-pooling approach, improving the precisionrecall curve significantly. However, PCNN method used only one sentence in the instance-set to predict the relation label and for backpropagation. [Lin et al., 2016] improves upon PCNN results by introducing an attention mechanism to select a set of sentences from instance set for relation label prediction. [ <cite>Zheng et al., 2016]</cite> aimed to leverage inter-sentence information for relation extraction in a ranking model. The hypothesis explored is that for a particular entity-pair, each mention alone may not be expressive enough of the relation in question, but information from several mentions may be required to decisively make a prediction. Recently, work by [Ye et al., 2016] exploit the connections between relation (class ties) to improve relation extraction performance. A few papers propose the addition of background knowledge to reduce noise in training data. [Weston et al., 2013] proposes a joint-embedding model for text and KB entities where the known part of the KB is utilized as part of the supervision signal. [Han and Sun, 2016] use indirect supervision like consistency between relation labels, consistency between relations and arguments, and consistency between neighbour instances using Markov logic networks.",
  "y": "background"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_8",
  "x": "Attention mechanisms in neural networks have been successfully applied to a variety of problems, like machine translation [Bahdanau et al., 2014] , image captioning [Xu et al., 2015] , supervised relation extraction [Shen and Huang, 2016] , distantly-supervised relation extraction<cite> [Zheng et al., 2016]</cite> etc. In our work, we focus on selecting the right words in a sentence using the word and entity-based attention mechanism. ---------------------------------- **CONCLUSION** Distant Supervision (DS) has emerged as a promising approach to bootstrap relation extractors with limited supervision. In this paper, we present three novel models for distantlysupervised relation extraction: (1) a Bi-GRU based word attention model (BGWA), (2) an entity-centric attention model (EA), and (3) and a weighted voting ensemble model, which combines multiple complementary models for improved relation extraction. We introduce GDS, a new distant supervision dataset for relation extraction. GDS removes test data noise present in all previous distant supervision benchmark datasets, making credible automatic evaluation possible. Combining proposed methods with attention-based sentence selection methods is left as future work.",
  "y": "background"
 },
 {
  "id": "c42e9d10ca8876af80eee021c969d7_0",
  "x": "Automatically detecting dialogue structure within corpora of human-human dialogue is the subject of increasing attention. In the domain of tutorial dialogue, automatic discovery of dialogue structure is of particular interest because these structures inherently represent tutorial strategies or modes, the study of which is key to the design of intelligent tutoring systems that communicate with learners through natural language. We propose a methodology in which a corpus of humanhuman tutorial dialogue is first manually annotated with dialogue acts. Dependent adjacency pairs of these acts are then identified through \u03c7 2 analysis, and hidden Markov modeling is applied to the observed sequences to induce a descriptive model of the dialogue structure. ---------------------------------- **INTRODUCTION** Automatically learning dialogue structure from corpora is an active area of research driven by a recognition of the value offered by data-driven approaches (e.g., Bangalore et al., 2006) . Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems <cite>(Forbes-Riley et al., 2007)</cite> , but also contribute to our understanding of the cognitive and affective processes involved in learning through tutoring (VanLehn et al., 2007) . Although traditional top-down approaches (e.g., Cade et al., 2008) and some empirical work on analyzing the structure of tutorial dialogue <cite>(Forbes-Riley et al., 2007)</cite> have yielded significant results, the field is limited by the lack of an automatic, data-driven approach to identifying dialogue structure.",
  "y": "background"
 },
 {
  "id": "c42e9d10ca8876af80eee021c969d7_1",
  "x": "In the domain of tutorial dialogue, automatic discovery of dialogue structure is of particular interest because these structures inherently represent tutorial strategies or modes, the study of which is key to the design of intelligent tutoring systems that communicate with learners through natural language. We propose a methodology in which a corpus of humanhuman tutorial dialogue is first manually annotated with dialogue acts. Dependent adjacency pairs of these acts are then identified through \u03c7 2 analysis, and hidden Markov modeling is applied to the observed sequences to induce a descriptive model of the dialogue structure. ---------------------------------- **INTRODUCTION** Automatically learning dialogue structure from corpora is an active area of research driven by a recognition of the value offered by data-driven approaches (e.g., Bangalore et al., 2006) . Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems <cite>(Forbes-Riley et al., 2007)</cite> , but also contribute to our understanding of the cognitive and affective processes involved in learning through tutoring (VanLehn et al., 2007) . Although traditional top-down approaches (e.g., Cade et al., 2008) and some empirical work on analyzing the structure of tutorial dialogue <cite>(Forbes-Riley et al., 2007)</cite> have yielded significant results, the field is limited by the lack of an automatic, data-driven approach to identifying dialogue structure. An empirical approach to identifying tutorial dialogue strategies, or modes, could address this limitation by providing a mechanism for describing in succinct probabilistic terms the tutorial strategies that actually occur in a corpus.",
  "y": "motivation background"
 },
 {
  "id": "c42e9d10ca8876af80eee021c969d7_2",
  "x": "This analysis uses a corpus of human-human tutorial dialogue collected in the domain of introductory computer science. Forty-three learners interacted remotely with a tutor through a keyboard-to-keyboard remote learning environment yielding 4,864 dialogue moves. The tutoring corpus was manually tagged with dialogue acts designed to capture the salient characteristics of the tutoring process (Table 1) . The correspondence between utterances and dialogue act tags is one-to-one. Compound utterances (i.e., a single utterance comprising more than one dialogue act) were split by the primary annotator prior to the inter-rater reliability study. ---------------------------------- **1** The importance of adjacency pairs is wellestablished in natural language dialogue (e.g., Schlegoff & Sacks, 1973) , and adjacency pair analysis has illuminated important phenomena in tutoring as well <cite>(Forbes-Riley et al., 2007)</cite> . For the current corpus, bigram analysis of dialogue acts yielded a set of commonly-occurring pairs.",
  "y": "background"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_0",
  "x": "In this dataset, target entities have been semi-automatically selected, and sentiment expressed towards multiple target entities as well as high-level topics in a tweet have been manually annotated. Unlike all existing studies on target-specific Twitter sentiment analysis, we move away from the assumption that each tweet mentions a single target; we introduce a more realistic and challenging task of identifying sentiment towards multiple targets within a tweet. To tackle this task, we propose TDParse, a method that divides a tweet into different segments building on the approach introduced by Vo and Zhang (2015) . TDParse exploits a syntactic dependency parser designed explicitly for tweets (Kong et al., 2014) , and combines syntactic information for each target with its left-right context. We evaluate and compare our proposed system both on our new multi-target UK election dataset, as well as on the benchmarking dataset for single-target dependent sentiment<cite> (Dong et al., 2014)</cite> . We show a clear state-of-the-art performance of TDParse over existing approaches for tweets with multiple targets, which encourages further research on the multi-target-specific sentiment recognition task. 2 2 Related Work: Target-dependent Sentiment Classification on Twitter The 2015 Semeval challenge introduced a task on target-specific Twitter sentiment (Rosenthal et al., 2015) which most systems (Boag et al., 2015; Plotnikova et al., 2015) treated in the same way as tweet level sentiment. The best performing system in the 2016 Semeval Twitter challenge substask B (Nakov et al., 2016) , named Tweester, also performs on tweet level sentiment classification.",
  "y": "uses"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_1",
  "x": "Also sentiment classification in formal text such as product reviews 2 The data and code can be found at https://goo.gl/ S2T1GO is very different from that in tweets. Recently Vargas et al. (2016) analysed the differences between the overall and target-dependent sentiment of tweets for three events containing 30 targets, showing many significant differences between the corresponding overall and target-dependent sentiment labels, thus confirming that these are distinct tasks. Early work tackling target-dependent sentiment in tweets (Jiang et al., 2011) designed targetdependent features manually, relying on the syntactic parse tree and a set of grammar-based rules, and incorporating the sentiment labels of related tweets to improve the classification performance. Recent work<cite> (Dong et al., 2014)</cite> used recursive neural networks and adaptively chose composition functions to combine child feature vectors according to their dependency type, to reflect sentiment signal propagation to the target. Their datadriven composition selection approach replies on the dependency types as features and a small set of rules for constructing target-dependent trees. Their manually annotated dataset contains only one target per tweet and has since been used for benchmarking by several subsequent studies (Vo and Zhang, 2015; Tang et al., 2016a; Zhang et al., 2016) . Vo and Zhang (2015) exploit the left and right context around a target in a tweet and combine low-dimensional embedding features from both contexts and the full tweet using a number of different pooling functions. Despite not fully capturing semantic and syntactic information given the target entity, they show a much better performance than<cite> Dong et al. (2014)</cite> , indicating useful signals in relation to the target can be drawn from such context representation. Both Tang et al. (2016a) and Zhang et al. (2016) adopt and integrate left-right target-dependent context into their recurrent neural network (RNN) respectively.",
  "y": "background"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_2",
  "x": "Their manually annotated dataset contains only one target per tweet and has since been used for benchmarking by several subsequent studies (Vo and Zhang, 2015; Tang et al., 2016a; Zhang et al., 2016) . Vo and Zhang (2015) exploit the left and right context around a target in a tweet and combine low-dimensional embedding features from both contexts and the full tweet using a number of different pooling functions. Despite not fully capturing semantic and syntactic information given the target entity, they show a much better performance than<cite> Dong et al. (2014)</cite> , indicating useful signals in relation to the target can be drawn from such context representation. Both Tang et al. (2016a) and Zhang et al. (2016) adopt and integrate left-right target-dependent context into their recurrent neural network (RNN) respectively. While Tang et al (2016a) propose two long shortterm memory (LSTM) models showing competitive performance to Vo and Zhang (2015) , Zhang et al (2016) design a gated neural network layer between the left and right context in a deep neural network structure but require a combination of three corpora for training and evaluation. Results show that conventional neural network models like LSTM are incapable of explicitly capturing important context information of a target (Tang et al., 2016b) . Tang et al. (2016a) also experiment with adding attention layers for LSTM but fail to achieve competitive results possibly due to the small training corpus. Going beyond the existing work we study the more challenging task of classifying sentiment towards multiple target entities within a tweet. Using the syntactic information drawn from tweetspecific parsing, in conjunction with the left-right contexts, we show the state-of-the-art performance in both single and multi-target classification tasks.",
  "y": "background"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_3",
  "x": "It is worth noting that the sentiment annotation for each target also involves choosing among not only positive/negative/neutral but also a fourth category 'doesnotapply'. The resulting dataset contains 4,077 tweets, with an average of 3.09 entity mentions (targets) per tweet. As many as 3,713 tweets have more than a single entity mention (target) per tweet, which makes the task different from 2015 Semeval 10 subtask C (Rosenthal et al., 2015) and a target-dependent benchmarking dataset of<cite> Dong et al. (2014)</cite> where each tweet has only one target annotated and thus one sentiment label assigned. The number of targets in the 4,077 tweets to be annotated originally amounted to 12,874. However, the annotators unhighlighted 975 of them, and added 688 new ones, so that the final number of targets in the dataset is 12,587. These are distributed as follows: 1,865 are positive, 4,707 are neutral, and 6,015 are negative. This distribution shows the tendency of a theme like politics, where users tend to have more negative opinions. This is different from the Semeval dataset, which has a majority of neutral sentiment. Looking at the annotations provided for different targets within each tweet, we observe that 2,051 tweets (50.3%) have all their targets consistently annotated with a single sentiment value, 1,753 tweets (43.0%) have two different sentiments, and 273 tweets (6.7%) have three different sentiment values.",
  "y": "differences"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_4",
  "x": "vector as shown in (2), where P (X) presents a list of k different pooling functions on an embedding matrix X. Not only does this proposed framework make the learning process efficient without labor intensive manual feature engineering and heavy architecture engineering for neural models, it has also shown that complex syntactic and semantic information can be effectively drawn by simply concatenating different types of context together without the use of deep learning (other than pretrained word embeddings). Data set: We evaluate and compare our proposed system to the state-of-the-art baselines on a benchmarking corpus<cite> (Dong et al., 2014</cite> ) that has been used by several previous studies (Vo and Zhang, 2015; Tang et al., 2016a; Zhang et al., 2016) . This corpus contains 6248 training tweets and 692 testing tweets with a sentiment class balance of 25% negative, 50% neutral and 25% positive. Although the original corpus has only annotated one target per tweet, without specifying the location of the target, we expand this notion to consider cases where the target entity may appear more than once at different locations in the tweet, e.g.: \"Nicki Minaj has brought back the female rapper. -really? Nicki Minaj is the biggest parody in popular music since the Lonely Island.\" Semantically it is more appropriate and meaningful to consider both target appearances when determining the sentiment polarity of \"Nicki Minaj\" expressed in this tweet. While it isn't clear if<cite> Dong et al. (2014)</cite> and Tang et al. (2016a) have considered this realistic same-target-multiappearance scenario, Vo et al. (2015) and Zhang et al. (2016) do not take it into account when extracting target-dependent contexts. Contrary to these studies we extend our system to fully incorporate the situation where a target appears multiple times at different locations in the tweet. We add another pooling layer in (2) where we apply a medium pooling function to combine extracted feature vectors from each target appearance together into the final feature vector for the sentiment classification of such targets.",
  "y": "uses"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_6",
  "x": "In tuning the cost factor C we perform five-fold cross validation on the training data over the same set of parameter values for both Vo and Zhang (2015) 's implementation and our system. This makes sure our proposed models are comparable with those of Vo and Zhang (2015) . Evaluation metrics: We follow previous work on target-dependent Twitter sentiment classification, and report our performance in accuracy, 3-class macro-averaged (i.e. negative, neutral and positive) F 1 score as well as 2-class macroaveraged (i.e. negative and positive) F 1 score 8 , as used by the Semeval competitions (Rosenthal et al., 2015) for measuring Twitter sentiment classification performance. ---------------------------------- **EXPERIMENTAL RESULTS AND COMPARISON WITH OTHER BASELINES** We report our experimental results in Table 2 on the single-target benchmarking corpus<cite> (Dong et al., 2014)</cite> , with three model categories: 1) tweet-level target-independent models, 2) targetdependent models without considering the 'sametarget-multi-appearance' scenario and 3) targetdependent models incorporating the 'same-targetmulti-appearance' scenario. We include the models presented in the previous section as well as models for target specific sentiment from the literature where possible. Among the target-independent baseline models Target-ind (Vo and Zhang, 2015) and Semevalbest have shown strong performance compared with SSWE and SVM-ind (Jiang et al., 2011) as they use more features, especially rich automatic features using the embeddings of Mikolov et al. (2013) . Interestingly they also perform better than some of the targetdependent baseline systems, namely SVM-dep (Jiang et al., 2011) , Recursive NN and AdaRNN<cite> (Dong et al., 2014)</cite> , showing the difficulty of fully extracting and incorporating target information in tweets.",
  "y": "uses"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_7",
  "x": "Among the target-independent baseline models Target-ind (Vo and Zhang, 2015) and Semevalbest have shown strong performance compared with SSWE and SVM-ind (Jiang et al., 2011) as they use more features, especially rich automatic features using the embeddings of Mikolov et al. (2013) . Interestingly they also perform better than some of the targetdependent baseline systems, namely SVM-dep (Jiang et al., 2011) , Recursive NN and AdaRNN<cite> (Dong et al., 2014)</cite> , showing the difficulty of fully extracting and incorporating target information in tweets. Basic LSTM models (Tang et al., 2016a) completely ignore such target information and as a result do not perform as well. Among the target-dependent systems neural network baselines have shown varying results. The adaptive recursive neural network, namely AdaRNN<cite> (Dong et al., 2014)</cite> , adaptively selects composition functions based on the input data and thus performs better than a standard recursive neural network model (Recursive NN<cite> (Dong et al., 2014)</cite> ). TD-LSTM and TC-LSTM from Tang et al. (2016a) model left-target-right contexts using two LSTM neural networks and by doing so incorporate target-dependent information. TD-LSTM uses two LSTM neural networks for modeling the left and right contexts respectively. TC-LSTM differs from (and outperforms) TD-LSTM in that it concatenates target word vectors with embedding vectors of each context word. We also test the Gated recurrent neural network models proposed by Zhang et al. (2016) on the same dataset.",
  "y": "background"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_8",
  "x": "---------------------------------- **EXPERIMENTAL RESULTS AND COMPARISON WITH OTHER BASELINES** We report our experimental results in Table 2 on the single-target benchmarking corpus<cite> (Dong et al., 2014)</cite> , with three model categories: 1) tweet-level target-independent models, 2) targetdependent models without considering the 'sametarget-multi-appearance' scenario and 3) targetdependent models incorporating the 'same-targetmulti-appearance' scenario. We include the models presented in the previous section as well as models for target specific sentiment from the literature where possible. Among the target-independent baseline models Target-ind (Vo and Zhang, 2015) and Semevalbest have shown strong performance compared with SSWE and SVM-ind (Jiang et al., 2011) as they use more features, especially rich automatic features using the embeddings of Mikolov et al. (2013) . Interestingly they also perform better than some of the targetdependent baseline systems, namely SVM-dep (Jiang et al., 2011) , Recursive NN and AdaRNN<cite> (Dong et al., 2014)</cite> , showing the difficulty of fully extracting and incorporating target information in tweets. Basic LSTM models (Tang et al., 2016a) completely ignore such target information and as a result do not perform as well. Among the target-dependent systems neural network baselines have shown varying results. The adaptive recursive neural network, namely AdaRNN<cite> (Dong et al., 2014)</cite> , adaptively selects composition functions based on the input data and thus performs better than a standard recursive neural network model (Recursive NN<cite> (Dong et al., 2014)</cite> ).",
  "y": "background"
 },
 {
  "id": "c57e98c9c07dd5d8653e172136c901_0",
  "x": "The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs. The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference. This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (Blei et al., 2010) , hierarchical Pitman-Yor process (Teh, 2006) , Indian buffet process (Ghahramani and Griffiths, 2005) , recurrent neural network (Mikolov et al., 2010; Van Den Oord et al., 2016) , long short-term memory (Hochreiter and Schmidhuber, 1997; , sequence-to-sequence model (Sutskever et al., 2014), variational auto-encoder (Kingma and Welling, 2014) , generative adversarial network (Goodfellow et al., 2014) , attention mechanism (Chorowski et al., 2015; <cite>Seo et al., 2016)</cite> , memory-augmented neural network (Graves et al., 2014; Graves et al., 2014) , stochastic neural network Miao et al., 2016) , predictive state neural network (Downey et al., 2017) , policy gradient (Yu et al., 2017) and reinforcement learning (Mnih et al., 2015) . We present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language. The variational inference and sampling method are formulated to tackle the optimization for complicated models (Rezende et al., 2014) . The word and sentence embeddings, clustering and co-clustering are merged with linguistic and semantic constraints. A series of case studies are presented to tackle different issues in deep Bayesian learning and understanding. At last, we point out a number of directions and outlooks for future studies. The presentation of this tutorial is arranged into five parts.",
  "y": "background"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_0",
  "x": "**INTRODUCTION** The field of compositional distributional semantics seeks principled ways to combine distributional representations of words to form larger units. Representations of full sentences, besides their theoretical interest, have the potential to be useful for tasks such as automatic summarisation and recognising textual entailment. A number of recent studies have investigated ways to combine distributional representations of Subject, Verb, Object (SVO) triples to form transitive sentences, that is, sentences based on a transitive verb [2-4, 6, 7, 12, 14] . Under the functional approach [1] [2] [3] [4] , argument-taking words such as verbs and adjectives are represented as tensors, which take word vectors as arguments. A transitive verb can be viewed as a third-order tensor with input dimensions for the subject and object, and an output dimension for the meaning of the sentence as a whole. This approach has achieved promising initial results [6] <cite>[7]</cite> [8] [9] 14] , but many questions remain. Two outstanding questions are the best method of learning verb tensors from a corpus, and the best sentence space for a variety of different tasks. This paper presents work in progress which addresses both of these questions.",
  "y": "motivation"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_2",
  "x": "**METHODS** In the definition of the functional approach to compositional distributional semantics [1] [2] [3] [4] , a transitive verb is a map that takes as arguments noun vectors representing the subject and object, and produces a vector in the sentence space. Typically, noun vectors for subject and object reside in a \"topic space\" where the dimensions correspond to co-occurrence features; we use a reduced space resulting from applying Singular Value Decomposition (SVD) to the raw co-occurrence space. The correct sentence space to use is less obvious; previous approaches have either mapped sentence meaning to the same topic-based noun space [6,<cite> 7]</cite> or defined a new space for sentence meaning, particularly plausibility space [11, 14] . If the verb function is a multi-linear map, then the verb is naturally represented by a third-order tensor. However, tensor training can be expensive and in practice, for some tasks, the verb can be approximated as a matrix<cite> [7,</cite> 14] . Below we describe two ways of learning a verb matrix. In the regression method, the learnt matrix consists of parameters from a plausibility classifier. The classifier is trained to distinguish plausible sentences like animals eat plants from implausible sentences like animals eat planets.",
  "y": "background"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_3",
  "x": "Typically, noun vectors for subject and object reside in a \"topic space\" where the dimensions correspond to co-occurrence features; we use a reduced space resulting from applying Singular Value Decomposition (SVD) to the raw co-occurrence space. The correct sentence space to use is less obvious; previous approaches have either mapped sentence meaning to the same topic-based noun space [6,<cite> 7]</cite> or defined a new space for sentence meaning, particularly plausibility space [11, 14] . If the verb function is a multi-linear map, then the verb is naturally represented by a third-order tensor. However, tensor training can be expensive and in practice, for some tasks, the verb can be approximated as a matrix<cite> [7,</cite> 14] . Below we describe two ways of learning a verb matrix. In the regression method, the learnt matrix consists of parameters from a plausibility classifier. The classifier is trained to distinguish plausible sentences like animals eat plants from implausible sentences like animals eat planets. In the distributional method, training is based on a sum of positive (i.e. attested) SVO triples. The acquisition of positive SVO data and plausibility training data is described in Section 2.2.",
  "y": "background"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_4",
  "x": "The regression algorithm is trained through gradient descent with Adagrad [5] and 10% of the training triples are used as a validation set for early stopping. ---------------------------------- **DISTRIBUTIONAL (DIST)** Following <cite>[7]</cite> , we generate a K \u00d7 K matrix for each verb as the average of outer products of subject and verb vectors from the positively labelled subset of the training data: where \u2297 is outer product and N p is the number of positive training examples. The intuition is that the matrix encodes higher weights for contextual features of frequently attested subjects and objects; for example, multiplying by the matrix for eat may yield a higher scalar value when its subject exhibits features common to animate nouns, and its object exhibits features common to edible nouns. ---------------------------------- **TRAINING DATA** In order to generate training data we find plausible SVO triples that occur in an October 2013 dump of Wikipedia.",
  "y": "uses"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_6",
  "x": "The meaning of a transitive sentence is a vector, obtained by: For the Relational method, sentence similarity is measured as the Frobenius inner product of the two sentence matrices. For Copy-subject and Verb-object, sentence similarity is measured as the cosine of the two sentence vectors. ---------------------------------- **TASKS** We investigate the performance of the regression learning method on two tasks: verb disambiguation, and transitive sentence similarity. In each case the system must compose SVO triples and compare the resulting semantic representations. For the verb disambiguation task we use the GS2011 dataset <cite>[7]</cite> . This dataset consists of pairs of SVO triples in which the subject and object are held constant, and the verb is manipulated to highlight different word senses.",
  "y": "uses"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_0",
  "x": "Despite the success of existing works on single-turn conversation generation, taking the coherence in consideration, human conversing is actually a context-sensitive process. Inspired by the existing studies, this paper proposed the static and dynamic attention based approaches for context-sensitive generation of open-domain conversational responses. Experimental results on two public datasets show that the proposed static attention based approach outperforms all the baselines on automatic and human evaluation. ---------------------------------- **INTRODUCTION** Until recently, training open-domain conversational systems that can imitate the way of human conversing is still not a well-solved problem and non-trivial task. Previous efforts focus on generating opendomain conversational responses as an unsupervised clustering process (Ritter et al., 2010) , a phrasebased statistical machine translation task (Ritter et al., 2011 ) and a search problem based on the vector space model (Banchs and Li, 2012) , etc. With the booming of deep learning, particularly the neural network based sequence-to-sequence models, generating open-domain conversational responses gradually turns into an end-to-end encoding and decoding process (Sutskever et al., 2014; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016b; Li et al., 2016a; Li et al., 2016b; Shao et al., 2017; Yao et al., 2017) . Despite the success of the above research on single-turn conversational response generation, human conversations are usually coherent (Li et al., 2016c) and context-sensitive (Tian et al., 2017;<cite> Xing et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_1",
  "x": "\u2022 First, existing studies of utterance modeling mainly focus on representing utterances by using bidirectional GRU<cite> (Xing et al., 2017)</cite> or unidirectional GRU (Tian et al., 2017 ). One is the attention-based approach<cite> (Xing et al., 2017)</cite> , the other is the sequential integration approach (Tian et al., 2017) . \u2022 Utterance Representations: Bidirectional GRU vs. Unidirectional GRU<cite> Xing et al. (2017)</cite> utilized a bidirectional GRU and a word-level attention mechanism to transfer word representations to utterance representations. \u2022 Inter-utterance Representations: Attention vs. Sequential Integration<cite> Xing et al. (2017)</cite> proposed a hierarchical attention mechanism to feed the utterance representations to a backward RNN to obtain contextual representation.",
  "y": "background"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_2",
  "x": "For utterance representation, we consider the advantages of the two state-of-the-art approaches to encoding contextual information for context-sensitive response generation <cite>(Xing et al., 2017</cite>; Tian et al., 2017) . We utilize a GRU model to obtain utterance representation. For inter-utterance representation, inspired by the above approaches of modeling inter-utterance representations, we proposed two attention mechanisms, namely dynamic and static attention, to weigh the importance of each utterance in a conversation and obtain the contextual representation. Figure 2 shows the framework of the proposed context-sensitive generation model. Drawing the advantages of attention mechanism on Here, u * denotes the * -th utterance in a conversation. weighing the importance of utterances for generating open-domain conversational responses<cite> (Xing et al., 2017)</cite> , we thus model the inter-utterance representation to obtain the context vector in two measures, namely static and dynamic attention, as shown in Figure 2 . We then formally describe the static and dynamic attention for decoding process. \u2022 Static Attention based Decoding As shown in Figure 2 , the static attention mechanism calculates the importance of each utterance as e i or \u03b1 i (i \u2208 {1, ..., s}). Here, h i and h s denote the representations of hidden state of the i-th and the last utterance in a conversation, respectively.",
  "y": "similarities uses"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_3",
  "x": "Here, V , W and U are also parameters that are independent to those in the static attention. T denotes the transposition operation of V . The e i,t and \u03b1 i,t are calculated in each time step t of decoding. The t-th hidden state s t in dynamic attention-based decoder can be calculated as follows: The main difference between our proposed conversational response generation model and the above two state-of-the-art models is the two attention mechanisms for obtaining the contextual representation of a conversation. Rather than use a hierarchical attention neural network<cite> (Xing et al., 2017)</cite> to obtain the contextual representation of a conversation, we propose two utterance-level attentions for weighting the importance of each utterance in the context, which is more simple in structure and has less number of parameters than the hierarchical attention approach. Meanwhile, rather than use a heuristic approach to weigh the importance of each utterance in the context (Tian et al., 2017) , in our proposed approach, the weights of utterance in the context are learned by two attention mechanisms from the data, which is more reasonable and flexible than the heuristic based approach. ---------------------------------- **EXPERIMENTAL RESULTS**",
  "y": "differences extends"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_4",
  "x": "\u2022 VHRED: The augmented HRED model, which incorporates a stochastic latent variable at utterance level for encoding and decoding, is proposed by Serban et al. (2017b) . \u2022 CVAE: The conditional variational autoencoder based approach, which is proposed by , to learn context diversity for conversational responses generation. \u2022 WSI and HRAN are proposed by Tian et al. (2017) and<cite> Xing et al. (2017)</cite> respectively. We detailed describe and compare the two models in Section 2.1 and 2.2 and their frameworks are shown in Figure 1 . ---------------------------------- **EVALUATION AND RESULTS** ---------------------------------- **AUTOMATIC EVALUATION** Until now, automatically evaluating the quality of open-domain conversational response is still an open problem.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_0",
  "x": "****CROSS-LINGUAL PARSING WITH POLYGLOT TRAINING AND MULTI-TREEBANK LEARNING: A FAROESE CASE STUDY**** **ABSTRACT** Cross-lingual dependency parsing involves transferring syntactic knowledge from one language to another. It is a crucial component for inducing dependency parsers in low-resource scenarios where no training data for a language exists. Using Faroese as the target language, we compare two approaches using annotation projection: first, projecting from multiple monolingual source models; second, projecting from a single polyglot model which is trained on the combination of all source languages. Furthermore, we reproduce <cite>multisource projection</cite> <cite>(Tyers et al., 2018)</cite> , in which dependency trees of multiple sources are combined. Finally, we apply multi-treebank modelling to the projected treebanks, in addition to or alternatively to polyglot modelling on the source side. We find that polyglot training on the source languages produces an overall trend of better results on the target language but the single best result for the target language is obtained by projecting from monolingual source parsing models and then training multi-treebank POS tagging and parsing models on the target side. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_1",
  "x": "This work focuses on dependency parsing for low-resource languages by means of annotation projection (Yarowsky et al., 2001) and synthetic treebank creation (Tiedemann and Agi\u0107, 2016) . We build on recent work by <cite>Tyers et al. (2018)</cite> who show that in the absence of annotated training data for the target language, a lexicalized treebank can be created by translating a target language corpus into a number of related source languages and parsing the translations using models trained on the source language treebanks. 1 These annotations are then projected to the target language using separate word alignments for each source language, combined into a single graph for each sentence and decoded (Sagae and Lavie, 2006) , resulting in a treebank for the target language, Faroese in the case of <cite>Tyers et al.'s</cite> and our experiments. Inspired by recent literature involving multilingual learning (Mulcaire et al., 2019; Vilares et al., 2016) , we investigate whether additional improvements can be made by: 1. using a single polyglot 2 parsing model which is trained on the combination of all source languages to create synthetic source treebanks (which are subsequently projected to the target language) 1 In this paper, source language and target language always refer to the projection, not the direction of translation. 2 We adopt the same terminology used in Mulcaire et al. (2019) , who use the term cross-lingual transfer to describe methods involving the use of one or more source languages to process a target language. They reserve the term polyglot learning for training a single model on multiple languages and where parameters are shared between languages. For the polyglot learning technique applied to multiple treebanks of a single language, we use the term multi-treebank learning. 2. training a multi-treebank model on the individually projected treebanks and the treebank produced with multi-source projections.",
  "y": "extends"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_2",
  "x": "Cross-lingual transfer methods, i. e. methods that transfer knowledge from one or more source languages to a target language, have led to substantial improvements for low-resource dependency parsing (Rosa and Mare\u010dek, 2018; Guo et al., 2015; Lynn et al., 2014; Mc-Donald et al., 2011; Hwa et al., 2005) and part-ofspeech (POS) tagging (Plank and Agi\u0107, 2018) . In low-resource scenarios, there may be not enough data for data-driven models to learn how to parse. In cases where no annotated data is available, knowledge is often transferred from annotated data in other languages and when there is only a small amount of annotated data, additional knowl-edge can be induced from external corpora such as by learning distributed word representations (Mikolov et al., 2013; Al-Rfou' et al., 2013) and more recent contextualized variants Devlin et al., 2019) . This work focuses on dependency parsing for low-resource languages by means of annotation projection (Yarowsky et al., 2001) and synthetic treebank creation (Tiedemann and Agi\u0107, 2016) . We build on recent work by <cite>Tyers et al. (2018)</cite> who show that in the absence of annotated training data for the target language, a lexicalized treebank can be created by translating a target language corpus into a number of related source languages and parsing the translations using models trained on the source language treebanks. 1 These annotations are then projected to the target language using separate word alignments for each source language, combined into a single graph for each sentence and decoded (Sagae and Lavie, 2006) , resulting in a treebank for the target language, Faroese in the case of <cite>Tyers et al.'s</cite> and our experiments. Inspired by recent literature involving multilingual learning (Mulcaire et al., 2019; Vilares et al., 2016) , we investigate whether additional improvements can be made by: 1. using a single polyglot 2 parsing model which is trained on the combination of all source languages to create synthetic source treebanks (which are subsequently projected to the target language) 1 In this paper, source language and target language always refer to the projection, not the direction of translation. 2 We adopt the same terminology used in Mulcaire et al. (2019) , who use the term cross-lingual transfer to describe methods involving the use of one or more source languages to process a target language.",
  "y": "similarities"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_3",
  "x": "1 These annotations are then projected to the target language using separate word alignments for each source language, combined into a single graph for each sentence and decoded (Sagae and Lavie, 2006) , resulting in a treebank for the target language, Faroese in the case of <cite>Tyers et al.'s</cite> and our experiments. Inspired by recent literature involving multilingual learning (Mulcaire et al., 2019; Vilares et al., 2016) , we investigate whether additional improvements can be made by: 1. using a single polyglot 2 parsing model which is trained on the combination of all source languages to create synthetic source treebanks (which are subsequently projected to the target language) 1 In this paper, source language and target language always refer to the projection, not the direction of translation. 2 We adopt the same terminology used in Mulcaire et al. (2019) , who use the term cross-lingual transfer to describe methods involving the use of one or more source languages to process a target language. They reserve the term polyglot learning for training a single model on multiple languages and where parameters are shared between languages. For the polyglot learning technique applied to multiple treebanks of a single language, we use the term multi-treebank learning. 2. training a multi-treebank model on the individually projected treebanks and the treebank produced with multi-source projections. The former differs from the approach of <cite>Tyers et al. (2018)</cite> , who use multiple discrete, monolingual models to parse the translated sentences, whereas in this work we use a single model trained on multiple source treebanks. The latter differs from training on the target treebank produced by multi-source projection in that the information of the individual projections is still available and training data is not reduced to cases where all source languages provide a projection.",
  "y": "differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_4",
  "x": "On the target side, we expect that combining different sources of information will result in a more robust target model. We evaluated our various models on the Faroese test set and experienced considerable gains for three of the four source languages (Danish, Norwegian Bokm\u00e5l and Swedish) by adopting a polyglot model. However, for Norwegian Nynorsk, a stronger monolingual model was able to outperform the polyglot approach. When we extended multi-treebank learning to the target side, we experienced additional gains for all cases. Our best result of 71.5 -an absolute improvement of 7.2 points over the result reported by <cite>Tyers et al. (2018)</cite> -was achieved with multi-treebank target learning over the monolingual projections. <cite>Tyers et al. (2018)</cite> describe a method for creating synthetic treebanks for Faroese based on previous work which uses machine translation and word alignments to transfer trees from source language(s) to the target language. Sentences from Faroese are translated into the four source languages Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l. The translated sentences are then tokenized, POS tagged and parsed using the relevant source language model trained on the source language treebank. The resulting trees are projected back to the Faroese sentences using word alignments.",
  "y": "differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_5",
  "x": "Our best result of 71.5 -an absolute improvement of 7.2 points over the result reported by <cite>Tyers et al. (2018)</cite> -was achieved with multi-treebank target learning over the monolingual projections. <cite>Tyers et al. (2018)</cite> describe a method for creating synthetic treebanks for Faroese based on previous work which uses machine translation and word alignments to transfer trees from source language(s) to the target language. Sentences from Faroese are translated into the four source languages Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l. The translated sentences are then tokenized, POS tagged and parsed using the relevant source language model trained on the source language treebank. The resulting trees are projected back to the Faroese sentences using word alignments. The four trees for each sentence are combined into a graph with edge scores one to four (the number of trees that support them), from which a single tree per sentence is produced using the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) . The resulting trees make up a synthetic treebank for Faroese which is then used to train a Faroese parsing model. The parser output is evaluated using the gold-standard Faroese test treebank developed by <cite>Tyers et al. (2018)</cite> . The approach is compared to a delexicalized baseline, which it outperforms by a large margin.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_6",
  "x": "The four trees for each sentence are combined into a graph with edge scores one to four (the number of trees that support them), from which a single tree per sentence is produced using the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) . The resulting trees make up a synthetic treebank for Faroese which is then used to train a Faroese parsing model. The parser output is evaluated using the gold-standard Faroese test treebank developed by <cite>Tyers et al. (2018)</cite> . The approach is compared to a delexicalized baseline, which it outperforms by a large margin. It is also shown that, for Faroese, a combination of the four source languages (multi-source projection) is superior to individual language projection. ---------------------------------- **BACKGROUND** The idea of annotation projection using wordalignments originates from (Yarowsky et al., 2001) who used word alignments to transfer information such as POS tags from source to target languages. This method was later used in dependency parsing by Hwa et al. (2005) , who project dependencies to a target language and use a set of heuristics to form dependency trees in the target language.",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_7",
  "x": "Various forms of projected annotation methods are compared to delexicalized baselines, and the use of machine translation instead of parallel corpora to produce synthetic treebanks in the target language is explored. In contrast to <cite>Tyers et al. (2018)</cite> , they translate a target sentence and project the source parse tree back to the target during test time instead of using this approach to obtain training data for the target language. leverage massively multilingual parallel corpora such as translations of the Bible and web-scraped data from the Watchtower Online Library website 3 for low-resource POS tagging and dependency parsing using annotation projection. They project weight matrices (as opposed to decoded dependency arcs) from multiple source languages and average the matrices weighted by word alignment confidences. They then decode the weight matrices into dependency trees on the target side, which are then used to train a parser. This approach utilizes dense information from multiple source languages, which helps reduce noise from source side predictions but to the best of our knowledge, the source-side parsing models learn information between source languages independently and the cross-lingual interaction only occurs when projecting the edge scores into multi-source weight matrices. The idea of projecting dense information in the form of a weighted graph has been further extended by Schlichtkrull and S\u00f8gaard (2017) who bypass the need to train the target parser on decoded trees and develop a parser which can be trained directly on weighted graphs. Plank and Agi\u0107 (2018) use annotation projection for POS tagging. They find that choosing high quality training instances results in superior accuracy than randomly sampling a larger training set.",
  "y": "differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_8",
  "x": "They include three experiments: first, training a monolingual model on a small number of sentences in the target language; second, training a cross-lingual model on related source languages which is then applied to the target data and lastly, training a multilingual model which includes target data as well as data from the related support languages. They found that training a monolingual model on the target data was always superior to training a cross-lingual model. Interestingly, they found that the best results were achieved by training a model on the various support languages as well as the target data, i. e. their multilingual model. While we do not combine ---------------------------------- **METHOD** We outline the process used for creating a synthetic treebank for cross-lingual dependency parsing. We use the following resources: raw Faroese sentences taken from Wikipedia, a machine translation system to translate these sentences into all source languages (Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l), a word-aligner to provide word alignments between the words in the target and source sentences, treebanks for the four source languages on which to train parsing models, POS tagging and parsing tools, and, lastly a target language test set. We use the same raw corpus, alignments and tokenized and segmented versions of the source translations 4 as <cite>Tyers et al. (2018)</cite> who release all of <cite>their data</cite>.",
  "y": "similarities uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_9",
  "x": "We outline the process used for creating a synthetic treebank for cross-lingual dependency parsing. We use the following resources: raw Faroese sentences taken from Wikipedia, a machine translation system to translate these sentences into all source languages (Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l), a word-aligner to provide word alignments between the words in the target and source sentences, treebanks for the four source languages on which to train parsing models, POS tagging and parsing tools, and, lastly a target language test set. We use the same raw corpus, alignments and tokenized and segmented versions of the source translations 4 as <cite>Tyers et al. (2018)</cite> who release all of <cite>their data</cite>. 5 In this way, the experimental pipeline is the same as <cite>theirs</cite> but we predict POS tags and dependency annotations using our own models. ---------------------------------- **TARGET LANGUAGE CORPUS** We use the target corpus built by <cite>Tyers et al. (2018)</cite> which comprises 28,862 sentences which were extracted from Faroese Wikipedia dumps 6 using the WikiExtractor script 7 and further pre-processed to remove any non-Faroese texts and other forms of unsuitable sentences. Machine Translation As noted by <cite>Tyers et al. (2018)</cite> , popular repositories for developing machine translation systems such as OPUS (Tiedemann, 2016) contain an inadequate amount of sentences to train a data-driven machine translation system for Faroese. For instance, there are fewer than 7,000 sentence pairs between Faroese and Danish, Faroese and English, Faroese and Norwegian and Faroese and Swedish.",
  "y": "similarities differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_10",
  "x": "We use the target corpus built by <cite>Tyers et al. (2018)</cite> which comprises 28,862 sentences which were extracted from Faroese Wikipedia dumps 6 using the WikiExtractor script 7 and further pre-processed to remove any non-Faroese texts and other forms of unsuitable sentences. Machine Translation As noted by <cite>Tyers et al. (2018)</cite> , popular repositories for developing machine translation systems such as OPUS (Tiedemann, 2016) contain an inadequate amount of sentences to train a data-driven machine translation system for Faroese. For instance, there are fewer than 7,000 sentence pairs between Faroese and Danish, Faroese and English, Faroese and Norwegian and Faroese and Swedish. Consequently, to create parallel source sentences, <cite>Tyers et al. (2018)</cite> use a rule-based machine translation system available in Apertium 8 to translate from Faroese to Norwegian Bokm\u00e5l. There also exists translation systems from Norwegian Bokm\u00e5l to Norwegian Nynorsk, Swedish and Danish in Apertium. As a result, the authors use pivot translation from Norwegian Bokm\u00e5l into the other source languages. The process is illustrated in Fig. 1 . For a more thorough description of the machine translation process and for resource creation in general, see the work of <cite>Tyers et al. (2018)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_11",
  "x": "We outline the process used for creating a synthetic treebank for cross-lingual dependency parsing. We use the following resources: raw Faroese sentences taken from Wikipedia, a machine translation system to translate these sentences into all source languages (Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l), a word-aligner to provide word alignments between the words in the target and source sentences, treebanks for the four source languages on which to train parsing models, POS tagging and parsing tools, and, lastly a target language test set. We use the same raw corpus, alignments and tokenized and segmented versions of the source translations 4 as <cite>Tyers et al. (2018)</cite> who release all of <cite>their data</cite>. 5 In this way, the experimental pipeline is the same as <cite>theirs</cite> but we predict POS tags and dependency annotations using our own models. ---------------------------------- **TARGET LANGUAGE CORPUS** We use the target corpus built by <cite>Tyers et al. (2018)</cite> which comprises 28,862 sentences which were extracted from Faroese Wikipedia dumps 6 using the WikiExtractor script 7 and further pre-processed to remove any non-Faroese texts and other forms of unsuitable sentences. Machine Translation As noted by <cite>Tyers et al. (2018)</cite> , popular repositories for developing machine translation systems such as OPUS (Tiedemann, 2016) contain an inadequate amount of sentences to train a data-driven machine translation system for Faroese. For instance, there are fewer than 7,000 sentence pairs between Faroese and Danish, Faroese and English, Faroese and Norwegian and Faroese and Swedish.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_12",
  "x": "Consequently, to create parallel source sentences, <cite>Tyers et al. (2018)</cite> use a rule-based machine translation system available in Apertium 8 to translate from Faroese to Norwegian Bokm\u00e5l. There also exists translation systems from Norwegian Bokm\u00e5l to Norwegian Nynorsk, Swedish and Danish in Apertium. As a result, the authors use pivot translation from Norwegian Bokm\u00e5l into the other source languages. The process is illustrated in Fig. 1 . For a more thorough description of the machine translation process and for resource creation in general, see the work of <cite>Tyers et al. (2018)</cite> . ---------------------------------- **WORD ALIGNMENTS** We use word alignments between the Faroese text and the source translations generated by <cite>Tyers et al. (2018)</cite> using fast align (Dyer et al., 2013) , a word alignment tool based on IBM Model 2. 9",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_13",
  "x": "As a result, the authors use pivot translation from Norwegian Bokm\u00e5l into the other source languages. The process is illustrated in Fig. 1 . For a more thorough description of the machine translation process and for resource creation in general, see the work of <cite>Tyers et al. (2018)</cite> . ---------------------------------- **WORD ALIGNMENTS** We use word alignments between the Faroese text and the source translations generated by <cite>Tyers et al. (2018)</cite> using fast align (Dyer et al., 2013) , a word alignment tool based on IBM Model 2. 9 Source Treebanks We use the Universal Dependencies v2.2 treebanks to train our source parsing models. This is the version used for the 2018 CoNLL shared task on Parsing Universal Dependencies .",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_14",
  "x": "For a more thorough description of the machine translation process and for resource creation in general, see the work of <cite>Tyers et al. (2018)</cite> . ---------------------------------- **WORD ALIGNMENTS** We use word alignments between the Faroese text and the source translations generated by <cite>Tyers et al. (2018)</cite> using fast align (Dyer et al., 2013) , a word alignment tool based on IBM Model 2. 9 Source Treebanks We use the Universal Dependencies v2.2 treebanks to train our source parsing models. This is the version used for the 2018 CoNLL shared task on Parsing Universal Dependencies . Source Tagging and Parsing Models In order for our parsers to work well with predicted POS tags, we follow the same steps as used in the 2018 CoNLL shared task for creating training and development treebanks with automatically predicted POS tags (henceforth referred to as silver POS). Since we are required to parse translated text which only has lexical features available, we 9 Note that previous related work report better results using IBM Model 1 with a more diverse language setup.",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_15",
  "x": "Future work should explore crosslingual word embeddings with limited amount of parallel data or use aligned contextual embeddings as in (Schuster et al., 2019) . Synthetic Source Treebanks Source translations are tokenized with UDPipe (Straka and Strakov\u00e1, 2017) by <cite>Tyers et al. (2018)</cite> . For each source language, the POS model trained on the full training data (see previous section) is used to tag the tokenized translations. Once the text is tagged, we predict dependency arcs and labels with the parsing models of the previous section, and use annotation projection (described below) to provide syntactic annotations for the target sentences. Annotation Projection Once the synthetic source treebanks are compiled, i. e. the translations are parsed, the annotations are then projected from the source translations to the target language using the word alignments and <cite>Tyers et al.'s projection tool</cite>, resulting in a Faroese treebank. In some cases, not all tokens are aligned and <cite>Tyers et al. (2018)</cite> work around this by falling back to a 1:1 mapping between the target index and the source index. There are also cases where there is a mismatch in length between the source and target sentences and some dependency structures cannot be projected to the target language. <cite>Tyers et al.'s projection setup</cite> removes unsuitable projected trees containing e. g. more than one root token, a token that is its own head or a token with a head outside the range of the sentence. Multi-source Projection For multi-source projection, the four source-language dependency trees for a Faroese sentence are projected into a single graph, scoring edges according to the number of trees that contain them (Sagae and Lavie, 2006; Nivre et al., 2007) .",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_16",
  "x": "In our experiments, we simply 10 We observe slightly lower POS tagging scores on fully annotated test sets than UDPipe, which uses gold lemmas, XPOS and morphological features to predict the UPOS label and therefore cannot be applied to the translated text without also building predictors for these features. 11 https://lindat.mff.cuni.cz/ repository/xmlui/handle/11234/1-1989 use the union of the word embeddings and average the word vector for words that occur in more than one language. Future work should explore crosslingual word embeddings with limited amount of parallel data or use aligned contextual embeddings as in (Schuster et al., 2019) . Synthetic Source Treebanks Source translations are tokenized with UDPipe (Straka and Strakov\u00e1, 2017) by <cite>Tyers et al. (2018)</cite> . For each source language, the POS model trained on the full training data (see previous section) is used to tag the tokenized translations. Once the text is tagged, we predict dependency arcs and labels with the parsing models of the previous section, and use annotation projection (described below) to provide syntactic annotations for the target sentences. Annotation Projection Once the synthetic source treebanks are compiled, i. e. the translations are parsed, the annotations are then projected from the source translations to the target language using the word alignments and <cite>Tyers et al.'s projection tool</cite>, resulting in a Faroese treebank. In some cases, not all tokens are aligned and <cite>Tyers et al. (2018)</cite> work around this by falling back to a 1:1 mapping between the target index and the source index. There are also cases where there is a mismatch in length between the source and target sentences and some dependency structures cannot be projected to the target language.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_17",
  "x": "Synthetic Source Treebanks Source translations are tokenized with UDPipe (Straka and Strakov\u00e1, 2017) by <cite>Tyers et al. (2018)</cite> . For each source language, the POS model trained on the full training data (see previous section) is used to tag the tokenized translations. Once the text is tagged, we predict dependency arcs and labels with the parsing models of the previous section, and use annotation projection (described below) to provide syntactic annotations for the target sentences. Annotation Projection Once the synthetic source treebanks are compiled, i. e. the translations are parsed, the annotations are then projected from the source translations to the target language using the word alignments and <cite>Tyers et al.'s projection tool</cite>, resulting in a Faroese treebank. In some cases, not all tokens are aligned and <cite>Tyers et al. (2018)</cite> work around this by falling back to a 1:1 mapping between the target index and the source index. There are also cases where there is a mismatch in length between the source and target sentences and some dependency structures cannot be projected to the target language. <cite>Tyers et al.'s projection setup</cite> removes unsuitable projected trees containing e. g. more than one root token, a token that is its own head or a token with a head outside the range of the sentence. Multi-source Projection For multi-source projection, the four source-language dependency trees for a Faroese sentence are projected into a single graph, scoring edges according to the number of trees that contain them (Sagae and Lavie, 2006; Nivre et al., 2007) . The dependency structure is first built by voting over the directed edges.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_18",
  "x": "Annotation Projection Once the synthetic source treebanks are compiled, i. e. the translations are parsed, the annotations are then projected from the source translations to the target language using the word alignments and <cite>Tyers et al.'s projection tool</cite>, resulting in a Faroese treebank. In some cases, not all tokens are aligned and <cite>Tyers et al. (2018)</cite> work around this by falling back to a 1:1 mapping between the target index and the source index. There are also cases where there is a mismatch in length between the source and target sentences and some dependency structures cannot be projected to the target language. <cite>Tyers et al.'s projection setup</cite> removes unsuitable projected trees containing e. g. more than one root token, a token that is its own head or a token with a head outside the range of the sentence. Multi-source Projection For multi-source projection, the four source-language dependency trees for a Faroese sentence are projected into a single graph, scoring edges according to the number of trees that contain them (Sagae and Lavie, 2006; Nivre et al., 2007) . The dependency structure is first built by voting over the directed edges. Afterward, dependency labels and POS tags are decided using the same voting procedure. The process is illustrated in Fig. 3 . Target Tagging and Parsing Models At this stage we have Faroese treebanks to train our POS tagging and parsing models.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_19",
  "x": "introduce the term \"proxy treebank\" to refer to cases where the test treebank is not in the training set and a treebank embedding from the training set must be used instead. ---------------------------------- **EXPERIMENTS** In this section, we describe our experiments, which include a replication of the main findings of <cite>Tyers et al. (2018)</cite> , using AllenNLP for POS tagging and parsing instead of UDPipe (Straka and Strakov\u00e1, 2017 Figure 3 : Multi-source projection. The source language is listed in brackets. ---------------------------------- **DETAILS** The hyper-parameters of our POS tagging and parsing models are given in Table 1 . For POS tagging, we adopt a standard architecture with a word and character-level Bi-LSTM Graves and Schmidhuber, 2005) to learn contextsensitive representations of our words.",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_20",
  "x": "The multi-source approach was not that effective in our case and some individual better sources were able to surpass this combination approach. One could argue that this may be due to the lower amount of training data when using the multisource treebank. We test this hypothesis by only including those sentences which contributed to multi-source projection in the single-source synthetic treebanks. The results are given in Table 5. Comparing the results in Tables 4 and 5, we see that LAS scores tend to be slightly lower than on the version which included all target sen-WORK RESULT Rosa and Mare\u010dek (2018) 49.4 <cite>Tyers et al. (2018)</cite> 64.4 Our implementation 68.0 of <cite>Tyers et al. (2018)</cite> Our Best Model 71.5 tences, indicating that we did lose some information by filtering out a large number of sentences. However, Norwegian Nynorsk still outperforms the multi-source model for the monolingual setting and both Norwegian models perform better than the multi-source model in the polyglot setting, suggesting that size alone does not explain the under-performance of the multi-source model. It is also worth noting that polyglot training is superior to all monolingual models which hints that for no nynorsk (the previously better performing model), the monolingual model was not able to achieve its full potential with the reduced data while the polyglot model was able to provide richer annotations. Another reason why the multi-source model does not work as well in our experiments as it does in those of <cite>Tyers et al. (2018)</cite> might be that we use pre-trained embeddings whereas <cite>Tyers et al. (2018)</cite> do not. In this way, our monolingual models are stronger and likely do not benefit as much from voting.",
  "y": "uses differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_21",
  "x": "On the source side, the monolingual Norwegian Nynorsk model also performed slightly better than the polyglot model ( guage with the highest LAS (Norwegian Bokm\u00e5l) is also the best choice for projection (in this single target model setting). The multi-source approach was not that effective in our case and some individual better sources were able to surpass this combination approach. One could argue that this may be due to the lower amount of training data when using the multisource treebank. We test this hypothesis by only including those sentences which contributed to multi-source projection in the single-source synthetic treebanks. The results are given in Table 5. Comparing the results in Tables 4 and 5, we see that LAS scores tend to be slightly lower than on the version which included all target sen-WORK RESULT Rosa and Mare\u010dek (2018) 49.4 <cite>Tyers et al. (2018)</cite> 64.4 Our implementation 68.0 of <cite>Tyers et al. (2018)</cite> Our Best Model 71.5 tences, indicating that we did lose some information by filtering out a large number of sentences. However, Norwegian Nynorsk still outperforms the multi-source model for the monolingual setting and both Norwegian models perform better than the multi-source model in the polyglot setting, suggesting that size alone does not explain the under-performance of the multi-source model. It is also worth noting that polyglot training is superior to all monolingual models which hints that for no nynorsk (the previously better performing model), the monolingual model was not able to achieve its full potential with the reduced data while the polyglot model was able to provide richer annotations. Another reason why the multi-source model does not work as well in our experiments as it does in those of <cite>Tyers et al. (2018)</cite> might be that we use pre-trained embeddings whereas <cite>Tyers et al. (2018)</cite> do not.",
  "y": "differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_22",
  "x": "The highest scoring system in the 2018 CoNLL shared task was that of Rosa and Mare\u010dek (2018) who achieved a LAS score of 49.4 on the Faroese test set. Note that they use predicted tokenization and segmentation whereas our experiments and <cite>Tyers et al.'s</cite> use gold tokenization and segmentation, which provides a small artificial boost. <cite>Tyers et al. (2018)</cite> report an LAS of 64.43 with a monolingual multi-source approach. Our implementation which uses a different parser (AllenNLP versus UDPipe) and pre-trained word embeddings achieves an LAS of 68. Our highest score of 71.51 is achieved through the combination of projecting from strong monolingual source models and then training multi-treebank POS tagging and parsing models on the outputs. ---------------------------------- **CONCLUSION** We have presented parsing results on Faroese, a low-resource language, using annotation projection from multiple monolingual sources versus a single polyglot model. We also extended the idea of multi-treebank learning to the target treebanks.",
  "y": "similarities"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_23",
  "x": "In this way, our monolingual models are stronger and likely do not benefit as much from voting. The second result column (MULTI) of Table 4 shows the effect of training a multi-treebank POS tagger and parser on the Faroese treebanks created by each of the four source languages as well as the treebank which is produced by multi-source projection. This experiment is orthogonal to the experiment using a polyglot model on the source side and so we also test a combination of polyglot source side parsing and multi-treebank target side parsing. We see improvements over the single treebank setting for all cases. 15 Table 6 places our systems in the context of previous results on the same Faroese test set. The highest scoring system in the 2018 CoNLL shared task was that of Rosa and Mare\u010dek (2018) who achieved a LAS score of 49.4 on the Faroese test set. Note that they use predicted tokenization and segmentation whereas our experiments and <cite>Tyers et al.'s</cite> use gold tokenization and segmentation, which provides a small artificial boost. <cite>Tyers et al. (2018)</cite> report an LAS of 64.43 with a monolingual multi-source approach. Our implementation which uses a different parser (AllenNLP versus UDPipe) and pre-trained word embeddings achieves an LAS of 68.",
  "y": "background"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_0",
  "x": "The agent must navigate through the environment, conditioning on the instruction as well as the visual imagery that it observes along the route, to stop at the location specified by the instruction (e.g. the mirror). Recent state-of-the-art models (Wang et al., 2018; Fried et al., 2018b;<cite> Ma et al., 2019)</cite> have demonstrated large gains in accuracy on the VLN task. However, it is unclear which modality these go past the couch \u2026 Figure 1 : We factor the grounding of language instructions into visual appearance, route structure, and object detections using a mixture-of-experts approach. substantial increases in task metrics can be attributed to, and, in particular, whether the gains in performance are due to stronger grounding into visual context or e.g. simply into the discrete, geometric structure of possible routes, such as turning left or moving forward (see Fig. 1 ---------------------------------- **, TOP VS. MIDDLE).** First, we analyze to what extent VLN models ground language into visual appearance and route structure by training versions of two state-ofthe-art models without visual features, using the benchmark Room-to-Room (R2R) dataset (Anderson et al., 2018) . We find that while grounding into route structure is useful, the models with visual features fail to learn generalizable visual grounding. Surprisingly, when trained without visual features, their performance on unseen environments is comparable or even better.",
  "y": "background"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_1",
  "x": "**RELATED WORK** Vision and Language Navigation. Vision-andLanguage Navigation (VLN) (Anderson et al., 2018; Chen et al., 2019) unites two lines of work: first, of following natural language navigational instructions in an environmental context (MacMahon et al., 2006; Vogel and Jurafsky, 2010; Tellex et al., 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015; Mei et al., 2016; Fried et al., 2018a; Misra et al., 2018) , and second, of vision-based navigation tasks (Mirowski et al., 2017; Yang et al., 2019; Mirowski et al., 2018; Cirik et al., 2018 ) that use visually-rich real-world imagery (Chang et al., 2017) . A number of methods for the VLN task have been recently proposed. Wang et al. (2018) use model-based and model-free reinforcement learning to learn an environmental model and optimize directly for navigation success. Fried et al. (2018b) use a separate instruction generation model to synthesize new instructions as data augmentation during training, and perform pragmatic inference at test time. Most recently,<cite> Ma et al. (2019)</cite> introduce a visual and textual co-attention mechanism and a route progress predictor. These approaches have significantly improved performance on the VLN task, when evaluated by metrics such as success rate. However, it is unclear where the high performance comes from.",
  "y": "background"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_2",
  "x": "**RELATED WORK** Vision and Language Navigation. Vision-andLanguage Navigation (VLN) (Anderson et al., 2018; Chen et al., 2019) unites two lines of work: first, of following natural language navigational instructions in an environmental context (MacMahon et al., 2006; Vogel and Jurafsky, 2010; Tellex et al., 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015; Mei et al., 2016; Fried et al., 2018a; Misra et al., 2018) , and second, of vision-based navigation tasks (Mirowski et al., 2017; Yang et al., 2019; Mirowski et al., 2018; Cirik et al., 2018 ) that use visually-rich real-world imagery (Chang et al., 2017) . A number of methods for the VLN task have been recently proposed. Wang et al. (2018) use model-based and model-free reinforcement learning to learn an environmental model and optimize directly for navigation success. Fried et al. (2018b) use a separate instruction generation model to synthesize new instructions as data augmentation during training, and perform pragmatic inference at test time. Most recently,<cite> Ma et al. (2019)</cite> introduce a visual and textual co-attention mechanism and a route progress predictor. These approaches have significantly improved performance on the VLN task, when evaluated by metrics such as success rate. However, it is unclear where the high performance comes from.",
  "y": "motivation"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_3",
  "x": "Recent work has also investigated singlemodality performance in vision-and-language embodiment tasks. Anand et al. (2018) find that stateof-the-art results can be achieved on the EmbodiedQA task (Das et al., 2018 ) using an agent without visual inputs. Work concurrent to ours evaluates the performance of single-modality models for several embodied tasks including VLN (Thomason et al., 2019) , finding that high performance can be achieved on the R2R dataset using a non-visual version of the baseline model (Anderson et al., 2018) . In this paper, we show that the same trends hold for two recent state-of-the-art architectures <cite>(Ma et al., 2019</cite>; Fried et al., 2018b) for the VLN task; we also analyze to what extent object-based representations and mixture-ofexperts methods can address these issues. in a connectivity graph determined by line-of-sight in the physical environment. See the top row of Fig. 1 for a top-down environment illustration. In the VLN task, a virtual agent is placed at a particular viewpoint in an environment, and is given a natural language instruction (written by a human annotator) to follow. At each timestep, the agent receives the panoramic image for the viewpoint it is currently located at, and either predicts to move to one of the adjacent connected viewpoints, or to stop. When the agent predicts the stop action, it is evaluated on whether it has correctly reached the end of the route that the human annotator was asked to describe.",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_4",
  "x": "These approaches have significantly improved performance on the VLN task, when evaluated by metrics such as success rate. However, it is unclear where the high performance comes from. In this paper, we find that agents without any visual input can achieve competitive performance, matching or even outperforming their vision-based counterparts under two state-of-theart model models (Fried et al., 2018b;<cite> Ma et al., 2019)</cite> . We also explore two approaches to make the agents better utilize their visual inputs. The role of vision in vision-and-language tasks. In several vision-and-language tasks, high performance can be achieved without effective modeling of the visual modality. Devlin et al. (2015) find that image captioning models can exploit regularity in the captions, showing that a nearestneighbor matching approach can achieve competitive performance to sophisticated language generation models. and find that neural captioning models often ground object mentions into incorrect objects due to correlations in the training data, and can hallucinate non-existing objects. Recent work has also investigated singlemodality performance in vision-and-language embodiment tasks. Anand et al. (2018) find that stateof-the-art results can be achieved on the EmbodiedQA task (Das et al., 2018 ) using an agent without visual inputs.",
  "y": "uses differences"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_5",
  "x": "In the VLN task, a virtual agent is placed at a particular viewpoint in an environment, and is given a natural language instruction (written by a human annotator) to follow. At each timestep, the agent receives the panoramic image for the viewpoint it is currently located at, and either predicts to move to one of the adjacent connected viewpoints, or to stop. When the agent predicts the stop action, it is evaluated on whether it has correctly reached the end of the route that the human annotator was asked to describe. In this work, we analyze two recent VLN models, which typify the visual grounding approaches of VLN work: the panoramic \"follower\" model from the Speaker-Follower (SF) system of Fried et al. (2018b) and the Self-Monitoring (SM) model of<cite> Ma et al. (2019)</cite> . These models obtained stateof-the-art results on the R2R dataset. Both models are based on the encoder-decoder approach (Cho et al., 2014 ) and map an instruction to a sequence of actions in context by encoding the instruction with an LSTM, and outputting actions using an LSTM decoder that conditions on the encoded instruction and visual features summarizing the agent's environmental context. Compared to the SF model, the SM model introduces an improved visual-textual co-attention mechanism and a progress monitor component. We refer to the original papers for details on the two models. To analyze the models' visual grounding ability, we focus on their core encoder-decoder components.",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_6",
  "x": "We refer to the original papers for details on the two models. To analyze the models' visual grounding ability, we focus on their core encoder-decoder components. In our experiments, we use models trained without data augmentation, and during inference predict actions with greedy search (i.e. without beam search, pragmatic, or progress monitorbased inference). For SF, we use the publicly released code. For SM, we use a reimplementation without the progress monitor, which was shown to be most important for search in inference<cite> (Ma et al., 2019)</cite> . We investigate how well these models ground instructions into visual features of the environment, by training and evaluating them without access to the visual context: setting their visual feature vectors to zeroes during training and testing. We compare performance on the validation sets of the R2R dataset: the val-seen split, consisting of the same environments as in training, and the val- Table 1 : Success rate (SR) of the vision-based full agent (\"RN\", using ResNet) and the non-visual agent (\"no vis.\", setting all visual features to zero) on the R2R dataset under different model architectures (SpeakerFollower (SF) (Fried et al., 2018b) and Self-Monitoring (SM)<cite> (Ma et al., 2019)</cite> ) and training schemes. unseen split of novel environments. Since we aim to evaluate how well the agents generalize to the unseen environments, we focus on the val-unseen split.",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_7",
  "x": "We refer to the original papers for details on the two models. To analyze the models' visual grounding ability, we focus on their core encoder-decoder components. In our experiments, we use models trained without data augmentation, and during inference predict actions with greedy search (i.e. without beam search, pragmatic, or progress monitorbased inference). For SF, we use the publicly released code. For SM, we use a reimplementation without the progress monitor, which was shown to be most important for search in inference<cite> (Ma et al., 2019)</cite> . We investigate how well these models ground instructions into visual features of the environment, by training and evaluating them without access to the visual context: setting their visual feature vectors to zeroes during training and testing. We compare performance on the validation sets of the R2R dataset: the val-seen split, consisting of the same environments as in training, and the val- Table 1 : Success rate (SR) of the vision-based full agent (\"RN\", using ResNet) and the non-visual agent (\"no vis.\", setting all visual features to zero) on the R2R dataset under different model architectures (SpeakerFollower (SF) (Fried et al., 2018b) and Self-Monitoring (SM)<cite> (Ma et al., 2019)</cite> ) and training schemes. unseen split of novel environments. Since we aim to evaluate how well the agents generalize to the unseen environments, we focus on the val-unseen split.",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_8",
  "x": "We thus propose an objectbased representation, where object detection results from a pretrained large-scale object detector are used as the environment representation. The object-based representation is intended to prevent overfitting to training scenes and to transfer to new environments better than CNN features. Both the SF and SM models represent the visual appearance at each location with a set of visual features {x img,i }, where x img,i is a vector extracted from an image patch at a particular orientation i using a CNN. Both models also use a visual attention mechanism to extract an attended visual feature x img,att from {x img,i }. For our objectbased representation, we use a Faster R-CNN (Ren et al., 2015) object detector trained on the Visual Genome dataset (Krishna et al., 2017) . We construct a set of vectors {x obj,j } representing detected objects and their attributes. Each vector x obj,j (j-th detected object in the scene) is a concatenation of summed GloVe vectors (Pennington et al., 2014) for the detected object label (e.g. door) and attribute labels (e.g. white) and a location vector from the object's bounding box coordinates. We then use the same visual attention mechanism as in Fried et al. (2018b) and<cite> Ma et al. (2019)</cite> to obtain an attended object representation x obj,att over these {x obj,j } vectors. We either substitute the ResNet CNN features x img,att (\"RN\") with our object representation x obj,att (\"Obj\"), or concatenate x img,att and x obj,att (\"RN+Obj\").",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_11",
  "x": "---------------------------------- **A DETAILS ON THE COMPARED VLN MODELS** The Speaker-Follower (SF) model (Fried et al., 2018b ) and the Self-Monitoring (SM) model<cite> (Ma et al., 2019)</cite> which we analyze both use sequenceto-sequence model (Cho et al., 2014) with attention (Bahdanau et al., 2015) as their base instruction-following agent. Both use an encoder LSTM (Hochreiter and Schmidhuber, 1997 ) to represent the instruction text, and a decoder LSTM to predict actions sequentially. At each timestep, the decoder LSTM conditions on the action previously taken, a representation of the visual context at the agent's current location, and an attended representation of the encoded instruction. While at a high level these models are similar (at least in terms of the base sequence-tosequence models -both papers additionally develop techniques to select routes from these base models during search-based inference techniques, either using a separate language generation model in SF, or a progress-monitor in SM), they differ in the mechanism by which they combine representations of the text instruction and visual input. The SM uses a co-grounded attention mechanism, where both the visual attention on image features and the textual attention on the instruction words are generated based on previous decoder LSTM hidden state h t\u22121 , and then the attended visual and textual features are used as LSTM inputs to produce h t . The SF model only uses attended visual features as LSTM inputs and then produces textual attention based on updated LSTM state h t . Also, the visual attention weights are calculated with an MLP and batch-normalization in SM, while only a linear dot-product visual attention is used in SF.",
  "y": "uses"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_0",
  "x": "**INTRODUCTION** Question generation (QG) task, which takes a context and an answer as input and generates a question that targets the given answer, have received tremendous interests in recent years from both industrial and academic communities (Zhao et al., 2018) (Zhou et al., 2017) <cite>(Du et al., 2017)</cite> . The state-of-the-art models mainly adopt neural approaches by training a neural network based on the sequence-to-sequence framework. So far, the best performing result is reported in (Zhao et al., 2018) , which advances the state-of-the-art results from 13.9 to 16.8 (BLEU 4) . The existing QG models mainly rely on recurrent neural networks (RNN) augmented by attention mechanisms. However, the inherent sequential nature of the RNN models suffers from the problem of handling long sequences. As a result, the existing QG models <cite>(Du et al., 2017)</cite> (Zhou et al., 2017 ) mainly use only sentence-level information as context. When applied to a paragraphlevel context, the existing models show significant performance degradation. However, as indicated by <cite>(Du et al., 2017)</cite> , providing paragraph-level information can improve QG performance.",
  "y": "motivation"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_1",
  "x": "---------------------------------- **INTRODUCTION** Question generation (QG) task, which takes a context and an answer as input and generates a question that targets the given answer, have received tremendous interests in recent years from both industrial and academic communities (Zhao et al., 2018) (Zhou et al., 2017) <cite>(Du et al., 2017)</cite> . The state-of-the-art models mainly adopt neural approaches by training a neural network based on the sequence-to-sequence framework. So far, the best performing result is reported in (Zhao et al., 2018) , which advances the state-of-the-art results from 13.9 to 16.8 (BLEU 4) . The existing QG models mainly rely on recurrent neural networks (RNN) augmented by attention mechanisms. However, the inherent sequential nature of the RNN models suffers from the problem of handling long sequences. As a result, the existing QG models <cite>(Du et al., 2017)</cite> (Zhou et al., 2017 ) mainly use only sentence-level information as context. When applied to a paragraphlevel context, the existing models show significant performance degradation.",
  "y": "background"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_2",
  "x": "The existing QG models mainly rely on recurrent neural networks (RNN) augmented by attention mechanisms. However, the inherent sequential nature of the RNN models suffers from the problem of handling long sequences. As a result, the existing QG models <cite>(Du et al., 2017)</cite> (Zhou et al., 2017 ) mainly use only sentence-level information as context. When applied to a paragraphlevel context, the existing models show significant performance degradation. However, as indicated by <cite>(Du et al., 2017)</cite> , providing paragraph-level information can improve QG performance. For handling long context, the work (Zhao et al., 2018) introduces a maxout pointer mechanism with gated self-attention encoder for processing paragraphlevel input. The work reports state-of-the-art performance for QG tasks. Recently, the NLP community has seen excitement around neural learning models that make use of pre-trained language models (Devlin et al., 2018) (Radford et al., 2018) . The latest development is BERT, which has shown significant performance improvement over various natural language understanding tasks, such as document summarization, document classification, etc.",
  "y": "motivation"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_3",
  "x": "Subsequently, the newly generated tokenq i is appended into X and the question generation process is repeated (as illustrated in Figure 2 ) with the new X until [SEP] is predicted. We report the generated tokens as the predicted question. ---------------------------------- **PERFORMANCE EVALUATION** ---------------------------------- **DATASETS** The SQuAD dataset contains 536 Wikipedia articles and around 100K reading comprehension questions (and the corresponding answers) posed about the articles. Answers of the questions are text spans in the articles. We follow the same data split settings as previous work on the QG tasks <cite>(Du et al., 2017)</cite> (Zhao et al., 2018) to directly compare the state-of-theart results on QG tasks.",
  "y": "uses similarities"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_4",
  "x": "**DATASETS** The SQuAD dataset contains 536 Wikipedia articles and around 100K reading comprehension questions (and the corresponding answers) posed about the articles. Answers of the questions are text spans in the articles. We follow the same data split settings as previous work on the QG tasks <cite>(Du et al., 2017)</cite> (Zhao et al., 2018) to directly compare the state-of-theart results on QG tasks. Table 1 summarizes some statistics for the compared datasets. \u2022 SQuAD 73K In this set, we follow the same setting as <cite>(Du et al., 2017)</cite> ; the accessible parts of the SQuAD training data are randomly divided into a training set (80%), a development set (10%), and a test set (10%). We report results on the 10% test set. \u2022 SQuAD 81K In this set, we follow the same setting as (Zhao et al., 2018) ; the accessible SQuAD development data set is divided into a development set (50%), and a test set (50%). ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_5",
  "x": "Table 1 summarizes some statistics for the compared datasets. \u2022 SQuAD 73K In this set, we follow the same setting as <cite>(Du et al., 2017)</cite> ; the accessible parts of the SQuAD training data are randomly divided into a training set (80%), a development set (10%), and a test set (10%). We report results on the 10% test set. \u2022 SQuAD 81K In this set, we follow the same setting as (Zhao et al., 2018) ; the accessible SQuAD development data set is divided into a development set (50%), and a test set (50%). ---------------------------------- **IMPLEMENTATION DETAILS** We use the PyTorch version of BERT 1 to train our BERT-QG and BERT-SQG models. The <cite>(Du et al., 2017)</cite> , and SQuAD 81K is the setting of (Zhao et al., 2018) . SQuAD 73K 73240 11877 10570  SQuAD 81K 81577 8964  8964 pre-trained model uses the officially provided BERT base model (12 layers, 768 hidden dimensions, and 12 attention heads.) with a vocab of 30522 words.",
  "y": "background"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_6",
  "x": "data for epoch model to make predictions and select the highest accuracy rate as our score evaluation model. Also, in our BERT-SQG model, we use the Beam Search strategy for sequence decoding. The beam size is set to 3. ---------------------------------- **TRAIN TEST DEV** ---------------------------------- **MODEL COMPARISON** In this paper, we compare our models with the best performing models <cite>(Du et al., 2017)</cite> (Zhao et al., 2018) in the literature. The compared models in the experiment are:",
  "y": "similarities"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_7",
  "x": "Also, in our BERT-SQG model, we use the Beam Search strategy for sequence decoding. The beam size is set to 3. ---------------------------------- **TRAIN TEST DEV** ---------------------------------- **MODEL COMPARISON** In this paper, we compare our models with the best performing models <cite>(Du et al., 2017)</cite> (Zhao et al., 2018) in the literature. The compared models in the experiment are: \u2022 NQG-RC <cite>(Du et al., 2017)</cite> : A seq2seq question generation model based on bidirectional LSTM.",
  "y": "uses similarities"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_0",
  "x": "Goodfellow et al. (2014) interprets by generating adversarial examples. However, Baehrens et al. (2010) and Bach et al. (2015) ; Montavon et al. (2017) explain neural network predictions by sensitivity analysis to different input features and decomposition of decision functions, respectively. Recurrent neural networks (RNNs) (Elman, 1990) are temporal networks and cumulative in nature to effectively model sequential data such as text or speech. RNNs and their variants such as LSTM (Hochreiter and Schmidhuber, 1997) have shown success in several natural language processing (NLP) tasks, such as entity extraction (Lample et al., 2016; Ma and Hovy, 2016) , relation extraction <cite>(Vu et al., 2016a</cite>; Miwa and Bansal, 2016; Gupta et al., 2016 Gupta et al., , 2018c , language modeling (Mikolov et al., 2010; Peters et al., 2018) , slot filling (Mesnil et al., 2015; Vu et al., 2016b) , machine translation (Bahdanau et al., 2014) , sentiment analysis (Wang et al., 2016; Tang et al., 2015) , semantic textual similarity (Mueller and Thyagarajan, 2016; Gupta et al., 2018a) and dynamic topic modeling (Gupta et al., 2018d) . Past works (Zeiler and Fergus, 2014; have mostly analyzed deep neural network, especially CNN in the field of computer vision to study and visualize the features learned by neurons. Recent studies have investigated visualization of RNN and its variants. Tang et al. (2017) visualized the memory vectors to understand the behavior of LSTM and gated recurrent unit (GRU) in speech recognition task. For given words in a sentence, employed heat maps to study sensitivity and meaning composition in recurrent networks. Ming et al. (2017)",
  "y": "background"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_2",
  "x": "---------------------------------- **CONNECTIONIST BI-DIRECTIONAL RNN** We adopt the bi-directional recurrent neural network architecture with ranking loss, proposed by<cite> Vu et al. (2016a)</cite> . The network consists of three parts: a forward pass which processes the original sentence word by word (Equation 1); a backward pass which processes the reversed sentence word by word (Equation 2); and a combination of both (Equation 3). The forward and backward passes are combined by adding their hidden layers. There is also a connection to the previous combined hidden layer with weight W bi with a motivation to include all intermediate hidden layers into the final decision of the network (see Equation 3). They named the neural architecture as 'Connectionist Bi-directional RNN' (C-BRNN). Figure 1 shows the C-BRNN architecture, where all the three parts are trained jointly. where w t is the word vector of dimension d for a word at time step t in a sentence of length n.",
  "y": "uses similarities"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_3",
  "x": "<e1> demolition </e1> <e1> demolition </e1> was <e1> demolition </e1> was the <e1> demolition </e1> was the cause <e1> demolition </e1> was the cause of C-BRNN C-BRNN <e1> demolition </e1> was the cause of <e2> <e1> demolition </e1> was the cause of <e2> terror D is the hidden unit dimension. U f \u2208 R d\u00d7D and U b \u2208 R d\u00d7D are the weight matrices between hidden units and input w t in forward and backward networks, respectively; W f \u2208 R D\u00d7D and W b \u2208 R D\u00d7D are the weights matrices connecting hidden units in forward and backward networks, respectively. W bi \u2208 R D\u00d7D is the weight matrix connecting the hidden vectors of the combined forward and backward network. Following Gupta et al. (2015) during model training, we use 3-gram and 5-gram representation of each word w t at timestep t in the word sequence, where a 3-gram for w t is obtained by concatenating the corresponding word embeddings, i.e., w t\u22121 w t w t+1 . Ranking Objective: Similar to Santos et al. (2015) and<cite> Vu et al. (2016a)</cite> , we applied the ranking loss function to train C-BRNN. The ranking scheme offers to maximize the distance between the true label y + and the best competitive label c \u2212 given a data point x. It is defined as- where s \u03b8 (x) y + and s \u03b8 (x) c \u2212 being the scores for the classes y + and c \u2212 , respectively. The parameter \u03b3 controls the penalization of the prediction errors and m + and m are margins for the correct and incorrect classes. Following<cite> Vu et al. (2016a)</cite> , we set \u03b3 = 2, m + = 2.5 and m \u2212 = 0.5.",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_4",
  "x": "Following Gupta et al. (2015) during model training, we use 3-gram and 5-gram representation of each word w t at timestep t in the word sequence, where a 3-gram for w t is obtained by concatenating the corresponding word embeddings, i.e., w t\u22121 w t w t+1 . Ranking Objective: Similar to Santos et al. (2015) and<cite> Vu et al. (2016a)</cite> , we applied the ranking loss function to train C-BRNN. The ranking scheme offers to maximize the distance between the true label y + and the best competitive label c \u2212 given a data point x. It is defined as- where s \u03b8 (x) y + and s \u03b8 (x) c \u2212 being the scores for the classes y + and c \u2212 , respectively. The parameter \u03b3 controls the penalization of the prediction errors and m + and m are margins for the correct and incorrect classes. Following<cite> Vu et al. (2016a)</cite> , we set \u03b3 = 2, m + = 2.5 and m \u2212 = 0.5. ---------------------------------- **MODEL TRAINING AND FEATURES:** We represent each word by the concatenation of its word embedding and position feature vectors.",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_5",
  "x": "As position features in relation classification experiments, we use position indicators (PI) (Zhang and Wang, 2015) in C-BRNN to annotate target entity/nominals in the word sequence, without necessity to change the input vectors, while it increases the length of the input word sequences, as four independent words, as position indicators (<e1>, </ e1>, <e2>, </e2>) around the relation arguments are introduced. In our analysis and interpretation of recurrent neural networks, we use the trained C-BRNN ( Figure 1 )<cite> (Vu et al., 2016a)</cite> model. ---------------------------------- **LISA AND EXAMPLE2PATTERN IN RNN** There are several aspects in interpreting the neural network, for instance via (1) Data: \"Which dimensions of the data are the most relevant for the task\" (2) Prediction or Decision: \"Explain why a certain pattern\" is classified in a certain way (3) Model: \"How patterns belonging to each category in the data look like according to the network\". In this work, we focus to explain RNN via decision and model aspects by finding the patterns that explains \"why\" a model arrives at a particular decision for each category in the data and verifies that model behaves as expected. To do so, we propose a technique named as LISA that interprets RNN about \"how it accumulates and builds meaningful semantics of a sentence word by word\" and \"how the saliency patterns look like according to the network\" for each category in the data while decision making. We extract the saliency patterns via example2pattern transformation. LISA Formulation: To explain the cumulative nature of recurrent neural networks, we show how does it build semantic meaning of a sentence word by word belonging to a particular category in the data and compute prediction scores for the expected category on different inputs, as shown in Figure 2 .",
  "y": "uses similarities"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_6",
  "x": "Therefore, each example input is transformed into a saliency pattern that informs us about the network learning. To do so, we first compute N-gram for each word w t in the sentence S. For instance, a 3-gram representation of w t is given by w t\u22121 , w t , w t+1 . Therefore, an N-gram (for N=3) sequence S of words is represented as [[w t\u22121 , w t , w t+1 ] n t=1 ], where w 0 and w n+1 are PADDING (zero) vectors of embedding dimension. Following<cite> Vu et al. (2016a)</cite> , we use N-grams (e.g., tri-grams) representation for each word in each subsequence S \u2264k that is input to C-BRNN to compute P (R|S \u2264k ), where the N-gram (N=3) subsequence S \u2264k is given by, for k \u2208 [1, n]. Observe that the 3-gram tri k con- (d) LISA for S4 < e 1 > c a r < / e 1 > l e f t t h e < e 2 > p l a n t < / e 2 > ---------------------------------- **ID** Relation/Slot Types Example Sentences Example2Pattern S1 cause-effect(e1, e2) <e1> demolition </e1> was the cause of <e2> terror </e2> cause of <e2> S2 cause-effect(e2, e1) <e1> damage </e1> caused by the <e2> bombing </e2> damage </e1> caused S3 component-whole(e1, e2) <e1> countyard </e1> of the <e2> castle </e2> </e1> of the S4 entity-destination(e1,e2) <e1> marble </e1> was dropped into the <e2> bowl </e2> dropped into the S5 entity-origin(e1, e2) <e1> car </e1> left the <e2> plant </e2> left the <e2> S6 product-produce(e1, e2) <e1> cigarettes </e1> by the major <e2> producer </e2> </e1> by the S7 instrument-agency(e1, e2) <e1> cigarettes </e1> are used by <e2> women </e2> </e1> are used S8 per:loc of birth(e1, e2) <e1> person </e1> was born in <e2> location </e2> born in <e2> S9 per:spouse(e1, e2) <e1> person </e1> married <e2> spouse </e2> </e1> married <e2> Table 1 : Example Sentences for LISA and example2pattern illustrations.",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_7",
  "x": "For a given example sentence S with its length n and category R, we extract the most salient N-gram (N=3, 5 or 7) pattern patt (the last N-gram in the N-gram subsequence S \u2264k ) that contributes the most in detecting the relation type R. The threshold parameter \u03c4 signifies the probability of prediction for the category R by the model M. For an input N-gram sequence S \u2264k of sentence S, we extract the last N-gram, e.g., tri k that detects the relation R with prediction score above \u03c4 . By manual inspection of patterns extracted at different values (0.4, 0.5, 0.6, 0.7) of \u03c4 , we found that \u03c4 = 0.5 generates the most salient and interpretable patterns. The saliency pattern detection follows LISA as demonstrated in Figure 2 , except that we use N-gram (N =3, 5 or 7) input to detect and extract the key relationship patterns. ---------------------------------- **ANALYSIS: RELATION CLASSIFICATION** Given a sentence and two annotated nominals, the task of binary relation classification is to predict the semantic relations between the pairs of nominals. In most cases, the context in between the two nominals define the relationship. However,<cite> Vu et al. (2016a)</cite> has shown that the extended context helps. In this work, we focus on the building semantics for a given sentence using relationship contexts between the two nominals.",
  "y": "background"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_8",
  "x": "**SEMEVAL10 SHARED TASK 8 DATASET** The relation classification dataset of the Semantic Evaluation 2010 (SemEval10) shared task 8 (Hendrickx et al., 2009) consists of 19 relations (9 directed relations and one artificial class Other), 8,000 training and 2,717 testing sentences. We split the training data into train (6.5k) and development (1.5k) sentences to optimize the C-BRNN Relation 3-gram Patterns 5-gram Patterns 7-gram Patterns </e1> cause <e2> the leading causes of <e2> is one of the leading causes of cause-</e1> caused a the main causes of <e2> is one of the main causes of effect(e1,e2) that cause respiratory </e1> leads to <e2> inspiration </e1> that results in <e2> hardening </e2> which cause acne </e1> that results in <e2> </e1> resulted in the <e2> loss </e2> leading causes of </e1> resulted in the <e2> <e1> sadness </e1> leads to <e2> inspiration caused due to </e1> has been caused by </e1> is caused by a <e2> comet comes from the </e1> are caused by the </e1> however has been caused by the causearose from an </e1> arose from an <e2> </e1> that has been caused by the effect (e2,e1) caused by the </e1> caused due to <e2> that has been caused by the <e2> radiated from a infection </e2> results in an <e1> product </e1> arose from an <e2> in a <e2> </e1> was contained in a </e1> was contained in a <e2> box was inside a </e1> was discovered inside a </e1> was in a <e2> suitcase </e2> contentcontained in a </e1> were in a <e2> </e1> were in a <e2> box </e2> container(e1,e2) hidden in a is hidden in a <e2> </e1> was inside a <e2> box </e2> stored in a </e1> was contained in a </e1> was hidden in an <e2> envelope </e1> released by </e1> issued by the <e2> <e1> products </e1> created by an <e2> product-</e1> issued by </e1> was prepared by <e2> </e1> by an <e2> artist </e2> who produce(e1,e2) </e1> created by was written by a <e2> </e1> written by most of the <e2> by the <e2> </e1> built by the <e2> temple </e1> has been built by <e2> of the <e1> </e1> are made by <e2> </e1> were founded by the <e2> potter </e1> of the </e1> of the <e2> device the <e1> timer </e1> of the <e2> whole(e1, e2) of the <e2> </e1> was a part of </e1> was a part of the romulan componentpart of the </e1> is part of the </e1> was the best part of the </e1> of <e2> is a basic element of </e1> is a basic element of the </e1> on a </e1> is part of a are core components of the <e2> solutions put into a have been moving into the </e1> have been moving back into <e2> released into the was dropped into the <e2> </e1> have been moving into the <e2> entity-</e1> into the </e1> moved into the <e2> </e1> have been dropped into the <e2> destination(e1,e2) moved into the were released into the <e2> </e1> have been released back into the added to the </e1> have been exported to power </e1> is exported to the <e2> </e1> are used </e1> assists the <e2> eye cigarettes </e1> are used by <e2> women used by <e2> </e1> are used by <e2> <e1> telescope </e1> assists the <e2> eye instrument-</e1> is used </e1> were used by some <e1> practices </e1> for <e2> engineers </e2> agency(e1,e2) set by the </e1> with which the <e2> the best <e1> tools </e1> for <e2> </e1> set by readily associated with the <e2> <e1> wire </e1> with which the <e2> The <e1> demolition </e1> was the cause of <e2> terror </e2> and communal divide is just a way of not letting truth prevail. \u2192 cause-effect(e1,e2) The terms demolition and terror are the relation arguments or nominals, where the phrase was the cause of is the relationship context between the two arguments. Table 1 shows the examples sentences (shortened to argument1+relationship context+argument2) drawn from the development and test sets that we employed to analyse the C-BRNN for semantic accumulation in our experiments. We use the similar experimental setup as<cite> Vu et al. (2016a)</cite> . LISA Analysis: As discussed in Section 3, we interpret C-BRNN by explaining its predictions via the semantic accumulation over the subsequences S \u2264k (Figure 2) for each sentence S. We select the example sentences S1-S7 (Table 1) for which the network predicts the correct relation type with high scores. For an example sentence S1, Table 2 illustrates how different subsequences are input to C-BRNN in order to compute prediction scores pp in the softmax layer for the relation cause-effect(e1, e2).",
  "y": "similarities uses"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_0",
  "x": "Beyond words, several works focused on the representation of sentences [11] , documents [9] , and also knowledge bases (KBs) [3, 18] . Within the latter work focusing on KBs, the goal is to exploit concepts and their relationships to obtain a latent representation of the KB. While some work focused on the representation of relations on the basis of triplets belonging to the KB [3] , other work proposed to enhance the distributed representation of words for representing their underlying concepts by taking into consideration the structure of the KB graph (e.g., concepts in the same category or their relationships with other concepts) <cite>[6,</cite> 18, 19] . A first work<cite> [6]</cite> proposes a \"retrofitting\" technique consisting in a leveraging of lexicon-derived relational information, namely adjacent words of concepts, to refine their associated word embeddings. The underlying intuition is that adjacent concepts in the KB should have similar embeddings while maintaining most of the semantic information in their prelearned distributed word representations. For each word, the retrofitting approach learns its new representation by minimizing both (1) its distance with the representation of all connected words in the semantic graph and (2) its distance with the pre-learned word embedding, namely its initial distributed representation. In contrast to<cite> [6]</cite> , other work [18, 19] proposes an endto-end oriented approach that rather adjusts the objective function of the neural language model. For instance, Xu et al. [18] propose the RC-NET model that leverages the relational and categorical knowledge to learn a higher quality word embeddings. This model extends the objective function of the skip-gram model [10] with two regularization functions based on relational and categorical knowledge from the external resource, respectively.",
  "y": "background"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_1",
  "x": "The potential of semantic representations of words learned through a neural approach has been introduced in [10, 15] , opening several perspectives in natural language processing and IR tasks. Beyond words, several works focused on the representation of sentences [11] , documents [9] , and also knowledge bases (KBs) [3, 18] . Within the latter work focusing on KBs, the goal is to exploit concepts and their relationships to obtain a latent representation of the KB. While some work focused on the representation of relations on the basis of triplets belonging to the KB [3] , other work proposed to enhance the distributed representation of words for representing their underlying concepts by taking into consideration the structure of the KB graph (e.g., concepts in the same category or their relationships with other concepts) <cite>[6,</cite> 18, 19] . A first work<cite> [6]</cite> proposes a \"retrofitting\" technique consisting in a leveraging of lexicon-derived relational information, namely adjacent words of concepts, to refine their associated word embeddings. The underlying intuition is that adjacent concepts in the KB should have similar embeddings while maintaining most of the semantic information in their prelearned distributed word representations. For each word, the retrofitting approach learns its new representation by minimizing both (1) its distance with the representation of all connected words in the semantic graph and (2) its distance with the pre-learned word embedding, namely its initial distributed representation. In contrast to<cite> [6]</cite> , other work [18, 19] proposes an endto-end oriented approach that rather adjusts the objective function of the neural language model. For instance, Xu et al. [18] propose the RC-NET model that leverages the relational and categorical knowledge to learn a higher quality word embeddings.",
  "y": "background"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_2",
  "x": "A first work<cite> [6]</cite> proposes a \"retrofitting\" technique consisting in a leveraging of lexicon-derived relational information, namely adjacent words of concepts, to refine their associated word embeddings. The underlying intuition is that adjacent concepts in the KB should have similar embeddings while maintaining most of the semantic information in their prelearned distributed word representations. For each word, the retrofitting approach learns its new representation by minimizing both (1) its distance with the representation of all connected words in the semantic graph and (2) its distance with the pre-learned word embedding, namely its initial distributed representation. In contrast to<cite> [6]</cite> , other work [18, 19] proposes an endto-end oriented approach that rather adjusts the objective function of the neural language model. For instance, Xu et al. [18] propose the RC-NET model that leverages the relational and categorical knowledge to learn a higher quality word embeddings. This model extends the objective function of the skip-gram model [10] with two regularization functions based on relational and categorical knowledge from the external resource, respectively. While the relationalbased regularization function characterizes the word relationships which are interpreted as translations in latent semantic space of word embeddings, the categorical-based one aims at minimizing the weighted distance between words with same attributes. With experiments on text mining and NLP tasks, the authors have reported that combining these two regularization functions allows to significantly improve the quality of word representations. In the same mind, Yu et al. [19] propose a relation constrained model (RCM) that extends the CBOW model [10] with a function based on prior relational knowledge issued from an external resource.",
  "y": "background"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_3",
  "x": "The reported literature review clearly highlights the potential of neural networks in one hand and the benefit of KBs, in the other hand, for ad-hoc search tasks. We believe that the integration of an external resource within a document-query neural matching process would allow benefiting from the symbolic semantics surrounding concepts and their relationships. Accordingly, such approach would impact the representation learning that could be performed at different levels. As illustrated in Figure 2 , we suggest using a deep neural approach to achieve two levels of representations: 1) an enhanced knowledge-based representation of the document and the query and 2) a distinct representation of the document and the query surrounding by a third KB-based representation aiming at improving the semantic closeness of document and query representations. While in the first approach, a KB is used as a mean of document and query representation enhancement, the KB is exploited in the latter approach as a mean for document-query translation. ---------------------------------- **LEVERAGING ENHANCED REPRESENTATIONS OF TEXT USING KB FOR IR** The first approach that we suggest for integrating KB within a deep neural network focuses on an enhanced representation of documents and queries as illustrated in Figure 2a . While a naive approach would be to exploit the concept embeddings learned from the KB distributed representation <cite>[6,</cite> 18] as input of the deep neural network, we believe that a hybrid representation of the distributional semantic (namely, word embeddings) and the symbolic semantics (namely, concept embeddings taking into account the graph structure) would allow enhancing the document-query matching.",
  "y": "differences"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_4",
  "x": "On the other hand, the semantic layer could be built by the representation of concepts (and their relationships) extracted from the plain text through a concept embedding<cite> [6]</cite> or a richer embedding representation of a KB sub-graph, as suggested in [2] . The latter presents the advantage to model the compositionality of concepts within the document. Similarly to previous approaches [8, 16, 17] , the enhanced representations of both document and query would be transformed into low-dimensional semantic feature vectors used within a similarity function. ---------------------------------- **USING KB TRANSLATION MODEL FOR IR** While the first model exploits knowledge bases to enhance the representation of a document-query pair and their similarity score, an alternative approach consists in a ranking model based on the translation role of the knowledge resource. As illustrated in Figure 2b , this second approach aims to take external knowledge resources as a third component of the deep neural network architecture. Intuitively, this third branch could be considered as a pivotal component bridging the semantic gap between the document and the query vocabulary. Indeed, the knowledge resource is here seen as a mediate component that helps to translate the deep representation of the query towards the deep representation of the document with respect to the ad-hoc IR task.",
  "y": "background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_0",
  "x": "VQA is a computer vision task: a system is given an arbitrary textbased question about an image, and then tasked to output the text-based answer of the given question about the image. Currently, most of the state-of-the-art VQA models (<cite>Antol et al. 2015</cite>; Chen et al. 2016; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) only focus on how to improve accuracy. However, accuracy is not the only metric to score a given VQA model. Robustness is also a crucial property. For the image part, there is already a rapidly growing research on evaluating the robustness of deep learning models (Fawzi, Moosavi Dezfooli, and Frossard 2017; Carlini and Wagner 2017; Xu, Caramanis, and Mannor 2009) . However, for the question part, we couldn't find any acceptable method to measure the robustness of VQA algorithms after extensive literature review. To Copyright c 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Regarding the input question of Module 2, it is the direct concatenation of a given main question with 3 basic questions of the main question.",
  "y": "motivation"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_1",
  "x": "In the VQABQ model, there are two modules: the basic question generation module (Module 1) and the visual question answering module (Module 2). We take the query question, called the main question (MQ), encoded by Skip-Thought Vectors , as the input of Module 1. In Module 1, we encode all of the questions, also by Skip-Thought Vectors, from the training and validation sets of <cite>VQA</cite> (<cite>Antol et al. 2015</cite>) dataset as a 4800 by 186027 dimension basic question (BQ) matrix, and then solve the LASSO optimization problem (Huang, Alfadly, and Ghanem 2017) , with MQ, to find the top 3 similar BQ of MQ. These BQ are the output of Module 1. Moreover, we take the direct concatenation of MQ and BQ and the given image as the input of Module 2, the general VQA module, and then it will output the answer prediction of MQ. We claim that the BQ of given MQ can be considered as the small noise of MQ and it will affect the accuracy of VQA model. Then, we verify the above claim by the detailed experiments and use the VQABQ method to analyze the robustness of 6 available pretrained stateof-the-art VQA models, provided by papers' authors, (<cite>Antol et al. 2015</cite>; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) . Note that the available pretrained VQA models can be categorized into two main categories, attention-based and non-attention-based VQA models. According to the results of our experiments, we discover that attention-based models not only have the higher accuracy but also the better robustness compared with nonattention-based models.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_2",
  "x": "In Module 1, we encode all of the questions, also by Skip-Thought Vectors, from the training and validation sets of <cite>VQA</cite> (<cite>Antol et al. 2015</cite>) dataset as a 4800 by 186027 dimension basic question (BQ) matrix, and then solve the LASSO optimization problem (Huang, Alfadly, and Ghanem 2017) , with MQ, to find the top 3 similar BQ of MQ. These BQ are the output of Module 1. Moreover, we take the direct concatenation of MQ and BQ and the given image as the input of Module 2, the general VQA module, and then it will output the answer prediction of MQ. We claim that the BQ of given MQ can be considered as the small noise of MQ and it will affect the accuracy of VQA model. Then, we verify the above claim by the detailed experiments and use the VQABQ method to analyze the robustness of 6 available pretrained stateof-the-art VQA models, provided by papers' authors, (<cite>Antol et al. 2015</cite>; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) . Note that the available pretrained VQA models can be categorized into two main categories, attention-based and non-attention-based VQA models. According to the results of our experiments, we discover that attention-based models not only have the higher accuracy but also the better robustness compared with nonattention-based models. In this work, our main contributions are summarized below: \u2022 We propose a new method to generate the basic questions of the given main question and utilize these basic questions to analyze the robustness of 6 available pretrained state-of-the-art VQA models.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_3",
  "x": "**RELATED WORK** Recently, there are many papers (<cite>Antol et al. 2015</cite>; Shih, Singh, and Hoiem 2016; Chen et al. 2016; Kafle and Kanan 2016; Ma, Lu, and Li 2016; Ren, Kiros, and Zemel 2015; Zhu et al. 2016; Wu et al. 2016; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) have proposed methods to solve the challenging VQA task. Our VQABQ method involves in different areas in Machine Learning, Natural Language Processing (NLP) and Computer Vision. The following, we discuss recent works related to our approach. Sequence Modeling by Recurrent Neural Networks. Recurrent Neural Networks (RNN) can handle the sequences of flexible length. Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber 1997) is a particular variant of RNN and in natural language tasks, such as machine translation (Sutskever, Vinyals, and Le 2014; , LSTM is a successful application. In (Ren, Kiros, and Zemel 2015) , they exploit RNN and Convolutional Neural Network (CNN) to build a question generation algorithm, but the generated question sometimes has invalid grammar. The input in (Malinowski, Rohrbach, and Fritz 2015; is the concatenation of each word embedding with the same feature vector of the image.",
  "y": "background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_4",
  "x": "In Section 3, we mainly discuss how to encode questions and generate BQ and how do we exploit BQ to do robustness analysis on the 6 available state-of-the-art VQA models. The overall architecture of our VQABQ model can be referred to Figure 1 . The model has two main parts, Module 1 and Module 2. Regarding Module 1, it will take the encoded MQ as the input and uses the encoded BQ matrix to output the BQ of query question. Then, Module 2 is a VQA model we want to analyze, and it will take the concatenation of the output of Module 1 and MQ, and the given image as input and then outputs the final answer of MQ. The detailed architecture of Module 1 also can be referred to Figure 1 . ---------------------------------- **QUESTION DATA PREPROCESSING** We take the most popular <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to develop our BQD.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_5",
  "x": "Regarding Module 1, it will take the encoded MQ as the input and uses the encoded BQ matrix to output the BQ of query question. Then, Module 2 is a VQA model we want to analyze, and it will take the concatenation of the output of Module 1 and MQ, and the given image as input and then outputs the final answer of MQ. The detailed architecture of Module 1 also can be referred to Figure 1 . ---------------------------------- **QUESTION DATA PREPROCESSING** We take the most popular <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to develop our BQD. At the beginning, we take all of the training and validation questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our basic question candidates. Then, we take all of the testing questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our main question candidates. That is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite> ).",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_6",
  "x": "We take the most popular <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to develop our BQD. At the beginning, we take all of the training and validation questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our basic question candidates. Then, we take all of the testing questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our main question candidates. That is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite> ). Because we model the basic question generation problem by LASSO, we remove the repeated basic question candidate and the basic question candidate exactly the same as any main question candidate. The above step guarantees that our LASSO can work well. ---------------------------------- **QUESTION ENCODING** There are many popular text encoders, such as Word2Vec (Mikolov et al. 2013) , GloVe (Pennington, Socher, and Manning 2014) and Skip-Thoughts .",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_7",
  "x": "Then, we take all of the testing questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our main question candidates. That is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite> ). Because we model the basic question generation problem by LASSO, we remove the repeated basic question candidate and the basic question candidate exactly the same as any main question candidate. The above step guarantees that our LASSO can work well. ---------------------------------- **QUESTION ENCODING** There are many popular text encoders, such as Word2Vec (Mikolov et al. 2013) , GloVe (Pennington, Socher, and Manning 2014) and Skip-Thoughts . In these encoders, Skip-Thoughts not only can focus on the word-toword meaning but also the whole sentence semantic meaning. So, we choose Skip-Thoughts to be our question encoding method.",
  "y": "similarities uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_8",
  "x": "Our idea is the BQ generation for MQ and, at the same time, we only want the minimum number of BQ to represent the MQ, so modeling our problem as LASSO optimization problem is an appropriate way: (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". , where A is the matrix of encoded BQ, b is the encode MQ and \u03bb is a parameter of the regularization term. ---------------------------------- **BASIC QUESTION GENERATION** We now describe how to generate the BQ of a query question, illustrated in Figure 1 . According to the above subsections, Question Encoding and Problem Formulation, we can encode all basic question candidates from the training and validation question sets of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) by Skip-Thought Vectors, and then we have a matrix of basic question candidates. Each column of the matrix is a vector representation, 4800 by 1 dimensions, of a specific basic question candidate and we have 186027 columns. That is, the dimension of BQ matrix, called A, is 4800 by 186027.",
  "y": "background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_9",
  "x": ", where A is the matrix of encoded BQ, b is the encode MQ and \u03bb is a parameter of the regularization term. ---------------------------------- **BASIC QUESTION GENERATION** We now describe how to generate the BQ of a query question, illustrated in Figure 1 . According to the above subsections, Question Encoding and Problem Formulation, we can encode all basic question candidates from the training and validation question sets of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) by Skip-Thought Vectors, and then we have a matrix of basic question candidates. Each column of the matrix is a vector representation, 4800 by 1 dimensions, of a specific basic question candidate and we have 186027 columns. That is, the dimension of BQ matrix, called A, is 4800 by 186027. Also, we encode the given query question as a column vector, 4800 by 1 dimensions, by Skip-Thought Vectors, called b. Regarding the selection of the parameter, \u03bb, we will discuss this in Section 4. Now, we can solve the LASSO optimization problem, mentioned in the above subsection of Problem Formulation, to get the solution, x. Here, we consider the elements of the solution vector, x, as the similarity score of the corresponding BQ in the BQ matrix, A. The first element of x corresponds to the first column, i.e. the first BQ, of A. Then, we rank all of the similarity scores in x and pick up the top 21 large weights with corresponding BQ to be the ranked BQ of the given query question. Intuitively, if a BQ has a larger similarity score to the given MQ, then this BQ can be considered as a question more similar to the given MQ.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_10",
  "x": "Also, we encode the given query question as a column vector, 4800 by 1 dimensions, by Skip-Thought Vectors, called b. Regarding the selection of the parameter, \u03bb, we will discuss this in Section 4. Now, we can solve the LASSO optimization problem, mentioned in the above subsection of Problem Formulation, to get the solution, x. Here, we consider the elements of the solution vector, x, as the similarity score of the corresponding BQ in the BQ matrix, A. The first element of x corresponds to the first column, i.e. the first BQ, of A. Then, we rank all of the similarity scores in x and pick up the top 21 large weights with corresponding BQ to be the ranked BQ of the given query question. Intuitively, if a BQ has a larger similarity score to the given MQ, then this BQ can be considered as a question more similar to the given MQ. Finally, we find the ranked BQ of all 244302 testing questions from the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) and collect them together, with the format {Image, M Q, 21 (BQ + corresponding similarity score)}, as our Basic Question Dataset (BQD). ---------------------------------- **BASIC QUESTION DATASET** We propose a novel large scale dataset, called Basic (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". (<cite>Antol et al. 2015</cite>) , and the corresponding similarity scores of BQ are generated by our basic question generation method, referring to Section 3. Note that we preprocess the training and validation question datasets from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) , so the total number of basic question candidates is less than the total number of training and validation question datasets in <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . In BQD, we have 81434 images, 244302 MQ and 5130342 (BQ + corresponding similarity score).",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_11",
  "x": "Each column of the matrix is a vector representation, 4800 by 1 dimensions, of a specific basic question candidate and we have 186027 columns. That is, the dimension of BQ matrix, called A, is 4800 by 186027. Also, we encode the given query question as a column vector, 4800 by 1 dimensions, by Skip-Thought Vectors, called b. Regarding the selection of the parameter, \u03bb, we will discuss this in Section 4. Now, we can solve the LASSO optimization problem, mentioned in the above subsection of Problem Formulation, to get the solution, x. Here, we consider the elements of the solution vector, x, as the similarity score of the corresponding BQ in the BQ matrix, A. The first element of x corresponds to the first column, i.e. the first BQ, of A. Then, we rank all of the similarity scores in x and pick up the top 21 large weights with corresponding BQ to be the ranked BQ of the given query question. Intuitively, if a BQ has a larger similarity score to the given MQ, then this BQ can be considered as a question more similar to the given MQ. Finally, we find the ranked BQ of all 244302 testing questions from the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) and collect them together, with the format {Image, M Q, 21 (BQ + corresponding similarity score)}, as our Basic Question Dataset (BQD). ---------------------------------- **BASIC QUESTION DATASET** We propose a novel large scale dataset, called Basic (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\".",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_12",
  "x": "Finally, we find the ranked BQ of all 244302 testing questions from the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) and collect them together, with the format {Image, M Q, 21 (BQ + corresponding similarity score)}, as our Basic Question Dataset (BQD). ---------------------------------- **BASIC QUESTION DATASET** We propose a novel large scale dataset, called Basic (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". (<cite>Antol et al. 2015</cite>) , and the corresponding similarity scores of BQ are generated by our basic question generation method, referring to Section 3. Note that we preprocess the training and validation question datasets from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) , so the total number of basic question candidates is less than the total number of training and validation question datasets in <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . In BQD, we have 81434 images, 244302 MQ and 5130342 (BQ + corresponding similarity score). Furthermore, we exploit the BQD to do robustness analysis of the 6 available pretrained state-of-the-art VQA models (<cite>Antol et al. 2015</cite>; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) in the next subsection. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_13",
  "x": "(<cite>Antol et al. 2015</cite>) , and the corresponding similarity scores of BQ are generated by our basic question generation method, referring to Section 3. Note that we preprocess the training and validation question datasets from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) , so the total number of basic question candidates is less than the total number of training and validation question datasets in <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . In BQD, we have 81434 images, 244302 MQ and 5130342 (BQ + corresponding similarity score). Furthermore, we exploit the BQD to do robustness analysis of the 6 available pretrained state-of-the-art VQA models (<cite>Antol et al. 2015</cite>; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) in the next subsection. ---------------------------------- **ROBUSTNESS ANALYSIS BY BASIC QUESTIONS** To measure the robustness of any model, we should evaluate it on clean and noisy input and compare the performance. The noise can be completely random, have a specific structure and/or be semantically relevant to the final task. For VQA the input is an image question pair and therefore the noise should be introduced to both. The noise to the question shouldn't be random and it should have some contextual semantics for the measure to be informative.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_14",
  "x": "The noise can be completely random, have a specific structure and/or be semantically relevant to the final task. For VQA the input is an image question pair and therefore the noise should be introduced to both. The noise to the question shouldn't be random and it should have some contextual semantics for the measure to be informative. For the image part, there is already a rapidly growing research on evaluating the robustness of deep learning models (Fawzi, Moosavi Dezfooli, and Frossard 2017;  Table 3 : MUTAN without Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". Carlini and Wagner 2017; Xu, Caramanis, and Mannor 2009) . However, for the question part, we couldn't find any acceptable method to measure the robustness of visual question answering algorithms after extensive literature review. Here we propose a novel robustness measure for VQA by introducing semantically relevant noise to the questions where we can control the strength of noisiness. First, we measure the accuracy of the model on the clean <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) and we call it Acc vqa .",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_15",
  "x": "First, we measure the accuracy of the model on the clean <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) and we call it Acc vqa . Then, we append the top ranked k BQs to each of the MQs in the clean dataset and recompute the accuracy of the model on this noisy input and we call it Acc bqd . Finally, we compute the absolute difference Acc dif f = |Acc vqa \u2212 Acc bqd | and we report the robustness score R score . ,where 0 \u2264 t < m \u2264 100. The parameters t and m are the tolerance and the maximum robustness limit, respectively, i.e., the robustness score R score decreases smoothly between 1 and 0 as Acc dif f moves from t to m and remain constant out side this range. The rate of change of this transition is exponentially decreasing from exponential to sublinear in the range [t, m] . The reasoning behind this is that we want the score to be sensitive if the difference is small, but not before t, and less sensitive if it is large, but not after m. ---------------------------------- **EXPERIMENT**",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_16",
  "x": "The parameters t and m are the tolerance and the maximum robustness limit, respectively, i.e., the robustness score R score decreases smoothly between 1 and 0 as Acc dif f moves from t to m and remain constant out side this range. The rate of change of this transition is exponentially decreasing from exponential to sublinear in the range [t, m] . The reasoning behind this is that we want the score to be sensitive if the difference is small, but not before t, and less sensitive if it is large, but not after m. ---------------------------------- **EXPERIMENT** In Section 4, we describe the details of our implementation and experimental results of the proposed method. ---------------------------------- **DATASETS** We conduct our experiments on BQD and <cite>VQA</cite> (<cite>Antol et al. 2015</cite>) dataset.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_17",
  "x": "---------------------------------- **DATASETS** We conduct our experiments on BQD and <cite>VQA</cite> (<cite>Antol et al. 2015</cite>) dataset. <cite>VQA dataset</cite> is based on the MS COCO Table 4 : MUTAN with Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". dataset (Lin et al. 2014) and it contains a large number of questions. There are questions, 248349 for training, 121512 for validation and 244302 for testing. In the <cite>VQA dataset</cite>, each question is associated with 10 answers annotated by different people from Amazon Mechanical Turk (AMT).",
  "y": "uses background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_18",
  "x": "---------------------------------- **EXPERIMENT** In Section 4, we describe the details of our implementation and experimental results of the proposed method. ---------------------------------- **DATASETS** We conduct our experiments on BQD and <cite>VQA</cite> (<cite>Antol et al. 2015</cite>) dataset. <cite>VQA dataset</cite> is based on the MS COCO Table 4 : MUTAN with Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\".",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_19",
  "x": "In the <cite>VQA dataset</cite>, each question is associated with 10 answers annotated by different people from Amazon Mechanical Turk (AMT). About 98% of answers do not exceed 3 words and 90% of answers have single words. Note that we only develop our work on the open-ended case in <cite>VQA dataset</cite> because it is the most popular task and we also think the open-ended task is closer to the real situation than multiple-choice one. Setup In our LASSO model, we use \u03bb = 10 \u22126 to be our parameter and in the later subsection, we will discuss how the \u03bb affects the quality of BQ. Furthermore, although we rank all of the basic question candidates for each MQ, we only collect top 21 BQ to put into our BQD. The most important reason is that the similarity scores are too small after twentyfirst BQ. Regarding the limit of number of words of question input, for most of available pretrained state-of-the-art VQA models they are trained under the condition maximum number of words of input 26 words. Based on the above limitation, we divide each 21 ranked BQs into 7 partitions to do detailed analysis, referring Table 1 to Table 5 , because the total number of words of each MQ with 3 BQs is less than or equal to 26 words. ----------------------------------",
  "y": "background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_20",
  "x": "We conduct our experiments on BQD and <cite>VQA</cite> (<cite>Antol et al. 2015</cite>) dataset. <cite>VQA dataset</cite> is based on the MS COCO Table 4 : MUTAN with Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". dataset (Lin et al. 2014) and it contains a large number of questions. There are questions, 248349 for training, 121512 for validation and 244302 for testing. In the <cite>VQA dataset</cite>, each question is associated with 10 answers annotated by different people from Amazon Mechanical Turk (AMT). About 98% of answers do not exceed 3 words and 90% of answers have single words. Note that we only develop our work on the open-ended case in <cite>VQA dataset</cite> because it is the most popular task and we also think the open-ended task is closer to the real situation than multiple-choice one.",
  "y": "uses motivation"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_21",
  "x": "**EVALUATION METRICS** <cite>VQA dataset</cite> provides multiple-choice and open-ended task for evaluation. Regarding open-ended task, the answer can be any phrase or word. However, in the multiple-choice task, an answer should be chosen from 18 candidate answers. For both cases, answers are evaluated by accuracy which can reflect human consensus. The accuracy is given by the following: Table 5 : MLB with Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". , where N is the total number of examples, I[\u00b7] denotes an indicator function, a i is the predicted answer and T i is an answer set of the i th example. That is, a predicted answer is considered as a correct one if at least 3 annotators agree with it and the score depends on the total number of agreements when the predicted answer is not correct.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_22",
  "x": "---------------------------------- **EVALUATION METRICS** <cite>VQA dataset</cite> provides multiple-choice and open-ended task for evaluation. Regarding open-ended task, the answer can be any phrase or word. However, in the multiple-choice task, an answer should be chosen from 18 candidate answers. For both cases, answers are evaluated by accuracy which can reflect human consensus. The accuracy is given by the following: Table 5 : MLB with Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". , where N is the total number of examples, I[\u00b7] denotes an indicator function, a i is the predicted answer and T i is an answer set of the i th example.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_23",
  "x": "Note that if a BQ can have highly enough similarity score and provide enough extra useful information to a given MQ, then we say that the BQ has the well enough quality. We discover that when \u03bb is greater than 10 \u22125 or less than 10 \u22126 , the quality of BQ is obviously not good based on the common sense knowledge. However, if we compare the quality of BQ of \u03bb equal to 10 \u22125 with \u03bb equal to 10 \u22126 , we think the BQ quality of \u03bb equal to 10 \u22126 is slightly better than \u03bb equal to 10 \u22125 based on our common sense knowledge. We will put some randomly selected BQ examples from our BQD in the supplementary material for references. Note that Figure 2 : The accuracy of state-of-the-art VQA models evaluated on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . Note that we divide the top 21 ranked BQs into 7 partitions and each partition contains 3 ranked BQs. Here, \"First top 3\" means the first partition, \"Second top 3\" means the second partition,..., and \"Seventh top 3\" means the seventh partition. Model LQI HAV HAR MU MUA MLB R score 0.19 0.48 0.45 0.30 0.34 0.36 we use the state-of-the-art question encoder, skip-thoughts , to encode all questions. iii.) Who Is The Robustest VQA Model? According to Table 6 , there are two categories, attention-based and nonattention-based, of VQA models.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_24",
  "x": "Based on the above observation, we can say that attention mechanism can help robustness. Finally, we also can discover that HieCoAtt is the most robust VQA model. In addition, when we compare HAV with HAR, the only difference between them is using the different neural networks to extract image features. Although using Resnet-200 instead of VGG-16 can help the accuracy of HieCoAtt VQA model , it reduces the robustness of the model. We conjecture that if a convolutional neural network is too deep, it probably can harm the robustness of VQA models. In the next subsection, we will do further analysis on the most robust HieCoAtt VQA model . iv.) Can Basic Questions Directly Help Accuracy? According to Table 6 , we know that HieCoAtt is the robustness VQA model. We want to do more advanced analysis on this model, so we claim that if the quality of Figure 3 : The accuracy decrement of state-of-the-art VQA models evaluated on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . Note that we divide the top 21 ranked BQs into 7 partitions and each partition contains 3 ranked BQs.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_26",
  "x": [
   "Then, we have 244302 testing questions, so that means the number of correctly answering questions of our method is more than state-of-the-art method 49 questions. In other words, if we have well enough basic question dataset, we can increase accuracy more, especially in the counting-type question, referring to supplementary material. We conjecture that because the Co-Attention Mechanism is good at localizing, the counting-type question is improved more than others. Accordingly, based on our experiment, we believe that basic questions with well enough quality can directly help accuracy by only using the naive concatenation method. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this paper, we propose a novel VQABQ method and Basic Question Dataset (BQD) for robustness analysis of VQA models. The VQABQ method has two main modules, Basic Question Generation Module and VQA Module. The former one can generate the basic questions for the query question, and the latter one can take an image, basic and query questions as the input and then output the text-based answer of the query question about the given image."
  ],
  "y": "uses"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_0",
  "x": "With the advances of deep learning, neural sequence labeling models have achieved state-ofthe-art for many tasks (Ling et al., 2015;<cite> Ma and Hovy, 2016</cite>; Peters et al., 2017) . Features are extracted automatically through network structures including long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and convolution neural network (CNN) (LeCun et al., 1989) with distributed word representations. Similar to discrete models, a CRF layer is used in many state-of-the-art neural sequence labeling models for capturing label dependencies (Collobert et al., 2011; Lample et al., 2016; Peters et al., 2017) . There exist several open-source statistical CRF sequence labeling toolkits, such as CRF++ 2 , CRFSuite (Okazaki, 2007) and FlexCRFs (Phan et al., 2004) , which provide users with flexible means of feature extraction, various training settings and decoding formats, facilitating quick implementation and extension on state-of-the-art models. On the other hand, there is limited choice for neural sequence labeling toolkits. Although many authors released their code along with their sequence labeling papers (Lample et al., 2016;<cite> Ma and Hovy, 2016</cite>; Liu et al., 2018) , the implementations are mostly focused on specific model structures and specific tasks. Modifying or extending can need enormous coding. In this paper, we present Neural CRF++ (NCRF++) 3 , a neural sequence labeling toolkit based on PyTorch, which is designed for solving general sequence labeling tasks with effective and efficient neural models. It can be regarded as the neural version of CRF++, with both take the CoNLL data format as input and can add hand- crafted features to CRF framework conveniently.",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_1",
  "x": "Although many authors released their code along with their sequence labeling papers (Lample et al., 2016;<cite> Ma and Hovy, 2016</cite>; Liu et al., 2018) , the implementations are mostly focused on specific model structures and specific tasks. Modifying or extending can need enormous coding. In this paper, we present Neural CRF++ (NCRF++) 3 , a neural sequence labeling toolkit based on PyTorch, which is designed for solving general sequence labeling tasks with effective and efficient neural models. It can be regarded as the neural version of CRF++, with both take the CoNLL data format as input and can add hand- crafted features to CRF framework conveniently. We take the layerwise implementation, which includes character sequence layer, word sequence layer and inference layer. NCRF++ is: \u2022 Fully configurable: users can design their neural models only through a configuration file without any code work. Figure 1 shows a segment of the configuration file. It builds a LSTM-CRF framework with CNN to encode character sequence (the same structure as <cite>Ma and Hovy (2016)</cite> ), plus POS and Cap features, within 10 lines.",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_2",
  "x": "In this paper, we present Neural CRF++ (NCRF++) 3 , a neural sequence labeling toolkit based on PyTorch, which is designed for solving general sequence labeling tasks with effective and efficient neural models. It can be regarded as the neural version of CRF++, with both take the CoNLL data format as input and can add hand- crafted features to CRF framework conveniently. We take the layerwise implementation, which includes character sequence layer, word sequence layer and inference layer. NCRF++ is: \u2022 Fully configurable: users can design their neural models only through a configuration file without any code work. Figure 1 shows a segment of the configuration file. It builds a LSTM-CRF framework with CNN to encode character sequence (the same structure as <cite>Ma and Hovy (2016)</cite> ), plus POS and Cap features, within 10 lines. This demonstrates the convenience of designing neural models using NCRF++. \u2022 Flexible with features: human-defined features have been proved useful in neural sequence labeling (Collobert et al., 2011; Chiu and Nichols, 2016) .",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_3",
  "x": "This demonstrates the convenience of designing neural models using NCRF++. \u2022 Flexible with features: human-defined features have been proved useful in neural sequence labeling (Collobert et al., 2011; Chiu and Nichols, 2016) . Similar to the statistical toolkits, NCRF++ supports user-defined features but using distributed representations through lookup tables, which can be initialized randomly or from external pretrained embeddings (embedding directory: emb dir in Figure 1 ). In addition, NCRF++ integrates several state-of-the-art automatic feature extractors, such as CNN and LSTM for character sequences, leading easy reproduction of many recent work (Lample et al., 2016; Chiu and Nichols, 2016;<cite> Ma and Hovy, 2016)</cite> . \u2022 Effective and efficient: we reimplement several state-of-the-art neural models (Lample et al., 2016;<cite> Ma and Hovy, 2016)</cite> using NCRF++. Experiments show models built in NCRF++ give comparable performance with reported results in the literature. Besides, NCRF++ is implemented using batch calculation, which can be accelerated using GPU. Our experiments demonstrate that NCRF++ as an effective and efficient toolkit. \u2022 Function enriched: NCRF++ extends the Viterbi algorithm (Viterbi, 1967) to enable decoding n best sequence labels with their probabilities.",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_4",
  "x": "\u2022 Flexible with features: human-defined features have been proved useful in neural sequence labeling (Collobert et al., 2011; Chiu and Nichols, 2016) . Similar to the statistical toolkits, NCRF++ supports user-defined features but using distributed representations through lookup tables, which can be initialized randomly or from external pretrained embeddings (embedding directory: emb dir in Figure 1 ). In addition, NCRF++ integrates several state-of-the-art automatic feature extractors, such as CNN and LSTM for character sequences, leading easy reproduction of many recent work (Lample et al., 2016; Chiu and Nichols, 2016;<cite> Ma and Hovy, 2016)</cite> . \u2022 Effective and efficient: we reimplement several state-of-the-art neural models (Lample et al., 2016;<cite> Ma and Hovy, 2016)</cite> using NCRF++. Experiments show models built in NCRF++ give comparable performance with reported results in the literature. Besides, NCRF++ is implemented using batch calculation, which can be accelerated using GPU. Our experiments demonstrate that NCRF++ as an effective and efficient toolkit. \u2022 Function enriched: NCRF++ extends the Viterbi algorithm (Viterbi, 1967) to enable decoding n best sequence labels with their probabilities. Taking NER, Chunking and POS tagging as typical examples, we investigate the performance of models built in NCRF++, the influence of humandefined and automatic features, the performance of nbest decoding and the running speed with the batch size.",
  "y": "similarities uses"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_5",
  "x": "**WORD SEQUENCE LAYER** Similar to the character sequence layer, NCRF++ supports both RNN and CNN as the word sequence feature extractor. The selection can be configurated through word seq feature in Figure 1 . The input of the word sequence layer is a word representation, which may include word embeddings, character sequence representations and handcrafted neural features (the combination depends on the configuration file). The word sequence layer can be stacked, building a deeper feature extractor. \u2022 Word RNN together with GRU and LSTM are available in NCRF++, which are popular structures in the recent literature (Huang et al., 2015; Lample et al., 2016;<cite> Ma and Hovy, 2016</cite>; Yang et al., 2017) . Bidirectional RNNs are supported to capture the left and right contexted information of each word. The hidden vectors for both directions on each word are concatenated to represent the corresponding word. \u2022 Word CNN utilizes the same sliding window as character CNN, while a nonlinear function (Glorot et al., 2011) is attached with the extracted features.",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_6",
  "x": "**SETTINGS** To evaluate the performance of our toolkit, we conduct the experiments on several datasets. Sang and Buchholz, 2000) , data split is following Reimers and Gurevych (2017) . For POS tagging, we use the same data and split with <cite>Ma and Hovy (2016)</cite> . We test different combinations of character representations and word sequence representations on these three benchmarks. Hyperparameters are mostly following <cite>Ma and Hovy (2016)</cite> and almost keep the same in all these experiments 5 . Standard SGD with a decaying learning rate is used as the optimizer. Table 1 shows the results of six CRF-based models with different character sequence and word sequence representations on three benchmarks. State-of-the-art results are also listed.",
  "y": "similarities uses"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_7",
  "x": "Sang and Buchholz, 2000) , data split is following Reimers and Gurevych (2017) . For POS tagging, we use the same data and split with <cite>Ma and Hovy (2016)</cite> . We test different combinations of character representations and word sequence representations on these three benchmarks. Hyperparameters are mostly following <cite>Ma and Hovy (2016)</cite> and almost keep the same in all these experiments 5 . Standard SGD with a decaying learning rate is used as the optimizer. Table 1 shows the results of six CRF-based models with different character sequence and word sequence representations on three benchmarks. State-of-the-art results are also listed. In this table, \"Nochar\" suggests a model without character sequence information. \"CLSTM\" and \"CCNN\" represent models using LSTM and CNN to encode character sequence, respectively.",
  "y": "similarities uses"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_8",
  "x": "Character information can improve model performance significantly, while using LSTM or CNN give similar improvement. Most of state-of-the-art models utilize the framework of word LSTM-CRF with character LSTM or CNN features (correspond to \"CLSTM+WLSTM+CRF\" and \"CCNN+WLSTM+CRF\" of our models) (Lample et al., 2016;<cite> Ma and Hovy, 2016</cite>; Yang et al., 2017; Peters et al., 2017) . Our implementations can achieve comparable results, with better NER and chunking performances and slightly lower POS tagging accuracy. Note that we use almost the same hyperparameters across all the experiments to achieve the results, which demonstrates the robustness of our implementation. The full experimental results and analysis are published in Yang et al. (2018) . ---------------------------------- **INFLUENCE OF FEATURES** We also investigate the influence of different features on system performance. Table 2 shows the results on the NER task.",
  "y": "background"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_0",
  "x": "Multimodal sentiment analysis research focuses on understanding how an individual modality conveys sentiment information (intra-modal dynamics), and how they interact with each other (intermodal dynamics). It is a challenging research area and state-of-the-art performance of automatic sentiment prediction has room for improvement compared to human performance<cite> (Zadeh et al., 2018a)</cite> . While multimodal approaches to sentiment analysis are relatively new in NLP, multimodal emotion recognition has long been a focus of Affective Computing. For example, De Silva and Ng (2000) combined facial expressions and speech acoustics to predict the Big-6 emotion categories (Ekman, 1992) . Emotions and sentiments are closely related concepts in Psychology and Cognitive Science research, and are often used interchangeably. Munezero et al. (2014) identified the main differences between sentiments and emotions to be that sentiments are more stable and dispositional than emotions, and sentiments are formed and directed toward a specific object. However, when adopting the cognitive definition of emotions which connects emotions to stimuli in the environment (Ortony et al., 1990) , the boundary between emotions and sentiments blurs. In particular, the circumplex model of emotions proposed by Russell (1980) describes emotions with two dimensions: Arousal which represents the level of excitement (active/inactive), and Valence which represents the level of liking (positive/negative). In many sentiment analysis studies, sentiments are defined using Likert scales with varying numbers of steps.",
  "y": "background"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_1",
  "x": "For each modality or fusion strategy we build four models: single-task sentiment regression model, bi-task sentiment regression model with polarity classification as the auxiliary task, bi-task sentiment regression model with intensity classification as the auxiliary task, and tri-task sentiment regression model with both polarity and intensity classification as the auxiliary tasks. In the bi-task and tri-task models, the main task loss is assigned a weight of 1.0, while the auxiliary task losses are assigned a weight of 0.5. Structures of the single-task and multi-task learning models only differ at the output layer: for sentiment score regression the output is a single node with tanh activation; for polarity classification the output is a single node with sigmoid activation; for intensity classification the output is 4 nodes with softmax activation. The main task uses mean absolute error as the loss function, while polarity classification uses binary cross-entropy as the loss function, and intensity classification uses categorical crossentropy as the loss function. Following state-ofthe-art on the CMU-MOSI database , during training we used Adam as the optimization function with a learning rate of 0.0005. We use the CMU Multimodal Data Software Development Kit (SDK)<cite> (Zadeh et al., 2018a)</cite> to load and pre-process the CMU-MOSI database, which splits the 2199 opinion segments into training (1283 segments), validation (229 segments), and test (686 segments) sets. 1 We implement the sentiment analysis models using the Keras deep learning library (Chollet et al., 2015) . ---------------------------------- **MULTIMODAL FEATURES**",
  "y": "uses"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_2",
  "x": "Unlike , we did not add the extra constant dimension with value 1 when computing the 3-fold Cartesian space in order to reduce the dimensionality of the multimodal input. In the LF model, as shown in Figure 5 , we concatenate the unimodal model top layers as the multimodal inputs. In the HF model, unimodal information is used in a hierarchy where the top layer of the lower unimodal model is concatenated with the input layer of the higher unimodal model, as shown in Figure 6 . We use the vocal modality at the bottom of the hierarchy while using the verbal modality at the top in HF fusion. This is because in previous studies (e.g., <cite>Zadeh et al. (2018a)</cite> ) the verbal modality was shown to be the most effective for unimodal sentiment analysis, while the vocal modality was shown to be the least effective. 4 ---------------------------------- **EXPERIMENTS AND RESULTS** Here we report our sentiment score prediction experiments.",
  "y": "similarities background"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_3",
  "x": "2 In Tables 2 and 3 , \"S\" is the singletask learning model; \"S+P\" is the bi-task learning model with polarity classification as the auxillary task; \"S+I\" is the bi-task learning model with intensity classification as the auxillary task; \"S+P+I\" is the tri-task learning model. To evaluate the performance of sentiment score prediction, following previous work<cite> (Zadeh et al., 2018a)</cite> , we Tables 2 and 3 , the numbers in bold are the best performance for each modality or fusion strategy. To identify the significant differences in results, we perform a two-sample Wilcoxon test on the sentiment score predictions given by each pair of models being compared and consider p < 0.05 as significant. We also include random prediction as a baseline and the human performance reported by . ---------------------------------- **UNIMODAL EXPERIMENTS** The results of unimodal sentiment prediction experiments are shown in Table 2 . 3 The verbal models have the best performance here, which is consistent with previous sentiment analysis studies on multiple databases (e.g., <cite>Zadeh et al. (2018a)</cite> ). This suggests that lexical information remains the most effective for sentiment analysis.",
  "y": "uses"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_4",
  "x": "---------------------------------- **UNIMODAL EXPERIMENTS** The results of unimodal sentiment prediction experiments are shown in Table 2 . 3 The verbal models have the best performance here, which is consistent with previous sentiment analysis studies on multiple databases (e.g., <cite>Zadeh et al. (2018a)</cite> ). This suggests that lexical information remains the most effective for sentiment analysis. On each modality, the best performance is achieved by a multi-task learning model. This answers our first research question and suggests that sentiment analysis can benefit from multi-task learning. In multi-task learning, the main task gains additional information from the auxillary tasks. Compared to the S model, the S+P model has increased focus on the polarity of sentiment, while the S+I model has increased focus on the intensity of sentiment.",
  "y": "similarities"
 },
 {
  "id": "cc5927700475b7abc0482a28ab209a_0",
  "x": "**INTRODUCTION** Word embeddings have revolutionized natural language processing by representing words as dense real-valued vectors in a low dimensional space. Pre-trained word embeddings such as Glove (Pennington et al., 2014) , word2vec (Mikolov et al., 2013) and fasttext (Bojanowski et al., 2017) , trained on large corpora are readily available for use in a variety of tasks. Subsequently, there has been emphasis on post-processing the embeddings to improve their performance on downstream tasks <cite>(Mu and Viswanath, 2018)</cite> or to induce linguistic properties (Mrk\u0161ic et al.; Faruqui et al., 2015) . In particular, the Principal Component Analysis (PCA) based post-processing algorithm proposed by <cite>(Mu and Viswanath, 2018)</cite> has led to significant gains in word and sentence similarity tasks, and has also proved useful in dimensionality reduction (Raunak, 2017) . Similarly, understanding the geometry of word embeddings is another area of active research (Mimno and Thompson, 2017) . Researchers have tried to ascertain the importance of dimensionality for word embeddings, with results from (Yin and Shen, 2018) answering the question of optimal dimensionality selection. In contrast to previous work, we explore the dimensional properties of existing pre-trained word embeddings through their principal components. Specifically, our contributions are as follows:",
  "y": "background"
 },
 {
  "id": "cc5927700475b7abc0482a28ab209a_1",
  "x": "We have released the source code along with the paper 1 . * equal contribution 1 Code Link: https://github.com/aclsrw/anonymized_code ---------------------------------- **INTRODUCTION** Word embeddings have revolutionized natural language processing by representing words as dense real-valued vectors in a low dimensional space. Pre-trained word embeddings such as Glove (Pennington et al., 2014) , word2vec (Mikolov et al., 2013) and fasttext (Bojanowski et al., 2017) , trained on large corpora are readily available for use in a variety of tasks. Subsequently, there has been emphasis on post-processing the embeddings to improve their performance on downstream tasks <cite>(Mu and Viswanath, 2018)</cite> or to induce linguistic properties (Mrk\u0161ic et al.; Faruqui et al., 2015) . In particular, the Principal Component Analysis (PCA) based post-processing algorithm proposed by <cite>(Mu and Viswanath, 2018)</cite> has led to significant gains in word and sentence similarity tasks, and has also proved useful in dimensionality reduction (Raunak, 2017) . Similarly, understanding the geometry of word embeddings is another area of active research (Mimno and Thompson, 2017) . Researchers have tried to ascertain the importance of dimensionality for word embeddings, with results from (Yin and Shen, 2018) answering the question of optimal dimensionality selection.",
  "y": "background"
 },
 {
  "id": "cc5927700475b7abc0482a28ab209a_2",
  "x": "In particular, the Principal Component Analysis (PCA) based post-processing algorithm proposed by <cite>(Mu and Viswanath, 2018)</cite> has led to significant gains in word and sentence similarity tasks, and has also proved useful in dimensionality reduction (Raunak, 2017) . Similarly, understanding the geometry of word embeddings is another area of active research (Mimno and Thompson, 2017) . Researchers have tried to ascertain the importance of dimensionality for word embeddings, with results from (Yin and Shen, 2018) answering the question of optimal dimensionality selection. In contrast to previous work, we explore the dimensional properties of existing pre-trained word embeddings through their principal components. Specifically, our contributions are as follows: 1. We analyze the word embeddings in terms of their principal components and demonstrate that their performance on both word similarity and sentence classification tasks saturates well before the full dimensionality. 2. We demonstrate that the amount of variance captured by the principal components is a poor representative for the downstream performance of the embeddings constructed using the very same principal components. 3. We investigate the reasons behind the aforementioned result through syntactic information based dimensional linguistic probing tasks and demonstrate that the syntactic information captured by a principal component is independent of the amount of variance it explains. 4. We point out the limitations of applying variance based post-processing <cite>(Mu and Viswanath, 2018)</cite> and demonstrate that it leads to a decrease in performance in sentence classification and machine translation arXiv:1910.02211v1 [cs.CL] 5 Oct 2019 In Section 1, we provide an introduction to the problem statement.",
  "y": "motivation"
 },
 {
  "id": "cc5927700475b7abc0482a28ab209a_3",
  "x": "Figure 3 depicts the scores (Test accuracy) on TopConst and TreeDepth tasks respectively. Evidently, no single principal component (dimension) achieves a significantly higher score in any of the two tasks and the performance across the dimensions does not have any particular trend (increasing or decreasing). This validates the hypothesis that the principal components do not vary disproportionately in terms of the syntactic information contained. ---------------------------------- **THE POST PROCESSING ALGORITHM (PPA)** In this section, we first describe and then evaluate the post-processing algorithm (PPA) proposed in <cite>(Mu and Viswanath, 2018)</cite> , which achieves high scores on Word and Semantic textual similarity tasks. The algorithm removes the projections of top principal components from each of the word vectors, making the individual word vectors more discriminative (Refer to Algorithm 1 for details). Algorithm 1: Post Processing Algorithm PPA(X, D) Data: Embedding Matrix X, Threshold Parameter D Result: Post-Processed Word Embedding Matrix X 1 X = X -X ; // Subtract Mean Embedding / * Compute PCA Components * / 2 ui = PCA(X), where i = 1, 2 . .",
  "y": "uses"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_0",
  "x": "Dashtipour et al. (2016) undertook a replication study in sentiment prediction, however this was at the document level and on different datasets and languages to the originals. In other areas of (aspectbased) sentiment analysis, releasing code for published systems has not been a high priority, e.g. in SemEval 2016 task 5 (Pontiki et al., 2016) only 1 out of 21 papers released their source code. In IR, specific reproducible research tracks have been created 3 and we are pleased to see the same happening at COLING 2018 4 . Turning now to the focus of our investigations, Target Dependent sentiment analysis (TDSA) research (Nasukawa and Yi, 2003) arose as an extension to the coarse grained analysis of document level sentiment analysis (Pang et al., 2002; Turney, 2002) . Since its inception, papers have applied different methods such as feature based (Kiritchenko et al., 2014) , Recursive Neural Networks (RecNN) <cite>(Dong et al., 2014)</cite> , Recurrent Neural Networks (RNN) (Tang et al., 2016a) , attention applied to RNN (Wang et al., 2016; Chen et al., 2017; Tay et al., 2017) , Neural Pooling (NP) Wang et al., 2017) , RNN combined with NP (Zhang et al., 2016) , and attention based neural networks (Tang et al., 2016b) . Others have tackled TDSA as a joint task with target extraction, thus treating it as a sequence labelling problem. Mitchell et al. (2013) carried out this task using Conditional Random Fields (CRF), and this work was then extended using a neural CRF . Both approaches found that combining the two tasks did not improve results compared to treating the two tasks separately, apart from when considering POS and NEG when the joint task performs better. Finally, created an attention RNN for this task which was evaluated on two very different datasets containing written and spoken (video-based) reviews where the domain adaptation between the two shows some promise.",
  "y": "background"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_1",
  "x": "Other studies have adopted more linguistic approaches. Wang et al. (2017) extended the work of by using the dependency linked words from the target. Dong et al. (2014) used the dependency tree to create a Recursive Neural Network (RecNN) inspired by Socher et al. (2013) but compared to Socher et al. (2013) they also utilised the dependency tags to create an Adaptive RecNN (ARecNN). Critically, the methods reported above have not been applied to the same datasets, therefore a true comparative evaluation between the different methods is somewhat difficult. This has serious implications for generalisability of methods. We correct that limitation in our study. There are two papers taking a similar approach to our work in terms of generalisability although they do not combine them with the reproduction issues that we highlight. First, Chen et al. (2017) compared results across SemEval's laptop and restaurant reviews in English (Pontiki et al., 2014) , a Twitter dataset <cite>(Dong et al., 2014)</cite> and their own Chinese news comments dataset. They did perform a comparison across different languages, domains, corpora types, and different methods; SVM with features (Kiritchenko et al., 2014) , Rec-NN <cite>(Dong et al., 2014)</cite> , TDLSTM (Tang et al., 2016a) , Memory Neural Network (MNet) (Tang et al., 2016b) and their own attention method.",
  "y": "background"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_2",
  "x": "There are two papers taking a similar approach to our work in terms of generalisability although they do not combine them with the reproduction issues that we highlight. First, Chen et al. (2017) compared results across SemEval's laptop and restaurant reviews in English (Pontiki et al., 2014) , a Twitter dataset <cite>(Dong et al., 2014)</cite> and their own Chinese news comments dataset. They did perform a comparison across different languages, domains, corpora types, and different methods; SVM with features (Kiritchenko et al., 2014) , Rec-NN <cite>(Dong et al., 2014)</cite> , TDLSTM (Tang et al., 2016a) , Memory Neural Network (MNet) (Tang et al., 2016b) and their own attention method. However, the Chinese dataset was not released, and the methods were not compared across all datasets. By contrast, we compare all methods across all datasets, using techniques that are not just from the Recurrent Neural Network (RNN) family. A second paper, by Barnes et al. (2017) compares seven approaches to (document and sentence level) sentiment analysis on six benchmark datasets, but does not systematically explore reproduction issues as we do in our paper. ---------------------------------- **DATASETS USED IN OUR EXPERIMENTS** We are evaluating our models over six different English datasets deliberately chosen to represent a range of domains, types and mediums.",
  "y": "background"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_3",
  "x": "Second, we only used datasets that contain three distinct sentiments (Wilson (2008) only has two). From the datasets we have used, we have only had issue with parsing Wang et al. (2017) where the annotations for the first set of the data contains the target span but the second set does not. Thus making it impossible to use the second set of annotation and forcing us to only use a subset of the dataset. An as example of this: \"Got rid of bureaucrats 'and we put that money, into 9000 more doctors and nurses'... to turn the doctors into bureaucrats#BattleForNumber10\" in that Tweet 'bureaucrats' was annotated as negative but it does not state if it was the first or second instance of 'bureaucrats' since it does not use target spans. As we can see from table 2, generally the social media datasets (Twitter and YouTube) contain more targets per sentence with the exception of<cite> Dong et al. (2014)</cite> and Mitchell et al. (2013) . The only dataset that has a small difference between the number of unique sentiments per sentence is the Wang et al. (2017) ---------------------------------- **REPRODUCTION STUDIES** In the following subsections, we present the three different methods that we are reproducing and how their results differ from the original analysis.",
  "y": "differences"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_4",
  "x": "Vo and Zhang (2015) created the first NP method for TDSA. All of their experiments are performed on<cite> Dong et al. (2014)</cite> Twitter data set.",
  "y": "background"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_5",
  "x": "However we agree that combining the two vectors is beneficial and that the rank of methods is the same in our observations. ---------------------------------- **SCALING AND FINAL MODEL COMPARISON** We test all of the methods on the test data set of<cite> Dong et al. (2014)</cite> and show the difference between the original and reproduced models in figure 2 . Finally, we show the effect of scaling using Max Min and not scaling the data. As stated before, we have been using Max Min scaling on the NP features, however did not mention scaling in their paper. The library they were using, LibLinear (Fan et al., 2008) , suggests in its practical guide (Hsu et al., 2003) to scale each feature to [0, 1] but this was not re-iterated by . We are using scikit-learn's (Pedregosa et al., 2011) LinearSVC which is a wrapper of LibLinear, hence making it appropriate to use here. As can be seen in figure 2, not scaling can affect the results by around one-third.",
  "y": "uses similarities"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_6",
  "x": "TDParse the feature of TDParse-and the left and right contexts, and 3. TDParse+ the features of TDParse and LS and RS contexts. The experiments are performed on the<cite> Dong et al. (2014)</cite> and Wang et al. (2017) Twitter datasets where we train and test on the previously specified train and test splits. We also scale our features using Max Min scaling before inputting into the SVM. We used all three sentiment lexicons as in the original paper, and we found the C-Value by performing 5 fold stratified cross validation on the training datasets. The results of these experiments can be seen in figure 3 10 . As found with the results of replication, scaling is very important but is typically overlooked when reporting. Tang et al. (2016a) was the first to use LSTMs specifically for TDSA. They created three different models: 1. LSTM a standard LSTM that runs over the length of the sentence and takes no target information into account, 2.",
  "y": "similarities uses"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_7",
  "x": "The experiments are performed on the<cite> Dong et al. (2014)</cite> dataset where we train and test on the specified splits. For the LSTMs we initialised the weights using uniform distribution U(0.003, 0.003), used Stochastic Gradient Descent (SGD) a learning rate of 0.01, cross entropy loss, padded and truncated sequence to the length of the maximum sequence in the training dataset as stated in the original paper, and we did not \"set the clipping threshold of softmax layer as 200\" (Tang et al., 2016a) as we were unsure what this meant. With regards to the number of epochs trained, we used early stopping with a patience of 10 and allowed 300 epochs. Within their experiments they used SSWE and Glove Twitter vectors 11 (Pennington et al., 2014) . As the paper being reproduced does not define the number of epochs they trained for, we use early stopping. Thus for early stopping we require to split the training data into train and validation sets to know when to stop. As it has been shown by Reimers and Gurevych (2017) that the random seed statistically significantly changes the results of experiments we ran each model over each word embedding thirty times, using a different seed value but keeping the same stratified train and validation split, and reported the results on the same test data as the original paper. As can be seen in Figure 4 , the initial seed value makes a large difference more so for the smaller embeddings. In table 5, we show the difference between our mean and maximum result and the original result for each model using the 200 dimension Glove Twitter vectors.",
  "y": "similarities uses"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_0",
  "x": "****DEPENDENCY LINK EMBEDDINGS: CONTINUOUS REPRESENTATIONS OF SYNTACTIC SUBSTRUCTURES**** **ABSTRACT** We present a simple method to learn continuous representations of dependency substructures (links), with the motivation of directly working with higher-order, structured embeddings and their hidden relationships, and also to avoid the millions of sparse, template-based word-cluster features in dependency parsing. These link embeddings allow a significantly smaller and simpler set of unary features for dependency parsing, while maintaining improvements similar to state-of-the-art, n-ary word-cluster features, and also stacking over them. Moreover, these link vectors (made publicly available) are directly portable as offthe-shelf, dense, syntactic features in various NLP tasks. As one example, we incorporate them into constituent parse reranking, where their small feature set again matches the performance of standard non-local, manuallydefined features, and also stacks over them. ---------------------------------- **INTRODUCTION** Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (Koo et al., 2008; Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou' et al., 2013; <cite>Bansal et al., 2014</cite>; Guo et al., 2014; Pennington et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wang et al., 2015) .",
  "y": "background"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_1",
  "x": "These link embeddings allow a significantly smaller and simpler set of unary features for dependency parsing, while maintaining improvements similar to state-of-the-art, n-ary word-cluster features, and also stacking over them. Moreover, these link vectors (made publicly available) are directly portable as offthe-shelf, dense, syntactic features in various NLP tasks. As one example, we incorporate them into constituent parse reranking, where their small feature set again matches the performance of standard non-local, manuallydefined features, and also stacks over them. ---------------------------------- **INTRODUCTION** Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (Koo et al., 2008; Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou' et al., 2013; <cite>Bansal et al., 2014</cite>; Guo et al., 2014; Pennington et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wang et al., 2015) . While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) . Some recent work has addressed this issue, via low-rank tensor mappings (Lei et al., 2014) , feature embeddings , or neural network parsers (Chen and Manning, 2014) .",
  "y": "background"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_2",
  "x": "**INTRODUCTION** Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (Koo et al., 2008; Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou' et al., 2013; <cite>Bansal et al., 2014</cite>; Guo et al., 2014; Pennington et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wang et al., 2015) . While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) . Some recent work has addressed this issue, via low-rank tensor mappings (Lei et al., 2014) , feature embeddings , or neural network parsers (Chen and Manning, 2014) . Secondly, it would also be useful to learn dense representations directly for the higher-order substructures (that structured NLP tasks factor on) so as to explicitly capture the useful, hidden relationships among these substructures, instead of relying on the sparse word-conjoined relationships. In this work, we propose to address both these issues by learning simple dependency link embeddings on 'head-argument' pairs (as a single concatenated unit), which allows us to work directly with linguistically-intuitive, higher-order substructures, and also fire significantly fewer and simpler features in dependency parsing, as opposed to word cluster and embedding features in previous work (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) , while still maintaining <cite>their</cite> strong accuracies. Trained using appropriate dependency-based context in word2vec, the fast neural language model of Mikolov et al. (2013a) , these link vectors allow a substantially smaller set of unary link features (as opposed to n-ary, conjoined features) which provide savings in parsing time and memory. Moreover, unlike conjoined features, link embeddings allow a tractable set of accurate per-dimension features, making the feature set even smaller and the featuregeneration process orders of magnitude faster (than hierarchical clustering features).",
  "y": "motivation differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_3",
  "x": "Secondly, it would also be useful to learn dense representations directly for the higher-order substructures (that structured NLP tasks factor on) so as to explicitly capture the useful, hidden relationships among these substructures, instead of relying on the sparse word-conjoined relationships. In this work, we propose to address both these issues by learning simple dependency link embeddings on 'head-argument' pairs (as a single concatenated unit), which allows us to work directly with linguistically-intuitive, higher-order substructures, and also fire significantly fewer and simpler features in dependency parsing, as opposed to word cluster and embedding features in previous work (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) , while still maintaining <cite>their</cite> strong accuracies. Trained using appropriate dependency-based context in word2vec, the fast neural language model of Mikolov et al. (2013a) , these link vectors allow a substantially smaller set of unary link features (as opposed to n-ary, conjoined features) which provide savings in parsing time and memory. Moreover, unlike conjoined features, link embeddings allow a tractable set of accurate per-dimension features, making the feature set even smaller and the featuregeneration process orders of magnitude faster (than hierarchical clustering features). At the same time, these link embedding features maintain dependency parsing improvements similar to the complex, template-based features on word clusters and embeddings by previous work (Koo et al., 2008; <cite>Bansal et al., 2014</cite> ) (up to 9% relative error reduction), and also stack statistically significantly over <cite>them</cite> (up to an additional 5% relative error reduction). Another advantage of this approach (versus <cite>previous work</cite> on feature embeddings or special neural networks for parsing) is that these link embeddings can be imported as off-the-shelf, dense, syntactic features into various other NLP tasks, similar to word embedding features, but now with richer, structured information, and in tasks where plain word embeddings have not proven useful . As an example, we incorporate them into a constituent parse reranker and see improvements that again match state-of-the-art, manually-defined, non-local reranking features and stack over them statistically significantly. We make our link embeddings publicly available 1 and hope that they will prove useful in various other NLP tasks in future work, e.g., as dense, syntactic features in sentence classification or as linguistically-intuitive, initial units in vectorspace composition. ----------------------------------",
  "y": "differences motivation"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_4",
  "x": "Another advantage of this approach (versus <cite>previous work</cite> on feature embeddings or special neural networks for parsing) is that these link embeddings can be imported as off-the-shelf, dense, syntactic features into various other NLP tasks, similar to word embedding features, but now with richer, structured information, and in tasks where plain word embeddings have not proven useful . As an example, we incorporate them into a constituent parse reranker and see improvements that again match state-of-the-art, manually-defined, non-local reranking features and stack over them statistically significantly. We make our link embeddings publicly available 1 and hope that they will prove useful in various other NLP tasks in future work, e.g., as dense, syntactic features in sentence classification or as linguistically-intuitive, initial units in vectorspace composition. ---------------------------------- **DEPENDENCY LINK EMBEDDINGS** To train the link embeddings, we use the speedy, skip-gram neural language model of Mikolov et al. (2013a; 2013b) via their toolkit word2vec. 2 We use the original skip-gram model and simply change the context tuple data on which the model is trained, similar to <cite>Bansal et al. (2014)</cite> and Levy and Goldberg (2014) . The goal is to learn similar embeddings for links with similar syntactic contextual properties like label, signed distance, ancestors, etc. To this end, we first parse the BLLIP corpus (minus the PTB portion) 3 using the baseline MSTParser (McDonald et al., 2005b) .",
  "y": "similarities"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_5",
  "x": "**FEATURES** The BROWN cluster features are based on <cite>Bansal et al. (2014)</cite> , <cite>who</cite> follow Koo et al. (2008) Koo et al. (2008) ). We have another feature that additionally includes the signed, bucketed distance of the particular link in the given sentence. Also note the difference of our unary bucket features from the binary bucket features of <cite>Bansal et al. (2014)</cite> , <cite>who</cite> had to work with pairwise, conjoined features of the head and the argument. Hence, <cite>they</cite> used features on conjunctions of the two bucket values from the head and argument word vectors, firing one pairwise feature per dimension, because firing features on all dimension pairs (corresponding to an outer product) led to an infeasible number of features. The result discussion of these feature differences in presented in \u00a73.2. Bit-string features: We first hierarchically cluster the link vectors via MATLAB's linkage function with {method=ward, metric=euclidean} to get 0-1 bit-strings (similar to BROWN). Next, we again fire a small set of unary indicator features that simply con- sist of the link's bit-string prefix, the prefix-length, and another feature that adds the signed, bucketed distance of that link in the sentence. 6",
  "y": "uses"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_6",
  "x": "We have another feature that additionally includes the signed, bucketed distance of the particular link in the given sentence. Also note the difference of our unary bucket features from the binary bucket features of <cite>Bansal et al. (2014)</cite> , <cite>who</cite> had to work with pairwise, conjoined features of the head and the argument. Hence, <cite>they</cite> used features on conjunctions of the two bucket values from the head and argument word vectors, firing one pairwise feature per dimension, because firing features on all dimension pairs (corresponding to an outer product) led to an infeasible number of features. The result discussion of these feature differences in presented in \u00a73.2. Bit-string features: We first hierarchically cluster the link vectors via MATLAB's linkage function with {method=ward, metric=euclidean} to get 0-1 bit-strings (similar to BROWN). Next, we again fire a small set of unary indicator features that simply con- sist of the link's bit-string prefix, the prefix-length, and another feature that adds the signed, bucketed distance of that link in the sentence. 6 ---------------------------------- **SETUP AND RESULTS**",
  "y": "differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_7",
  "x": "Next, we will present empirical results on feature space reduction and on parsing performance on both in-domain and out-of-domain datasets. ---------------------------------- **FEATURES** The BROWN cluster features are based on <cite>Bansal et al. (2014)</cite> , <cite>who</cite> follow Koo et al. (2008) Koo et al. (2008) ). We have another feature that additionally includes the signed, bucketed distance of the particular link in the given sentence. Also note the difference of our unary bucket features from the binary bucket features of <cite>Bansal et al. (2014)</cite> , <cite>who</cite> had to work with pairwise, conjoined features of the head and the argument. Hence, <cite>they</cite> used features on conjunctions of the two bucket values from the head and argument word vectors, firing one pairwise feature per dimension, because firing features on all dimension pairs (corresponding to an outer product) led to an infeasible number of features. The result discussion of these feature differences in presented in \u00a73.2. Bit-string features: We first hierarchically cluster the link vectors via MATLAB's linkage function with {method=ward, metric=euclidean} to get 0-1 bit-strings (similar to BROWN).",
  "y": "differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_8",
  "x": "Bit-string features: We first hierarchically cluster the link vectors via MATLAB's linkage function with {method=ward, metric=euclidean} to get 0-1 bit-strings (similar to BROWN). Next, we again fire a small set of unary indicator features that simply con- sist of the link's bit-string prefix, the prefix-length, and another feature that adds the signed, bucketed distance of that link in the sentence. 6 ---------------------------------- **SETUP AND RESULTS** For all experiments (unless otherwise noted), we follow the 2nd-order MSTParser setup of <cite>Bansal et al. (2014)</cite> , in terms of data splits, parameters, preprocessing, and feature thresholding. Statistical significance is reported based on the bootstrap test (Efron and Tibshirani, 1994) with 1 million samples. First, we compare the number of features in Table 2 . Our dense, unary, link-embedding based Bucket and Bit-string features are substantially fewer than the sparse, n-ary, template-based features used in the MSTParser baseline, in BROWN, and in the word embedding SKIP DEP result of B<cite>ansal et al. (2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_9",
  "x": "For all experiments (unless otherwise noted), we follow the 2nd-order MSTParser setup of <cite>Bansal et al. (2014)</cite> , in terms of data splits, parameters, preprocessing, and feature thresholding. Statistical significance is reported based on the bootstrap test (Efron and Tibshirani, 1994) with 1 million samples. First, we compare the number of features in Table 2 . Our dense, unary, link-embedding based Bucket and Bit-string features are substantially fewer than the sparse, n-ary, template-based features used in the MSTParser baseline, in BROWN, and in the word embedding SKIP DEP result of B<cite>ansal et al. (2014)</cite> . This in turn also improves our parsing speed and memory. Moreover, regarding the preprocessing time taken to generate these various feature types, our Bucket features, which just need the fast word2vec training, take 2-3 orders of magnitude lesser time than the BROWN features (15 mins. versus 2.5 days) 7 ; this is also advantageous when 6 We again used prefixes of length 4, 6, 8, 12, same as the BROWN feature setting. For unknown links' features, we replace the bucket or bit-string prefix with a special 'UNK' string. 7 Based on a modern 3.50 GHz desktop and 1 thread.",
  "y": "differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_10",
  "x": "Table 3 shows the main UAS (unlabeled attachment score) results on WSJ, where each '+ X' row denotes adding type X features to the MSTParser baseline. All the final test improvements, i.e., Bucket (92.3) and Bit-string (92.6) w.r.t. Baseline (91.9), and BROWN + Bucket (93.0) and BROWN + Bit-string (93.1) w.r.t. BROWN (92.7), are statistically significant at p < 0.01. Moreover, the Bit-string result (92.6) is the same, i.e., has no statistically significant difference from the BROWN result (92.7), and also from the <cite>Bansal et al. (2014</cite>) SKIP DEP result (92.7). Therefore, the main contribution of these link embeddings is that their significantly simpler, smaller, and faster set of unary features can match the performance of complex, template-based BROWN features (and of the dependency-based word embedding features of <cite>Bansal et al. (2014)</cite> ), and also stack over them. We also get similar trends of improvements on the labeled attachment score (LAS) metric. 8 Moreover, unlike <cite>Bansal et al. (2014)</cite> , our Bucket features achieve statistically significant improvements, most likely because <cite>they</cite> fired D pairwise, conjoined features, one per dimension d, consisting of the two bucket values from the head and argument word vectors. This would disallow the classifier to learn useful linear combinations of the various dimensions.",
  "y": "similarities"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_11",
  "x": "Moreover, the Bit-string result (92.6) is the same, i.e., has no statistically significant difference from the BROWN result (92.7), and also from the <cite>Bansal et al. (2014</cite>) SKIP DEP result (92.7). Therefore, the main contribution of these link embeddings is that their significantly simpler, smaller, and faster set of unary features can match the performance of complex, template-based BROWN features (and of the dependency-based word embedding features of <cite>Bansal et al. (2014)</cite> ), and also stack over them. We also get similar trends of improvements on the labeled attachment score (LAS) metric. 8 Moreover, unlike <cite>Bansal et al. (2014)</cite> , our Bucket features achieve statistically significant improvements, most likely because <cite>they</cite> fired D pairwise, conjoined features, one per dimension d, consisting of the two bucket values from the head and argument word vectors. This would disallow the classifier to learn useful linear combinations of the various dimensions. Firing D 2 features on all dimension pairs (corresponding to an outer product) would lead to an infeasible number of features. On the other hand, we have a single vector for head+argument, allowing us to fire just D features (one per dimension) and still learn useful dimension combinations in linear space. We also report out-of-domain performance, in Table 4, on the Web treebank (Petrov and McDonald, 2012 ) test sets, directly using the WSJ-trained models. Again, both our Bucket and Bit-string linkembedding features achieve decent improvements over Baseline and they stack over BROWN, while using much fewer features.",
  "y": "differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_12",
  "x": "9 Table 5 shows these reranking results, where 1-best and log p(t|w) are the two Berkeley parser baselines, and where Config is the state-of-the-art, nonlocal, configurational feature set of Huang (2008) , which in turn is a simplified merge of Charniak and Johnson (2005) and Collins (2000) (here configurational). Again, all our test improvements are statistically significant at p < 0.01: Bit-string (90.9) over both the baselines (90.2, 89.9); and Config + Bit-string (91.4) over Config (91.1). Moreover, the Bit-string result (90.9) is the same (i.e., no statistically significant difference) as the Config result (91.1). Therefore, we can again match the improvements of complex, manually-defined, nonlocal reranking features with a much smaller set of simple, dense, off-the-shelf, link-embedding features, and also complement them statistically significantly. ---------------------------------- **RELATED WORK** As mentioned earlier, there has been a lot of useful, previous work on using word embeddings for NLP tasks such as similarity, tagging, NER, sentiment analysis, and parsing (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Huang et al., 2012; Al-Rfou' et al., 2013; Hisamoto et al., 2013; Andreas and Klein, 2014; <cite>Bansal et al., 2014;</cite> Guo et al., 2014; Pennington et al., 2014; Wang et al., 2015) , inter alia. In related work, <cite>Bansal et al. (2014)</cite> also use dependency context to tailor word embeddings to dependency parsing. However, <cite>their</cite> embedding features are still based on the sparse set of n-ary, word-based templates from previous work (McDonald et al., 2005a; Koo et al., 2008) .",
  "y": "background"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_13",
  "x": "Moreover, the Bit-string result (90.9) is the same (i.e., no statistically significant difference) as the Config result (91.1). Therefore, we can again match the improvements of complex, manually-defined, nonlocal reranking features with a much smaller set of simple, dense, off-the-shelf, link-embedding features, and also complement them statistically significantly. ---------------------------------- **RELATED WORK** As mentioned earlier, there has been a lot of useful, previous work on using word embeddings for NLP tasks such as similarity, tagging, NER, sentiment analysis, and parsing (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Huang et al., 2012; Al-Rfou' et al., 2013; Hisamoto et al., 2013; Andreas and Klein, 2014; <cite>Bansal et al., 2014;</cite> Guo et al., 2014; Pennington et al., 2014; Wang et al., 2015) , inter alia. In related work, <cite>Bansal et al. (2014)</cite> also use dependency context to tailor word embeddings to dependency parsing. However, <cite>their</cite> embedding features are still based on the sparse set of n-ary, word-based templates from previous work (McDonald et al., 2005a; Koo et al., 2008) . Our structured link embeddings achieve similar improvements <cite>as theirs</cite> (and better in the case of direct, per-dimension bucket features) with a substantially smaller and simpler (unary) set of features that are aimed to directly capture hidden relationships between the substructures that dependency parsing factors on. Moreover, we hope that similar to word embeddings, these link embeddings will also prove useful when imported into various other NLP tasks as dense, continuous features, but now with additional syntactic information.",
  "y": "background"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_14",
  "x": "In related work, <cite>Bansal et al. (2014)</cite> also use dependency context to tailor word embeddings to dependency parsing. However, <cite>their</cite> embedding features are still based on the sparse set of n-ary, word-based templates from previous work (McDonald et al., 2005a; Koo et al., 2008) . Our structured link embeddings achieve similar improvements <cite>as theirs</cite> (and better in the case of direct, per-dimension bucket features) with a substantially smaller and simpler (unary) set of features that are aimed to directly capture hidden relationships between the substructures that dependency parsing factors on. Moreover, we hope that similar to word embeddings, these link embeddings will also prove useful when imported into various other NLP tasks as dense, continuous features, but now with additional syntactic information. There has also been some recent, useful work on reducing the sparsity of features in dependency parsing, e.g., via low-rank tensors (Lei et al., 2014) and via neural network parsers that learn tag and label embeddings (Chen and Manning, 2014) . In related work, learn dense feature embeddings for dependency parsing; however, they still work with the large number of manuallydefined feature templates from previous work and train embeddings for all those templates, with an aim to discover hidden, shared information among the large set of sparse features. We get similar improvements with a much smaller and simpler set of unary link features; also, our link embeddings are more portable to other NLP tasks than template-based embeddings specific to dependency parsing. Other work includes learning distributed structured output via dense label vectors (Srikumar and Manning, 2014) , learning bilexical operator embeddings (Madhyastha et al., 2014) , and learning joint word embeddings and composition functions based on predicate-argument compositionality (Hashimoto et al., 2014) . Our main goal is to directly learn embeddings on linguistically-intuitive units like dependency links, so that they can be used as non-sparse, unary features in dependency parsing, and also as off-theshelf, dense, syntactic features in other NLP tasks (versus more intrinsic approaches based on feature embeddings or neural network parsers, which are harder to export).",
  "y": "differences motivation"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_0",
  "x": "****EXEMPLAR-BASED WORD SENSE DISAMBIGUATION: SOME RECENT IMPROVEMENTS**** **ABSTRACT** In this paper, we report recent improvements to the exemplar-based learning approach for word sense disambiguation that have achieved higher disambiguation accuracy. By using a larger value of k, the number of nearest neighbors to use for determining the class of a test example, and through 10-fold cross validation to automatically determine the best k, we have obtained improved disambiguation accuracy on a large sense-tagged corpus first used in <cite>(Ng and Lee, 1996)</cite> . The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in (Mooney, 1996) to have the highest disambiguation accuracy among seven state-of-the-art machine learning algorithms. ---------------------------------- **INTRODUCTION** Much recent research on word sense disambiguation (WSD) has adopted a corpus-based, learning approach. Many different learning approaches have been used, including neural networks (Leacock et al., 1993) , probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al., 1992a; Gale et al., 1995; Leacock et al., 1993; Yarowsky, 1992) , decision lists (Yarowsky, 1994) , exemplar-based learning algorithms (Cardie, 1993; <cite>Ng and Lee, 1996)</cite> , etc.",
  "y": "differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_1",
  "x": "**INTRODUCTION** Much recent research on word sense disambiguation (WSD) has adopted a corpus-based, learning approach. Many different learning approaches have been used, including neural networks (Leacock et al., 1993) , probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al., 1992a; Gale et al., 1995; Leacock et al., 1993; Yarowsky, 1992) , decision lists (Yarowsky, 1994) , exemplar-based learning algorithms (Cardie, 1993; <cite>Ng and Lee, 1996)</cite> , etc. In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word \"line\". The seven algorithms that he evaluated are: a Naive-Bayes classifier (Duda and Hart, 1973) , a perceptron (Rosenblatt, 1958) , a decisiontree learner (Quinlan, 1993) , a k nearest-neighbor classifier (exemplar-based learner) (Cover and Hart, 1967) , logic-based DNF and CNF learners (Mooney, 1995) , and a decision-list learner (Rivest, 1987) . His results indicate that the simple Naive-Bayes algorithm gives the highest accuracy on the \"line\" corpus tested. Past research in machine learning has also reported that the Naive-Bayes algorithm achieved good performance on other machine learning tasks (Clark and Niblett, 1989; Kohavi, 1996) . This is in spite of the conditional independence assumption made by the Naive-Bayes algorithm, which may be unjustified in the domains tested. Gale, Church and Yarowsky (Gale et al., 1992a; Gale et al., 1995; Yarowsky, 1992) have also successfully used the Naive-Bayes algorithm (and several extensions and variations) for word sense disambiguation.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_2",
  "x": "Past research in machine learning has also reported that the Naive-Bayes algorithm achieved good performance on other machine learning tasks (Clark and Niblett, 1989; Kohavi, 1996) . This is in spite of the conditional independence assumption made by the Naive-Bayes algorithm, which may be unjustified in the domains tested. Gale, Church and Yarowsky (Gale et al., 1992a; Gale et al., 1995; Yarowsky, 1992) have also successfully used the Naive-Bayes algorithm (and several extensions and variations) for word sense disambiguation. On the other hand, <cite>our past work</cite> on WSD <cite>(Ng and Lee, 1996)</cite> used an exemplar-based (or nearest neighbor) learning approach. Our WSD program, LEXAS, extracts a set of features, including part of speech and morphological form, surrounding words, local collocations, and verb-object syntactic relation from a sentence containing the word to be disambiguated. These features from a sentence form an example. LEXAS then uses the exemplar-based learning algorithm PEBLS (Cost and Salzberg, 1993) to find the sense (class) of the word to be disambiguated. In this paper, we report recent improvements to the exemplar-based learning approach for WSD that have achieved higher disambiguation accuracy. The exemplar-based learning algorithm PEBLS contains a number of parameters that must be set before running the algorithm.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_3",
  "x": "By using 10-fold cross validation (Kohavi and John, 1995) on the training set to automatically determine the best k to use, we have obtained improved disambiguation accuracy on a large sensetagged corpus first used in <cite>(Ng and Lee, 1996)</cite> . The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in (Mooney, 1996) to have the highest disambiguation accuracy among seven stateof-the-art machine learning algorithms. The rest of this paper is organized as follows. Section 2 gives a brief description of the exemplar-based algorithm PEBLS and the Naive-Bayes algorithm. Section 3 describes the 10-fold cross validation training procedure to determine the best k number of nearest neighbors to use. Section 4 presents the disambiguation accuracy of PEBLS and Naive-Bayes on the large corpus of <cite>(Ng and Lee, 1996)</cite> . Section 5 discusses the implications of the results. Section 6 gives the conclusion. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_4",
  "x": "Section 2 gives a brief description of the exemplar-based algorithm PEBLS and the Naive-Bayes algorithm. Section 3 describes the 10-fold cross validation training procedure to determine the best k number of nearest neighbors to use. Section 4 presents the disambiguation accuracy of PEBLS and Naive-Bayes on the large corpus of <cite>(Ng and Lee, 1996)</cite> . Section 5 discusses the implications of the results. Section 6 gives the conclusion. ---------------------------------- **LEARNING ALGORITHMS** ---------------------------------- **PEBLS**",
  "y": "uses"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_5",
  "x": "3 Improvements to Exemplar-Based WSD PEBLS contains a number of parameters that must be set before running the algorithm. These parameters include k (the number of nearest neighbors to use for determining the class of a test example), exemplar weights, feature weights, etc. Each of these parameters has a default value in PEBLS, eg., k = 1, no exemplar weighting, no feature weighting, etc. <cite>We</cite> have used the default values for all parameter settings in <cite>our previous work</cite> on exemplar-based WSD reported in <cite>(Ng and Lee, 1996)</cite> . However, our preliminary investigation indicates that, among the various learning parameters of PEBLS, the number k of nearest neighbors used has a considerable impact on the accuracy of the induced exemplar-based classifier. Cross validation is a well-known technique that can be used for estimating the expected error rate of a classifier which has been trained on a particular data set. For instance, the C4.5 program (Quinlan, 1993) contains an option for running cross validation to estimate the expected error rate of an induced rule set. Cross validation has been proposed as a general technique to automatically determine the parameter settings of a given learning algorithm using a particular data set as training data (Kohavi and John, 1995) . In m-fold cross validation, a training data set is partitioned into m (approximately) equal-sized blocks, and the learning algorithm is run m times.",
  "y": "uses"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_6",
  "x": "**4** Experimental Results Mooney (1996) has reported that the Naive-Bayes algorithm gives the best performance on disambiguating six senses of the word \"line\", among seven state-of-the-art learning algorithms tested. However, his comparative study is done on only one word using a data set of 2,094 examples. In our present study, we evaluated PEBLS and Naive-Bayes on a much larger corpus containing sense-tagged occurrences of 121 nouns and 70 verbs. This corpus was first reported in <cite>(Ng and Lee, 1996)</cite> , and it contains about 192,800 sense-tagged word occurrences of 191 most frequently occurring and ambiguous words of English. 1 These 191 words have been tagged with senses from WOI:tDNET (Miller, 1990) , an on-line, electronic dictionary available publicly. For this set of 191 words, the average number of senses per noun is 7.8, while the average number of senses per verb is 12.0. The sentences in this corpus were drawn from the combined corpus of the i million word Brown corpus and the 2.5 million word Wall Street Journal (WSJ) corpus. We tested both algorithms on two test sets from this corpus.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_7",
  "x": "We compute the error rate for each k, and choose the value of k with the minimum error rate. Note that the automatic determination of the best k through 10-fold cross validation makes use of only the training set, without looking at the test set at all. ---------------------------------- **4** Experimental Results Mooney (1996) has reported that the Naive-Bayes algorithm gives the best performance on disambiguating six senses of the word \"line\", among seven state-of-the-art learning algorithms tested. However, his comparative study is done on only one word using a data set of 2,094 examples. In our present study, we evaluated PEBLS and Naive-Bayes on a much larger corpus containing sense-tagged occurrences of 121 nouns and 70 verbs. This corpus was first reported in <cite>(Ng and Lee, 1996)</cite> , and it contains about 192,800 sense-tagged word occurrences of 191 most frequently occurring and ambiguous words of English. 1 These 191 words have been tagged with senses from WOI:tDNET (Miller, 1990) , an on-line, electronic dictionary available publicly.",
  "y": "uses"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_8",
  "x": "This corpus was first reported in <cite>(Ng and Lee, 1996)</cite> , and it contains about 192,800 sense-tagged word occurrences of 191 most frequently occurring and ambiguous words of English. 1 These 191 words have been tagged with senses from WOI:tDNET (Miller, 1990) , an on-line, electronic dictionary available publicly. For this set of 191 words, the average number of senses per noun is 7.8, while the average number of senses per verb is 12.0. The sentences in this corpus were drawn from the combined corpus of the i million word Brown corpus and the 2.5 million word Wall Street Journal (WSJ) corpus. We tested both algorithms on two test sets from this corpus. The first test set, named BC50, consists of 7,119 occurrences of the 191 words appearing in 50 text files of the Brown corpus. The second test set, named WSJ6, consists of 14,139 occurrences of the 191 words appearing in 6 text files of the WSJ corpus. Both test sets are identical to the ones reported in <cite>(Ng and Lee, 1996)</cite> . Since the primary aim of our present study is the comparative evaluation of learning algorithms, not feature representation, we have chosen, for simplicity, to use local collocations as the only features in the example representation.",
  "y": "similarities"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_9",
  "x": "Since the primary aim of our present study is the comparative evaluation of learning algorithms, not feature representation, we have chosen, for simplicity, to use local collocations as the only features in the example representation. Local collocations have been found to be the single most informative set of features for WSD <cite>(Ng and Lee, 1996)</cite> . That local collocation knowledge provides important clues to WSD has also been pointed out previously by Yarowsky (1993) . Let w be the word to be disambiguated, and let 12 ll w rl r2 be the sentence fragment containing w. In the present study, we used seven features in the representation of an example, which are the local collocations of the surrounding 4 words. These seven features are: 12-11, ll-rl, rl-r2, ll, rl, 12 , and r2. The first three features are concatenation of two words. 2 The experimental results obtained are tabulated in Table 1 . The first three rows of accuracy fig-1This corpus is available from the Linguistic Data Consortium (LDC).",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_10",
  "x": "The first three rows of accuracy fig-1This corpus is available from the Linguistic Data Consortium (LDC). Contact the LDC at ldc@unagi.cis.upenn.edu for details. 2The first five of these seven features were also used in <cite>(Ng and Lee, 1996)</cite> . , 1996) . The default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating WSD programs (Gale et al., 1992b; Miller et al., 1994) . There are two instantiations of this strategy in our current evaluation. Since WORDNET orders its senses such that sense 1 is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment. This assignment method does not even need to look at the training examples. We call this method \"Sense 1\" in Table 1 . Another assignment method is to determine the most frequently occurring sense in the training examples, and to assign this sense to all test examples.",
  "y": "similarities"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_11",
  "x": "We call this method \"Most Frequent\" in Table 1 . The accuracy figures of LEXAS as reported in <cite>(Ng and Lee, 1996)</cite> are reproduced in the third row of Table 1 . These figures were obtained using all features including part of speech and morphological form, surrounding words, local collocations, and verb-object syntactic relation. However, the feature value pruning method of <cite>(Ng and Lee, 1996)</cite> only selects surrounding words and local collocations as feature values if they are indicative of some sense class as measured by conditional probability (See <cite>(Ng and Lee, 1996)</cite> for details). The next three rows show the accuracy figures of PEBLS using the parameter setting of k = 1, k = 20, and 10-fold cross validation for finding the best k, respectively. The last row shows the accuracy figures of the Naive-Bayes algorithm. Accuracy figures of the last four rows are all based on only seven collocation features as described earlier in this section. However, all possible feature values (collocated words) are used, without employing the feature value pruning method used in <cite>(Ng and Lee, 1996)</cite> . Note that the accuracy figures of PEBLS with k = 1 are 1.0% and 1.6% higher than the accuracy figures of <cite>(Ng and Lee, 1996)</cite> in the third row, also with k = 1.",
  "y": "uses"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_12",
  "x": "Another assignment method is to determine the most frequently occurring sense in the training examples, and to assign this sense to all test examples. We call this method \"Most Frequent\" in Table 1 . The accuracy figures of LEXAS as reported in <cite>(Ng and Lee, 1996)</cite> are reproduced in the third row of Table 1 . These figures were obtained using all features including part of speech and morphological form, surrounding words, local collocations, and verb-object syntactic relation. However, the feature value pruning method of <cite>(Ng and Lee, 1996)</cite> only selects surrounding words and local collocations as feature values if they are indicative of some sense class as measured by conditional probability (See <cite>(Ng and Lee, 1996)</cite> for details). The next three rows show the accuracy figures of PEBLS using the parameter setting of k = 1, k = 20, and 10-fold cross validation for finding the best k, respectively. The last row shows the accuracy figures of the Naive-Bayes algorithm. Accuracy figures of the last four rows are all based on only seven collocation features as described earlier in this section. However, all possible feature values (collocated words) are used, without employing the feature value pruning method used in <cite>(Ng and Lee, 1996)</cite> .",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_13",
  "x": "Accuracy figures of the last four rows are all based on only seven collocation features as described earlier in this section. However, all possible feature values (collocated words) are used, without employing the feature value pruning method used in <cite>(Ng and Lee, 1996)</cite> . Note that the accuracy figures of PEBLS with k = 1 are 1.0% and 1.6% higher than the accuracy figures of <cite>(Ng and Lee, 1996)</cite> in the third row, also with k = 1. The feature value pruning method of <cite>(Ng and Lee, 1996)</cite> is intended to keep only feature values deemed important for classification. It seems that the pruning method has filtered out some usefifl collocation values that improve classification accuracy, such that this unfavorable effect outweighs the additional set of features (part of speech and morphological form, surrounding words, and verb-object syntactic relation) used. Our results indicate that although Naive-Bayes performs better than PEBLS with k = 1, PEBLS with k = 20 achieves comparable performance. Furthermore, PEBLS with 10-fold cross validation to select the best k yields results slightly better than the Naive-Bayes algorithm. ---------------------------------- **DISCUSSION**",
  "y": "differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_14",
  "x": "These figures were obtained using all features including part of speech and morphological form, surrounding words, local collocations, and verb-object syntactic relation. However, the feature value pruning method of <cite>(Ng and Lee, 1996)</cite> only selects surrounding words and local collocations as feature values if they are indicative of some sense class as measured by conditional probability (See <cite>(Ng and Lee, 1996)</cite> for details). The next three rows show the accuracy figures of PEBLS using the parameter setting of k = 1, k = 20, and 10-fold cross validation for finding the best k, respectively. The last row shows the accuracy figures of the Naive-Bayes algorithm. Accuracy figures of the last four rows are all based on only seven collocation features as described earlier in this section. However, all possible feature values (collocated words) are used, without employing the feature value pruning method used in <cite>(Ng and Lee, 1996)</cite> . Note that the accuracy figures of PEBLS with k = 1 are 1.0% and 1.6% higher than the accuracy figures of <cite>(Ng and Lee, 1996)</cite> in the third row, also with k = 1. The feature value pruning method of <cite>(Ng and Lee, 1996)</cite> is intended to keep only feature values deemed important for classification. It seems that the pruning method has filtered out some usefifl collocation values that improve classification accuracy, such that this unfavorable effect outweighs the additional set of features (part of speech and morphological form, surrounding words, and verb-object syntactic relation) used.",
  "y": "differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_15",
  "x": "Note that the accuracy figures of PEBLS with k = 1 are 1.0% and 1.6% higher than the accuracy figures of <cite>(Ng and Lee, 1996)</cite> in the third row, also with k = 1. The feature value pruning method of <cite>(Ng and Lee, 1996)</cite> is intended to keep only feature values deemed important for classification. It seems that the pruning method has filtered out some usefifl collocation values that improve classification accuracy, such that this unfavorable effect outweighs the additional set of features (part of speech and morphological form, surrounding words, and verb-object syntactic relation) used. Our results indicate that although Naive-Bayes performs better than PEBLS with k = 1, PEBLS with k = 20 achieves comparable performance. Furthermore, PEBLS with 10-fold cross validation to select the best k yields results slightly better than the Naive-Bayes algorithm. ---------------------------------- **DISCUSSION** To understand why larger values of k are needed, we examined the performance of PEBLS when tested on the WSJ6 test set. During 10-fold cross validation runs on the training set, for each of the 191 words, we compared two error rates: the minimum expected error rate of PEBLS using the best k, and the expected error rate of the most frequent classifter.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_16",
  "x": "It demonstrates that an exemplar-based learning approach is suitable for the WSD task, achieving high disambiguation accuracy. One potential drawback of an exemplar-based learning approach is the testing time required, since each test example must be compared with every training example, and hence the required testing time grows linearly with the size of the training set. However, more sophisticated indexing methods such as that reported in (Friedman et al., 1977) can reduce this to logarithmic expected time, which will significantly reduce testing time. In the present study, we have focused on the comparison of learning algorithms, but not on feature representation of examples. <cite>Our past work</cite> <cite>(Ng and Lee, 1996)</cite> suggests that multiple sources of knowledge are indeed useful for WSD. Future work will explore the addition of these other features to further improve disambiguation accuracy. Besides the parameter k, PEBLS also contains other learning parameters such as exemplar weights and feature weights. Exemplar weighting has been found to improve classification performance (Cost and Saizberg, 1993) . Also, given the relative importance of the various knowledge sources as reported in <cite>(Ng and Lee, 1996)</cite> , it may be possible to improve disambignation performance by introducing feature weighting.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_17",
  "x": "Besides the parameter k, PEBLS also contains other learning parameters such as exemplar weights and feature weights. Exemplar weighting has been found to improve classification performance (Cost and Saizberg, 1993) . Also, given the relative importance of the various knowledge sources as reported in <cite>(Ng and Lee, 1996)</cite> , it may be possible to improve disambignation performance by introducing feature weighting. Future work can explore the effect of exemplar weighting and feature weighting on disambiguation accuracy. ---------------------------------- **CONCLUSION** In summary, we have presented improvements to the exemplar-based learning approach for WSD. By using a larger value of k, the number of nearest neighbors to use for determining the class of a test example, and through 10-fold cross validation to automatically determine the best k, we have obtained improved disambignation accuracy on a large sensetagged corpus. The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was recently reported to have the highest disambignation accuracy among seven state-of-the-art machine learning algorithms.",
  "y": "future_work"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_0",
  "x": "More importantly, KBQG can improve the ability of machines to actively ask questions on human-machine conversations (Duan et al., 2017; Sun et al., 2018) . Therefore, this task has attracted more attention in recent years (Serban et al., 2016;<cite> Elsahar et al., 2018)</cite> . Specifically, KBQG is the task of generating natural language questions according to the input facts from a knowledge base with triplet form, like <subject, predicate, object>. For example, as illustrated in Figure 1 , KBQG aims at generating a question \"Which city is Statue of Liberty located in?\" (Q3) for the input factual triplet Which city is Statue of Liberty located in? Figure 1 : Examples of KBQG. We aims at generating questions like Q3 which expresses (matches) the given predicate and refers to a definitive answer. \"<Statue of Liberty, location/containedby 1 , New York City>\". Here, the generated question is associated to the subject \"Statue of Liberty\" and the predicate fb:location/containedby) of the input fact, and the answer corresponds to the object \"New York City\". As depicted by Serban et al. (2016) , KBQG is required to transduce the triplet fact into a question about the subject and predicate, where the object is the correct answer. Therefore, it is a key issue for KBQG to correctly understand the knowledge symbols (subject, predicate and object in the triplet fact) and then generate corresponding text descriptions. More recently, some researches have striven toward this task, where the behind intuition is to construct implicit associations between facts and texts.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_1",
  "x": "---------------------------------- **MODEL** Naturalness Serban et al. (2016) 2.96 <cite>Elsahar et al. (2018)</cite> 2.23 Our Model ans loss 3.56 Human evaluation is important for generated questions. Following <cite>Elsahar et al. (2018)</cite> , we sample 100 questions from each system, and then two annotators measure the naturalness by a score of 0-5. The Kappa coefficient for inter-annotator is 0.629, and p-value for all scores is less than 0.005. As shown in Table 5 , <cite>Elsahar et al. (2018)</cite> Pre-trained KB embeddings may provide rich structured relational information among entities. However, it heavily relies on large-scale triplets, which is time and resource-intensive. To investigate the effectiveness of pre-trained KB embedding for KBQG, we report the performance of KBQG whether using pre-trained KB embeddings by simply applying TransE. Table 6 shows that the performance of KBQG is degraded without TransE embeddings. In comparison, <cite>Elsahar et al. (2018)</cite> obtain obvious degradation on all metrics while there is only a slight decline in our model.",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_2",
  "x": "As depicted by Serban et al. (2016) , KBQG is required to transduce the triplet fact into a question about the subject and predicate, where the object is the correct answer. Therefore, it is a key issue for KBQG to correctly understand the knowledge symbols (subject, predicate and object in the triplet fact) and then generate corresponding text descriptions. More recently, some researches have striven toward this task, where the behind intuition is to construct implicit associations between facts and texts. Specifically, Serban et al. (2016) designed an encoder-decoder architecture to generate questions from structured triplet facts. In order to improve the generalization for KBQG, <cite>Elsahar et al. (2018)</cite> utilized extra contexts as input via distant supervisions (Mintz et al., 2009) , then a decoder is equipped with attention and part-ofspeech (POS) copy mechanism to generate questions. Finally, this model obtained significant improvements. Nevertheless, we observe that there are still two important research issues (RIs) which are not processed well or even neglected. 1 We omit the domain of the predicate for sake of brevity. ----------------------------------",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_3",
  "x": "Here, the generated question is associated to the subject \"Statue of Liberty\" and the predicate fb:location/containedby) of the input fact, and the answer corresponds to the object \"New York City\". As depicted by Serban et al. (2016) , KBQG is required to transduce the triplet fact into a question about the subject and predicate, where the object is the correct answer. Therefore, it is a key issue for KBQG to correctly understand the knowledge symbols (subject, predicate and object in the triplet fact) and then generate corresponding text descriptions. More recently, some researches have striven toward this task, where the behind intuition is to construct implicit associations between facts and texts. Specifically, Serban et al. (2016) designed an encoder-decoder architecture to generate questions from structured triplet facts. In order to improve the generalization for KBQG, <cite>Elsahar et al. (2018)</cite> utilized extra contexts as input via distant supervisions (Mintz et al., 2009) , then a decoder is equipped with attention and part-ofspeech (POS) copy mechanism to generate questions. Finally, this model obtained significant improvements. Nevertheless, we observe that there are still two important research issues (RIs) which are not processed well or even neglected. 1 We omit the domain of the predicate for sake of brevity.",
  "y": "background motivation"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_4",
  "x": "**RI-1:** The generated question is required to express the given predicate in the fact. For example in Figure 1 , Q1 does not express (match) the predicate (fb:location/containedby) while it is expressed in Q2 and Q3. Previous work<cite> (Elsahar et al., 2018)</cite> usually obtained predicate textual contexts through distant supervision. However, the distant supervision is noisy or even wrong (e.g. \"X is the husband of Y\" is the relational pattern for the predicate fb:marriage/spouse, so it is wrong when \"X\" is a woman). Furthermore, many predicates in the KB have no predicate contexts. We make statistic in the resources released by <cite>Elsahar et al. (2018)</cite> , and find that only 44% predicates have predicate textual context 2 . Therefore, it is prone to generate error questions from such without-context predicates. RI-2: The generated question is required to contain a definitive answer.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_5",
  "x": "The generated question is required to express the given predicate in the fact. For example in Figure 1 , Q1 does not express (match) the predicate (fb:location/containedby) while it is expressed in Q2 and Q3. Previous work<cite> (Elsahar et al., 2018)</cite> usually obtained predicate textual contexts through distant supervision. However, the distant supervision is noisy or even wrong (e.g. \"X is the husband of Y\" is the relational pattern for the predicate fb:marriage/spouse, so it is wrong when \"X\" is a woman). Furthermore, many predicates in the KB have no predicate contexts. We make statistic in the resources released by <cite>Elsahar et al. (2018)</cite> , and find that only 44% predicates have predicate textual context 2 . Therefore, it is prone to generate error questions from such without-context predicates. RI-2: The generated question is required to contain a definitive answer. A definitive answer means that one question only associates with a determinate answer rather than alternative answers.",
  "y": "uses motivation"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_6",
  "x": "Therefore, it is prone to generate error questions from such without-context predicates. RI-2: The generated question is required to contain a definitive answer. A definitive answer means that one question only associates with a determinate answer rather than alternative answers. As an example in Figure 1 , Q2 may contain ambiguous answers since it does not express the refined answer type. As a result, different answers including \"United State\", \"New York City\", etc. may be correct. In contrast, Q3 refers to a definitive answer (the object \"New York City\" in the given fact) by restraining the answer type to a city. We believe that Q3, which expresses the given predicate and refers to a definitive answer, is a better question than Q1 and Q2. In previous work, <cite>Elsahar et al. (2018)</cite> only regarded a most frequently mentioned entity type as the textual context for the subject or object in the triplet. In fact, most answer entities have multiple types, where the most frequently mentioned type tends to be universal (e.g. a broad type \"administrative region\" rather than a refined type \"US state\" for the entity \"New York\").",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_7",
  "x": "A definitive answer means that one question only associates with a determinate answer rather than alternative answers. As an example in Figure 1 , Q2 may contain ambiguous answers since it does not express the refined answer type. As a result, different answers including \"United State\", \"New York City\", etc. may be correct. In contrast, Q3 refers to a definitive answer (the object \"New York City\" in the given fact) by restraining the answer type to a city. We believe that Q3, which expresses the given predicate and refers to a definitive answer, is a better question than Q1 and Q2. In previous work, <cite>Elsahar et al. (2018)</cite> only regarded a most frequently mentioned entity type as the textual context for the subject or object in the triplet. In fact, most answer entities have multiple types, where the most frequently mentioned type tends to be universal (e.g. a broad type \"administrative region\" rather than a refined type \"US state\" for the entity \"New York\"). Therefore, generated questions from <cite>Elsahar et al. (2018)</cite> may be difficult to contain definitive answers. To address the aforementioned two issues, we exploit more diversified contexts for the given facts as textual contexts in an encoder-decoder model.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_8",
  "x": "As an example in Figure 1 , Q2 may contain ambiguous answers since it does not express the refined answer type. As a result, different answers including \"United State\", \"New York City\", etc. may be correct. In contrast, Q3 refers to a definitive answer (the object \"New York City\" in the given fact) by restraining the answer type to a city. We believe that Q3, which expresses the given predicate and refers to a definitive answer, is a better question than Q1 and Q2. In previous work, <cite>Elsahar et al. (2018)</cite> only regarded a most frequently mentioned entity type as the textual context for the subject or object in the triplet. In fact, most answer entities have multiple types, where the most frequently mentioned type tends to be universal (e.g. a broad type \"administrative region\" rather than a refined type \"US state\" for the entity \"New York\"). Therefore, generated questions from <cite>Elsahar et al. (2018)</cite> may be difficult to contain definitive answers. To address the aforementioned two issues, we exploit more diversified contexts for the given facts as textual contexts in an encoder-decoder model. Specifically, besides using predicate contexts from the distant supervision utilized by <cite>Elsahar et al. (2018)</cite> , we further leverage the domain, range and even topic for the given predicate as contexts, which are off-the-shelf in KBs (e.g. the range and the topic for the predicate fb:location/containedby are \"location\" and \"containedby\", respectively 1 ).",
  "y": "extends"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_9",
  "x": "In previous work, <cite>Elsahar et al. (2018)</cite> only regarded a most frequently mentioned entity type as the textual context for the subject or object in the triplet. In fact, most answer entities have multiple types, where the most frequently mentioned type tends to be universal (e.g. a broad type \"administrative region\" rather than a refined type \"US state\" for the entity \"New York\"). Therefore, generated questions from <cite>Elsahar et al. (2018)</cite> may be difficult to contain definitive answers. To address the aforementioned two issues, we exploit more diversified contexts for the given facts as textual contexts in an encoder-decoder model. Specifically, besides using predicate contexts from the distant supervision utilized by <cite>Elsahar et al. (2018)</cite> , we further leverage the domain, range and even topic for the given predicate as contexts, which are off-the-shelf in KBs (e.g. the range and the topic for the predicate fb:location/containedby are \"location\" and \"containedby\", respectively 1 ). Therefore, 100% predicates (rather than 44% 2 of those in Elsahar et al.) have contexts. Furthermore, in addition to the most frequently mentioned entity type as contexts used by <cite>Elsahar et al. (2018)</cite> , we leverage the type that best describes the entity as contexts (e.g. a refined entity type 3 \"US state\" combines a broad type \"administrative region\" for the entity \"New York\"), which is helpful to refine the entity information. Finally, in order to make full use of these contexts, we propose context-augmented fact encoder and multi-level copy mechanism (KB copy and context copy) to integrate diversified contexts, where the multilevel copy mechanism can copy from KB and textual contexts simultaneously. For the purpose of further making generated questions correspond to definitive answers, we propose the answer-aware loss by optimizing the cross-entropy between the generated question and answer type words, which is beneficial to generate precise questions.",
  "y": "extends"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_10",
  "x": "In contrast to general Sequence-to-Sequence (Seq2Seq) model (Sutskever et al., 2014) , the input fact is not a word sequence but instead a structured triplet F = (s, p, o). We employ a fact encoder to transform each atom in the fact into a fixed embedding, and the embedding is obtained from a KB embedding matrix. For example, the subject embedding e s \u2208 R d is looked up from the KB embedding matrix E f \u2208 R k,d , where k represents the size of KB vocabulary, and the size of KB embedding is equal to the number of hidden units (d) in Equation 3. Similarly, the predicate embedding e p and the object embedding e o are mapped from the KB embedding matrix E f , where E f is pre-trained using TransE (Bordes et al., 2013) to capture much more fact information in previous work<cite> (Elsahar et al., 2018)</cite> . In our model, E f can be pre-trained or randomly initiated (Details in Sec. 4.7.1). ---------------------------------- **CONTEXT-AUGMENTED FACT ENCODER** In order to combine both the context encoder information and the fact encoder information, we propose a context-augmented fact encoder which applies the gated fusion unit (Gong and Bowman, 2018) to integrate the context matrix and the fact embedding. For example, the subject context matrix C s = {c s 1 , c s 2 , ..., c s |s| } and the subject embedding vector e s are integrated by the following gated fusion:",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_11",
  "x": "<cite>Elsahar et al. (2018)</cite> demonstrated the effectiveness of POS copy for the context. However, such a copy mechanism heavily relies on POS tagging. Inspired by the CopyNet (Gu et al., 2016) , we directly copy words in the textual contexts C, and it does not rely on any POS tagging. Specifically, the input sequence \u03c7 for the context copy is the concatenation of all words in the textual contexts C. Unfortunately, \u03c7 is prone to contain repeated words because it consists of rich contexts for subject, predicate and object. The repeated words in the input sequence tend to cause repetition problems in output sequences (Tu et al., 2016) . We adopt the maxout pointer to address the repetition problem. Instead of summing all the probabilistic scores for repeated input words, we limit the probabilistic score of repeated words to their maximum score as Equation 13: ---------------------------------- **CONTEXT COPY**",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_12",
  "x": "---------------------------------- **EXPERIMENTAL DATA DETAILS** We conduct experiments on the SimpleQuestion dataset (Bordes et al., 2015) , and there are 75910/10845/21687 question answering pairs (QA-pairs) for training/validation/test. In order to obtain diversified contexts, we additionally employ domain, range and topic of the predicate to improve the coverage of predicate contexts. In this way, 100% predicates (rather than 44% 2 of those in Elsahar et al.) have contexts. For the subject and object context, we combine the most frequently mentioned entity type<cite> (Elsahar et al., 2018)</cite> with the type that best describe the entity 3 . The KB copy needs subject names as the copy source, and we map entities with their names similar to those in Mohammed et al. (2018) . The data details are in Appendix A and submitted Supplementary Data. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_13",
  "x": "Following (Serban et al., 2016;<cite> Elsahar et al., 2018)</cite> , we adopt some word-overlap based metrics (WBMs) for natural language generation including BLEU-4 (Papineni et al., 2002) , ROUGE L (Lin, 2004) and METEOR (Denkowski and Lavie, 2014) . However, such metrics still suffer from some limitations (Novikova et al., 2017) . Crucially, it might be difficult for them to measure whether generated questions that express the given predicate and refer to definitive answers. To better evaluate generated questions, we run two further evaluations as follows. (1) Predicate identification: Following Mohammed et al. (2018), we employ annotators to judge whether the generated question expresses the given predicate in the fact or not. The score for predicate identification is the percentage of generated questions that express the given predicate. (2) Answer coverage: We define a novel metric called answer coverage to identify whether the generated question refers to a definitive answer. Specifically, answer coverage is obtained by automatically calculating the percentage of questions that contain answer type words, and answer type words are object contexts (entity types for the object are regarded as answer type words). Furthermore, it is hard to automatically evaluate the naturalness of generated questions.",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_14",
  "x": "For the subject and object context, we combine the most frequently mentioned entity type<cite> (Elsahar et al., 2018)</cite> with the type that best describe the entity 3 . The KB copy needs subject names as the copy source, and we map entities with their names similar to those in Mohammed et al. (2018) . The data details are in Appendix A and submitted Supplementary Data. ---------------------------------- **EVALUATION METRICS** Following (Serban et al., 2016;<cite> Elsahar et al., 2018)</cite> , we adopt some word-overlap based metrics (WBMs) for natural language generation including BLEU-4 (Papineni et al., 2002) , ROUGE L (Lin, 2004) and METEOR (Denkowski and Lavie, 2014) . However, such metrics still suffer from some limitations (Novikova et al., 2017) . Crucially, it might be difficult for them to measure whether generated questions that express the given predicate and refer to definitive answers. To better evaluate generated questions, we run two further evaluations as follows.",
  "y": "extends"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_15",
  "x": "---------------------------------- **COMPARISON WITH STATE-OF-THE-ARTS** We compare our model with following methods. (1) Template: A baseline in Serban et al. (2016) , it randomly chooses a candidate fact F c in the training data to generate the question, where F c shares the same predicate with the input fact. (2) Serban et al. (2016): We compare our methods with the single placeholder model, which performs best in Serban et al. (2016) . (3) <cite>Elsahar et al. (2018)</cite> : We compare our methods with the model utilizing copy actions, the best performing model in <cite>Elsahar et al. (2018)</cite> . Although this model is designed to a zero-shot setting (for unseen predicates and entity type), it has good abilities to generate better questions (on known or unknown predicates and entity types) represented in the additional context input and SPO copy mechanism. ---------------------------------- **IMPLEMENTATION DETAILS**",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_16",
  "x": "To make our model comparable to the comparison methods, we keep most parameter values the same as <cite>Elsahar et al. (2018)</cite> . We utilize RMSProp algorithm with a decreasing learning rate (0.001), batch size (200) to optimize the model. The size of KB embeddings is 200, and KB embeddings are pre-trained by TransE (Bordes et al., 2013) . The word embeddings are initialized by the pre-trained Glove word vectors 4 with 200 dimensions. In the transformer, we set the hidden units d to 200, and we employ 4 paralleled attention head and a stack of 5 identical layers. We set the weight (\u03bb) of the answer-aware loss to 0.2. In Table 1 , we compare our model with the typical baselines on word-overlap based metrics. It is evident that our model is remarkably better than baselines on all metrics, where the BLEU4 score increases 4.53 compared with the strongest baseline<cite> (Elsahar et al., 2018)</cite> . Especially, incorporating answer-aware loss (the last line in Table 1 ) further improves the performance (+5.16 BLEU4).",
  "y": "similarities"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_17",
  "x": "The size of KB embeddings is 200, and KB embeddings are pre-trained by TransE (Bordes et al., 2013) . The word embeddings are initialized by the pre-trained Glove word vectors 4 with 200 dimensions. In the transformer, we set the hidden units d to 200, and we employ 4 paralleled attention head and a stack of 5 identical layers. We set the weight (\u03bb) of the answer-aware loss to 0.2. In Table 1 , we compare our model with the typical baselines on word-overlap based metrics. It is evident that our model is remarkably better than baselines on all metrics, where the BLEU4 score increases 4.53 compared with the strongest baseline<cite> (Elsahar et al., 2018)</cite> . Especially, incorporating answer-aware loss (the last line in Table 1 ) further improves the performance (+5.16 BLEU4). ---------------------------------- **OVERALL COMPARISONS**",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_18",
  "x": "It is evident that our model is remarkably better than baselines on all metrics, where the BLEU4 score increases 4.53 compared with the strongest baseline<cite> (Elsahar et al., 2018)</cite> . Especially, incorporating answer-aware loss (the last line in Table 1 ) further improves the performance (+5.16 BLEU4). ---------------------------------- **OVERALL COMPARISONS** ---------------------------------- **PERFORMANCES ON PREDICATE IDENTIFICATION** To evaluate the ability of our model on predicate identification, we sample 100 generated questions Model Pred. Identification Serban et al. (2016) 53.5 <cite>Elsahar et al. (2018)</cite> 71.5 Our Model ans loss 75.5 from each model, and then two annotators are employed to judge whether the generated question expresses the given predicate. The Kappa for inter-annotator statistics is 0.611, and p-value for all scores is less than 0.005.",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_19",
  "x": "(3) It tends to correspond to alternative answers (object in the triplet fact) for some predicates such as fb:location/containedby, while other predicates (e.g. fb:person/gender) may refer to a definitive answer. To investigate our model, by incorporating answer-aware loss, does not generate an answer type word in a mandatory way, we found 20.5% predicate corresponds to the generated questions without answer type words when our model obtains the highest Ans cov (\u03bb=0.5), and it is very close to 21.7% for the one in human-annotated questions. This demonstrates that the answer-aware loss does not force all predicates to generate questions with answer type words. Table 4 : Ablation study by removing the main components, where \"w/o\" means without, and \"w/o diversified contexts\" represents that diversified contexts are replaced by contexts used in <cite>Elsahar et al. (2018)</cite> . ---------------------------------- **ABLATION STUDY** In order to validate the effectiveness of model components, we remove some important components in our model, including context copy, KB copy, answer-aware loss and diversified contexts. The results are shown in Table 4 . We can see that removing any component brings performance decline on all metrics.",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_20",
  "x": "Table 4 : Ablation study by removing the main components, where \"w/o\" means without, and \"w/o diversified contexts\" represents that diversified contexts are replaced by contexts used in <cite>Elsahar et al. (2018)</cite> . ---------------------------------- **ABLATION STUDY** In order to validate the effectiveness of model components, we remove some important components in our model, including context copy, KB copy, answer-aware loss and diversified contexts. The results are shown in Table 4 . We can see that removing any component brings performance decline on all metrics. It demonstrates that all these components are useful. Specifically, the last line in Table 4 , replacing diversified contexts with contexts used in <cite>Elsahar et al. (2018)</cite> , has more obvious performance degradation. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_21",
  "x": "It demonstrates that all these components are useful. Specifically, the last line in Table 4 , replacing diversified contexts with contexts used in <cite>Elsahar et al. (2018)</cite> , has more obvious performance degradation. ---------------------------------- **PERFORMANCES ON NATURALNESS** ---------------------------------- **MODEL** Naturalness Serban et al. (2016) 2.96 <cite>Elsahar et al. (2018)</cite> 2.23 Our Model ans loss 3.56 Human evaluation is important for generated questions. Following <cite>Elsahar et al. (2018)</cite> , we sample 100 questions from each system, and then two annotators measure the naturalness by a score of 0-5. The Kappa coefficient for inter-annotator is 0.629, and p-value for all scores is less than 0.005.",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_22",
  "x": "Naturalness Serban et al. (2016) 2.96 <cite>Elsahar et al. (2018)</cite> 2.23 Our Model ans loss 3.56 Human evaluation is important for generated questions. Following <cite>Elsahar et al. (2018)</cite> , we sample 100 questions from each system, and then two annotators measure the naturalness by a score of 0-5. The Kappa coefficient for inter-annotator is 0.629, and p-value for all scores is less than 0.005. As shown in Table 5 , <cite>Elsahar et al. (2018)</cite> Pre-trained KB embeddings may provide rich structured relational information among entities. However, it heavily relies on large-scale triplets, which is time and resource-intensive. To investigate the effectiveness of pre-trained KB embedding for KBQG, we report the performance of KBQG whether using pre-trained KB embeddings by simply applying TransE. Table 6 shows that the performance of KBQG is degraded without TransE embeddings. In comparison, <cite>Elsahar et al. (2018)</cite> obtain obvious degradation on all metrics while there is only a slight decline in our model. We believe that it may owe to the contextaugmented fact encoder since our model drops to 40.87 on the BLEU4 score without contextaugmented fact encoder and transE embeddings. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_23",
  "x": "---------------------------------- **MODEL** Naturalness Serban et al. (2016) 2.96 <cite>Elsahar et al. (2018)</cite> 2.23 Our Model ans loss 3.56 Human evaluation is important for generated questions. Following <cite>Elsahar et al. (2018)</cite> , we sample 100 questions from each system, and then two annotators measure the naturalness by a score of 0-5. The Kappa coefficient for inter-annotator is 0.629, and p-value for all scores is less than 0.005. As shown in Table 5 , <cite>Elsahar et al. (2018)</cite> Pre-trained KB embeddings may provide rich structured relational information among entities. However, it heavily relies on large-scale triplets, which is time and resource-intensive. To investigate the effectiveness of pre-trained KB embedding for KBQG, we report the performance of KBQG whether using pre-trained KB embeddings by simply applying TransE. Table 6 shows that the performance of KBQG is degraded without TransE embeddings. In comparison, <cite>Elsahar et al. (2018)</cite> obtain obvious degradation on all metrics while there is only a slight decline in our model.",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_24",
  "x": "However, it heavily relies on large-scale triplets, which is time and resource-intensive. To investigate the effectiveness of pre-trained KB embedding for KBQG, we report the performance of KBQG whether using pre-trained KB embeddings by simply applying TransE. Table 6 shows that the performance of KBQG is degraded without TransE embeddings. In comparison, <cite>Elsahar et al. (2018)</cite> obtain obvious degradation on all metrics while there is only a slight decline in our model. We believe that it may owe to the contextaugmented fact encoder since our model drops to 40.87 on the BLEU4 score without contextaugmented fact encoder and transE embeddings. ---------------------------------- **THE EFFECTIVENESS OF GENERATED QUESTIONS FOR ENHANCING QUESTION ANSWERING OVER KNOWLEDGE BASES** Data Type Accuracy human-labeled data 68.97 + gen data (Serban et al., 2016) 68.53 + gen data<cite> (Elsahar et al., 2018)</cite> 69.13 + gen data (Our Model ans loss ) 69.57 Previous experiments demonstrate that our model can deliver more precise questions. To further prove the effectiveness of our model, we will see how useful the generated questions are for training a question answering system over knowledge bases. Specifically, we combine humanlabeled data with the same amount of modelgenerated data to a typical QA system (Mohammed et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_25",
  "x": "In ID 2, although all questions express the target predicate correctly, only our question refers to a definitive answer since it contains an answer type word \"city\" (marked as bold). It should be emphasized that the questions, generated by our method with answer-aware loss, do not always contain answer type words (ID 1 and 3). ---------------------------------- **RELATED WORK** Our work is inspired by a large number of successful applications using neural encoder-decoder frameworks on NLP tasks such as machine translation (Cho et al., 2014a) and dialog generation (Vinyals and Le, 2015) . Our work is also inspired by the recent work for KBQG based on encoderdecoder frameworks. Serban et al. (2016) first proposed a neural network for mapping KB facts into natural language questions. To improve the generalization, <cite>Elsahar et al. (2018)</cite> introduced extra contexts for the input fact, which achieved significant performances. However, these contexts may make it difficult to generate questions that express the given predicate and associate with a definitive answer.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_26",
  "x": "---------------------------------- **RELATED WORK** Our work is inspired by a large number of successful applications using neural encoder-decoder frameworks on NLP tasks such as machine translation (Cho et al., 2014a) and dialog generation (Vinyals and Le, 2015) . Our work is also inspired by the recent work for KBQG based on encoderdecoder frameworks. Serban et al. (2016) first proposed a neural network for mapping KB facts into natural language questions. To improve the generalization, <cite>Elsahar et al. (2018)</cite> introduced extra contexts for the input fact, which achieved significant performances. However, these contexts may make it difficult to generate questions that express the given predicate and associate with a definitive answer. Therefore, we focus on the two research issues: expressing the given predicate and referring to a definitive answer for generated questions. Moreover, our work also borrows the idea from copy mechanisms.",
  "y": "motivation background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_27",
  "x": "Bao et al. (2018) proposed to copy elements in the table (KB). <cite>Elsahar et al. (2018)</cite> exploited POS copy action to better capture textual contexts. To incorporate advantages from above copy mechanisms, we introduce KB copy and context copy which can copy KB element and textual context, and they do not rely on POS tagging. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this paper, we focus on two crucial research issues for the task of question generation over knowledge bases: generating questions that express the given predicate and refer to definitive answers rather than alternative answers. For this purpose, we present a neural encoder-decoder model which integrates diversified off-the-shelf contexts and multi-level copy mechanisms. Moreover, we design an answer-aware loss to generate questions that refer to definitive answers. Experiments show that our model achieves state-of-the-art performance on automatic and manual evaluations.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_28",
  "x": "To improve the generalization, <cite>Elsahar et al. (2018)</cite> introduced extra contexts for the input fact, which achieved significant performances. However, these contexts may make it difficult to generate questions that express the given predicate and associate with a definitive answer. Therefore, we focus on the two research issues: expressing the given predicate and referring to a definitive answer for generated questions. Moreover, our work also borrows the idea from copy mechanisms. Point network predicted the output sequence directly from the input, and it can not generate new words while CopyNet (Gu et al., 2016) combined copying and generating. Bao et al. (2018) proposed to copy elements in the table (KB). <cite>Elsahar et al. (2018)</cite> exploited POS copy action to better capture textual contexts. To incorporate advantages from above copy mechanisms, we introduce KB copy and context copy which can copy KB element and textual context, and they do not rely on POS tagging. ----------------------------------",
  "y": "background differences"
 },
 {
  "id": "d2c95c3198f21e793549d7b16bdaf8_0",
  "x": "Our demo relies on online web services which allow for an easy access to our entity linking approaches and can disambiguate against DBpedia and Wikidata. During the demo, we will show how to use MAG by means of POST requests as well as using its user-friendly web interface. All data used in the demo is available at https://hobbitdata.informatik.uni-leipzig.de/agdistis/ ---------------------------------- **INTRODUCTION** A recent survey by IBM 3 suggests that more than 2.5 quintillion bytes of data are produced on the Web every day. Entity Linking (EL), also known as Named Entity Disambiguation (NED), is one of the most important Natural Language Processing (NLP) techniques for extracting knowledge automatically from this huge amount of data. The goal of an EL approach is as follows: Given a piece of text, a reference knowledge base K and a set of entity mentions in that text, map each entity mention to the corresponding resource in K <cite>[4]</cite> . A large number of challenges has to be addressed while performing a disambiguation.",
  "y": "background"
 },
 {
  "id": "d2c95c3198f21e793549d7b16bdaf8_1",
  "x": "A portion of these approaches claim to be multilingual and most of them rely on models which are trained on English corpora with cross-lingual dictionaries. However, MAG (Multilingual AGDISTIS) <cite>[4]</cite> showed that the underlying models being trained on English corpora make them prone to failure when migrated to a different language. Additionally, these approaches hardly make their models or data available on more than three languages [6] . The new version of MAG (which is the quintessence of this demo) provides support for 40 different languages using sophisticated indices 4 . For the sake of server space, we deployed MAG-based web services for 9 languages and offer the other 31 languages for download. Additionally, we provide an English index using Wikidata to show the knowledge-base agnosticism of MAG. During the demo, we will show how to use the web services as well as MAG's user interface. ---------------------------------- **MAG ENTITY LINKING SYSTEM**",
  "y": "motivation"
 },
 {
  "id": "d2c95c3198f21e793549d7b16bdaf8_2",
  "x": "During the online phase, the EL is carried out in two steps: 1) candidate generation and 2) disambiguation. The goal of the candidate generation step is to retrieve a tractable number of candidates for each mention. These candidates are later inserted into the disambiguation graph, which is used to determine the mapping between entities and mentions. MAG implements two graph-based algorithms to disambiguate entities, i.e., PageRank and HITS. Independently of the chosen graph algorithm, the highest candidate score among the set of candidates is chosen as correct disambiguation for a given mention <cite>[4]</cite> . ---------------------------------- **DEMONSTRATION** Our demonstration will show the capabilities of MAG for different languages. We provide a graphical, web-based user interface (GUI).",
  "y": "background"
 },
 {
  "id": "d2c95c3198f21e793549d7b16bdaf8_3",
  "x": "Other parameters. The user can also define more parameters to fine-tune the disambiguation. These parameters have to be set up within the properties file 5 or via environment variables while deploying it locally. Below, we describe all the parameters. -Popularity -The user can set it as popularity=false or popularity=true. It allows MAG to use either the Page Rank or the frequency of a candidate to sort while candidate retrieval. -Graph-based algorithm -The user can choose which graph-based algorithm to use for disambiguating among the candidates per mentions. The current implementation offers HITS and PageRank as algorithms, algorithm=hits or algorithm =pagerank. -Search by Context -This boolean parameter provides a search of candidates using a context index <cite>[4]</cite> .",
  "y": "uses background"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_0",
  "x": "[wsj 0209] The training and the development data for the shared task was adapted from the Penn Discourse Treebank 2.0 (PDTB-2.0) (Prasad et al., 2008) . Our system was trained on the training partition and tuned using the development data. Results in the paper are reported for the development and the test sets from PDTB, as well as for the blind test. ---------------------------------- **SYSTEM DESCRIPTION** The system consists of multiple modules that are applied in a pipeline fashion. This architecture is a standard approach that was originally proposed in Lin et al. (2014) and was followed with slight variations by systems in the last year competition (Xue et al., 2015) . Our design most closely resembles the pipeline proposed by the top system last year<cite> (Wang and Lan, 2015)</cite> , in that argument extraction for explicit relations is performed separately for Arg1 and Arg2, the non-explicit sense classifier is run twice. The overall architecture of the system is shown in Figure 1 .",
  "y": "similarities"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_1",
  "x": "We use the training data to generate a list of 145 connective words and phrases that may function as discourse connectives. Only consecutive connectives that contain up to three tokens are addressed. The features are based on previous work (Pitler et al., 2009; Lin et al., 2014;<cite> Wang and Lan, 2015)</cite> . Our classifier is a Maximum Entropy classifier implemented with the NLTK toolkit (Bird, 2006) . ---------------------------------- **IDENTIFYING ARG1 POSITION** For explicit relations, position of Arg2 is fixed to be the sentence where the connective itself occurs. Arg1, on the other hand, can be located in the same sentence as the connective or in a previous sentence. Given a connective and the sentence in which it occurs, the goal of the position classifier is to determine the location of Arg1.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_2",
  "x": "Arg1, on the other hand, can be located in the same sentence as the connective or in a previous sentence. Given a connective and the sentence in which it occurs, the goal of the position classifier is to determine the location of Arg1. This is a binary classifier with two classes: SS and PS. We employ the features proposed in Lin et al. (2014) and additional features described in last year's top system<cite> (Wang and Lan, 2015)</cite> . The position classifier is trained using the Maximum Entropy algorithm and achieves an F1 score of 99.186% on the development data. In line with prior work<cite> (Wang and Lan, 2015)</cite> , we consider PS to be the sentence that immediately precedes the connective. About 10% of explicit discourse relations have Arg1 occurring in a sentence that does not immediately precede the connective. These are missed at this point. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_3",
  "x": "SS Argument Extractor: SS argument extractor identifies spans of Arg1 and Arg2 of explicit relations where Arg1 occurs in the same sentence, as the connective and Arg2. We follow the constituent-based approach proposed in Kong et al. (2014) , without the joint inference and enhance it using features in<cite> Wang and Lan (2015)</cite> . This component is also trained with the Maximum Entropy algorithm. PS Arg1 Extractor: We implement features described in<cite> Wang and Lan (2015)</cite> and add novel features. To identify candidate constituents, we follow Kong et al. (2014) , where constituents are defined loosely based on punctuation occurring in the sentence and clause boundaries as defined by SBAR tags. We used the constituent split implemented in<cite> Wang and Lan (2015)</cite> . Based on earlier work <cite>(Wang and Lan, 2015</cite>; Lin et al., 2014) , we implement the following features: surface form of the verbs in the sentence (three features), last word of the current constituent (curr), last word of the previous constituent (prev), the first word of curr, and the lowercased form of the connective. The novel features that we add are shown in Table 1. These features use POS information of tokens in the constituents, punctuation between the constituents, and feature conjunctions.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_4",
  "x": "About 10% of explicit discourse relations have Arg1 occurring in a sentence that does not immediately precede the connective. These are missed at this point. ---------------------------------- **EXPLICIT RELATIONS: ARGUMENT EXTRACTION** SS Argument Extractor: SS argument extractor identifies spans of Arg1 and Arg2 of explicit relations where Arg1 occurs in the same sentence, as the connective and Arg2. We follow the constituent-based approach proposed in Kong et al. (2014) , without the joint inference and enhance it using features in<cite> Wang and Lan (2015)</cite> . This component is also trained with the Maximum Entropy algorithm. PS Arg1 Extractor: We implement features described in<cite> Wang and Lan (2015)</cite> and add novel features. To identify candidate constituents, we follow Kong et al. (2014) , where constituents are defined loosely based on punctuation occurring in the sentence and clause boundaries as defined by SBAR tags.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_5",
  "x": "This component is also trained with the Maximum Entropy algorithm. PS Arg1 Extractor: We implement features described in<cite> Wang and Lan (2015)</cite> and add novel features. To identify candidate constituents, we follow Kong et al. (2014) , where constituents are defined loosely based on punctuation occurring in the sentence and clause boundaries as defined by SBAR tags. We used the constituent split implemented in<cite> Wang and Lan (2015)</cite> . Based on earlier work <cite>(Wang and Lan, 2015</cite>; Lin et al., 2014) , we implement the following features: surface form of the verbs in the sentence (three features), last word of the current constituent (curr), last word of the previous constituent (prev), the first word of curr, and the lowercased form of the connective. The novel features that we add are shown in Table 1. These features use POS information of tokens in the constituents, punctuation between the constituents, and feature conjunctions. PS Arg2 Extractor: Similar to PS Arg1 extractor, for this component we implement features described in<cite> Wang and Lan (2015)</cite> and add novel features. The novel features are the same as those introduced for PS Arg1 but also include the following additional features:",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_6",
  "x": "Based on earlier work <cite>(Wang and Lan, 2015</cite>; Lin et al., 2014) , we implement the following features: surface form of the verbs in the sentence (three features), last word of the current constituent (curr), last word of the previous constituent (prev), the first word of curr, and the lowercased form of the connective. The novel features that we add are shown in Table 1. These features use POS information of tokens in the constituents, punctuation between the constituents, and feature conjunctions. PS Arg2 Extractor: Similar to PS Arg1 extractor, for this component we implement features described in<cite> Wang and Lan (2015)</cite> and add novel features. The novel features are the same as those introduced for PS Arg1 but also include the following additional features: \u2022 nextFirstW&puncBefore -the first word token of next and the punctuation before next. \u2022 prevLastW&puncAfter -the last word token of prev and the punctuation after prev. \u2022 POS of the connective string. \u2022 The distance between the connective and the position of curr in the sentence.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_7",
  "x": "We use lexical and syntactic features based on previous work and also introduce new features: \u2022 C (Connective) string, C POS, prev + C, proposed in Lin et al. (2014) . \u2022 C self-category, parent-category of C, leftsibling-category of C, right-sibling-category of C, 4 C-Syn interactions, and 6 Syn-Syn interactions, introduced in Pitler et al. (2009) . \u2022 C parent-category linked context, previous connective and its POS of \"as\"(the connective and its POS of previous relation, if the connective of current relation is \"as\"), previous connective and its POS of \"when\", adopted from<cite> Wang and Lan (2015)</cite> . \u2022 Our new features: first token of C, second token of C (if exists), next word (next), C + next, prev + next, prev + C + next. Table 1 : Novel features used in the PS Arg1 and PS Arg2 extractors. Curr, prev, and next refer to the current, previous, and next constituent in the same sentence, respectively. W denotes word token, and POS denotes the part-of-speech tag of a word. For example, currFirstWAndCurrSecondW refers to the first two word tokens in curr, while prevLastPOS refers to the POS of the last token of prev, and nextFirstPOS refers to the POS of the first token of next.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_8",
  "x": "**IDENTIFYING NON-EXPLICIT RELATIONS** The first step in identifying non-explicit relations is the generation of sentence pairs that are candidate arguments for a non-explicit relation. Following<cite> Wang and Lan (2015)</cite>, we extract sentence pairs that satisfy the following three criteria: \u2022 Sentences are adjacent \u2022 Sentences occur within the same paragraph \u2022 Neither sentence participates in an explicit relation For all pairs of sentences that meet those criteria, we take the first sentence to be the location of Arg1, and the second sentence -the location of Arg2. This approach is quite noisy since about 24% of all consecutive sentence pairs in the training data do not participate in a discourse relation. We leave this for future work. ---------------------------------- **NON-EXPLICIT SENSE CLASSIFIER** Following previous work on non-explicit sense classification (Lin et al., 2009; Pitler et al., 2009; Rutherford and Xue, 2014) , we define four sets of binary feature groups: Brown clustering pairs, Brown clustering arguments, first-last words, and production rules.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_9",
  "x": "Candidate sentences are split into constituents based on punctuation symbols and clause boundaries using the SBAR tag. We use features in Lin et al. (2009) and<cite> Wang and Lan (2015)</cite> and augment these with novel features. Implicit Arg1 Extractor: The Implicit Arg1 extractor employs a rich set of features. Most of these are similar to those presented for PS Arg1 and PS Arg2 extractors in that we take into account POS information, punctuation symbols that occur on the boundaries of the constituents, as well as dependency relations in the constituent itself. One key distinction of how we define the depen-dency relation features is that, in contrast to prior work that treats each dependency relation as a separate binary feature, we only consider the first two relations (r1 and r2, respectively) in curr, prev, and next, and take their conjunctions. Our intuition is that the relations in the beginning of a constituent are most important, while the other relations are not that relevant. This approach to feature generation also avoids sparseness, which was found to be a problem in earlier work. Overall, we generate seven features that use dependency relations. Implicit Arg2 Extractor: We use most of the features in Lin et al. (2014) and<cite> Wang and Lan (2015)</cite> to train the Arg2 extractor (for more details and explanation about the features, we refer the reader to the respective papers):",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_10",
  "x": "Overall, we generate seven features that use dependency relations. Implicit Arg2 Extractor: We use most of the features in Lin et al. (2014) and<cite> Wang and Lan (2015)</cite> to train the Arg2 extractor (for more details and explanation about the features, we refer the reader to the respective papers): \u2022 Lowercased and lemmatized verbs in curr \u2022 The first and last terms of curr \u2022 The last term of prev \u2022 The first term of next \u2022 The last term of prev + the first term of curr \u2022 The last term of curr + the first term of next \u2022 The position of curr in the sentence: start, middle, end, or whole sentence",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_12",
  "x": "The new set of features is presented in Section 3. Evaluation using gold connectives and argument boundaries (no EP). Table 3 : PS Arg1 extractor, no EP. Baseline denotes taking the entire sentence as argument span. ---------------------------------- **MODEL** Base features refer to features used in<cite> Wang and Lan (2015)</cite> . sense classification indicated that Averaged Perceptron should be preferred for these sub-tasks. Due to time constraints, we did not compare all three algorithms on all sub-tasks.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_13",
  "x": "We compare our baseline model that implements the features proposed in<cite> Wang and Lan (2015)</cite> with the model that employs additional features introduced in 4.4. Our baseline model performs slightly better than the one reported in<cite> Wang and Lan (2015)</cite> : we obtain 90.55 vs. 90.14, as reported in<cite> Wang and Lan (2015)</cite> . Table 6 : Evaluation of each component on the development set (no EP). duced. We implement the features in<cite> Wang and Lan (2015)</cite> and add our novel features shown in Table 1 . Results for PS Arg1 extractor are shown in Table 3 . The baseline refers to taking the entire sentence as argument span. Overall, we obtain a 5 point improvement over the baseline method. Similarly, Table 4 shows results for PS Arg2 extractor.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_14",
  "x": "Explicit Sense Classifier: Table 2 evaluates the explicit sense classifier. We compare our baseline model that implements the features proposed in<cite> Wang and Lan (2015)</cite> with the model that employs additional features introduced in 4.4. Our baseline model performs slightly better than the one reported in<cite> Wang and Lan (2015)</cite> : we obtain 90.55 vs. 90.14, as reported in<cite> Wang and Lan (2015)</cite> . Table 6 : Evaluation of each component on the development set (no EP). duced. We implement the features in<cite> Wang and Lan (2015)</cite> and add our novel features shown in Table 1 . Results for PS Arg1 extractor are shown in Table 3 . The baseline refers to taking the entire sentence as argument span. Overall, we obtain a 5 point improvement over the baseline method.",
  "y": "differences"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_15",
  "x": "Explicit Sense Classifier: Table 2 evaluates the explicit sense classifier. We compare our baseline model that implements the features proposed in<cite> Wang and Lan (2015)</cite> with the model that employs additional features introduced in 4.4. Our baseline model performs slightly better than the one reported in<cite> Wang and Lan (2015)</cite> : we obtain 90.55 vs. 90.14, as reported in<cite> Wang and Lan (2015)</cite> . Table 6 : Evaluation of each component on the development set (no EP). duced. We implement the features in<cite> Wang and Lan (2015)</cite> and add our novel features shown in Table 1 . Results for PS Arg1 extractor are shown in Table 3 . The baseline refers to taking the entire sentence as argument span. Overall, we obtain a 5 point improvement over the baseline method.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_16",
  "x": "Results for PS Arg1 extractor are shown in Table 3 . The baseline refers to taking the entire sentence as argument span. Overall, we obtain a 5 point improvement over the baseline method. Similarly, Table 4 shows results for PS Arg2 extractor. For PS Arg2 extractor, the classifiers are able to obtain a larger improvement compared to the baseline method. Adding new features improves the results by three points. We note that in<cite> Wang and Lan (2015)</cite> the numbers that correspond to the entire sentence baselines are not the same as those that we obtain, so we do not report a direct comparison with their models. However, our base models implement the features they use. Implicit Arg1 Extractor: In Table 5 , we evaluate the Implicit Arg1 extractor.",
  "y": "differences"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_0",
  "x": "Natural language texts are, more often than not, a result of a deliberate cognitive effort of an author and as such consist of semantically coherent segments. Text segmentation deals with automatically breaking down the structure of text into such topically contiguous segments, i.e., it aims to identify the points of topic shift (Hearst 1994; Choi 2000; Brants, Chen, and Tsochantaridis 2002; Riedl and Biemann 2012; Du, Buntine, and Johnson 2013; Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> . Reliable segmentation results with texts that are more readable for humans, but also facilitates downstream tasks like automated text summarization (Angheluta, De Busser, and Moens 2002; Bokaei, Sameti, and Liu 2016) , passage retrieval (Huang et al. 2003; Shtekh et al. 2018) , topical classification (Zirn et al. 2016) , or dialog modeling (Manuvinakurike et al. 2016; Zhao and Kawahara 2017) . Text coherence is inherently tied to text segmentationintuitively, the text within a segment is expected to be more coherent than the text spanning different segments. Consider, e.g., the text in Figure 1 , with two topical segments. Snippets T 1 and T 2 are more coherent than T 3 and T 4 : all T 1 sentences relate to Amsterdam's history, and all T 2 sentences to Amsterdam's geography; in contrast, T 3 and T 4 contain sentences Amsterdam is younger than Dutch cities such as Nijmegen, Rotterdam, and Utrecht. Amsterdam was granted city rights in either 1300 or 1306. In the 14th century Amsterdam flourished because of trade with the Hanseatic League. Amsterdam is located in the Western Netherlands.",
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_1",
  "x": "Similarly, a recently proposed state-of-the-art supervised neural segmentation model<cite> (Koshorek et al. 2018</cite> ) directly learns to predict binary sentence-level segmentation decisions and has no explicit mechanism for modeling coherence. In this work, in contrast, we propose a supervised neural model for text segmentation that explicitly takes coherence into account: we augment the segmentation prediction objective with an auxiliary coherence modeling objective. Our proposed model, dubbed Coherence-Aware Text Segmentation (CATS), encodes a sentence sequence using two hierarchically connected Transformer networks (Vaswani et al. 2017; Devlin et al. 2018 ). Similar to<cite> (Koshorek et al. 2018)</cite> , CATS' main learning objective is a binary sentence-level segmentation prediction. However, CATS augments the segmentation objective with an auxiliary coherence-based objec-tive which pushes the model to predict higher coherence for original text snippets than for corrupt (i.e., fake) sentence sequences. We empirically show (1) that even without the auxiliary coherence objective, the Two-Level Transformer model for Text Segmentation (TLT-TS) yields state-of-the-art performance across multiple benchmarks, (2) that the full CATS model, with the auxiliary coherence modeling, further significantly improves the segmentation, and (3) that both TLT-TS and CATS are robust in domain transfer. Furthermore, we demonstrate models' effectiveness in zero-shot language transfer. Coupled with a cross-lingual word embedding space, 1 our models trained on English Wikipedia successfully segment texts from unseen languages, outperforming the best-performing unsupervised segmentation model (Glava\u0161, Nanni, and Ponzetto 2016) by a wide margin. CATS: Coherence-Aware Two-Level Transformer for Text Segmentation Figure 2 illustrates the high-level architecture of the CATS model.",
  "y": "differences background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_2",
  "x": "Similar to<cite> (Koshorek et al. 2018)</cite> , CATS' main learning objective is a binary sentence-level segmentation prediction. However, CATS augments the segmentation objective with an auxiliary coherence-based objec-tive which pushes the model to predict higher coherence for original text snippets than for corrupt (i.e., fake) sentence sequences. We empirically show (1) that even without the auxiliary coherence objective, the Two-Level Transformer model for Text Segmentation (TLT-TS) yields state-of-the-art performance across multiple benchmarks, (2) that the full CATS model, with the auxiliary coherence modeling, further significantly improves the segmentation, and (3) that both TLT-TS and CATS are robust in domain transfer. Furthermore, we demonstrate models' effectiveness in zero-shot language transfer. Coupled with a cross-lingual word embedding space, 1 our models trained on English Wikipedia successfully segment texts from unseen languages, outperforming the best-performing unsupervised segmentation model (Glava\u0161, Nanni, and Ponzetto 2016) by a wide margin. CATS: Coherence-Aware Two-Level Transformer for Text Segmentation Figure 2 illustrates the high-level architecture of the CATS model. A snippet of text -a sequence of sentences of fixed length -is an input to the model. Token encodings are a concatenation of a pretrained word embedding and a positional embedding. Sentences are first encoded from their tokens with a token-level Transformer (Vaswani et al. 2017 ).",
  "y": "similarities differences"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_3",
  "x": [
   "**DATA** WIKI-727K Corpus. Koshorek et al. (2018) leveraged the manual structuring of Wikipedia pages into sections to automatically create a large segmentation-annotated corpus. WIKI-727K consists of 727,746 documents created from English (EN) Wikipedia pages, divided into training (80%), development (10%), and test portions (10%). We train, optimize, and evaluate our models on respective portions of the WIKI-727K dataset. Standard Test Corpora. Koshorek et al. (2018) additionally created a small evaluation set WIKI-50 to allow for comparative evaluation against unsupervised segmentation models, e.g., the GRAPHSEG model of Glava\u0161, Nanni, and Ponzetto (2016) , for which evaluation on large datasets is prohibitively slow. For years, the synthetic dataset of Choi (2000) was used as a standard becnhmark for text segmentation models. CHOI dataset contains 920 documents, each of which is a concatenation of 10 paragraphs randomly sampled from the Brown corpus."
  ],
  "y": "uses background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_4",
  "x": [
   "WIKI-727K consists of 727,746 documents created from English (EN) Wikipedia pages, divided into training (80%), development (10%), and test portions (10%). We train, optimize, and evaluate our models on respective portions of the WIKI-727K dataset. Standard Test Corpora. Koshorek et al. (2018) additionally created a small evaluation set WIKI-50 to allow for comparative evaluation against unsupervised segmentation models, e.g., the GRAPHSEG model of Glava\u0161, Nanni, and Ponzetto (2016) , for which evaluation on large datasets is prohibitively slow. For years, the synthetic dataset of Choi (2000) was used as a standard becnhmark for text segmentation models. CHOI dataset contains 920 documents, each of which is a concatenation of 10 paragraphs randomly sampled from the Brown corpus. CHOI dataset is divided into subsets containing only documents with specific variability of segment lengths (e.g., segments with 3-5 or with 9-11 sentences). 7 Finally, we evaluate the performance of our models on two small datasets, CITIES and ELEMENTS, created by Chen et al. (2009) from Wikipedia pages dedicated to the cities of the world and chemical elements, respectively. Other Languages."
  ],
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_5",
  "x": "In order to test the performance of our Transformer-based models in zero-shot language transfer setup, we prepared small evaluation datasets in other languages. Analogous to the WIKI-50 dataset created by<cite> Koshorek et al. (2018)</cite> from English (EN) Wikipedia, we created WIKI-50-CS, WIKI-50-FI, and WIKI-50-TR datasets consisting of 50 randomly selected pages from Czech (CS), Finnish (FI), and Turkish (TR) Wikipedia, respectively. 8 ---------------------------------- **COMPARATIVE EVALUATION** Evaluation Metric. Following previous work (Riedl and Biemann 2012; Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we also adopt the standard text segmentation measure P k (Beeferman, Berger, and Lafferty 1999) as our evaluation metric. P k score is the probability that a model makes a wrong prediction as to whether the first and last sentence of a randomly sampled snippet of k sentences belong to the same segment (i.e., the probability of the model predicting the same segment for the sentences from different segment or different segments for the sentences from the same segment). Following (Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we set k to the half of the average ground truth segment size of the dataset.",
  "y": "similarities extends"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_6",
  "x": "---------------------------------- **COMPARATIVE EVALUATION** Evaluation Metric. Following previous work (Riedl and Biemann 2012; Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we also adopt the standard text segmentation measure P k (Beeferman, Berger, and Lafferty 1999) as our evaluation metric. P k score is the probability that a model makes a wrong prediction as to whether the first and last sentence of a randomly sampled snippet of k sentences belong to the same segment (i.e., the probability of the model predicting the same segment for the sentences from different segment or different segments for the sentences from the same segment). Following (Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we set k to the half of the average ground truth segment size of the dataset. Baseline Models. We compare CATS against the state-ofthe-art neural segmentation model of<cite> Koshorek et al. (2018)</cite> and against GRAPHSEG (Glava\u0161, Nanni, and Ponzetto 2016) , the state-of-the-art unsupervised text segmentation model. Additionally, as a sanity check, we evaluate the RANDOM baseline -it assigns a positive segmentation label to a sentence with the probability that corresponds to the ratio of the total number of segments (according to the gold segmentation) and total number of sentences in the dataset.",
  "y": "uses"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_7",
  "x": "**COMPARATIVE EVALUATION** Evaluation Metric. Following previous work (Riedl and Biemann 2012; Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we also adopt the standard text segmentation measure P k (Beeferman, Berger, and Lafferty 1999) as our evaluation metric. P k score is the probability that a model makes a wrong prediction as to whether the first and last sentence of a randomly sampled snippet of k sentences belong to the same segment (i.e., the probability of the model predicting the same segment for the sentences from different segment or different segments for the sentences from the same segment). Following (Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we set k to the half of the average ground truth segment size of the dataset. Baseline Models. We compare CATS against the state-ofthe-art neural segmentation model of<cite> Koshorek et al. (2018)</cite> and against GRAPHSEG (Glava\u0161, Nanni, and Ponzetto 2016) , the state-of-the-art unsupervised text segmentation model. Additionally, as a sanity check, we evaluate the RANDOM baseline -it assigns a positive segmentation label to a sentence with the probability that corresponds to the ratio of the total number of segments (according to the gold segmentation) and total number of sentences in the dataset. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_8",
  "x": "Evaluation Metric. Following previous work (Riedl and Biemann 2012; Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we also adopt the standard text segmentation measure P k (Beeferman, Berger, and Lafferty 1999) as our evaluation metric. P k score is the probability that a model makes a wrong prediction as to whether the first and last sentence of a randomly sampled snippet of k sentences belong to the same segment (i.e., the probability of the model predicting the same segment for the sentences from different segment or different segments for the sentences from the same segment). Following (Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we set k to the half of the average ground truth segment size of the dataset. Baseline Models. We compare CATS against the state-ofthe-art neural segmentation model of<cite> Koshorek et al. (2018)</cite> and against GRAPHSEG (Glava\u0161, Nanni, and Ponzetto 2016) , the state-of-the-art unsupervised text segmentation model. Additionally, as a sanity check, we evaluate the RANDOM baseline -it assigns a positive segmentation label to a sentence with the probability that corresponds to the ratio of the total number of segments (according to the gold segmentation) and total number of sentences in the dataset. ---------------------------------- **MODEL CONFIGURATION**",
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_10",
  "x": "We then report and analyze models' performance in the cross-lingual zero-shot transfer experiments. 9 https://tinyurl.com/y6j4gh9a 10 Given the large hyperparameter space and large training set, we only searched over a limited-size grid of hyperparameter configurations. It is thus likely that a better-performing configuration than the one reported can be found with a more extensive grid search. 11 We do not tune other transformer hyperparameters, but rather adopt the recommended values from (Vaswani et al. 2017) : filter size of 1024 and dropout probabilities of 0.1 for both attention layers and feed-forward ReLu layers. Table 1 shows models' performance on five EN evaluation datasets. Both our Transformer-based models -TLT-TS and CATS -outperform the competing supervised model of<cite> Koshorek et al. (2018)</cite> , a hierarchical encoder based on recurrent components, across the board. The improved performance that TLT-TS has with respect to the model of<cite> Koshorek et al. (2018)</cite> is consistent with improvements that Transformer-based architectures yield in comparison with models based on recurrent components in other NLP tasks (Vaswani et al. 2017; Devlin et al. 2018) . The gap in performance is particularly wide (>20 P k points) for the EL-EMENTS dataset. Evaluation on the ELEMENTS test set is, arguably, closest to a true domain-transfer setting: 12 while the train portion of the WIKI-727K set contains pages similar in type to those found in WIKI-50 and CITIES test sets, it does not contain any Wikipedia pages about chemical elements (all such pages are in the ELEMENTS test set).",
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_12",
  "x": "Moreover,<cite> Koshorek et al. (2018)</cite> report human performance on the WIKI-50 dataset of 14.97, which is a mere one P k point better than the performance of our coherence-aware CATS model. The unsupervised GRAPHSEG model of Glava\u0161, Nanni, and Ponzetto (2016) seems to outperform all supervised models on the synthetic CHOI dataset. We believe that this is primarily because (1) by being synthetic, the CHOI dataset can be accurately segmented based on simple lexical overlaps and word embedding similarities (and GRAPHSEG relies on similarities between averaged word embeddings) and because (2) by being trained on a much more challenging real-world WIKI-727K dataset -on which lexical overlap is insufficient for accurate segmentation -supervised models learn to segment based on deeper natural language understanding (and learn not to encode lexical overlap as reliable segmentation signal). Additionally, GRAPHSEG is evaluated separately on each subset of the CHOI dataset, for each of which it is provided the (gold) minimal segment size, which further facilitates and improves its predicted segmentations. ---------------------------------- **ZERO-SHOT CROSS-LINGUAL TRANSFER** In Table 2 we show the results of our zero-shot cross-lingual transfer experiments. In this setting, we use our Transformerbased models, trained on the English WIKI-727K dataset, to segment texts from the WIKI-50-X (X \u2208 {CS, FI, TR}) datasets in other languages. As a baseline, we additionally evaluate GRAPHSEG (Glava\u0161, Nanni, and Ponzetto 2016) , as a language-agnostic model requiring only pretrained word embeddings of the test language as input.",
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_13",
  "x": [
   "Because our CATS model has an auxiliary coherencebased objective, we additionally provide a brief overview of research on modeling text coherence. ---------------------------------- **TEXT SEGMENTATION** Text segmentation tasks come in two main flavors: (1) linear (i.e., sequential) text segmentation and (2) hierarchical segmentation in which top-level segments are further broken down into sub-segments. While the hierarchical segmentation received a non-negligible research attention (Yaari 1997; Eisenstein 2009; Du, Buntine, and John-son 2013) , the vast majority of the proposed models (including this work) focus on linear segmentation (Hearst 1994; Beeferman, Berger, and Lafferty 1999; Choi 2000; Brants, Chen, and Tsochantaridis 2002; Misra et al. 2009; Riedl and Biemann 2012; Glava\u0161, Nanni, and Ponzetto 2016; Koshorek et al. 2018, inter alia) . In one of the pioneering segmentation efforts, Hearst (1994) proposed an unsupervised TextTiling algorithm based on the lexical overlap between adjacent sentences and paragraphs. Choi (2000) computes the similarities between sentences in a similar fashion, but renormalizes them within the local context; the segments are then obtained through divisive clustering. Utiyama and Isahara (2001) and Fragkou, Petridis, and Kehagias (2004) minimize the segmentation cost via exhaustive search with dynamic programming. Following the assumption that topical cohesion guides the segmentation of the text, a number of segmentation approaches based on topic models have been proposed."
  ],
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_14",
  "x": "Finally,<cite> Koshorek et al. (2018)</cite> identify Wikipedia as a free large-scale source of manually segmented texts that can be used to train a supervised segmentation model. They train a neural model that hierarchically combines two bidirectional LSTM networks and report massive improvements over unsupervised segmentation on a range of evaluation datasets. The model we presented in this work has a similar hierarchical architecture, but uses Transfomer networks instead of recurrent encoders. Crucially, CATS additionally defines an auxiliary coherence objective, which is coupled with the (primary) segmentation objective in a multi-task learning model. ---------------------------------- **TEXT COHERENCE** Measuring text coherence amounts to predicting a score that indicates how meaningful the order of the information in the text is. The majority of the proposed text coherence models are grounded in formal theories of text coherence, among which the entity grid model (Barzilay and Lapata 2008) , based on the centering theory of Grosz, Weinstein, and Joshi (1995) , is arguably the most popular. The entity grid model represent texts as matrices encoding the grammatical roles that the same entities have in different sentences.",
  "y": "motivation differences background"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_0",
  "x": "Word embedding has been extensively studied in recent years (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012) . Following the idea that the meaning of a word can be determined by 'the company it keeps' (Baroni and Zamparelli, 2010) , i.e., the words that it co-occurs with, word embedding projects discrete words to a low-dimensional and continuous vector space where co-occurred words are located close to each other. Compared to conventional discrete representations (e.g., the one-hot encoding), word embedding provides more robust representations for words, particulary for those that infrequently appear in the training data. More importantly, the embedding encodes syntactic and semantic content implicitly, so that relations among words can be simply computed as the distances among their embeddings, or word vectors. A well-known efficient word embedding approach was recently proposed by (Mikolov et al., 2013a) , where two log-linear models (CBOW and skip-gram) are proposed to learn the neighboring relation of words in context. A following work proposed by the same authors introduces some modifications that largely improve the efficiency of model training (Mikolov et al., 2013c ). An interesting property of word vectors learned by the log-linear model is that the relations among relevant words seem linear and can be computed by simple vector addition and substraction (Mikolov et al., 2013d) . For example, the following relation approximately holds in the word vector space: ParisFrance + Rome = Italy. In<cite> (Mikolov et al., 2013b)</cite> , the linear relation is extended to the bilingual scenario, where a linear transform is learned to project semantically identical words from one language to another.",
  "y": "background"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_1",
  "x": "Finally, the cosine distance is used when we train the orthogonal transform, in order to achieve full consistence. ---------------------------------- **RELATED WORK** This work largely follows the methodology and experimental settings of<cite> (Mikolov et al., 2013b)</cite> , while we normalize the embedding and use an orthogonal transform to conduct bilingual translation. Multilingual learning can be categorized into projection-based approaches and regularizationbased approaches. In the projection-based approaches, the embedding is performed for each language individually with monolingual data, and then one or several projections are learned using multilingual data to represent the relation between languages. Our method in this paper and the linear projection method in <cite>(Mikolov et al., 2013b</cite> ) both belong to this category. Another interesting work proposed by (Faruqui and Dyer, 2014) learns linear transforms that project word vectors of all languages to a common low-dimensional space, where the correlation of the multilingual word pairs is maximized with the canonical correlation analysis (CCA). The regularization-based approaches involve the multilingual constraint in the objective function for learning the embedding.",
  "y": "uses similarities"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_2",
  "x": "Finally, the cosine distance is used when we train the orthogonal transform, in order to achieve full consistence. ---------------------------------- **RELATED WORK** This work largely follows the methodology and experimental settings of<cite> (Mikolov et al., 2013b)</cite> , while we normalize the embedding and use an orthogonal transform to conduct bilingual translation. Multilingual learning can be categorized into projection-based approaches and regularizationbased approaches. In the projection-based approaches, the embedding is performed for each language individually with monolingual data, and then one or several projections are learned using multilingual data to represent the relation between languages. Our method in this paper and the linear projection method in <cite>(Mikolov et al., 2013b</cite> ) both belong to this category. Another interesting work proposed by (Faruqui and Dyer, 2014) learns linear transforms that project word vectors of all languages to a common low-dimensional space, where the correlation of the multilingual word pairs is maximized with the canonical correlation analysis (CCA). The regularization-based approaches involve the multilingual constraint in the objective function for learning the embedding.",
  "y": "similarities"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_3",
  "x": "Solving this problem by Lagrange multipliers is possible, but here we simply divide a vector by its l-2 norm whenever the vector is updated. This does not involve much code change and is efficient enough. 1 The consequence of the normalization is that all the word vectors are located on a hypersphere, as illustrated in Figure 1 . In addition, by the normalization, the inner product falls back to the cosine distance, hence solving the inconsistence between the embedding learning and the distance measurement. ---------------------------------- **ORTHOGONAL TRANSFORM** The bilingual word translation provided by <cite>(Mikolov et al., 2013b</cite> ) learns a linear transform from the source language to the target language by the linear regression. The objective function is as follows: 1 For efficiency, this normalization can be conducted every n mini-batches.",
  "y": "background"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_4",
  "x": "The objective function is as follows: 1 For efficiency, this normalization can be conducted every n mini-batches. The performance is expected to be not much impacted, given that n is not too large. where W is the projection matrix to be learned, and x i and z i are word vectors in the source and target language respectively. The bilingual pair (x i , z i ) indicates that x i and z i are identical in semantic meaning. A high accuracy was reported on a word translation task, where a word projected to the vector space of the target language is expected to be as close as possible to its translation<cite> (Mikolov et al., 2013b)</cite> . However, we note that the 'closeness' of words in the projection space is measured by the cosine distance, which is fundamentally different from the Euler distance in the objective function (3) and hence causes inconsistence. We solve this problem by using the cosine distance in the transform learning, so the optimization task can be redefined as follows: Note that the word vectors in both the source and target vector spaces are normalized, so the inner product in (4) is equivalent to the cosine distance.",
  "y": "background"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_5",
  "x": "We first present the data profile and configurations used to learn monolingual word vectors, and then examine the learning quality on the word similarity task. Finally, a comparative study is reported on the bilingual word translation task, with Mikolov's linear transform and the orthogonal transform proposed in this paper. ---------------------------------- **MONOLINGUAL WORD EMBEDDING** The monolingual word embedding is conducted with the data published by the EMNLP 2011 SMT workshop (WMT11) 2 . For an easy comparison, we largely follow Mikolov's settings in<cite> (Mikolov et al., 2013b)</cite> and set English and Spanish as the source and target language, respectively. The data preparation involves the following steps. Firstly, the text was tokenized by the standard scripts provided by WMT11 3 , and then duplicated sentences were removed. The numerical expressions were tokenized as 'NUM', and special characters (such as !?,:) were removed.",
  "y": "uses similarities"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_6",
  "x": "The proportions that the correct translations are in the top 1 and top 5 candidate list are reported as P@1 and P@5 respectively. As can be seen, the best dimension setting is 800 for English and 200 for Spanish, and the corresponding P@1 and P@5 are 35.36% and 53.96%, respectively. These results are comparable with the results reported in <cite>(Mikolov et al., 2013b</cite> ---------------------------------- **RESULTS WITH ORTHOGONAL TRANSFORM** The results with the normalized word vectors and the orthogonal transform are reported in Table 2 . It can be seen that the results with the orthogonal transform are consistently better than those reported in Table1 which are based on the linear transform. This confirms our conjecture that bilingual translation can be largely improved by the normalized embedding and the accompanied orthogonal transform. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_0",
  "x": "**PARAPHRASES FROM DISTRIBUTIONAL PROFILES** A distributional profile (DP) of a word or phrase was first proposed in (Rapp, 1995) for SMT. Given a word f , its distributional profile is: V is the vocabulary and the surrounding words w i are taken from a monolingual corpus using a fixed window size. We use a window size of 4 words based on the experiments in<cite> (Razmara et al., 2013)</cite> . DPs need an association measure A(\u00b7, \u00b7) to compute distances between potential paraphrases. A comparison of different association measures appears in (Marton et al., 2009; <cite>Razmara et al., 2013</cite>; Saluja et al., 2014) and our preliminary experiments validated the choice of the same association measure as in these papers, namely Pointwise Mutual Information (Lin, 1998) (PMI) . For each potential context word w i : To evaluate the similarity between two phrases we use cosine similarity.",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_1",
  "x": "We use a window size of 4 words based on the experiments in<cite> (Razmara et al., 2013)</cite> . DPs need an association measure A(\u00b7, \u00b7) to compute distances between potential paraphrases. A comparison of different association measures appears in (Marton et al., 2009; <cite>Razmara et al., 2013</cite>; Saluja et al., 2014) and our preliminary experiments validated the choice of the same association measure as in these papers, namely Pointwise Mutual Information (Lin, 1998) (PMI) . For each potential context word w i : To evaluate the similarity between two phrases we use cosine similarity. The cosine coefficient of two phrases f 1 and f 2 is: where V is the vocabulary. Note that in Eqn. (2) w i 's are the words that appear in the context of f 1 or f 2 , otherwise the PMI values would be zero.",
  "y": "similarities"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_2",
  "x": "Thus, we use the heuristic applied in previous works (Marton et al., 2009; <cite>Razmara et al., 2013</cite>; Saluja et al., 2014) to reduce the search space. For each phrase we keep candidate paraphrases which appear in one of the surrounding context (e.g. Left Right) among all occurrences of the phrase. ---------------------------------- **PARAPHRASES FROM BILINGUAL PIVOTING** Bilingual pivoting uses parallel corpora between the source language, F , and a pivot language T . If two phrases, f 1 and f 2 , in a same language are paraphrases, then they share a translation in other languages with p(f 1 |f 2 ) as a paraphrase score: (1 and 6) are phrases from the SMT phrase table (unfilled nodes are not). Edge weights are set using a log-linear combination of scores from PPDB. Phrase #6 has different senses ('gold' or 'left'); and it has a paraphrase in phrase #7 for the 'gold' sense and a paraphrase in phrase #2 for the 'left' sense. After propagation, phrase #2 receives translation candidates from phrase #6 and phrase #1 reducing the probability of translation from unrelated senses (like the 'gold' sense).",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_3",
  "x": "It leverages syntactic information and other resources to filters and scores each paraphrase pair using a large set of features. These features can be used by a log linear model to score paraphrases (Zhao et al., 2008) . We used a linear combination of these features using the equation in Sec. 3 of (Ganitkevitch and Callison-Burch, 2014) to score paraphrase pairs. PPDB version 1 is broken into different levels of coverage. The smaller sizes contain only better-scoring, high-precision paraphrases, while larger sizes aim for high coverage. ---------------------------------- **METHODOLOGY** After paraphrase extraction we have paraphrase pairs, (f 1 , f 2 ) and a score S(f 1 , f 2 ) we can induce new translation rules for OOV phrases using the steps in Algo. (1): 1) A graph of source phrases is constructed as in<cite> (Razmara et al., 2013)</cite> ; 2) translations are propagated as labels through the graph as explained in Fig. 2 ; and 3) new translation rules obtained from graph-propagation are integrated with the original phrase table.",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_4",
  "x": "In this objective function, \u00b5 i and P i,v are hyperparameters (\u2200v : \u03a3 i P i,v = 1). R v \u2208 \u2206 m+1 is our prior belief about labeling. First component of the function tries to minimize the difference of new distribution to the original distribution for the seed nodes. The second component insures that nearby neighbours have similar distributions, and the final component is to make sure that the distribution does not stray from a prior distribution. At the end of propagation, we wish to find a label distribution for our OOV phrases. We describe in Sec. 4.2.2 the reasons for choosing MAD over other graph propagation algorithms. The MAD graph propagation generalizes the approach used in<cite> (Razmara et al., 2013)</cite> . The Structured Label Propagation algorithm (SLP) was used in (Saluja et al., 2014; Zhao et al., 2015) which uses a graph structure on the target side phrases as well. However, we have found that in our diverse experimental settings (see Sec. 5) MAD had two properties we needed compared to SLP: one was the use of graph random walks which allowed us to control translation candidates and MAD also has the ability to penalize nodes with a large number of edges (also see Sec. 4.2.2).",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_5",
  "x": "---------------------------------- **PRE-STRUCTURING THE GRAPH** Razmara et al. (2013) avoid a fully connected graph structure. They pre-structure the graph into bipartite graphs (only connections between phrases with known translation and OOV phrases) and tripartite graphs (connections can also go from a known phrasal node to an OOV phrasal node through one node that is a paraphrase of both but does not have translations, i.e. it is an unlabeled node). In these pre-structured graphs there are no connections between nodes of the same type (known, OOV or unlabeled). We apply this method in our low resource setting experiments (Sec. 5.3) to compare our bipartite and tripartite results to<cite> Razmara et al. (2013)</cite> . In the rest of the experiments we use the tripartite approach since it outperforms the bipartite approach. ---------------------------------- **GRAPH RANDOM WALKS**",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_6",
  "x": "In each case, we compare results of using paraphrases extracted by Distributional Profile (DP) and PPDB in an end-to-end SMT system. Important: no subset of the test data sentences are used in the bilingual corpora for paraphrase extraction process. ---------------------------------- **EXPERIMENTAL SETUP** We use CDEC 1 (Dyer et al., 2010) as an endto-end SMT pipeline with its standard features 2 . fast align (Dyer et al., 2013 ) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA (Crammer and Singer, 2003) . This setup is used for most of our experiments: oracle (Sec. 5.2), domain adaptation (Sec. 5.4) and morphologically complex languages (Sec. 5.5). But as we wish to fairly compare our approach with<cite> Razmara et al. (2013)</cite> on low resource setting, we follow their setup in Sec. 5.3: Moses (Koehn et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Och, 2003) for tuning. We add our own feature to the SMT log-linear model as described in Sec. KenLM (Heafield, 2011 ) is used to train a 5-gram language model on English Gigaword (V5: LDC2011T07). For scalable graph propagation we use the Junto framework 3 .",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_7",
  "x": "WMT 2011 and WMT 2012 are used as dev and test data respectively. Table 4 shows the results in terms of BLEU on dev and test. The first row is baseline which simply copies OOVs to output. The second and third rows show the result of augmenting phrase-table by adding translations for single-word OOVs and phrases containing OOVs. The last row shows the oracle result where dev and test sentences exist inside the training data and all the OOVs are known (Fully observers cannot avoid model and search errors). ---------------------------------- **CASE 1: LIMITED PARALLEL DATA** In this experiment we use a setup similar to (Razmara et al., 2013 we use 10K French-English parallel sentences, randomly chosen from Europarl to train translation system, as reported in<cite> (Razmara et al., 2013)</cite> . ACL/WMT 2005 4 is used for dev and test data.",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_0",
  "x": "Stemmers usually remove affixes from the words to present them in the form of their morphological roots. Conventional rule-based stemmers tailor the linguistic knowledge of experts. On the other hand, statistical stemmers provide languageindependent approaches which generally group related words based on various string-similarity measures. Such approaches often involve n-grams; equivalence classes can be formed from words that share the same properties: word-initial letter n-grams, common n-grams throughout the word, or by refining these classes with clustering techniques. This kind of statistical stemming has been shown to be effective for many languages, including English, Turkish, and Malay. For example, Bhat introduced a method for Kannada where the similarity of two words is determined by three distance measures based on prefix and suffix matching and the first mismatch point in the words [2] . Defining precise rules for morphologically complex texts, especially for the purpose of infix removal is sometimes impossible<cite> [5]</cite> . Informal/irregular forms usually do not obey the conventional rules in the languages. For instance, 'khunh' (home) is a frequent form for 'khanh' in Persian conversations or 'goood ' and 'good ' are used interchangeably in English tweets.",
  "y": "motivation"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_1",
  "x": "In highly inflected languages, bilingual dictionaries contain only original forms of the words. Therefore, in dictionary-based CLIR, retrieval systems are obliged either to stem documents and queries, or to leave them intact [8, 4, 12] , or expand the query with inflections. We opted the query expansion approach which is a widely used approach to compensate the shortage of inflections [9, 3, <cite>5]</cite> . We used the following probabilistic framework to this end<cite> [5]</cite> : where q i is a query term and c i is the set of translation candidates provided in a bilingual dictionary for q i . c i is the set of the most probable inflections of the words appeared in c i selected by a tuned threshold. Then, we compute the translation probability of c i,j or c i,j for the given q i . To avoid adding noisy terms, we only compute the joint probabilities between either a pair of translation candidates from the dictionary (c i,j and c i ,j ) or a pair of a candidate from the dictionary and an inflection from the collection (c i,j and c i ,j )<cite> [5]</cite> . Our goal is to findc i using the proposed SS4MCT (i.e. set of top-ranked c i ,j according to p t (c i ,j |c i,j )) and then evaluate its impact on the performance of the CLIR task.",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_2",
  "x": "We used the following probabilistic framework to this end<cite> [5]</cite> : where q i is a query term and c i is the set of translation candidates provided in a bilingual dictionary for q i . c i is the set of the most probable inflections of the words appeared in c i selected by a tuned threshold. Then, we compute the translation probability of c i,j or c i,j for the given q i . To avoid adding noisy terms, we only compute the joint probabilities between either a pair of translation candidates from the dictionary (c i,j and c i ,j ) or a pair of a candidate from the dictionary and an inflection from the collection (c i,j and c i ,j )<cite> [5]</cite> . Our goal is to findc i using the proposed SS4MCT (i.e. set of top-ranked c i ,j according to p t (c i ,j |c i,j )) and then evaluate its impact on the performance of the CLIR task. Figure 1 shows the whole process of extracting rules (off-line part) and the evaluation framework (on-line part). ---------------------------------- **EXPERIMENTS**",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_3",
  "x": "Then, we compute the translation probability of c i,j or c i,j for the given q i . To avoid adding noisy terms, we only compute the joint probabilities between either a pair of translation candidates from the dictionary (c i,j and c i ,j ) or a pair of a candidate from the dictionary and an inflection from the collection (c i,j and c i ,j )<cite> [5]</cite> . Our goal is to findc i using the proposed SS4MCT (i.e. set of top-ranked c i ,j according to p t (c i ,j |c i,j )) and then evaluate its impact on the performance of the CLIR task. Figure 1 shows the whole process of extracting rules (off-line part) and the evaluation framework (on-line part). ---------------------------------- **EXPERIMENTS** ---------------------------------- **EXPERIMENTAL SETUP** The statistics of the collection used for both rule extraction and evaluation is provided in Table 2 .",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_4",
  "x": "The statistics of the collection used for both rule extraction and evaluation is provided in Table 2 . We employed the statistical language modeling framework with Kullback-Leibler similarity measure of Lemur toolkit for our retrieval task. Dirichlet Prior is selected as our document smoothing strategy. Top 30 documents are used for the mixture pseudo-relevance feedback algorithm. Queries are expanded by the top 50 terms generated by the feedback model [14, 6] . We removed Persian stop words from the queries and documents [4, <cite>5]</cite> . We used STeP1 [13] in our stemming process in Persian. We also stem the source English queries in all experiments with the Porter stemmer. We use Google EnglishPersian dictionary 1 as the translation resource.",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_5",
  "x": "Dirichlet Prior is selected as our document smoothing strategy. Top 30 documents are used for the mixture pseudo-relevance feedback algorithm. Queries are expanded by the top 50 terms generated by the feedback model [14, 6] . We removed Persian stop words from the queries and documents [4, <cite>5]</cite> . We used STeP1 [13] in our stemming process in Persian. We also stem the source English queries in all experiments with the Porter stemmer. We use Google EnglishPersian dictionary 1 as the translation resource. Dadashkarimi et al., demonstrated that Google has better coverage compared to other English-Persian dictionaries<cite> [5]</cite> . We have exploited 40 Persian POS tags in our experiments.",
  "y": "background"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_6",
  "x": "To this end we compare the proposed SS4MCT with a number of dictionary-based CLIR methods; the 5-gram truncation method (SPLIT) proposed in [11] , rule-based query expansion (RBQE) based on inflectional/derivation rules from Farazzin machine translator 3 , and the STeP1 stemmer [13] are the morphological processing approaches for the retrieval system. On the other hand, we run another set of experiments without applying any morphological processing method similar to the Persian state-of-the-art CLIR methods. Iterative translation disambiguation (ITD) [11] , joint cross-lingual topical relevance model (JCLTRLM) [7] , top-ranked translation (TOP-1), and the bi-gram coherence translation method (BiCTM), introduced in<cite> [5]</cite> (assume |c i | = 0), are the baselines without any morphological processing units. As shown in Table 3 BiCTM outperforms all the baselines when there is no morphological processing unit. Although the improvement compared to JCLTRLM is not statistically significant, for simplicity we assume this model as a base of comparisons in the next set of experiments. In other words, we study the effect of the morphological processing units on the performance of BiCTM. As shown in Table 3 the performance of the CLIR task degraded when we use the SPLIT approach. It is due to expanding the query with irrelevant tokens (e.g., normal/abnormal ). RBQE suffers from a similar problem to some extent; for example jat is a valid suffix for sabzi (=vegetable) in Persian whereas it is an invalid suffix for ketab (=book).",
  "y": "uses"
 },
 {
  "id": "d5a71358168d262dd1e9734c80234b_0",
  "x": "This can be a problem if one rejects some aspects of that theory. Also one may object to a particular system of annotation because some theories generalize to cover new ground (e.g., new languages) better than others. Nevertheless, advantages of accepting a corpus as standard include the following: It is straight-forward to compare the performance of the set of systems that produce the same form of output, e.g., Penn Treebank-based parsers can be compared in terms of how well they reproduce the Penn Treebank. Alternative systems based on a standard are largely interchangeable. Thus a system that uses one PennTreebank-based parser as a component can easily be adapted to use another better performing PennTreebank-based parser. Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) . It is my view that these advantages outweigh the disadvantages.",
  "y": "background"
 },
 {
  "id": "d5a71358168d262dd1e9734c80234b_1",
  "x": "Nevertheless, advantages of accepting a corpus as standard include the following: It is straight-forward to compare the performance of the set of systems that produce the same form of output, e.g., Penn Treebank-based parsers can be compared in terms of how well they reproduce the Penn Treebank. Alternative systems based on a standard are largely interchangeable. Thus a system that uses one PennTreebank-based parser as a component can easily be adapted to use another better performing PennTreebank-based parser. Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004), English <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) , Czech (Sgall et al., 2004) and German (Baumann et al., 2004).",
  "y": "background"
 },
 {
  "id": "d5a71358168d262dd1e9734c80234b_2",
  "x": "Alternative systems based on a standard are largely interchangeable. Thus a system that uses one PennTreebank-based parser as a component can easily be adapted to use another better performing PennTreebank-based parser. Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004) , English <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) , Czech (Sgall et al., 2004) and German (Baumann et al., 2004) . The sixth, seventh and eighth papers (Baumann et al., 2004; \u00c7 mejrek et al., 2004; Helmreich et al., 2004) explore questions of multilingual annotation of syntax and semantics, beginning to answer the question of how annotation systems can be made compatible across languages. Indeed (Helmreich et al., 2004) explores the question of integration across languages, as well as levels of annotation.",
  "y": "background"
 },
 {
  "id": "d5a71358168d262dd1e9734c80234b_3",
  "x": "It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004) , English <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) , Czech (Sgall et al., 2004) and German (Baumann et al., 2004) . The sixth, seventh and eighth papers (Baumann et al., 2004; \u00c7 mejrek et al., 2004; Helmreich et al., 2004) explore questions of multilingual annotation of syntax and semantics, beginning to answer the question of how annotation systems can be made compatible across languages. Indeed (Helmreich et al., 2004) explores the question of integration across languages, as well as levels of annotation. (Baumann et al., 2004) also describes how a number of different linguistic levels can be related in annotation (pragmatic and prosodic) among two languages (English and German). The ninth and tenth papers (Langone et al., 2004; Zabokrtsk\u00fd and Lopatkov\u00e1, 2004) are respectively about a corpus related to a lexicon and the reverse: a lexicon related to a corpus. This opens up the wider theme of the intergration of a number of different linguistic resources. As the natural language community produces more and more linguistic resources, especially corpora, it seems important to step back and look at the larger picture.",
  "y": "background"
 },
 {
  "id": "d61f75366022f043d4c3a005b5a73d_0",
  "x": "**INTRODUCTION** Word embedding models learn a space of continuous word representations, in which similar words are expected to be close to each other. Traditionally, the term similar refers to semantic similarity (e.g. walking should be close to hiking, and happiness to joy), hence the model performance is usually evaluated using semantic similarity datasets. Recently, several works introduced morphology-driven models motivated by the poor performance of traditional models on morphologically complex words. Such words are often rare, and there is not enough evidence to model them correctly. The morphology-driven models allow pooling evidence from different words which have the same base form. These models work by learning per-morpheme representations rather than just per-word ones, and compose the representing vector of each word from those of its morphemes -as derived from a supervised or unsupervised morphological analysis -and (optionally) its surface form (e.g. walking = f (v walk , v ing , v walking )). The works differ in the way they acquire morphological knowledge (from using linguistically derived morphological analyzers on one end, to approximating morphology using substrings while relying on the concatenative nature of morphology, on the other) and in the model form (cDSMs (Lazaridou et al., 2013) , RNN (Luong et al., 2013) , LBL (Botha and Blunsom, 2014) , CBOW (Qiu et al., 2014) , SkipGram (Soricut and Och, 2015; <cite>Bojanowski et al., 2016)</cite> , GGM (Cotterell et al., 2016) ). But essentially, they all show that breaking a word into morphological components (base form, affixes and potentially also the complete surface form), learning a vector for each component, and representing a word as a composition of these vectors improves the models semantic performance, especially on rare words.",
  "y": "background"
 },
 {
  "id": "d61f75366022f043d4c3a005b5a73d_1",
  "x": "Our model form is a generalization of the fastText model<cite> (Bojanowski et al., 2016)</cite> , which in turn extends the skip-gram model of Mikolov et al (2013) . The skip-gram model takes a sequence of words w 1 , ..., w T and a function s assigning scores to (word, context) pairs, and maximizes where \u2113 is the log-sigmoid loss function, C t is a set of context words, and N t is a set of negative examples sampled from the vocabulary. s(w t , w c ) is defined as s(w t , w c ) = v \u22a4 wt u wc (where v wt and u wc are the embeddings of the focus and the context words). Bojanowski et al (2016) replace the word representation v wt with the set of character ngrams appearing in it: v wt = g\u2208G(wt) v g where G(w t ) is the set of n-grams appearing in w t . The n-grams are used to approximate the morphemes in the target word. We generalize<cite> Bojanowski et al (2016)</cite> by replacing the set of ngrams G(w) with a set P(w) of explicit linguistic properties. Each word w t is then composed as the sum of the vectors of its linguistic properties: v wt = p\u2208P(wt) v p . The linguistic properties we consider are the surface form of the word (W), it's lemma (L) and its morphological tag (M) 1 .",
  "y": "extends"
 },
 {
  "id": "d61f75366022f043d4c3a005b5a73d_2",
  "x": "To the best of our knowledge, this work is the first to evaluate both aspects. While our experiments focus on Modern Hebrew due to the availability of a reliable semantic similarity dataset, we believe our conclusions hold more generally. ---------------------------------- **MODELS** Our model form is a generalization of the fastText model<cite> (Bojanowski et al., 2016)</cite> , which in turn extends the skip-gram model of Mikolov et al (2013) . The skip-gram model takes a sequence of words w 1 , ..., w T and a function s assigning scores to (word, context) pairs, and maximizes where \u2113 is the log-sigmoid loss function, C t is a set of context words, and N t is a set of negative examples sampled from the vocabulary. s(w t , w c ) is defined as s(w t , w c ) = v \u22a4 wt u wc (where v wt and u wc are the embeddings of the focus and the context words). Bojanowski et al (2016) replace the word representation v wt with the set of character ngrams appearing in it: v wt = g\u2208G(wt) v g where G(w t ) is the set of n-grams appearing in w t .",
  "y": "similarities extends"
 },
 {
  "id": "d61f75366022f043d4c3a005b5a73d_3",
  "x": "---------------------------------- **EXPERIMENTS AND RESULTS** Our implementation is based on the fastText 2 library<cite> (Bojanowski et al., 2016)</cite> , which we modify as described above. We train the models on the Hebrew Wikipedia (\u223c4M sentences), using a window size of 2 to each side of the focus word, and dimensionality of 200. We use the morphological disambiguator of Adler (2007) to assign words with their morphological tags, and the inflection dictionary of MILA (Itai and Wintner, 2008) Semantic Evaluation Measure The common datasets for semantic similarity 4 have some notable shortcomings as noted in (Avraham and Goldberg, 2016; Faruqui et al., 2016; Batchkarov et al., 2016; Linzen, 2016) . We use the evaluation method (and corresponding Hebrew similarity dataset) that we have introduced in a previous work (Avraham and Goldberg, 2016) (AG). The AG method defines an annotation task which is more natural for human judges, resulting in datasets with improved annotator-agreement scores. Furthermore, the AG's evaluation metric takes annotator agreement into account, by putting less weight on similarities that have lower annotator agreement. An AG dataset is a collection of target-groups, where each group contains a target word (e.g. singer) and three types of candidate words: positives which are words \"similar\" to the target (e.g. musician), distractors which are words \"related but dissimilar\" to the target (e.g. microphone), and randoms which are not related to the target at all (e.g laptop).",
  "y": "extends"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_0",
  "x": "---------------------------------- **INTRODUCTION** Spontaneous speech typically contains a significant amount of variation, which makes it difficult to model in automatic speech recognition (ASR) systems. Such variability stems from varying speakers, pronunciation variations, speaker stylistic differences, varying recording conditions and many other factors. Recognizing words from conversational telephone speech (CTS) can be quite difficult due to the spontaneous nature of speech, its informality, speaker variations, hesitations, disfluencies etc. The Switchboard and Fisher [1] data collections are large collection of CTS datasets that have been used extensively by researchers working on conversational speech recognition [2, 3, 4, 5, 6] . Recent trends in speech recognition [7, <cite>8,</cite> 9] have demonstrated impressive performance on Switchboard and Fisher data. Deep neural network (DNN) based acoustic modeling has become the state-of-the-art in automatic speech recognition (ASR) systems [10, 11] . It has demonstrated impressive performance gains for almost all tried languages and ___________________________________________________________ *The author performed this work while at SRI International and is currently working at Apple Inc. acoustic conditions.",
  "y": "background"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_1",
  "x": "It has demonstrated impressive performance gains for almost all tried languages and ___________________________________________________________ *The author performed this work while at SRI International and is currently working at Apple Inc. acoustic conditions. Advanced variants of DNNs, such as convolutional neural nets (CNNs) [12] , recurrent neural nets (RNNs) [13] , long short-term memory nets (LSTMs) [14] , time-delay neural nets (TDNNs) [15, 29] , <cite>VGG-nets</cite> <cite>[8]</cite> , have significantly improved recognition performance, bringing them closer to human performance [9] . Both abundance of data and sophistication of deep learning algorithms have symbiotically contributed to the advancement of speech recognition performance. The role of acoustic features has not been explored in comparable detail, and their potential contribution to performance gains is unknown. This paper focuses on acoustic features and investigates how their selection improves recognition performance using benchmark training datasets: Switchboard and Fisher, when evaluated on the NIST 2000 CTS test set [2] . We investigated a traditional CNN model and explored the following: (1) Use of multiple features both in isolation and in combination. Our experiments demonstrated that the use of feature combinations helped to improve performance over individual features in isolation and over traditionally used mel-filterbank (MFB) features. Articulatory features were found to be useful for improving recognition performance on both Switchboard and CallHome subsets of the NIST 2000 CTS test set. These findings indicate that the use of better acoustic features can help improve speech recognition performance when using standard acoustic modeling techniques, and can demonstrate performance as good as those obtained from more sophisticated acoustic models that exploit temporal memory.",
  "y": "background"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_2",
  "x": "These findings indicate that the use of better acoustic features can help improve speech recognition performance when using standard acoustic modeling techniques, and can demonstrate performance as good as those obtained from more sophisticated acoustic models that exploit temporal memory. For the sake of simplicity, we used a CNN acoustic model in our experiment, where the baseline system's performance is directly comparable to the state-of-the-art CNN performance reported in <cite>[8]</cite> . We expect our results using the CNN to carry over into other neural network architectures as well. The outline of the paper is as follows. In Section 2 we present the dataset and the recognition task. In Section 3 we describe the acoustic features and the articulatory features that were used in our experiments. Section 4 presents the acoustic and language models used in our experiments, followed by experimental results in Section 5 and conclusion and future directions in Section 6. ---------------------------------- **DATA AND TASK**",
  "y": "uses"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_3",
  "x": "Table 1 shows that the performance of fMLLR transforms learned from the cepstral version of the features are better than the ones directly from the filterbank features, which is expected, as the cepstral features are uncorrelated, which adheres to the diagonal covariance assumption of the GMM models used to learn those transforms. Table 1 demonstrates that the fMLLR transformed features always performed better than the features without fMLLR transform. Also, the CNN models always gave better results, confirming similar observations from studies reported earlier <cite>[8]</cite> . Also, note that Table 1 shows that the DOC features performed slightly better than the MFB features after the fMLLR transform, where the performance improvement was more pronounced for the CH subset of the NIST 2000 CTS test set. As a next step, we investigated the efficacy of feature combination and focused only on the CNN acoustic models. We appended the articulatory features (TVs) extracted from the SWB training set, dev04 and NIST 2000 CTS test sets, and combined them with MFB+fMLLR and DOC+fMLLR features, respectively. Finally, we combined the MFB+fMLLR and DOC+fMLLR features and added the TVs to them. Table 2 Table 2 shows that the use of articulatory features helped to lower the WER in all the cases. The DOC feature was always found to perform slightly better than the MFBs and the best results were obtained when all the features were combined together, indicating the benefit of using multiview features.",
  "y": "background"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_4",
  "x": "Note that the MFB DNN baseline model was used to generate the alignments for the FSH part of the 2000 hours CTS training set and as a consequence the number of senone labels remained the same as the 360-hour SWB models. Table 3 presents the results from the 2000 hours CTS trained models. The model configurations and their parameter size were kept the same as the 360-hour SWB models. Figure 3 shows that the use of the additional FSH training data resulted in significant performance improvement for both SWB and the CH subsets of the NIST 2000 CTS test set. Adding the FSH dataset resulted in relative WER reduction of 4.4% and 12% respectively for SWB and CH subsets of the NIST 2000 CTS test set, using MFB+fMLLR features. Similar improvement was observed from the DOC+fMLLR features as well, where 8% and 12% relative reduction in WER for SWB and CH subsets was observed when FSH data was added to the training data. Note that the CH subset of the NIST 2000 CTS test set was more challenging than the SWB subset, as it contains non-native speakers of English, hence introducing accented speech into the evaluation set. The use of articulatory features helped to reduce the error rates for both SWB and CH test sets, indicating their robustness to model spontaneous speech in both native (SWB) and non-native (CH) speaking styles. The FSH corpus contains speech from quite a diverse set of speakers, helping to reduce the WER of the CH subset more significantly than the SWB subset, a trend reflected in results reported in the literature <cite>[8]</cite> .",
  "y": "similarities"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_5",
  "x": "The DOC feature was always found to perform slightly better than the MFBs and the best results were obtained when all the features were combined together, indicating the benefit of using multiview features. Note that only 100 additional neurons were used to accommodate the TV features, hence all the models were of comparable sizes. The benefit of the articulatory features stemmed from the complementary information that they contain (reflecting degree and location of articulatory constrictions in the vocal tract), as demonstrated by earlier studies [22] [23] [24] . Overall the f-CNN-DNN system trained with the combined feature set, MFB+fMLLR + DOC+fMLLR + TV, demonstrated a relative reduction in WER of 7% and 9% compared to the MFB+fMLLR CNN baseline for SWB and CH subsets of the NIST 2000 CTS test set. Table 1 and 2 also demonstrates that sequence training always gave additive performance gain over crossentropy training, supporting the in <cite>[8,</cite> 21] . As a next step, we focused on training the acoustic models using the 2000-hour SWB+FSH CTS data, focusing on the CNN acoustic models and multi-view features. Note that the MFB DNN baseline model was used to generate the alignments for the FSH part of the 2000 hours CTS training set and as a consequence the number of senone labels remained the same as the 360-hour SWB models. Table 3 presents the results from the 2000 hours CTS trained models. The model configurations and their parameter size were kept the same as the 360-hour SWB models.",
  "y": "uses similarities"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_6",
  "x": [
   "We reported the results exploring multiple features for ASR on English CTS data. We observed that the fMLLR transform helped reduce the WER of the baseline system significantly. We observed that using multiple acoustic features helped in improving the overall accuracy of the system. Use of robust features and articulatory features significantly reduced the WER for the more challenging CallHome subset of the NIST 2000 CTS evaluation set, with accented speech in that subset. We developed a fused-CNN-DNN architecture, where input convolution was only performed on the acoustic features and the articulatory features were process by a feed-forward layer. We found this architecture effective for combining acoustic features and articulatory features. The robust features and articulatory features capture complementary information, and the addition of them resulted in the best single system performance, with 12% relative reduction of WER on SWB and CH evaluation sets respectively, compared to the MFB+fMLLR CNN baseline. Note that in this study the language model has not been optimized. Future studies should investigate RNN or other neural network-based language modeling techniques that are known to perform better than word n-gram LMs."
  ],
  "y": "future_work"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_7",
  "x": "Table 1 presents the word error rates (WER) from the baseline CNN model trained with the SWB data when evaluated on the NIST 2000 CTS test set, for both cross-entropy (CE) training and sequence training (ST) using MMI. Table 1 also shows the results obtained from the DOC features with and without a fMLLR transform. We present results from ST as they were found to be always better than the results CE training. We explored learning the fMLLR transform directly from the filterbank features (MFB_fMLLR and DOC_fMLLR) and learning the fMLLR transforms on the full dimensional cepstral versions of the features, applying the transform and then performing IDCT (MFB+fMLLR and DOC+fMLLR). Table 1 shows that the performance of fMLLR transforms learned from the cepstral version of the features are better than the ones directly from the filterbank features, which is expected, as the cepstral features are uncorrelated, which adheres to the diagonal covariance assumption of the GMM models used to learn those transforms. Table 1 demonstrates that the fMLLR transformed features always performed better than the features without fMLLR transform. Also, the CNN models always gave better results, confirming similar observations from studies reported earlier <cite>[8]</cite> . Also, note that Table 1 shows that the DOC features performed slightly better than the MFB features after the fMLLR transform, where the performance improvement was more pronounced for the CH subset of the NIST 2000 CTS test set. As a next step, we investigated the efficacy of feature combination and focused only on the CNN acoustic models.",
  "y": "uses similarities"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_0",
  "x": "**ABSTRACT** This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer, syntactic representations of units of bilingual language models (BiLMs). Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm. The approach is evaluated in a series of ArabicEnglish and Chinese-English translation experiments. The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline, as well as over the lexicalized BiLM by <cite>Niehues et al. (2011)</cite> . Further improvements of up to 0.45 BLEU for ArabicEnglish and up to 0.59 BLEU for ChineseEnglish are obtained by combining our dependency BiLM with a lexicalized BiLM. An improvement of 0.98 BLEU is obtained for Chinese-English in the setting of an increased distortion limit. ---------------------------------- **INTRODUCTION**",
  "y": "background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_1",
  "x": "The simplest reordering model is linear distortion (Koehn et al., 2003) which scores the distance between phrases translated at steps t and t + 1 of the derivation. This model ignores any contextual information, as the distance between translated phrases is its only parameter. Lexical distortion modeling (Tillmann, 2004) conditions reordering probabilities on the phrase pairs translated at the current and previous steps. Unlike linear distortion, it characterizes reordering not in terms of distance but type: monotone, swap, or discontinuous. In this paper, we base our approach to reordering on bilingual language models (Marino et al., 2006;<cite> Niehues et al., 2011)</cite> . Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process. 1 Originally, Marino et al. (2006) used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems<cite> (Niehues et al., 2011)</cite> . We adopt and generalize the approach of <cite>Niehues et al. (2011)</cite> to investigate several variations of bilingual language models. Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties.",
  "y": "similarities background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_2",
  "x": "This problem has received a lot of attention in the literature (see, e.g., Tillmann (2004) , Zens and Ney (2003) , Al-Onaizan and Papineni (2006) ), as choosing the correct reordering improves readability of the translation and can have a substantial impact on translation quality (Birch, 2011) . In this paper, we only consider those approaches that include a reordering feature function into the loglinear interpolation used during decoding. The simplest reordering model is linear distortion (Koehn et al., 2003) which scores the distance between phrases translated at steps t and t + 1 of the derivation. This model ignores any contextual information, as the distance between translated phrases is its only parameter. Lexical distortion modeling (Tillmann, 2004) conditions reordering probabilities on the phrase pairs translated at the current and previous steps. Unlike linear distortion, it characterizes reordering not in terms of distance but type: monotone, swap, or discontinuous. In this paper, we base our approach to reordering on bilingual language models (Marino et al., 2006;<cite> Niehues et al., 2011)</cite> . Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process. 1 Originally, Marino et al. (2006) used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems<cite> (Niehues et al., 2011)</cite> .",
  "y": "background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_3",
  "x": "We adopt and generalize the approach of <cite>Niehues et al. (2011)</cite> to investigate several variations of bilingual language models. Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties. What kind of contextual information should be incorporated in a reordering model? Lexical information has been used by Tillmann (2004) but is known to suffer from data sparsity (Galley and Manning, 2008) . Also previous contributions to bilingual language modeling (Marino et al., 2006;<cite> Niehues et al., 2011)</cite> have mostly used lexical information, although Crego and Yvon (2010a) and Crego and Yvon (2010b) label bilingual to-kens with a rich set of POS tags. But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (Fox, 2002; Cherry, 2008) . Syntactic information is incorporated in tree-based approaches in SMT, allowing one to provide a more detailed definition of translation events and to redefine decoding as parsing of a source string (Liu et al., 2006; Huang et al., 2006; Marton and Resnik, 2008) , of a target string (Shen et al., 2008) , or both (Chiang, 2007; Chiang, 2010) . Reordering is a result of a given derivation, and CYK-based decoding used in tree-based approaches is more syntax-aware than the simple PBSMT decoding algorithm. Although tree-based approaches potentially offer a more accurate model of translation, they are also a lot more complex and requiring more intricate optimization and estimation techniques (Huang and Mi, 2010) .",
  "y": "uses extends"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_4",
  "x": "In this paper, we base our approach to reordering on bilingual language models (Marino et al., 2006;<cite> Niehues et al., 2011)</cite> . Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process. 1 Originally, Marino et al. (2006) used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems<cite> (Niehues et al., 2011)</cite> . We adopt and generalize the approach of <cite>Niehues et al. (2011)</cite> to investigate several variations of bilingual language models. Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties. What kind of contextual information should be incorporated in a reordering model? Lexical information has been used by Tillmann (2004) but is known to suffer from data sparsity (Galley and Manning, 2008) . Also previous contributions to bilingual language modeling (Marino et al., 2006;<cite> Niehues et al., 2011)</cite> have mostly used lexical information, although Crego and Yvon (2010a) and Crego and Yvon (2010b) label bilingual to-kens with a rich set of POS tags. But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (Fox, 2002; Cherry, 2008) .",
  "y": "motivation background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_5",
  "x": "We incrementally build up the syntactic representation of a translation during decoding by adding precomputed fragments from the source parse tree. The idea to combine the merits of the two SMT paradigms has been proposed before, where Huang and Mi (2010) introduce incremental decoding for a tree-based model. On a very general level, our approach is similar to theirs in that it keeps track of a sequence of source syntactic subtrees that are being translated at consecutive decoding steps. An important difference is that they keep track of whether the visited subtrees have been fully translated, while in our approach, once a syntactic structural unit has been added to the history, it is not updated anymore. In this paper, we focus on source syntactic information. During decoding we have full access to the source sentence, which allows us to obtain a better syntactic analysis (than for a partial sentence) and to precompute the units that the model operates with. We investigate the following research questions: How well can we capture reordering regularities of a language pair by incorporating source syntactic parameters into the units of a bilingual language model? What kind of source syntactic parameters are necessary and sufficient? Our contributions can be summarized as follows: We argue that the contextual information used in the original bilingual models<cite> (Niehues et al., 2011)</cite> is insufficient and introduce a simple model that exploits source-side syntax to improve reordering (Sections 2 and 3). We perform a thorough comparison between different variants of our general model and compare them to the original approach.",
  "y": "background motivation"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_6",
  "x": "The richer representation allows for a finer distinction between reorderings. For example, Arabic has a morphological marker of definiteness on both nouns and adjectives. If we first translate a definite adjective and then an indefinite noun, it will probably not be a likely sequence according to the translation model. This kind of intuition underlies the model of <cite>Niehues et al. (2011)</cite> , a bilingual LM (BiLM), which defines elementary translation events t 1 , ..., t n as follows: where e i is the i-th target word and A : E \u2192 P(F ) is an alignment function, E and F referring to target and source sentences, and P(\u00b7) is the powerset function. In other words, the i-th translation event consists of the i-th target word and all source words aligned to it. <cite>Niehues et al. (2011)</cite> refer to the defined translation events t i as bilingual tokens and we adopt this terminology. There are alternative definitions of bilingual language models. Our choice of the above definition is supported by the fact that it produces an unambiguous segmentation of a parallel sentence into tokens.",
  "y": "background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_7",
  "x": "where e i is the i-th target word and A : E \u2192 P(F ) is an alignment function, E and F referring to target and source sentences, and P(\u00b7) is the powerset function. In other words, the i-th translation event consists of the i-th target word and all source words aligned to it. <cite>Niehues et al. (2011)</cite> refer to the defined translation events t i as bilingual tokens and we adopt this terminology. There are alternative definitions of bilingual language models. Our choice of the above definition is supported by the fact that it produces an unambiguous segmentation of a parallel sentence into tokens. Ambiguous segmentation is undesirable because it increases the token vocabulary, and thus the model sparsity. Another disadvantage comes from the fact that we want to compare permutations of the same set of elements. For example, the two different segmentations of ba into [ba] and [b] [a] still represent the same permutation of the sequence ab. In Figure 1 one can produce a segmentation of (AsEAr Albtrwl, oil prices) into (Albtrwl, oil) and (AsEAr, prices) or leave it as is. If we allow for both segmentations, the learnt probability parameters may be different for the sum of (Albtrwl, oil) and (AsEAr, prices) and for the unsegmented phrase.",
  "y": "similarities background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_8",
  "x": "Since <cite>Niehues et al. (2011)</cite> have shown their model to work successfully as an additional feature in combination with commonly used standard phrase-based features, we use their approach as the main point of reference and base our approach on their segmentation method. In the rest of the text we refer to <cite>Niehues et al. (2011)</cite> as the original BiLM. 4 At the same time, we do not see any specific obstacles for combining our work with MTUs. ---------------------------------- **SUITABILITY OF LEXICALIZED BILM TO MODEL REORDERING** As mentioned in the introduction, lexical information is not very well-suited to capture reordering regularities. Consider Figure 2 .a. The extracted sequence of bilingual tokens is produced by aligning source words with respect to target words (so that they are in the same order), as demonstrated by the shaded part of the picture. If we substituted the Arabic translation of Egyptian for the Arabic translation of Israeli, the reordering should remain the same. What matters for reordering is the syntactic role or context of a word.",
  "y": "uses"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_10",
  "x": "Consider Figure 2 .a. The extracted sequence of bilingual tokens is produced by aligning source words with respect to target words (so that they are in the same order), as demonstrated by the shaded part of the picture. If we substituted the Arabic translation of Egyptian for the Arabic translation of Israeli, the reordering should remain the same. What matters for reordering is the syntactic role or context of a word. By using unnecessarily fine-grained categories we risk running into sparsity issues. <cite>Niehues et al. (2011)</cite> also described an alternative variant of the original BiLM, where words are substituted by their POS tags (Figure 2 .a, shaded part). Also, however, POS information by itself may be insufficiently expressive to separate cor- , it still is a likely sequence. Indeed, the log-probabilities of the two sequences with respect to a 4-gram BiLM model 5 result in a higher probability of \u221210.25 for the incorrect reordering than for the correct one (\u221210.39). Since fully lexicalized bilingual tokens suffer from data sparsity and POS-based bilingual tokens are insufficiently expressive, the question is which level of syntactic information strikes the right balance between expressiveness and generality. 5 Section 4 contains details about data and software setup.",
  "y": "motivation background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_11",
  "x": "If we incorporate dependency relations into the representation of bilingual tokens, the incorrect reordering in Figure 2 .b will produce a highly unlikely sequence. For example, we can substitute each source word with its POS tag and its parent's POS tag (Figure 3 ). Again, we computed 4-gram log-probabilities for the corresponding sequences: the correct reordering results in a substantially higher probability of \u221210.58 than the incorrect one (\u221213.48). We may consider situations where more fine-grained distinctions are required. In the next section, we explore different representations based on source dependency trees. ---------------------------------- **DEPENDENCY-BASED BILM** In this section, we introduce our model which combines the BiLM from <cite>Niehues et al. (2011)</cite> with source dependency information. We further give details on how the proposed models are trained and integrated into a phrase-based decoder.",
  "y": "similarities background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_13",
  "x": "This is followed by a preliminary analysis of observed reorderings where we compare 4-gram precision results and conduct experiments with an increased distortion limit. ---------------------------------- **ARABIC-ENGLISH TRANSLATION EXPERIMENTS** We are interested in how a translation system with an integrated dependency-based BiLM feaand several gale corpora. ture performs as compared to the standard PB-SMT baseline and, more importantly, to the original BiLM model. We consider two variants of BiLM discussed by <cite>Niehues et al. (2011)</cite> : the standard one, Lex\u2022Lex, and the simplest syntactic one, Pos\u2022Pos. Results for the experiments can be found in Table 2 . In the discussion below we mostly focus on the experimental results for the large, combined test set MT08+MT09. Table 2 .a-b compares the performance of the baseline and original BiLM systems.",
  "y": "background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_14",
  "x": "---------------------------------- **CONCLUSIONS** In this paper, we have introduced a simple, yet effective way to include syntactic information into phrase-based SMT. Our method consists of enriching the representation of units of a bilingual language model (BiLM). We argued that the very limited contextual information used in the original bilingual models<cite> (Niehues et al., 2011)</cite> can capture reorderings only to a limited degree and proposed a method to incorporate information from a source dependency tree in bilingual units. In a series of translation experiments we performed a thorough comparison between various syntacticallyenriched BiLMs and competing models. The results demonstrated that adding syntactic information from a source dependency tree to the representations of bilingual tokens in an n-gram model can yield statistically significant improvements over the competing systems. A number of additional evaluations provided an indication for better modeling of reordering phenomena. The proposed dependency-based BiLMs resulted in an increase in 4-gram precision and provided further significant improvements over all considered metrics in experiments with an increased distortion limit.",
  "y": "background motivation"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_0",
  "x": "---------------------------------- **INTRODUCTION** Neural Machine Translation (NMT) has rapidly become the new Machine Translation (MT) paradigm, significantly improving over the traditional statistical machine translation procedure . Recently, several models and variants have been proposed with increased research efforts towards multilingual machine translation (Firat et al., 2016; Lakew et al., 2018; Wang et al., 2018; Blackwood et al., 2018; Lu et al., 2018) . The main motivation of multilingual models is the effect of transfer learning that enables machine translation systems to benefit from relationships between languages and training signals that come from different datasets (Ha et al., 2016; Johnson et al., 2017; Gu et al., 2018) . Another aspect that draws interest in translation models is the effective computation of sentence representations using the translation task as an auxiliary semantic signal (Hill et al., 2016; McCann et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018 ). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings <cite>(C\u00edfka and Bojar, 2018)</cite> . However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear. Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (Schwenk, 2018) , on translation quality (Lu et al., 2018) , on speed comparison (Britz et al., 2017) , or only exploring a bilingual scenario <cite>(C\u00edfka and Bojar, 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_1",
  "x": "---------------------------------- **INTRODUCTION** Neural Machine Translation (NMT) has rapidly become the new Machine Translation (MT) paradigm, significantly improving over the traditional statistical machine translation procedure . Recently, several models and variants have been proposed with increased research efforts towards multilingual machine translation (Firat et al., 2016; Lakew et al., 2018; Wang et al., 2018; Blackwood et al., 2018; Lu et al., 2018) . The main motivation of multilingual models is the effect of transfer learning that enables machine translation systems to benefit from relationships between languages and training signals that come from different datasets (Ha et al., 2016; Johnson et al., 2017; Gu et al., 2018) . Another aspect that draws interest in translation models is the effective computation of sentence representations using the translation task as an auxiliary semantic signal (Hill et al., 2016; McCann et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018 ). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings <cite>(C\u00edfka and Bojar, 2018)</cite> . However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear. Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (Schwenk, 2018) , on translation quality (Lu et al., 2018) , on speed comparison (Britz et al., 2017) , or only exploring a bilingual scenario <cite>(C\u00edfka and Bojar, 2018)</cite> .",
  "y": "motivation"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_2",
  "x": "Recently, several models and variants have been proposed with increased research efforts towards multilingual machine translation (Firat et al., 2016; Lakew et al., 2018; Wang et al., 2018; Blackwood et al., 2018; Lu et al., 2018) . The main motivation of multilingual models is the effect of transfer learning that enables machine translation systems to benefit from relationships between languages and training signals that come from different datasets (Ha et al., 2016; Johnson et al., 2017; Gu et al., 2018) . Another aspect that draws interest in translation models is the effective computation of sentence representations using the translation task as an auxiliary semantic signal (Hill et al., 2016; McCann et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018 ). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings <cite>(C\u00edfka and Bojar, 2018)</cite> . However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear. Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (Schwenk, 2018) , on translation quality (Lu et al., 2018) , on speed comparison (Britz et al., 2017) , or only exploring a bilingual scenario <cite>(C\u00edfka and Bojar, 2018)</cite> . In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model. This shared layer serves as a fixedsize sentence representation that can be straightforwardly applied to downstream tasks. We examine this model with a systematic evaluation on different sizes of the attention bridge and extensive experiments to study the abstractions it learns from multiple translation tasks.",
  "y": "background"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_3",
  "x": "In contrast to previous work <cite>(C\u00edfka and Bojar, 2018)</cite> , we demonstrate that there is a correlation between translation performance and trainable downstream tasks when adjusting the size of the intermediate layer. The trend is different for non-trainable tasks that benefit from the increased compression that denser representations achieve, which typically hurts the translation performance because of the decreased capacity of the model. We also show that multilingual models improve trainable downstream tasks even further, demonstrating the additional abstraction that is pushed into the representations through additional translation tasks involved in training. ---------------------------------- **ARCHITECTURE** Our architecture follows the standard setup of an encoder-decoder model in machine translation with a traditional attention mechanism (Luong et al., 2015) . However, we augment the network with language specific encoders and decoders to enable multilingual training as in Lu et al. (2018) , plus we introduce an inner-attention layer (Liu et al., 2016; Lin et al., 2017) that summarizes the encoder information in a fixed-size vector representation that can easily be shared among different translation tasks with the language-specific encoders and decoders connecting to it. The overall architecture is illustrated in Figure 1 (see also V\u00e1zquez et al., 2019) . Due to the attentive connection between encoders and decoders we call this layer attention bridge, and its architecture is an adaptation from the model proposed by<cite> C\u00edfka and Bojar (2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_4",
  "x": "However, we augment the network with language specific encoders and decoders to enable multilingual training as in Lu et al. (2018) , plus we introduce an inner-attention layer (Liu et al., 2016; Lin et al., 2017) that summarizes the encoder information in a fixed-size vector representation that can easily be shared among different translation tasks with the language-specific encoders and decoders connecting to it. The overall architecture is illustrated in Figure 1 (see also V\u00e1zquez et al., 2019) . Due to the attentive connection between encoders and decoders we call this layer attention bridge, and its architecture is an adaptation from the model proposed by<cite> C\u00edfka and Bojar (2018)</cite> . Finally, each decoder follows a common attention mechanism in NMT, with the only exception that the context vector is computed on the attention bridge, and the initialization is performed by a mean pooling over it. Hence, the decoder receives the information only through the shared attention bridge. The fixed-sized representation coming out of the shared layer can immediately be applied to downstream tasks. 1 However, selecting a reasonable size of the attention bridge in terms of attention heads (m i in Figure 1 ) is crucial for the performance both in a bilingual and multilingual sce-1 As in Lu et al. (2018) , we note that the attention bridge is independent of the underlying encoder and decoder. While we use LSTM, it could be easily replaced with a transformer type network (Vaswani et al., 2017) or with a CNN (Gehring et al., 2017) . nario as we will see in the experiments below.",
  "y": "uses"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_5",
  "x": "We are also interested in the translation quality to verify the appropriateness of our models with respect to the main objective they are trained for. For this, we adopt the in-domain development and evaluation dataset from the ACL-WMT07 shared task. Sentences are encoded using Byte-Pair Encoding (Sennrich et al., 2016) , with 32,000 merge operations for each language. 4 SentEval: Classification tasks Table 1 shows the performance of our models on two popular tasks (SNLI and SICK-E) as in<cite> C\u00edfka and Bojar (2018)</cite> as well as the average of all 10 SentEval downstream tasks. The experiments reveal two important findings: (1) In contrast with the results from<cite> C\u00edfka and Bojar (2018)</cite>, our scores demonstrate that an increasing number of attention heads is beneficial for classification-based downstream tasks. All models perform best with more than one attention head and the general trend is that the accuracies improve with larger representations. The previous claim was that there is the opposite effect and lower numbers of attention heads lead to higher performances in downstream tasks, but we do not see that effect in our setup, at least not in the classification tasks. (2) The second outcome is the positive effect 3 Due to the large number of SentEval tasks, we report results on natural language inference (SNLI, SICK-E/SICK-R) and the average of all tasks.",
  "y": "similarities"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_6",
  "x": "We are also interested in the translation quality to verify the appropriateness of our models with respect to the main objective they are trained for. For this, we adopt the in-domain development and evaluation dataset from the ACL-WMT07 shared task. Sentences are encoded using Byte-Pair Encoding (Sennrich et al., 2016) , with 32,000 merge operations for each language. 4 SentEval: Classification tasks Table 1 shows the performance of our models on two popular tasks (SNLI and SICK-E) as in<cite> C\u00edfka and Bojar (2018)</cite> as well as the average of all 10 SentEval downstream tasks. The experiments reveal two important findings: (1) In contrast with the results from<cite> C\u00edfka and Bojar (2018)</cite>, our scores demonstrate that an increasing number of attention heads is beneficial for classification-based downstream tasks. All models perform best with more than one attention head and the general trend is that the accuracies improve with larger representations. The previous claim was that there is the opposite effect and lower numbers of attention heads lead to higher performances in downstream tasks, but we do not see that effect in our setup, at least not in the classification tasks. (2) The second outcome is the positive effect 3 Due to the large number of SentEval tasks, we report results on natural language inference (SNLI, SICK-E/SICK-R) and the average of all tasks.",
  "y": "differences"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_7",
  "x": "The previous claim was that there is the opposite effect and lower numbers of attention heads lead to higher performances in downstream tasks, but we do not see that effect in our setup, at least not in the classification tasks. (2) The second outcome is the positive effect 3 Due to the large number of SentEval tasks, we report results on natural language inference (SNLI, SICK-E/SICK-R) and the average of all tasks. Table 2 : Results from supervised similarity tasks (SICK-R and STSB), measured using Pearson's (r) and Spearman's (\u03c1) correlation coefficients (r/\u03c1). The average across unsupervised similarity tasks on Pearson's measures are displayed in the right-most column. Results with \u2020 taken from<cite> C\u00edfka and Bojar (2018).</cite> of multilingual training. We can see that multilingual training objectives are generally helpful for the trainable downstream tasks. Particularly interesting is the fact that the Manyto-Many model performs best on average even though it does not add any further training examples for English (compared to the other multilingual models), which is the target language of the downstream tasks. This suggests that the model is able to improve generalizations even from other language pairs (DE-ES, FR-ES, FR-DE) that are not directly involved in training the representations of English sentences.",
  "y": "uses"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_8",
  "x": "model is provided by a bilingual setting with only one attention head. This is in line with the findings of<cite> C\u00edfka and Bojar (2018)</cite> and could also be expected as the model is more strongly pushed into a dense semantic abstraction that is beneficial for measuring similarities without further training. More surprising is the negative effect of the multilingual models. We believe that the multilingual information encoded jointly in the attention bridge hampers the results for the monolingual semantic similarity measured with the cosine distance, while it becomes easier in a bilingual scenario where the vector encodes only one source language, English in this case. ii) On the supervised textual similarity tasks, we find a similar trend as in the previous section for SICK: both a higher number of attention heads and multilinguality contribute to better scores, while for STSB, we notice a different pattern. This general discrepancy between results in supervised and unsupervised tasks is not new in the literature (Hill et al., 2016) . We hypothesize that the training procedure is able to pick up the information needed for the task, while in the unsupervised case a more dense representation is essential. ---------------------------------- **TRANSLATION QUALITY**",
  "y": "similarities"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_0",
  "x": "More specifically, they do not evaluate the systems correctly for situations when there is no answer available and thus agents optimized for these metrics are poor at modeling confidence. We introduce a simple new performance metric for evaluating question-answering agents that is more representative of practical usage conditions, and optimize for this metric by extending the binary reward structure used in prior work to a ternary reward structure which also rewards an agent for not answering a question rather than giving an incorrect answer. We show that this can drastically improve the precision of answered questions while only not answering a limited number of previously correctly answered questions. Employing a supervised learning strategy using depth-first-search paths to bootstrap the reinforcement learning algorithm further improves performance. ---------------------------------- **INTRODUCTION** A number of approaches for question answering have been proposed recently that use reinforcement learning to reason over a knowledge graph<cite> (Das et al., 2018</cite>; Lin et al., 2018; Chen et al., 2018; Zhang et al., 2018) . In these methods the input question is first parsed into a constituent question entity and relation. The answer entity is then identified by sequentially taking a number of steps (or 'hops') over the knowledge graph (KG) starting from the question entity.",
  "y": "background"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_1",
  "x": "The closest works to ours are the works by Lin et al. (2018) , Zhang et al. (2018) and <cite>Das et al. (2018)</cite> , which consider the question answering task in a reinforcement learning setting in which the agent always chooses to answer. 1 Other approaches consider this as a link prediction problem in which multi-hop reasoning can be used to learn relational paths that link two entities. One line of work focuses on composing embeddings (Neelakantan et al., 2015; Guu et al., 2015; Toutanova et al., 2016) initially introduced for link prediction, e.g., TransE (Bordes et al., 2013) , ComplexE (Trouillon et al., 2016) or ConvE (Dettmers et al., 2018 ). Another line of work focuses on logical rule learning such as neural logical programming and neural theorem proving (Rockt\u00e4schel and Riedel, 2017) . Here, we focus on question answering rather than link prediction or rule mining and use reinforcement learning to circumvent that we do not have ground truth paths leading to the answer entity. Recently, popular textual QA datasets have been extended with not-answerable questions (Trischler et al., 2017; Rajpurkar et al., 2018) . Questions that cannot be answered are labeled with 'no answer' option which allows for supervised training. This is different from our setup in which there are no ground truth 'no answer' labels. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_2",
  "x": "Questions that cannot be answered are labeled with 'no answer' option which allows for supervised training. This is different from our setup in which there are no ground truth 'no answer' labels. ---------------------------------- **BACKGROUND: REINFORCEMENT LEARNING** We base our work on the recent reinforcement learning approaches introduced in <cite>Das et al. (2018)</cite> and Lin et al. (2018) . We denote the knowledge graph as G, the set of entities as E, the set of relations as R and the set of directed edges L between entities of the form l = (e 1 , r, e 2 ) with e 1 , e 2 \u2208 E and r \u2208 R. The goal is to find an answer entity e a given a question entity e q and the question relation r q , when (e q , r q , e a ) is not part of graph G. We formulate this problem as a Markov Decision Problem (MDP) (Sutton and Barto, 1998) with the following states, actions, transition function and rewards: States. At every timestep t, the state s t is defined by the current entity e t , the question entity e q and relation r q , for which e t , e q \u2208 E and r q \u2208 R. More formally, s t = (e t , e q , r q ).",
  "y": "uses"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_3",
  "x": "The transition function \u03b4 maps s t to a new state s t+1 based on the action taken by the agent. Consequently, s t+1 = \u03b4(s t , A t ) = \u03b4(e t , e q , r q , A t ). Rewards. The agent is rewarded based on the final state. For example, in <cite>Das et al. (2018)</cite> and Lin et al. (2018) the agent obtains a reward of 1 if the correct answer entity is reached as the final state and 0 otherwise (i.e., R(s T ) = I{e T = e a }). Figure 2a illustrates the LSTM which encodes history of the path taken. The output at timestep t is used as input to the policy network, illustrated in Figure 2b , to determine which action to take next. ---------------------------------- **TRAINING**",
  "y": "background"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_4",
  "x": "We can achieve this by adding a synthetic 'no answer' action that leads to a special entity e N OAN SW ER . This is illustrated in Figure 3 . In the framework of <cite>Das et al. (2018)</cite> a binary reward is used which rewards the learner for the answer being wrong or correct. Following a similar protocol, we could award a score of 1 to return 'no answer' when there is no answer available in the KG. However, we cannot achieve reasonable training with such reward structure. This is because there is no specific pattern for 'no answer' that could be directly learned. Hence, if we reward a system equally for correct or no answer, it learns to always predict 'no answer'. We therefore propose a ternary reward structure in which a positive reward is given to a correct answer, a neutral reward when e N OAN SW ER is selected as an answer, and a negative reward for an does not exist in the graph and thus an alternative path needs to be used that leads to the correct answer. To avoid that the agent returns an incorrect answer when not finding the correct answer, a 'no answer' relation is added between every entity node and a special 'no answer' node, to be able to return 'no answer'. incorrect answer.",
  "y": "background"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_5",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** We evaluate our proposed approach on a publicly available dataset, FB15k-237 (Toutanova and Chen, 2015) which is based on the Freebase knowledge graph and a proprietary dataset Alexa69k-378 which is a sample of Alexa's proprietary knowledge graph. Both the public dataset and the proprietary dataset are <cite>Das et al. (2018)</cite> , using the same train/val/test splits for FB15k-237. For Alexa69k-378 we use 10% of the full dataset for validation and test. For both datasets, we add the reverse relations of all relations in the training set in order to facilitate backward navigation following the approach of previous work. Similarly, a 'no op' relation is added for each entity between the entity and itself, which allows the agent to loop/reason multiple consecutive steps over the same entity. An overview of both datasets can be found in Table 3 . We extend the publicly available implementation of <cite>Das et al. (2018)</cite> for our experimentation.",
  "y": "similarities uses"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_6",
  "x": "Unlike <cite>Das et al. (2018)</cite> , we also train entity embeddings after initializing them with random values. This resulted in the final QA Score of 47.58%, around 8% higher than standard RL and 12% higher than <cite>Das et al. (2018)</cite> . The final QA Score also increased from 28.72% to 39.55%, and also significantly improved over <cite>Das et al. (2018)</cite> and Lin et al. (2018) .",
  "y": "differences extends"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_0",
  "x": "We start from a baseline joint model that performs the tasks of named entity recognition (NER) and relation extraction at once. Previously proposed models (summarized in Section 2) exhibit several issues that the neural network-based baseline approach (detailed in Section 3.1) overcomes: (i) our model uses automatically extracted features without the need of external parsers nor manually extracted features (see Gupta et al. (2016) ; Miwa and Bansal (2016) ; Li et al. (2017) ), (ii) all entities and the corresponding relations within the sentence are extracted at once, instead of examining one pair of entities at a time (see Adel and Sch\u00fctze (2017) ), and (iii) we model relation extraction in a multi-label setting, allowing multiple relations per entity (see Katiyar and Cardie (2017) ; <cite>Bekoulis et al. (2018a)</cite> ). The core contribution of the paper is the use of AT as an extension in the training procedure for the joint extraction task (Section 3.2). To evaluate the proposed AT method, we perform a large scale experimental study in this joint task (see Section 4), using datasets from different contexts (i.e., news, biomedical, real estate) and languages (i.e., English, Dutch). We use a strong baseline that outperforms all previous models that rely on automatically extracted features, achieving state-of-the-art performance (Section 5). Compared to the baseline model, applying AT during training leads to a consistent additional increase in joint extraction effectiveness. ---------------------------------- **RELATED WORK** Joint entity and relation extraction: Joint models (Li and Ji, 2014; Miwa and Sasaki, 2014) that are based on manually extracted features have been proposed for performing both the named entity recognition (NER) and relation extraction subtasks at once.",
  "y": "differences"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_1",
  "x": "Specifically, Miwa and Bansal (2016) as well as Li et al. (2017) apply bidirectional tree-structured RNNs for different contexts (i.e., news, biomedical) to capture syntactic information (using external dependency parsers). Gupta et al. (2016) propose the use of various manually extracted features along with RNNs. Adel and Sch\u00fctze (2017) solve the simpler problem of entity classification (EC, assuming entity boundaries are given), instead of NER, and they replicate the context around the entities, feeding entity pairs to the relation extraction layer. Katiyar and Cardie (2017) investigate RNNs with attention without taking into account that relation labels are not mutually exclusive. Finally, <cite>Bekoulis et al. (2018a)</cite> use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part. Our baseline model enables simultaneous extraction of multiple relations from the same input. Then, we further extend this strong baseline using adversarial training. Adversarial training (AT) (Goodfellow et al., 2015) has been proposed to make classifiers more robust to input perturbations in the context of image recognition. In the context of NLP, several variants have been proposed for different tasks such as text classification (Miyato et al., 2017) , relation extraction (Wu et al., 2017) and POS tagging (Yasunaga et al., 2018) .",
  "y": "background"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_2",
  "x": "Neural network methods have been exploited to overcome this feature design issue and usually involve RNNs and CNNs (Miwa and Bansal, 2016; Zheng et al., 2017) . Specifically, Miwa and Bansal (2016) as well as Li et al. (2017) apply bidirectional tree-structured RNNs for different contexts (i.e., news, biomedical) to capture syntactic information (using external dependency parsers). Gupta et al. (2016) propose the use of various manually extracted features along with RNNs. Adel and Sch\u00fctze (2017) solve the simpler problem of entity classification (EC, assuming entity boundaries are given), instead of NER, and they replicate the context around the entities, feeding entity pairs to the relation extraction layer. Katiyar and Cardie (2017) investigate RNNs with attention without taking into account that relation labels are not mutually exclusive. Finally, <cite>Bekoulis et al. (2018a)</cite> use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part. Our baseline model enables simultaneous extraction of multiple relations from the same input. Then, we further extend this strong baseline using adversarial training. Adversarial training (AT) (Goodfellow et al., 2015) has been proposed to make classifiers more robust to input perturbations in the context of image recognition.",
  "y": "differences"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_3",
  "x": "Adel and Sch\u00fctze (2017) solve the simpler problem of entity classification (EC, assuming entity boundaries are given), instead of NER, and they replicate the context around the entities, feeding entity pairs to the relation extraction layer. Katiyar and Cardie (2017) investigate RNNs with attention without taking into account that relation labels are not mutually exclusive. Finally, <cite>Bekoulis et al. (2018a)</cite> use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part. Our baseline model enables simultaneous extraction of multiple relations from the same input. Then, we further extend this strong baseline using adversarial training. Adversarial training (AT) (Goodfellow et al., 2015) has been proposed to make classifiers more robust to input perturbations in the context of image recognition. In the context of NLP, several variants have been proposed for different tasks such as text classification (Miyato et al., 2017) , relation extraction (Wu et al., 2017) and POS tagging (Yasunaga et al., 2018) . AT is considered as a regularization method. Unlike other regularization methods (i.e., dropout (Srivastava et al., 2014) , word dropout (Iyyer et al., 2015) ) that introduce random noise, AT generates perturbations that are variations of examples easily misclassified by the model.",
  "y": "extends differences"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_4",
  "x": "**EXPERIMENTAL SETUP** We evaluate our models on four datasets, using the code as available from our github codebase. 1 Specifically, we follow the 5-fold crossvalidation defined by Miwa and Bansal (2016) for the ACE04 (Doddington et al., 2004) dataset. For the CoNLL04 (Roth and Yih, 2004 ) EC task (assuming boundaries are given), we use the same splits as in Gupta et al. (2016) ; Adel and Sch\u00fctze (2017) . We also evaluate our models on the NER task similar to Miwa and Sasaki (2014) in the same dataset using 10-fold cross validation. For the Dutch Real Estate Classifieds, DREC (Bekoulis et al., 2017) dataset, we use train-test splits as in <cite>Bekoulis et al. (2018a)</cite> . For the Adverse Drug Events, ADE (Gurulingappa et al., 2012) , we perform 10-fold cross-validation similar to Li et al. (2017) . To obtain comparable results that are not affected by the input embeddings, we use the embeddings of the previous works. We employ early stopping in all of the experiments.",
  "y": "uses"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_5",
  "x": "We use the relaxed evaluation similar to Gupta et al. (2016) ; Adel and Sch\u00fctze (2017) on the EC task. The baseline model outperforms the state-of-the-art models that do not rely on manually extracted features (>4% improvement for both tasks), since we directly model the whole sentence, instead of just considering pairs of entities. Moreover, compared to the model of Gupta et al. (2016) that relies on complex features, the baseline model performs within a margin of 1% in terms of overall F 1 score. We also report NER results on the same dataset and improve overall F 1 score with \u223c1% compared to Miwa and Sasaki (2014) , indicating that our automatically extracted features are more informative than the hand-crafted ones. These automatically extracted features exhibit their performance improvement mainly due to the shared LSTM layer that learns to automatically generate feature representations of entities and their corresponding relations within a single model. For the DREC dataset, we use two evaluation methods. In the boundaries evaluation, the baseline has an improvement of \u223c3% on both tasks compared to <cite>Bekoulis et al. (2018a)</cite> , whose quadratic scoring layer complicates NER. Table 1 and Fig. 2 show the effectiveness of the adversarial training on top of the baseline model. In all of the experiments, AT improves the predictive performance of the baseline model in the joint setting.",
  "y": "differences"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_0",
  "x": "Compositionality is the degree to which the meaning of an MWE is predictable from the meanings of its component words. It is typically viewed as lying on a continuum, with expressions such as speed limit and gravy train lying towards the compositional and non-compositional ends of the spectrum, respectively, and expressions such as rush hour and fine line falling somewhere in between as semi-compositional. 1 Compositionality can also be viewed with respect to an individual component word of an MWE, where an MWE component word is compositional if its meaning is reflected in the meaning of the expression. For example, in spelling bee and grandfather clock, the first and second component words, respectively, are compositional, while the others are not. Knowledge of multiword expressions is important for natural language processing (NLP) tasks such as parsing (Korkontzelos and Manandhar, 2010) and machine translation (Carpuat and Diab, 2010) . In the case of translation, compositionality is particularly important because a word-for-word translation would typically be incorrect for a non-compositional expression. Much research has therefore focused on compositionality prediction of MWEs, primarily at the type level. One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; <cite>Salehi et al., 2015</cite>) . The hypothesis behind <cite>this line of work</cite> is that the representation of a compositional MWE will be more similar to the representations of its component words than the representation of a non-compositional MWE will be to those of its component words.",
  "y": "background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_1",
  "x": "Much research has therefore focused on compositionality prediction of MWEs, primarily at the type level. One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; <cite>Salehi et al., 2015</cite>) . The hypothesis behind <cite>this line of work</cite> is that the representation of a compositional MWE will be more similar to the representations of its component words than the representation of a non-compositional MWE will be to those of its component words. One issue faced by <cite>such approaches</cite> is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them. Token-level MWE identification has been studied for specific types of MWEs such as verb-particle constructions (e.g., and verb-noun idioms (e.g., Salton et al., 2016) . Broad coverage MWE identification has also been studied, and remains a challenge (Schneider et al., 2014; Gharbieh et al., 2017) . Language models are common throughout NLP in tasks including machine translation (Brants et al., 2007) , speech recognition (Collins et al., 2005) , and question answering (Chen et al., 2006) . Although word-level language models are widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012) . Owing to this advantage, character-level language models have been applied in a range of NLP tasks, including authorship attribution, (Peng et al., 2003) , part-of-speech tagging (Santos and Zadrozny, 2014), case restoration (Susanto et al., 2016) , and stock price prediction (dos Santos Pinheiro and Dras, 2017).",
  "y": "background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_2",
  "x": "One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; <cite>Salehi et al., 2015</cite>) . The hypothesis behind <cite>this line of work</cite> is that the representation of a compositional MWE will be more similar to the representations of its component words than the representation of a non-compositional MWE will be to those of its component words. One issue faced by <cite>such approaches</cite> is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them. Token-level MWE identification has been studied for specific types of MWEs such as verb-particle constructions (e.g., and verb-noun idioms (e.g., Salton et al., 2016) . Broad coverage MWE identification has also been studied, and remains a challenge (Schneider et al., 2014; Gharbieh et al., 2017) . Language models are common throughout NLP in tasks including machine translation (Brants et al., 2007) , speech recognition (Collins et al., 2005) , and question answering (Chen et al., 2006) . Although word-level language models are widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012) . Owing to this advantage, character-level language models have been applied in a range of NLP tasks, including authorship attribution, (Peng et al., 2003) , part-of-speech tagging (Santos and Zadrozny, 2014), case restoration (Susanto et al., 2016) , and stock price prediction (dos Santos Pinheiro and Dras, 2017). Moreover, character-level information can be composed to form representations of words (Ling et al., 2015) .",
  "y": "motivation background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_3",
  "x": "One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; <cite>Salehi et al., 2015</cite>) . One issue faced by <cite>such approaches</cite> is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them. Although word-level language models are widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012) . In this paper we consider whether character-level neural network language models capture knowledge of MWE compositionality.",
  "y": "motivation"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_4",
  "x": "In order to obtain vectors representing each of an MWE and its component words through a character-level neural network language model, each of the MWE and its component words are considered as a sequence of characters. Each of these character sequences includes a special end-of-sequence character. In the case of an MWE, the character sequence includes a space character between the component words. For example, the MWE ivory tower is represented as the sequence < i, v, o, r, y, , t, o, w, e, r, END >. These character sequences are fed to the neural network language model, and the hidden state of the neural network at the end of the sequence is taken as the vector representation for that sequence. 2 Once vector representations of an MWE and its component words are obtained, following <cite>Salehi et al. (2015)</cite> , the following equations are then used to compute the compositionality of an MWE: where MWE is the vector representation of the MWE, and C 1 and C 2 are vector representations for the first and second components of the MWE, respectively. 3 In both cases, we use cosine as the similarity measure. comp 1 is based on Reddy et al. (2011) .",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_5",
  "x": "The batch size, learning rate, and dropout are set 20, 0.002, and 0, respectively. 5 We consider some alternative parameter settings to these defaults in section \u00a74. ---------------------------------- **TRAINING CORPUS** We train language models over a portion of English and German Wikipedia dumps -following <cite>Salehi et al. (2015)</cite> -from 20 January 2018. The raw dumps are preprocessed using WP2TXT 6 to remove wikimarkup, metadata, and XML and HTML tags. The text from Wikipedia contains many characters that are not typically found in MWEs, for example, non-ASCII characters. Such characters drastically increase the size of the vocabulary of the language model, which leads to very long training times. We therefore remove all non-ASCII characters from the English dump, and all non-ASCII characters other than\u00e4,\u00c4,\u00f6,\u00d6,\u00fc,\u00dc, \u00df from the German dump.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_6",
  "x": "Such characters drastically increase the size of the vocabulary of the language model, which leads to very long training times. We therefore remove all non-ASCII characters from the English dump, and all non-ASCII characters other than\u00e4,\u00c4,\u00f6,\u00d6,\u00fc,\u00dc, \u00df from the German dump. Training the character-level language model over the Wikipedia dumps in their entirety would take a prohibitively long time due to their size. We therefore instead carry out experiments training on a 1% sample of the English dump, and a 2% sample of the German dump (to give a corpus of similar size to the English one). Details of the resulting training corpora are provided in table 1. ---------------------------------- **EVALUATION DATA** The proposed model is evaluated over the same three datasets as <cite>Salehi et al. (2015)</cite> , <cite>which</cite> cover two languages (English and German) and two kinds of MWEs (noun compounds and verb-particle constructions). ENC This dataset contains 90 English noun compounds (e.g., game plan, gravy train) which are annotated on a scale of [0, 5] for both their overall compositionality, and the compositionality of each of their component words (Reddy et al., 2011) .",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_7",
  "x": "ENC This dataset contains 90 English noun compounds (e.g., game plan, gravy train) which are annotated on a scale of [0, 5] for both their overall compositionality, and the compositionality of each of their component words (Reddy et al., 2011) . (Mikolov et al., 2013) , are also shown. EVPC This dataset consists of 160 English verb-particle constructions (e.g., add up, figure out) which are rated on a binary scale for the compositionality of each of the verb and particle component words (Bannard, 2006) by multiple annotators; no ratings for the overall compositionality of MWEs are provided in this dataset. The binary compositionality judgements are converted to continuous values as in <cite>Salehi et al. (2015)</cite> by dividing the number of judgements that an expression is compositional by the total number of judgements. GNC This dataset contains 244 German noun compounds (e.g., Ahornblatt 'maple leaf', Knoblauch 'garlic') which are annotated on a scale of [1, 7] for their overall compositionality, and the compositionality of each component word (von der Heide and Borgwaldt, 2009). ---------------------------------- **EVALUATION METHODOLOGY** We evaluate our proposed approach following <cite>Salehi et al. (2015)</cite> by computing Pearson's correlation between the predicted compositionality (i.e., from either comp 1 or comp 2 ) and human ratings for overall compositionality. For EVPC, no overall compositionality ratings are provided.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_8",
  "x": "The proposed model is evaluated over the same three datasets as <cite>Salehi et al. (2015)</cite> , <cite>which</cite> cover two languages (English and German) and two kinds of MWEs (noun compounds and verb-particle constructions). ENC This dataset contains 90 English noun compounds (e.g., game plan, gravy train) which are annotated on a scale of [0, 5] for both their overall compositionality, and the compositionality of each of their component words (Reddy et al., 2011) . (Mikolov et al., 2013) , are also shown. EVPC This dataset consists of 160 English verb-particle constructions (e.g., add up, figure out) which are rated on a binary scale for the compositionality of each of the verb and particle component words (Bannard, 2006) by multiple annotators; no ratings for the overall compositionality of MWEs are provided in this dataset. The binary compositionality judgements are converted to continuous values as in <cite>Salehi et al. (2015)</cite> by dividing the number of judgements that an expression is compositional by the total number of judgements. GNC This dataset contains 244 German noun compounds (e.g., Ahornblatt 'maple leaf', Knoblauch 'garlic') which are annotated on a scale of [1, 7] for their overall compositionality, and the compositionality of each component word (von der Heide and Borgwaldt, 2009). ---------------------------------- **EVALUATION METHODOLOGY** We evaluate our proposed approach following <cite>Salehi et al. (2015)</cite> by computing Pearson's correlation between the predicted compositionality (i.e., from either comp 1 or comp 2 ) and human ratings for overall compositionality.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_9",
  "x": "**RESULTS** We begin by considering results using the default settings (described in section \u00a73.1) using both comp 1 and comp 2 . For comp 1 , we set \u03b1 to 0.7 for ENC and GNC following <cite>Salehi et al. (2015)</cite> ; for EVPC we set \u03b1 to 0.5. Results are shown in table 2. For ENC, and the particle component of EVPC, both comp 1 and comp 2 achieve significant correlations (i.e., p < 0.05). However, for GNC, and the verb component of EVPC, neither approach to predicting compositionality gives significant correlations. These correlations are well below those of previous work. For example, using comp 1 with representations of the MWE and component words obtained from word2vec (Mikolov et al., 2013) , <cite>Salehi et al. (2015)</cite> achieve correlations of 0.717, 0.289, and 0.400 for ENC, the verb component of EVPC, and GNC, respectively. 8 Nevertheless, the results in table 2, and in particular the significant correlations for ENC and the particle component of EVPC, indicate that character-level neural network language models do capture some information about the compositionality of MWEs, at least for certain types of expressions.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_10",
  "x": "For example, using comp 1 with representations of the MWE and component words obtained from word2vec (Mikolov et al., 2013) , <cite>Salehi et al. (2015)</cite> achieve correlations of 0.717, 0.289, and 0.400 for ENC, the verb component of EVPC, and GNC, respectively. 8 Nevertheless, the results in table 2, and in particular the significant correlations for ENC and the particle component of EVPC, indicate that character-level neural network language models do capture some information about the compositionality of MWEs, at least for certain types of expressions. We now consider the compositionality of individual component words. Because of the low correlations on GNC in the previous experiments, we do not consider it further here. In this case, we compute the compositionality of a specific component word as below, where C is the vector representation of a component word. Note that this corresponds to comp 1 with \u03b1 = 1 or 0, in the case of the first and second component words, respectively. We compare these compositionality predictions with the human judgements for Table 4 : Pearson's correlation (r) for MWEs that are attested, and unattested, in each dataset, using comp 1 and comp 2 . Significant correlations (p < 0.05) are indicated with *. The number of attested and unattested MWEs in each dataset is also shown.",
  "y": "background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_11",
  "x": "For ENC, and the particle component of EVPC, both comp 1 and comp 2 achieve significant correlations (i.e., p < 0.05). However, for GNC, and the verb component of EVPC, neither approach to predicting compositionality gives significant correlations. These correlations are well below those of previous work. For example, using comp 1 with representations of the MWE and component words obtained from word2vec (Mikolov et al., 2013) , <cite>Salehi et al. (2015)</cite> achieve correlations of 0.717, 0.289, and 0.400 for ENC, the verb component of EVPC, and GNC, respectively. 8 Nevertheless, the results in table 2, and in particular the significant correlations for ENC and the particle component of EVPC, indicate that character-level neural network language models do capture some information about the compositionality of MWEs, at least for certain types of expressions. We now consider the compositionality of individual component words. Because of the low correlations on GNC in the previous experiments, we do not consider it further here. In this case, we compute the compositionality of a specific component word as below, where C is the vector representation of a component word. Note that this corresponds to comp 1 with \u03b1 = 1 or 0, in the case of the first and second component words, respectively.",
  "y": "differences"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_12",
  "x": "There are 71 such expressions. For the compositionality of the particle component, comp 1 and comp 2 achieve correlations of 0.327 and 0.308, respectively. These correlations are significant (p < 0.05). Word embedding models -such as that used in the approach to predicting compositionality of <cite>Salehi et al. (2015)</cite> -typically do not learn representations for low frequency items. 9 These results demonstrate that the proposed model is able to predict the compositionality for low frequency items, that would not typically be in-vocabulary for word embedding models, and for which compositionality models based only on word embeddings would not be able to make predictions. 10 For GNC, and the verb component of EVPC, in line with the previous results over the entire dataset, neither compositionality measure gives significant correlations, with the exception of the verb component of EVPC using comp 2 for unattested expressions, although again the number of expressions here is relatively small. In an effort to improve on the default setup we considered a range of model variations. In particular we considered an RNN and GRU (instead of an LSTM), character embeddings of size 25 and 50 (instead of a one-hot representation), increasing the batch size to 100 (from 20), using dropout between 0.2-0.6, and using a bi-directional LSTM. None of these variations led to consistent improvements over the default setup.",
  "y": "background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_13",
  "x": "To further investigate this, we focused on expressions in EVPC with less than 5 usages in the training corpus. There are 71 such expressions. For the compositionality of the particle component, comp 1 and comp 2 achieve correlations of 0.327 and 0.308, respectively. These correlations are significant (p < 0.05). Word embedding models -such as that used in the approach to predicting compositionality of <cite>Salehi et al. (2015)</cite> -typically do not learn representations for low frequency items. 9 These results demonstrate that the proposed model is able to predict the compositionality for low frequency items, that would not typically be in-vocabulary for word embedding models, and for which compositionality models based only on word embeddings would not be able to make predictions. 10 For GNC, and the verb component of EVPC, in line with the previous results over the entire dataset, neither compositionality measure gives significant correlations, with the exception of the verb component of EVPC using comp 2 for unattested expressions, although again the number of expressions here is relatively small. In an effort to improve on the default setup we considered a range of model variations. In particular we considered an RNN and GRU (instead of an LSTM), character embeddings of size 25 and 50 (instead of a one-hot representation), increasing the batch size to 100 (from 20), using dropout between 0.2-0.6, and using a bi-directional LSTM.",
  "y": "differences"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_0",
  "x": "Various methods have been proposed for aligning texts in two or more languages such as the Canadian Parliamentary Debates (Hansards). Some of these methods generate a bilingual lexicon as a by-product. We present an alternative alignment strategy which we call K-vec, that starts by estimating the lexicon. For example, it discovers that the English word fisheries is similar to the French p~ches by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French. K-vec does not depend on sentence boundaries. There have been quite a number of recent papers on parallel text: Brown et al ---------------------------------- **MOTIVATION** There have been quite a number of recent papers on parallel text: Brown et al (1990 Brown et al ( , 1991 Brown et al ( , 1993 , Chen (1993) , <cite>Church (1993)</cite> , , Dagan et al (1993) , Church (1991, 1993) , Isabelle (1992) , Kay and Rgsenschein (1993) , Klavans and Tzoukermann (1990) , Kupiec (1993) , Matsumoto (1991) , Ogden and Gonzales (1993) , Shemtov (1993) , Simard et al (1992) , WarwickArmstrong and Russell (1990) , Wu (to appear).",
  "y": "background"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_1",
  "x": "For example, it discovers that the English word fisheries is similar to the French p~ches by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French. K-vec does not depend on sentence boundaries. There have been quite a number of recent papers on parallel text: Brown et al ---------------------------------- **MOTIVATION** There have been quite a number of recent papers on parallel text: Brown et al (1990 Brown et al ( , 1991 Brown et al ( , 1993 , Chen (1993) , <cite>Church (1993)</cite> , , Dagan et al (1993) , Church (1991, 1993) , Isabelle (1992) , Kay and Rgsenschein (1993) , Klavans and Tzoukermann (1990) , Kupiec (1993) , Matsumoto (1991) , Ogden and Gonzales (1993) , Shemtov (1993) , Simard et al (1992) , WarwickArmstrong and Russell (1990) , Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese. In previous work , we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual (Aho, Kernighan, Weinberger (1980) ), using charalign<cite> (Church, 1993)</cite> , a method that looks for character sequences that are the same in both the source and target.",
  "y": "motivation background"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_2",
  "x": "**MOTIVATION** There have been quite a number of recent papers on parallel text: Brown et al (1990 Brown et al ( , 1991 Brown et al ( , 1993 , Chen (1993) , <cite>Church (1993)</cite> , , Dagan et al (1993) , Church (1991, 1993) , Isabelle (1992) , Kay and Rgsenschein (1993) , Klavans and Tzoukermann (1990) , Kupiec (1993) , Matsumoto (1991) , Ogden and Gonzales (1993) , Shemtov (1993) , Simard et al (1992) , WarwickArmstrong and Russell (1990) , Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese. In previous work , we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual (Aho, Kernighan, Weinberger (1980) ), using charalign<cite> (Church, 1993)</cite> , a method that looks for character sequences that are the same in both the source and target. The charalign method was designed for European language pairs, where cognates often share character sequences, e.g., government and gouvernement. In general, this approach doesn't work between languages such as English and Japanese which are written in different alphabets. The AWK manual happens to contain a large number of examples and technical words that are the same in the English source and target Japanese. It remains an open question how we might be able to align a broader class of texts, especially those that are written in different character sets and share relatively few character sequences.",
  "y": "background"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_3",
  "x": "There have been quite a number of recent papers on parallel text: Brown et al ---------------------------------- **MOTIVATION** There have been quite a number of recent papers on parallel text: Brown et al (1990 Brown et al ( , 1991 Brown et al ( , 1993 , Chen (1993) , <cite>Church (1993)</cite> , , Dagan et al (1993) , Church (1991, 1993) , Isabelle (1992) , Kay and Rgsenschein (1993) , Klavans and Tzoukermann (1990) , Kupiec (1993) , Matsumoto (1991) , Ogden and Gonzales (1993) , Shemtov (1993) , Simard et al (1992) , WarwickArmstrong and Russell (1990) , Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese. In previous work , we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual (Aho, Kernighan, Weinberger (1980) ), using charalign<cite> (Church, 1993)</cite> , a method that looks for character sequences that are the same in both the source and target. The charalign method was designed for European language pairs, where cognates often share character sequences, e.g., government and gouvernement. In general, this approach doesn't work between languages such as English and Japanese which are written in different alphabets.",
  "y": "background motivation"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_4",
  "x": "The K-vec method attempts to address this question. ---------------------------------- **THE K-VEC ALGORITHM** K-vec starts by estimating the lexicon. Consider the example: fisheries --~ p~ches. The K-vec algorithm will discover this fact by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French. The concordances for fisheries and p~ches are shown in Tables 1 and 2 (at the end of this paper). 1 1. These tables were computed from a small fragment of the Canadian Hansards that has been used in a number of other studies: <cite>Church (1993)</cite> and Simard et al (1992 show where the concordances were found in the texts.",
  "y": "uses"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_5",
  "x": "Ideally, we would like to apply the K-vec algorithm to all pairs of English and French words, but unfortunately, there are too many such pairs to consider. We therefore limited the search to pairs of words in the frequency range: 3-10. This heuristic makes the search practical, and catches many interesting pairs) ---------------------------------- **RESULTS** This algorithm was applied to a fragment of the Canadian Hansards that has been used in a number of other studies: <cite>Church (1993)</cite> and Simard et al (1992) . The 30 significant pairs with the largest mutual information values are shown in Table 9 . As can be seen, the results provide a quick-anddirty estimate of a bilingual lexicon. When the pair is not a direct translation, it is often the translation of a collocate, as illustrated by acheteur ~ Limited and Santd -~ Welfare.",
  "y": "uses"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_6",
  "x": "---------------------------------- **CONCLUSIONS** The K-vec algorithm generates a quick-and-dirty estimate of a bilingual lexicon. This estimate could be used as a starting point for a more detailed alignment algorithm such as word_align . In this way, we might be able to apply word_align to a broader class of language combinations including possibly English-Japanese and English-Chinese. Currently, word_align depends on charalign<cite> (Church, 1993)</cite> to generate a starting point, which limits its applicability to European languages since char_align was designed for language pairs that share a common alphabet. ---------------------------------- **REFERENCES** Aho, Kernighan, Weinberger (1980) \"The AWK Programming",
  "y": "uses background"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_0",
  "x": "Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the crosslingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques. ---------------------------------- **INTRODUCTION** Recent work demonstrated that word embeddings induced from large text collections encode many human biases (e.g., Bolukbasi et al., 2016;<cite> Caliskan et al., 2017)</cite> .",
  "y": "motivation background"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_1",
  "x": "While biases encoded in word embeddings can be a useful data source for diachronic analyses of societal biases (e.g., Garg et al., 2018) , they may cause ethical problems for many downstream applications and NLP models. In order to measure the extent to which various societal biases are captured by word embeddings,<cite> Caliskan et al. (2017)</cite> proposed the Word Embedding Association Test (WEAT). WEAT measures semantic similarity, computed through word embeddings, between two sets of target words (e.g., insects vs. flowers) and two sets of attribute words (e.g., pleasant vs. unpleasant words). While they test a number of biases, the analysis is limited in scope to English as the only language, GloVe (Pennington et al., 2014) as the embedding model, and Common Crawl as the type of text. Following the same methodology, McCurdy and Serbetci (2017) extend the analysis to three more languages (German, Dutch, Spanish) , but test only for gender bias. In this work, we present the most comprehensive study of biases captured by distributional word vector to date. We create XWEAT, a collection of multilingual and cross-lingual versions of the WEAT dataset, by translating WEAT to six other languages and offer a comparative analysis of biases over seven diverse languages. Furthermore, we measure the consistency of WEAT biases across different embedding models and types of corpora. What is more, given the recent surge of models for inducing cross-lingual embedding spaces (Mikolov et al., 2013a; Hermann and Blunsom, 2014; Smith et al., 2017; Conneau et al., 2018; Artetxe et al., 2018; Hoshen and Wolf, 2018, inter alia) and their ubiquitous application in cross-lingual transfer of NLP models for downstream tasks, we investigate cross-lingual biases encoded in cross-lingual embedding spaces and compare them to bias effects present of corresponding monolingual embeddings.",
  "y": "background"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_2",
  "x": "---------------------------------- **DATA FOR MEASURING BIASES** We first introduce the WEAT dataset<cite> (Caliskan et al., 2017)</cite> and then describe XWEAT, our multilingual and cross-lingual extension of WEAT designed for comparative bias analyses across languages and in cross-lingual embedding spaces. ---------------------------------- **WEAT** The Word Embedding Association Test (WEAT)<cite> (Caliskan et al., 2017)</cite> is an adaptation of the Implicit Association Test (IAT) (Nosek et al., 2002) . Whereas IAT measures biases based on response times of human subjects to provided stimuli, WEAT quantifies the biases using semantic similarities between word embeddings of the same stimuli. For each bias test, WEAT specifies four stimuli sets: two sets of target words and two sets of attribute words. The sets of target words represent stimuli between which we want to measure the bias (e.g., for gender biases, one target set could contain male names and the other females names).",
  "y": "uses"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_3",
  "x": "We first introduce the WEAT dataset<cite> (Caliskan et al., 2017)</cite> and then describe XWEAT, our multilingual and cross-lingual extension of WEAT designed for comparative bias analyses across languages and in cross-lingual embedding spaces. ---------------------------------- **WEAT** The Word Embedding Association Test (WEAT)<cite> (Caliskan et al., 2017)</cite> is an adaptation of the Implicit Association Test (IAT) (Nosek et al., 2002) . Whereas IAT measures biases based on response times of human subjects to provided stimuli, WEAT quantifies the biases using semantic similarities between word embeddings of the same stimuli. For each bias test, WEAT specifies four stimuli sets: two sets of target words and two sets of attribute words. The sets of target words represent stimuli between which we want to measure the bias (e.g., for gender biases, one target set could contain male names and the other females names). The attribute words, on the other hand, represent stimuli towards which the bias should be measured (e.g., one list could contain pleasant stimuli like health and love and the other negative war and death). The WEAT dataset defines ten bias tests, each containing two target and two attribute sets.",
  "y": "background"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_4",
  "x": "We adopt the general bias-testing framework from<cite> Caliskan et al. (2017)</cite> , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora. Furthermore, unlike<cite> Caliskan et al. (2017)</cite> , we test whether biases depend on the selection of the similarity metric. Finally, given the ubiquitous adoption of cross-lingual embeddings (Ruder et al., 2017; Glava\u0161 et al., 2019) , we investigate biases in a variety of bilingual embedding spaces. Bias-Testing Framework. We first describe the WEAT framework<cite> (Caliskan et al., 2017)</cite> . Let X and Y be two sets of targets, and A and B two sets of attributes (see \u00a72.1). The tested statistic is the difference between X and Y in average similarity of their terms with terms from A and B: with association difference for term t computed as: where t is the distributional vector of term t and f is a similarity or distance metric, fixed to cosine similarity in the original work<cite> (Caliskan et al., 2017)</cite> .",
  "y": "extends uses"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_5",
  "x": "We did not translate or modify proper names from WEAT sets 3-6. In our multilingual and cross-lingual experiments we, however, discard the (translations of) WEAT tests for which we cannot find more than 20% of words from some target or attribute set in the embedding vocabulary of the respective language. This strategy eliminates tests 3-5 and 10 which include proper American names, majority of which can not be found in distributional vocabularies of other languages. The exception to this is test 6, containing frequent English first names (e.g., Paul, Lisa), which we do find in distributional vocabularies of other languages as well. In summary, for languages other than EN and for cross-lingual settings, we execute six bias tests (T1, T2, T6-T9). ---------------------------------- **METHODOLOGY** We adopt the general bias-testing framework from<cite> Caliskan et al. (2017)</cite> , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora. Furthermore, unlike<cite> Caliskan et al. (2017)</cite> , we test whether biases depend on the selection of the similarity metric.",
  "y": "extends"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_6",
  "x": "In summary, for languages other than EN and for cross-lingual settings, we execute six bias tests (T1, T2, T6-T9). ---------------------------------- **METHODOLOGY** We adopt the general bias-testing framework from<cite> Caliskan et al. (2017)</cite> , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora. Furthermore, unlike<cite> Caliskan et al. (2017)</cite> , we test whether biases depend on the selection of the similarity metric. Finally, given the ubiquitous adoption of cross-lingual embeddings (Ruder et al., 2017; Glava\u0161 et al., 2019) , we investigate biases in a variety of bilingual embedding spaces. Bias-Testing Framework. We first describe the WEAT framework<cite> (Caliskan et al., 2017)</cite> . Let X and Y be two sets of targets, and A and B two sets of attributes (see \u00a72.1).",
  "y": "extends"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_7",
  "x": "We adopt the general bias-testing framework from<cite> Caliskan et al. (2017)</cite> , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora. Furthermore, unlike<cite> Caliskan et al. (2017)</cite> , we test whether biases depend on the selection of the similarity metric. Finally, given the ubiquitous adoption of cross-lingual embeddings (Ruder et al., 2017; Glava\u0161 et al., 2019) , we investigate biases in a variety of bilingual embedding spaces. Bias-Testing Framework. We first describe the WEAT framework<cite> (Caliskan et al., 2017)</cite> . Let X and Y be two sets of targets, and A and B two sets of attributes (see \u00a72.1). The tested statistic is the difference between X and Y in average similarity of their terms with terms from A and B: with association difference for term t computed as: where t is the distributional vector of term t and f is a similarity or distance metric, fixed to cosine similarity in the original work<cite> (Caliskan et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_8",
  "x": "The tested statistic is the difference between X and Y in average similarity of their terms with terms from A and B: with association difference for term t computed as: where t is the distributional vector of term t and f is a similarity or distance metric, fixed to cosine similarity in the original work<cite> (Caliskan et al., 2017)</cite> . The effect size, that is, the \"amount of bias\", is computed as the normalized measure of separation between association distributions: where \u00b5 denotes the mean and \u03c3 standard deviation. ---------------------------------- **DIMENSIONS OF BIAS ANALYSIS.** We analyze the bias effects across multiple dimensions. First, we analyze the effect that different embedding models have: we compare biases of distributional spaces induced from English Wikipedia, using CBOW (Mikolov et al., 2013b) , GLOVE (Pennington et al., 2014) , FASTTEXT (Bojanowski et al., 2017) , and DICT2VEC algorithms (Tissier et al., 2017) .",
  "y": "background"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_9",
  "x": "Corpora. In Table 4 we compare the biases of embeddings trained with the same model (GLOVE) but on different corpora: Common Crawl (i.e., noisy web content), Wikipedia (i.e., encyclopedic the definition of A and vice versa (Tissier et al., 2017) . 5 This is consistent with the original results obtained by<cite> Caliskan et al. (2017)</cite> . Corpus T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 WIKI 1.4 1.5 1.2 1.4 1.4 1.8 1.2 1.3 1.3 1.2 CC 1.5 1.6 1.5 1.6 1.4 1.9 1.1 1.3 1.4 1.3 TWEETS 1.2 1.0 1.1 1.2 1.2 1.2 \u22120.2 * 0.6 * 0.7 * 0.8 * Table 6 : XWEAT bias effects (aggregated over all six tests) for cross-lingual word embedding spaces. Rows: targets language; columns: attributes language. Asterisks indicate the inclusion of bias effects sizes in the aggregation that were insignificant at \u03b1 < 0.05. more pronounced for embeddings trained on the Common Crawl than for those obtained on encyclopedic texts (Wikipedia). Countering our intuition, the corpus of tweets seems to be consistently less biased (across all tests) than Wikipedia. In fact, the biases covered by tests T7-T10 are not even significantly present in the vectors trained on tweets.",
  "y": "similarities"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_10",
  "x": "**MULTILINGUAL COMPARISON.** ---------------------------------- **CONCLUSION** In this paper, we have presented the largest study to date on biases encoded in distributional word vector spaces. To this end, we have extended previous analyses based on the WEAT test<cite> (Caliskan et al., 2017</cite>; McCurdy and Serbetci, 2017) in multiple dimensions: across seven languages, four embedding models, and three different types of text. We find that different models may produce embeddings with very different biases, which stresses the importance of embedding model selection when fair text representations are to be created. Surprisingly, we find that the user-generated texts, such as tweets, may be less biased than redacted content. Furthermore, we have investigated the bias effects in cross-lingual embedding spaces and have shown that they may be predicted from the biases of corresponding monolingual embeddings. We make the XWEAT dataset and the testing code publicly available, 7 hoping to fuel further research on biases encoded in word representations.",
  "y": "extends uses"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_0",
  "x": "La Lituanie, la Lettonie et l'Estonie s'ouvrent ainsi au multipartisme. 1 The reformative push, initiated from Moscow in 1985, comes back like a boomerang towards the USSR. Lithuania, Latvia and Estonia thus open themselves to the multiparty system. 2 However, discourse connectives do not always mark the presence of discourse relations. For example, while the word 'et' is not a discourse connective in (1) , it signals a continuation relation in (2). (2) La f\u00e9d\u00e9ration CGT des transports s'est \u00e9lev\u00e9e contre \"l'absence de concertation\" et estime que les salari\u00e9s \"n'ont rien de bon \u00e0 attendre de cette restructuration\". 1 The CGT transport federation have risen against \"the lack of consultation\" and consider that employees have \"nothing positive to expect from this restructuring.\" 2 While studies have shown that discourse usage of discourse connectives can be accurately identified for English [13, <cite>20]</cite> , only a few studies have focused on the disambiguation of discourse connectives in other languages. In this paper, we investigate the usefulness of features proposed in the literature for the disambiguation of English discourse connectives for French discourse connectives. 3 This paper is organized as follow: Section 2 reviews related work.",
  "y": "motivation background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_1",
  "x": "To automatically disambiguate discourse connectives, discourse annotated corpora such as the Penn Discourse Treebank (PDTB) [22] are instrumented. The PDTB is the largest corpus of discourse annotated texts. It contains articles from the Wall Street Journal, where discourse connectives that are used in discourse-usage have been annotated by the discourse relation that they signal. The same approach has been used in the French Discourse Treebank (FDTB) [7] , however to date, only discourse-usage and non-discourse-usage of French discourse connectives have been annotated in the FDTB. Most of previous work on the disambiguation of discourse connectives have focused on English discourse connectives [13, 14, <cite>20]</cite> . One of earliest and pioneer work on the disambiguation of discourse connectives, Pitler and Nenkova<cite> [20]</cite> , showed that four syntactic features (see Section 3.4 for details about the features) and the connective itself can disambiguate discourse connectives with an accuracy of 95.04% within the PDTB [22] . Their approach used these features not only to disambiguate discourse connectives between discourse-usage and non-discourse-usage, but also to tag the discourse relation signalled by the discourse connectives. Later, Lin et al. [13] used the context of the connective (i.e. the previous and the following word of the connective) and added seven lexico-syntactic features to the feature set proposed by Pitler and Nenkova<cite> [20]</cite> . In doing so, Lin et al. achieved an accuracy of 97.34% for the disambiguation of discourse connectives in the PDTB.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_2",
  "x": "To automatically disambiguate discourse connectives, discourse annotated corpora such as the Penn Discourse Treebank (PDTB) [22] are instrumented. The PDTB is the largest corpus of discourse annotated texts. It contains articles from the Wall Street Journal, where discourse connectives that are used in discourse-usage have been annotated by the discourse relation that they signal. The same approach has been used in the French Discourse Treebank (FDTB) [7] , however to date, only discourse-usage and non-discourse-usage of French discourse connectives have been annotated in the FDTB. Most of previous work on the disambiguation of discourse connectives have focused on English discourse connectives [13, 14, <cite>20]</cite> . One of earliest and pioneer work on the disambiguation of discourse connectives, Pitler and Nenkova<cite> [20]</cite> , showed that four syntactic features (see Section 3.4 for details about the features) and the connective itself can disambiguate discourse connectives with an accuracy of 95.04% within the PDTB [22] . Their approach used these features not only to disambiguate discourse connectives between discourse-usage and non-discourse-usage, but also to tag the discourse relation signalled by the discourse connectives. Later, Lin et al. [13] used the context of the connective (i.e. the previous and the following word of the connective) and added seven lexico-syntactic features to the feature set proposed by Pitler and Nenkova<cite> [20]</cite> . In doing so, Lin et al. achieved an accuracy of 97.34% for the disambiguation of discourse connectives in the PDTB.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_3",
  "x": "Later, Lin et al. [13] used the context of the connective (i.e. the previous and the following word of the connective) and added seven lexico-syntactic features to the feature set proposed by Pitler and Nenkova<cite> [20]</cite> . In doing so, Lin et al. achieved an accuracy of 97.34% for the disambiguation of discourse connectives in the PDTB. On the other hand, the disambiguation of discourse connectives in languages other than English has received much less attention. Due to syntactic differences across languages and different discourse annotation methodologies, the techniques developed for one language may not be as effective in another. For example, English discourse connectives include mostly subordinating conjunctions (e.g. when) or coordinating conjunctions (e.g. but). In addition, only a few connectives are disjoint (e.g. On the one hand ... On the other hand). This is not the case for Chinese which uses many more disjoint connectives [26] . Inspired by Pitler and Nenkova<cite> [20]</cite> , Alsaif and Markert [4] proposed an approach for the disambiguation of Arabic Discourse connectives. Alsaif and Markert have shown that the features proposed by Pitler and Nenkova<cite> [20]</cite> work well for Arabic with an accuracy of 91.2%.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_4",
  "x": "Inspired by Pitler and Nenkova<cite> [20]</cite> , Alsaif and Markert [4] proposed an approach for the disambiguation of Arabic Discourse connectives. Alsaif and Markert have shown that the features proposed by Pitler and Nenkova<cite> [20]</cite> work well for Arabic with an accuracy of 91.2%. Moreover, they further improved the result of their system by considering Arabic-specific morphological features and achieved an accuracy of 92.4%. Today, due to the availability of discourse annotated corpora such as the French Discourse Treebank [6] , it is possible to analyse how the features developed for English behave when applied to French. ---------------------------------- **CORPUS** To evaluate the disambiguation of French discourse connectives, we used the French Discourse Treebank (FDTB) [6] which constitutes the largest publicly available discourse annotated corpus for French. The corpus contains the annotation of more than 10K connectives used in discourse usage in the French Treebank corpus [1] . The FDTB uses the French discourse connectives of the LEXCONN resource 4 [23] , a lexicon of 328 French discourse connectives (e.g. 'alors que') and their morphological variations (e.g. 'alors qu' ').",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_5",
  "x": "Due to syntactic differences across languages and different discourse annotation methodologies, the techniques developed for one language may not be as effective in another. For example, English discourse connectives include mostly subordinating conjunctions (e.g. when) or coordinating conjunctions (e.g. but). In addition, only a few connectives are disjoint (e.g. On the one hand ... On the other hand). This is not the case for Chinese which uses many more disjoint connectives [26] . Inspired by Pitler and Nenkova<cite> [20]</cite> , Alsaif and Markert [4] proposed an approach for the disambiguation of Arabic Discourse connectives. Alsaif and Markert have shown that the features proposed by Pitler and Nenkova<cite> [20]</cite> work well for Arabic with an accuracy of 91.2%. Moreover, they further improved the result of their system by considering Arabic-specific morphological features and achieved an accuracy of 92.4%. Today, due to the availability of discourse annotated corpora such as the French Discourse Treebank [6] , it is possible to analyse how the features developed for English behave when applied to French. ----------------------------------",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_6",
  "x": "Regardless of its source, this disparity motivated us to investigate the applicability of features proposed for the disambiguation of English discourse connectives for French. ---------------------------------- **FEATURES** As mentioned in Section 2, Pitler and Nenkova<cite> [20]</cite> have shown that the context of discourse connectives in the syntactic tree is very discriminating for the disambiguation of English discourse connectives. They proposed four syntactic features: 1. SelfCat: The highest node in the parse tree that covers the connective words but nothing more. 2. SelfCatParent: The parent of the SelfCat. To illustrate these four features, consider the parse tree of the second sentence in Example (1) shown in Figure 1 and the discourse connective 'ainsi'. The SelfCat node is the 'ADV' node in the parse tree and its parent, left and right siblings are the 'S', 'VN' and 'PP' nodes, respectively.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_7",
  "x": "Table 4 shows the entropy of French and English discourse connectives that signal the Cause relation with their most likely translations 6 as identified by Zufferey and Cartoni [27] . As Table 4 shows, there does not seem to be a direct rela- 6 Note that some translations of discourse connectives such as '\u00e9tant donn\u00e9 que' are not considered discourse connectives in the FDTB and the PDTB because they do not fit the formal definition of discourse connectives. Therefore, we do not list their entropy in Table 4 . (1) tionship between the entropy of the mapped discourse connectives. For example, while the French discourse connective 'car' has an entropy of 0.05 (i.e. 'car' is more than 99% of the time used in discourse-usage in the FDTB), its translations in English (i.e. 'because', 'since', and 'as') are very ambiguous. The disparity between the entropy of discourse connectives in the FDTB and the PDTB can be explained by the differences between the languages and the annotation methodology. Regardless of its source, this disparity motivated us to investigate the applicability of features proposed for the disambiguation of English discourse connectives for French. ---------------------------------- **FEATURES** As mentioned in Section 2, Pitler and Nenkova<cite> [20]</cite> have shown that the context of discourse connectives in the syntactic tree is very discriminating for the disambiguation of English discourse connectives.",
  "y": "uses"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_8",
  "x": "1. SelfCat: The highest node in the parse tree that covers the connective words but nothing more. 2. SelfCatParent: The parent of the SelfCat. To illustrate these four features, consider the parse tree of the second sentence in Example (1) shown in Figure 1 and the discourse connective 'ainsi'. The SelfCat node is the 'ADV' node in the parse tree and its parent, left and right siblings are the 'S', 'VN' and 'PP' nodes, respectively. In addition to the four features above, Pitler and Nenkova<cite> [20]</cite> used the discourse connective itself (case sensitive) as an additional feature for the classifier. The purpose of using the case sensitive version is to distinguish connectives positioned at the beginning of sentences. We slightly modified this feature by using the case-folded version of the discourse connective (called the Conn Feature). However, we created a new feature (called the Pos Feature) to explicitly indicate the position of the discourse connective within the sentence (i.e. ----------------------------------",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_9",
  "x": "The SelfCat node is the 'ADV' node in the parse tree and its parent, left and right siblings are the 'S', 'VN' and 'PP' nodes, respectively. In addition to the four features above, Pitler and Nenkova<cite> [20]</cite> used the discourse connective itself (case sensitive) as an additional feature for the classifier. The purpose of using the case sensitive version is to distinguish connectives positioned at the beginning of sentences. We slightly modified this feature by using the case-folded version of the discourse connective (called the Conn Feature). However, we created a new feature (called the Pos Feature) to explicitly indicate the position of the discourse connective within the sentence (i.e. ---------------------------------- **AT-THE-BEGINNING OR NOT-AT-THE-BEGINNING).** These two features are as informative as the case-sensitive connective string proposed by Pitler and Nenkova<cite> [20]</cite> , however, separating these features gives the classifier more flexibility when building its model. In Example (1), these two features are 'ainsi' and 'not-at-the-beginning', respectively.",
  "y": "extends"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_10",
  "x": "We slightly modified this feature by using the case-folded version of the discourse connective (called the Conn Feature). However, we created a new feature (called the Pos Feature) to explicitly indicate the position of the discourse connective within the sentence (i.e. ---------------------------------- **AT-THE-BEGINNING OR NOT-AT-THE-BEGINNING).** These two features are as informative as the case-sensitive connective string proposed by Pitler and Nenkova<cite> [20]</cite> , however, separating these features gives the classifier more flexibility when building its model. In Example (1), these two features are 'ainsi' and 'not-at-the-beginning', respectively. ---------------------------------- **DATA PREPARATION** Although the focus of the work is the disambiguation of French discourse connectives, we performed the same experiments with English discourse connectives as well.",
  "y": "similarities differences"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_11",
  "x": "Similarly to Pitler and Nenkova<cite> [20]</cite> , we report results using a maximum entropy classifier using ten-fold cross-validation over the extracted datasets. We used the off-the-shelf implementation of the maximum entropy classifier available in WEKA [9] for our experiments. Table 5 shows the overall performance of the classifier for the disambiguation of French and English discourse connectives. The results show that the classifier can distinguish between discourse-usage and non-discourse-usage of French discourse connectives with an accuracy of 94.2% and an FMeasure of 86.2%. This is close to the results achieved for English discourse connectives over the PDTB (accuracy of 93.6% and F-score of 88.9%). ---------------------------------- **FEATURE ANALYSIS** To evaluate the contribution of each feature, we ranked the features by their information gain for both languages. As Table 6 shows, with our datasets, the syntactic features provide less information about discourse-usage or non-discourseusage for French discourse connectives than they do for English.",
  "y": "similarities"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_12",
  "x": "The five discourse connectives 7 that achieve the lowest accuracy are listed in Table 9 . All the discourse connectives in Table 9 have very high entropy. For example, both 'effectivement' and 'alors' are among the top three ambiguous discourse connectives (see Table 3 ). Even though the accuracy of the classifier is higher than the baseline (except for the 'maintenant' discourse connective), the increase is small or not statistically significant. For example, the accuracy for the discourse connective 'effectivement' is 55.56% which is not statistically better than the baseline. These results show that for some connectives, the features proposed for English are sufficient (see Table 8 ), but for others, using only the connective and the syntactic features is not sufficient. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this paper, we have investigated the applicability of the syntactic and lexical features proposed by Pitler and Nenkova<cite> [20]</cite> for the disambiguation of English discourse connectives for French.",
  "y": "extends"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_0",
  "x": "Our results on both MBN and MFCC features are significantly higher than the previous state-of-the-art. The largest improvement comes from using the learned MBN features but our approach also improves results for MFCCs, which are the same features as were used in <cite>[15]</cite> . The learned MBN features provide better performance whereas the MFCCs are more cognitively plausible input features. The probing task shows that the model learns to recognise these words in the input. The system is not explicitly optimised to do so, but our results show that the lower layers learn to recognise this form related information from the input. After layer 2, the performance starts to decrease slightly which might indicate that these layers learn a more task-specific representation and it is to be expected that the final attention layer specialises in mapping from audio features to the multi-modal embedding space. In conclusion, we presented what are, to the best of our knowledge, the best results on spoken-caption to image retrieval. Our results improve significantly over previous approaches for both untrained and trained audio features. In a probing task, we show that the model learns to recognise words in the input speech signal.",
  "y": "similarities differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_1",
  "x": "As such, spoken input cannot be represented by conventional word embeddings (e.g. word2vec [4] , GloVe [5] ). These textbased embeddings are trained to encode word-level semantic knowledge and have become a mainstay in work on sentence representations (e.g. [6, 7] ). When we want to learn language directly from speech, we will have to do so in a more end-to-end fashion, without prior lexical level knowledge in terms of both form and semantics. In previous work [8] we used image-caption retrieval, where given a written caption the model must return the matching image and vice versa. We trained deep neural networks (DNNs) to create sentence embeddings without the use of prior knowledge of lexical semantics (see [7, 9, 10] for other studies on this task). The visually grounded sentence embeddings that arose capture semantic information about the sentence as measured by the Semantic Textual Similarity task (see [11] ), performing comparably to text-only methods that require word embeddings. In the current study we present an image-caption retrieval model that extends our previous work to spoken input. In [12, 13] , the authors adapted text based caption-image retrieval (e.g. [9] ) and showed that it is possible to perform speech-image retrieval using convolutional neural networks on spectral features. Our work is most closely related to the models presented in [12, 13, 14,<cite> 15]</cite> .",
  "y": "similarities"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_2",
  "x": "We trained deep neural networks (DNNs) to create sentence embeddings without the use of prior knowledge of lexical semantics (see [7, 9, 10] for other studies on this task). The visually grounded sentence embeddings that arose capture semantic information about the sentence as measured by the Semantic Textual Similarity task (see [11] ), performing comparably to text-only methods that require word embeddings. In the current study we present an image-caption retrieval model that extends our previous work to spoken input. In [12, 13] , the authors adapted text based caption-image retrieval (e.g. [9] ) and showed that it is possible to perform speech-image retrieval using convolutional neural networks on spectral features. Our work is most closely related to the models presented in [12, 13, 14,<cite> 15]</cite> . In the current study we improve upon these previous approaches to visual grounding of speech and present state-of-the-art image-caption retrieval results. The work by [12, 13, 14,<cite> 15]</cite> and the results presented here are a step towards more cognitively plausible models of language learning as it is more natural to learn language without prior assumptions about the lexical level. For instance, research indicates that the adult lexicon contains many relatively fixed multi-word expressions (e.g., 'how-are-you-doing') [16] . Furthermore, early during language acquisition the lexicon consists of entire utterances before a child's language use becomes more adult-like [16, 17, 18, 19] .",
  "y": "uses"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_3",
  "x": "In the current study we improve upon these previous approaches to visual grounding of speech and present state-of-the-art image-caption retrieval results. The work by [12, 13, 14,<cite> 15]</cite> and the results presented here are a step towards more cognitively plausible models of language learning as it is more natural to learn language without prior assumptions about the lexical level. For instance, research indicates that the adult lexicon contains many relatively fixed multi-word expressions (e.g., 'how-are-you-doing') [16] . Furthermore, early during language acquisition the lexicon consists of entire utterances before a child's language use becomes more adult-like [16, 17, 18, 19] . Image to spoken-caption retrieval models do not know a priori which constituents of the input are important and have no prior knowledge of lexical level semantics. We probe the resulting model to investigate whether it learns to recognise lexical units in the input without being explicitly trained to do so. We test two types of acoustic features; Mel Frequency Cepstral Coefficients (MFCCs) and Multilingual Bottleneck (MBN) features. MFCCs are features that can be computed for any speech signal without needing any other data, while the MBN features are 'learned' features that result from training a network on top of MFCCs in order to recognise phoneme states. While MBN features have been shown to be useful in several speech recognition tasks (e.g. [20, 21] ), learned audio features face the same issue as word embeddings, as humans learn to extract useful features from the audio signal as a result of learning to understand language and not as a separate process.",
  "y": "background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_4",
  "x": "The MBN features are created using a pre-trained DNN made available by [21] . In short, the network is trained on multilingual speech data (11 languages, no English) to classify phoneme states. The MBN features consist of the outputs of intermediate network layers where the network is compressed from 1500 features to 30 features (see [21] for the full details of the network and training). ---------------------------------- **MODEL ARCHITECTURE** Our multimodal encoder maps images and their corresponding captions to a common embedding space. The idea is to make matching images and captions lie close together and mismatched images and captions lie far apart in the embedding space. Our model consists of two parts; an image encoder and a sentence encoder as depicted in Figure 1 . The approach is based on our own text-based model described in [8] and on the speech-based models described in [13,<cite> 15]</cite> and we refer to those studies for more details.",
  "y": "uses background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_5",
  "x": "---------------------------------- **TRAINING** Following [8] , the model is trained to embed the images and captions such that the cosine similarity between image and caption pairs is larger (by a certain margin) than the similarity be-tween mismatching pairs. This so called hinge loss L as a function of the network parameters \u03b8 is given by: where the other caption-image pairs in the batch serve to create mismatched pairs (c, i \u2032 ) and (c \u2032 , i). We take the cosine similarity cos(x, y) and subtract the similarity of the mismatched pairs from the matching pairs such that the loss is only zero when the matching pair is more similar than the mismatched pairs by a margin \u03b1. We use importance sampling to select the mismatched pairs; rather than using all the other samples in the mini-batch as mismatched pairs (as done in [8,<cite> 15]</cite> ), we calculate the loss using only the hardest examples (i.e. mismatched pairs with high cosine similarity). While [10] used only the single hardest example in the batch for text-captions, we found that this did not work for the spoken captions. Instead we found that using the hardest 25 percent worked well.",
  "y": "differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_6",
  "x": "We train the networks for 32 epochs and take a snapshot for ensembling at every fourth epoch. For ensembling we use the two snapshots with the highest performance on the development data and simply sum their embeddings. The main differences with the approaches described in [13,<cite> 15]</cite> are the use of multi-layered GRUs, importance sampling, the cyclic learning rate, snapshot ensembling and the use of vectorial rather than scalar attention. ---------------------------------- **WORD PRESENCE DETECTION** While our model is not explicitly trained to recognise words or segment the speech signal, previous work has shown that such information can be extracted by visual grounding models<cite> [15,</cite> 28] . <cite>[15]</cite> use a binary decision task: given a word and a sentence embedding, decide if the word occurs in the sentence. Our approach is similar to the spoken-bag-of-words prediction task described in [28] . Given a sentence embedding created by our model, a classifier has to decide which of the words in its vocabulary occur in the sentence.",
  "y": "differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_7",
  "x": "While our model is not explicitly trained to recognise words or segment the speech signal, previous work has shown that such information can be extracted by visual grounding models<cite> [15,</cite> 28] . <cite>[15]</cite> use a binary decision task: given a word and a sentence embedding, decide if the word occurs in the sentence. Our approach is similar to the spoken-bag-of-words prediction task described in [28] . Given a sentence embedding created by our model, a classifier has to decide which of the words in its vocabulary occur in the sentence. Based on the original written captions, our database contains 7,374 unique words with a combined occurrence frequency of 324,480. From these we select words that occur between 50 and a 1,000 times and are over 3 characters long so that there are enough examples in the data that the model might actually learn to recognise them, and to filter out punctuation, spelling mistakes, numerals and most function words. This leaves 460 unique words, mostly verbs and nouns, with a combined occurrence frequency of 87,020 in our data. We construct a vector for each sentence in Flickr8k indicating which of these words is present. We do not encode multiple occurrences of the same word in one sentence.",
  "y": "background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_8",
  "x": "---------------------------------- **WORD PRESENCE DETECTION** While our model is not explicitly trained to recognise words or segment the speech signal, previous work has shown that such information can be extracted by visual grounding models<cite> [15,</cite> 28] . <cite>[15]</cite> use a binary decision task: given a word and a sentence embedding, decide if the word occurs in the sentence. Our approach is similar to the spoken-bag-of-words prediction task described in [28] . Given a sentence embedding created by our model, a classifier has to decide which of the words in its vocabulary occur in the sentence. Based on the original written captions, our database contains 7,374 unique words with a combined occurrence frequency of 324,480. From these we select words that occur between 50 and a 1,000 times and are over 3 characters long so that there are enough examples in the data that the model might actually learn to recognise them, and to filter out punctuation, spelling mistakes, numerals and most function words. This leaves 460 unique words, mostly verbs and nouns, with a combined occurrence frequency of 87,020 in our data.",
  "y": "background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_9",
  "x": "We compare our models to [12] and <cite>[15]</cite> , and include our own character-based model for comparison. [12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention. The character-based model is similar to the model we use here and was trained on the original Flickr8k text captions (see [8] for a full description). Both our MFCC and MBN based model significantly outperform previous spoken captionto-image methods on the Flickr8k dataset. The largest improvement is the MBN model which outperforms the results reported in <cite>[15]</cite> by as much as 23.2 percentage points on R@10. The MFCC model also improves on previous results but scores significantly lower than the MBN model across the board, improving as much as 12.3 percentage points over previous work. There is a large performance gap between the text-caption to image retrieval results and the spoken-caption to image results, showing there is still a lot of room for improvement. ---------------------------------- **RESULTS**",
  "y": "uses"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_10",
  "x": "We use the same data split that was used for training the multi-modal encoder, so that we test word presence detection on data that was not seen by either the encoder or the decoder. Table 1 shows the performance of our models on the imagecaption retrieval task. The caption embeddings are ranked by cosine distance to the image and vice versa where R@N is the percentage of test items for which the correct image or caption was in the top N results. We compare our models to [12] and <cite>[15]</cite> , and include our own character-based model for comparison. [12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention. The character-based model is similar to the model we use here and was trained on the original Flickr8k text captions (see [8] for a full description). Both our MFCC and MBN based model significantly outperform previous spoken captionto-image methods on the Flickr8k dataset. The largest improvement is the MBN model which outperforms the results reported in <cite>[15]</cite> by as much as 23.2 percentage points on R@10. The MFCC model also improves on previous results but scores significantly lower than the MBN model across the board, improving as much as 12.3 percentage points over previous work.",
  "y": "uses"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_11",
  "x": "The acoustic features consist of 30 (MBN) or 39 (MFCC) features for each time step, so we apply the convolutional layer followed by an untrained GRU layer to the input features, use average-pooling and normalise the result to have unit L2 norm. The word detection networks are trained for 32 epochs using Adam [25] with a constant learning rate of 0.001. We use the same data split that was used for training the multi-modal encoder, so that we test word presence detection on data that was not seen by either the encoder or the decoder. Table 1 shows the performance of our models on the imagecaption retrieval task. The caption embeddings are ranked by cosine distance to the image and vice versa where R@N is the percentage of test items for which the correct image or caption was in the top N results. We compare our models to [12] and <cite>[15]</cite> , and include our own character-based model for comparison. [12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention. The character-based model is similar to the model we use here and was trained on the original Flickr8k text captions (see [8] for a full description). Both our MFCC and MBN based model significantly outperform previous spoken captionto-image methods on the Flickr8k dataset.",
  "y": "background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_12",
  "x": "Table 1 shows the performance of our models on the imagecaption retrieval task. The caption embeddings are ranked by cosine distance to the image and vice versa where R@N is the percentage of test items for which the correct image or caption was in the top N results. We compare our models to [12] and <cite>[15]</cite> , and include our own character-based model for comparison. [12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention. The character-based model is similar to the model we use here and was trained on the original Flickr8k text captions (see [8] for a full description). Both our MFCC and MBN based model significantly outperform previous spoken captionto-image methods on the Flickr8k dataset. The largest improvement is the MBN model which outperforms the results reported in <cite>[15]</cite> by as much as 23.2 percentage points on R@10. The MFCC model also improves on previous results but scores significantly lower than the MBN model across the board, improving as much as 12.3 percentage points over previous work. There is a large performance gap between the text-caption to image retrieval results and the spoken-caption to image results, showing there is still a lot of room for improvement.",
  "y": "differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_13",
  "x": "[12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention. The character-based model is similar to the model we use here and was trained on the original Flickr8k text captions (see [8] for a full description). Both our MFCC and MBN based model significantly outperform previous spoken captionto-image methods on the Flickr8k dataset. The largest improvement is the MBN model which outperforms the results reported in <cite>[15]</cite> by as much as 23.2 percentage points on R@10. The MFCC model also improves on previous results but scores significantly lower than the MBN model across the board, improving as much as 12.3 percentage points over previous work. There is a large performance gap between the text-caption to image retrieval results and the spoken-caption to image results, showing there is still a lot of room for improvement. ---------------------------------- **RESULTS** The results of the word presence detection task are shown in Figure 2 and Table 2 .",
  "y": "differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_14",
  "x": "After layer 2, the performance starts to decrease slightly which might indicate that these layers learn a more task-specific representation and it is to be expected that the final attention layer specialises in mapping from audio features to the multi-modal embedding space. In conclusion, we presented what are, to the best of our knowledge, the best results on spoken-caption to image retrieval. Our results improve significantly over previous approaches for both untrained and trained audio features. In a probing task, we show that the model learns to recognise words in the input speech signal. We are currently collecting the Semantic Textual Similarity (STS) database in spoken format and the next step will be to investigate whether the model presented here also learns to capture sentence level semantic information and understand language in a deeper sense than recognising word presence. The work presented in <cite>[15]</cite> has made the first efforts in this regard and we aim to extend this to a larger database with sentences from multiple domains. Furthermore, we want to investigate the linguistic units that our model learns to recognise. In the current study, we only investigated whether the model learns to recognise words, but the potential benefit of our model is that it might learn multi-word statements or might even learn to look at sub-lexical level information. [14, 29] have recently shown that the speech-to-image retrieval approach can be used to detect word boundaries and even discover sub-word units.",
  "y": "uses future_work"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_15",
  "x": "After layer 2, the performance starts to decrease slightly which might indicate that these layers learn a more task-specific representation and it is to be expected that the final attention layer specialises in mapping from audio features to the multi-modal embedding space. In conclusion, we presented what are, to the best of our knowledge, the best results on spoken-caption to image retrieval. Our results improve significantly over previous approaches for both untrained and trained audio features. In a probing task, we show that the model learns to recognise words in the input speech signal. We are currently collecting the Semantic Textual Similarity (STS) database in spoken format and the next step will be to investigate whether the model presented here also learns to capture sentence level semantic information and understand language in a deeper sense than recognising word presence. The work presented in <cite>[15]</cite> has made the first efforts in this regard and we aim to extend this to a larger database with sentences from multiple domains. Furthermore, we want to investigate the linguistic units that our model learns to recognise. In the current study, we only investigated whether the model learns to recognise words, but the potential benefit of our model is that it might learn multi-word statements or might even learn to look at sub-lexical level information. [14, 29] have recently shown that the speech-to-image retrieval approach can be used to detect word boundaries and even discover sub-word units.",
  "y": "future_work"
 },
 {
  "id": "e803782890224294066ce447671981_0",
  "x": "****FINDING NON-LOCAL DEPENDENCIES: BEYOND PATTERN MATCHING**** **ABSTRACT** We describe an algorithm for recovering non-local dependencies in syntactic dependency structures. The <cite>patternmatching approach</cite> proposed by <cite>Johnson (2002)</cite> for a similar task for phrase structure trees is extended with machine learning techniques. The algorithm is essentially a classifier that predicts a nonlocal dependency given a connected fragment of a dependency structure and a set of structural features for this fragment. Evaluating the algorithm on the Penn Treebank shows an improvement of both precision and recall, compared to the results presented in <cite>(Johnson, 2002)</cite> . ---------------------------------- **INTRODUCTION** Non-local dependencies (also called long-distance, long-range or unbounded) appear in many frequent linguistic phenomena, such as passive, WHmovement, control and raising etc.",
  "y": "extends"
 },
 {
  "id": "e803782890224294066ce447671981_1",
  "x": "The <cite>patternmatching approach</cite> proposed by <cite>Johnson (2002)</cite> for a similar task for phrase structure trees is extended with machine learning techniques. The algorithm is essentially a classifier that predicts a nonlocal dependency given a connected fragment of a dependency structure and a set of structural features for this fragment. Evaluating the algorithm on the Penn Treebank shows an improvement of both precision and recall, compared to the results presented in <cite>(Johnson, 2002)</cite> . ---------------------------------- **INTRODUCTION** Non-local dependencies (also called long-distance, long-range or unbounded) appear in many frequent linguistic phenomena, such as passive, WHmovement, control and raising etc. Although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention. In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while <cite>Johnson (2002)</cite> presents a method for recovering non-local dependencies after parsing has been performed. More specifically, <cite>Johnson (2002)</cite> describes a <cite>pattern-matching algorithm</cite> for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies.",
  "y": "differences"
 },
 {
  "id": "e803782890224294066ce447671981_2",
  "x": "In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while <cite>Johnson (2002)</cite> presents a method for recovering non-local dependencies after parsing has been performed. More specifically, <cite>Johnson (2002)</cite> describes a <cite>pattern-matching algorithm</cite> for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies. From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent. In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem. Moreover, <cite>the author</cite> suggests that <cite>the algorithm</cite> may suffer from overlearning, and using more abstract \"skeletal\" patterns may be helpful to avoid this. In an attempt to overcome these problems we developed a similar approach using dependency structures rather than phrase structure trees, which, moreover, extends bare pattern matching with machine learning techniques. A different definition of pattern allows us to significantly reduce the number of patterns extracted from the same corpus. Moreover, the patterns we obtain are quite general and in most cases directly correspond to specific linguistic phenomena.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_3",
  "x": "Evaluating the algorithm on the Penn Treebank shows an improvement of both precision and recall, compared to the results presented in <cite>(Johnson, 2002)</cite> . ---------------------------------- **INTRODUCTION** Non-local dependencies (also called long-distance, long-range or unbounded) appear in many frequent linguistic phenomena, such as passive, WHmovement, control and raising etc. Although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention. In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while <cite>Johnson (2002)</cite> presents a method for recovering non-local dependencies after parsing has been performed. More specifically, <cite>Johnson (2002)</cite> describes a <cite>pattern-matching algorithm</cite> for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies. From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_4",
  "x": "---------------------------------- **INTRODUCTION** Non-local dependencies (also called long-distance, long-range or unbounded) appear in many frequent linguistic phenomena, such as passive, WHmovement, control and raising etc. Although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention. In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while <cite>Johnson (2002)</cite> presents a method for recovering non-local dependencies after parsing has been performed. More specifically, <cite>Johnson (2002)</cite> describes a <cite>pattern-matching algorithm</cite> for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies. From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent. In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_5",
  "x": "Evaluating the algorithm on the Penn Treebank shows an improvement of both precision and recall, compared to the results presented in <cite>(Johnson, 2002)</cite> . ---------------------------------- **INTRODUCTION** Non-local dependencies (also called long-distance, long-range or unbounded) appear in many frequent linguistic phenomena, such as passive, WHmovement, control and raising etc. Although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention. In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while <cite>Johnson (2002)</cite> presents a method for recovering non-local dependencies after parsing has been performed. More specifically, <cite>Johnson (2002)</cite> describes a <cite>pattern-matching algorithm</cite> for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies. From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_6",
  "x": "---------------------------------- **INTRODUCTION** Non-local dependencies (also called long-distance, long-range or unbounded) appear in many frequent linguistic phenomena, such as passive, WHmovement, control and raising etc. Although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention. In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while <cite>Johnson (2002)</cite> presents a method for recovering non-local dependencies after parsing has been performed. More specifically, <cite>Johnson (2002)</cite> describes a <cite>pattern-matching algorithm</cite> for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies. From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent. In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem.",
  "y": "motivation"
 },
 {
  "id": "e803782890224294066ce447671981_7",
  "x": "In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while <cite>Johnson (2002)</cite> presents a method for recovering non-local dependencies after parsing has been performed. More specifically, <cite>Johnson (2002)</cite> describes a <cite>pattern-matching algorithm</cite> for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies. From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent. In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem. Moreover, <cite>the author</cite> suggests that <cite>the algorithm</cite> may suffer from overlearning, and using more abstract \"skeletal\" patterns may be helpful to avoid this. In an attempt to overcome these problems we developed a similar approach using dependency structures rather than phrase structure trees, which, moreover, extends bare pattern matching with machine learning techniques. A different definition of pattern allows us to significantly reduce the number of patterns extracted from the same corpus. Moreover, the patterns we obtain are quite general and in most cases directly correspond to specific linguistic phenomena.",
  "y": "motivation"
 },
 {
  "id": "e803782890224294066ce447671981_8",
  "x": "In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem. Moreover, <cite>the author</cite> suggests that <cite>the algorithm</cite> may suffer from overlearning, and using more abstract \"skeletal\" patterns may be helpful to avoid this. In an attempt to overcome these problems we developed a similar approach using dependency structures rather than phrase structure trees, which, moreover, extends bare pattern matching with machine learning techniques. A different definition of pattern allows us to significantly reduce the number of patterns extracted from the same corpus. Moreover, the patterns we obtain are quite general and in most cases directly correspond to specific linguistic phenomena. This helps us to understand what information about syntactic structure is important for the recovery of non-local dependencies and in which cases lexicalization (or even semantic analysis) is required. On the other hand, using these simplified patterns, we may loose some structural information important for recovery of non-local dependencies. To avoid this, we associate patterns with certain structural features and use statistical classifi- cation methods on top of pattern matching. The evaluation of our algorithm on data automatically derived from the Penn Treebank shows an increase in both precision and recall in recovery of non-local dependencies by approximately 10% over the results reported in <cite>(Johnson, 2002)</cite> .",
  "y": "differences"
 },
 {
  "id": "e803782890224294066ce447671981_9",
  "x": "As will be described below, this allows us to \"factor out\" tense and modality of finite clauses from our patterns, making the patterns more general. ---------------------------------- **PATTERN EXTRACTION AND MATCHING** After converting the Penn Treebank to a dependency treebank, we first extracted non-local dependency patterns. As in <cite>(Johnson, 2002)</cite> , our patterns are minimal connected fragments containing both nodes involved in a non-local dependency. However, in our case these fragments are not connected sets of local trees, but shortest paths in local dependency graphs, leading from heads to non-local dependents. Patterns do not include POS tags of the involved words, but only labels of the dependencies. Thus, a pattern is a directed graph with labeled edges, and two distinguished nodes: the head and the dependent of a corresponding non-local dependency. When several patterns intersect, as may be the case, for example, when a word participates in more than one nonlocal dependency, these patterns are handled independently.",
  "y": "uses similarities"
 },
 {
  "id": "e803782890224294066ce447671981_10",
  "x": "When several patterns intersect, as may be the case, for example, when a word participates in more than one nonlocal dependency, these patterns are handled independently. Figure 2 shows examples of dependency graphs (above) and extracted patterns (below, with filled bullets corresponding to the nodes of a nonlocal dependency). As before, dotted lines denote non-local dependencies. The definition of a structure matching a pattern, and the algorithms for pattern matching and pattern extraction from a corpus are straightforward and similar to those described in <cite>(Johnson, 2002)</cite> . The total number of non-local dependencies found in the Penn WSJ is 57325. The number of different extracted patterns is 987. The 80 most frequent patterns (those that we used for the evaluation of our algorithm) cover 53700 out of all 57325 nonlocal dependencies (93,7%). These patterns were further cleaned up manually, e.g., most Penn functional tags (-TMP, -CLR etc., but not -OBJ, -SBJ, -PRD) were removed. Thus, we ended up with 16 structural patterns (covering the same 93,7% of the Penn Treebank).",
  "y": "similarities"
 },
 {
  "id": "e803782890224294066ce447671981_12",
  "x": "We performed experiments with two statistical classifiers: the decision tree induction system C4.5 (Quinlan, 1993) and the Tilburg Memory-Based Learner (TiMBL) (Daelemans et al., 2002) . In most cases TiBML performed slightly better. The results described in this section were obtained using TiMBL. For each of the 16 structural patterns, a separate classifier was trained on the set of (feature-vector, label) pairs extracted from the training corpus, and then evaluated on the pairs from the test corpus. Table 1 shows the results for some of the most frequent patterns, using conventional metrics: precision (the fraction of the correctly labeled dependencies among all the dependencies found), recall (the fraction of the correctly found dependencies among all the dependencies with a given label) and f-score (harmonic mean of precision and recall). The table also shows the number of times a pattern (together with a specific non-local dependency label) actually occurs in the whole Penn Treebank corpus (the column Dependency count). In order to compare our results to the results presented in <cite>(Johnson, 2002)</cite> , we measured the overall performance of the algorithm across patterns and non-local dependency labels. This corresponds to the row \"Overall\" of Table 4 in <cite>(Johnson, 2002)</cite> , repeated here in Table 4 . We also evaluated the procedure on NP traces across all patterns, i.e., on nonlocal dependencies with NP-SBJ, NP-OBJ or NP-PRD labels.",
  "y": "similarities"
 },
 {
  "id": "e803782890224294066ce447671981_13",
  "x": "In order to compare our results to the results presented in <cite>(Johnson, 2002)</cite> , we measured the overall performance of the algorithm across patterns and non-local dependency labels. This corresponds to the row \"Overall\" of Table 4 in <cite>(Johnson, 2002)</cite> , repeated here in Table 4 . We also evaluated the procedure on NP traces across all patterns, i.e., on nonlocal dependencies with NP-SBJ, NP-OBJ or NP-PRD labels. This corresponds to rows 2, 3 and 4 of Table 4 in <cite>(Johnson, 2002)</cite> . Our results are presented in Table 3 . The first three columns show the results for those non-local dependencies that are actually covered by our 16 patterns (i.e., for 93.7% of all non-local dependencies). The last three columns present the evaluation with respect to all non-local dependencies, thus the precision is the same, but recall drops accordingly. These last columns give the results that can be compared to <cite>Johnson's results</cite> for section 23 (Table 4) Table 3 : Overall performance of our algorithm. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "e803782890224294066ce447671981_17",
  "x": "---------------------------------- **ON SECTION 23** On parser output P R f P R f Overall 0.80 0.70 0.75 0.73 0.63 0.68 Table 4 : Results from <cite>(Johnson, 2002)</cite> . It is difficult to make a strict comparison of our results and those in <cite>(Johnson, 2002)</cite> . The two algorithms are designed for slightly different purposes: while <cite>Johnson's approach</cite> allows one to recover free empty nodes (without antecedents), we look for nonlocal dependencies, which corresponds to identification of co-indexed empty nodes (note, however, the modifications we describe in Section 2, when we actually transform free empty nodes into co-indexed empty nodes). ---------------------------------- **DISCUSSION** The results presented in the previous section show that it is possible to improve over the simple <cite>pattern matching algorithm</cite> of <cite>(Johnson, 2002)</cite> , using dependency rather than phrase structure information, more skeletal patterns, as was suggested by <cite>Johnson</cite>, and a set of features associated with instances of patterns. One of the reasons for this improvement is that our approach allows us to discriminate between different syntactic phenomena involving non-local dependencies.",
  "y": "differences"
 },
 {
  "id": "e803782890224294066ce447671981_18",
  "x": "**DISCUSSION** The results presented in the previous section show that it is possible to improve over the simple <cite>pattern matching algorithm</cite> of <cite>(Johnson, 2002)</cite> , using dependency rather than phrase structure information, more skeletal patterns, as was suggested by <cite>Johnson</cite>, and a set of features associated with instances of patterns. One of the reasons for this improvement is that our approach allows us to discriminate between different syntactic phenomena involving non-local dependencies. In most cases our patterns correspond to linguistic phenomena. That helps to understand why a particular construction is easy or difficult for our approach, and in many cases to make the necessary modifications to the algorithm (e.g., adding other features to instances of patterns). For example, for patterns 11 and 12 (see Tables 1 and 2 ) our classifier distinguishes subject and object reasonably well, apparently, because the feature has a local object is explicitly present for all instances (for the examples 11 and 12 in Table 2 , expand has a local object, but do doesn't). Another reason is that the patterns are general enough to factor out minor syntactic differences in linguistic phenomena (e.g., see example 4 in Table 2). Indeed, the most frequent 16 patterns cover 93.7% of all non-local dependencies in the corpus. This is mainly due to our choices in the dependency representation, such as making the main verb a head of a verb phrase.",
  "y": "differences"
 },
 {
  "id": "e803782890224294066ce447671981_19",
  "x": "In other words, we first take an \"oversimplified\" representation of the data, and then try to find what other data features can be useful. This strategy appears to be successful, because it allows us to identify which information is important for the recovery of non-local dependencies. More generally, the reasonable overall performance of the algorithm is due to the fact that for the most common non-local dependencies (extraction in relative clauses and reduced relative clauses, passivization, control and raising) the structural information we extract is enough to robustly identify non-local dependencies in a local dependency graph: the most frequent patterns in Table 1 are also those with best scores. However, many less frequent phenomena appear to be much harder. For example, performance for relative clauses with extracted objects or adverbs is much worse than for subject relative clauses (e.g., patterns 2 and 3 vs. 1 in Table 1 ). Apparently, in most cases this is not due to the lack of training data, but because structural information alone is not enough and lexical preferences, subcategorization information, or even semantic properties should be considered. We think that the aproach allows us to identify those \"hard\" cases. The natural next step in evaluating our algorithm is to work with the output of a parser instead of the original local structures from the Penn Treebank. Obviously, because of parsing errors the performance drops significantly: e.g., in the experiments reported in <cite>(Johnson, 2002 )</cite> the overall fscore decreases from 0.75 to 0.68 when evaluating on parser output (see Table 4 ).",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_20",
  "x": "Since our algorithm is sensitive to the exact labels of the dependencies, it suffers from these systematic errors. One possible solution to that problem could be to extract patterns and train the classification algorithm not on the training part of the Penn Treebank, but on the parser output for it. This would allow us to train and test our algorithm on data of the same nature. ---------------------------------- **CONCLUSIONS AND FUTURE WORK** We have presented an algorithm for recovering longdistance dependencies in local dependency structures. We extend the pattern matching approach of <cite>Johnson (2002)</cite> with machine learning techniques, and use dependency structures instead of constituency trees. Evaluation on the Penn Treebank shows an increase in accuracy. However, we do not have yet satisfactory results when working on a parser output.",
  "y": "extends differences"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_0",
  "x": "---------------------------------- **INTRODUCTION** Large-scale knowledge bases (KBs), such as Freebase [Bollacker et al., 2008] , WordNet<cite> [Miller, 1995]</cite> , Yago<cite> [Suchanek et al., 2007]</cite> , and NELL [Carlson et al., 2010] , are critical to natural language processing applications, e.g., question answering [Dong et al., 2015] , relation extraction<cite> [Riedel et al., 2013]</cite> , and language modeling [Ahn et al., 2016] . These KBs generally contain billions of facts, and each fact is organized into a triple base format (head entity, relation, tail entity), abbreviated as (h,r,t). However, the coverage of such KBs is still far from complete compared with real-world knowledge [Dong et al., 2014] . Traditional KB completion approaches, such as Markov logic networks<cite> [Richardson and Domingos, 2006]</cite> , suffer from feature sparsity and low efficiency. Recently, encoding the entire knowledge base into a lowdimensional vector space to learn latent representations of entity and relation has attracted widespread attention. These knowledge embedding models yield better performance in terms of low complexity and high scalability compared with previous works. Among these methods, TransE [Bordes et al., 2013 ] is a classical neural-based model, which assumes that each relation can be regarded as a translation from head to tail and uses a score function S(h,r,t)= h + r \u2212 t to measure the plausibility for triples.",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_1",
  "x": "Large-scale knowledge bases (KBs), such as Freebase [Bollacker et al., 2008] , WordNet<cite> [Miller, 1995]</cite> , Yago<cite> [Suchanek et al., 2007]</cite> , and NELL [Carlson et al., 2010] , are critical to natural language processing applications, e.g., question answering [Dong et al., 2015] , relation extraction<cite> [Riedel et al., 2013]</cite> , and language modeling [Ahn et al., 2016] . These KBs generally contain billions of facts, and each fact is organized into a triple base format (head entity, relation, tail entity), abbreviated as (h,r,t). However, the coverage of such KBs is still far from complete compared with real-world knowledge [Dong et al., 2014] . Traditional KB completion approaches, such as Markov logic networks<cite> [Richardson and Domingos, 2006]</cite> , suffer from feature sparsity and low efficiency. Recently, encoding the entire knowledge base into a lowdimensional vector space to learn latent representations of entity and relation has attracted widespread attention. These knowledge embedding models yield better performance in terms of low complexity and high scalability compared with previous works. Among these methods, TransE [Bordes et al., 2013 ] is a classical neural-based model, which assumes that each relation can be regarded as a translation from head to tail and uses a score function S(h,r,t)= h + r \u2212 t to measure the plausibility for triples. TransH [Wang et al., 2014] and TransR<cite> [Lin et al., 2015b]</cite> are representative variants of TransE. These variants consider entities from multiple aspects and various relations on different aspects. However, the majority of these approaches only exploit direct links that connect head and tail entities to predict potential relations between entities.",
  "y": "motivation"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_2",
  "x": "These KBs generally contain billions of facts, and each fact is organized into a triple base format (head entity, relation, tail entity), abbreviated as (h,r,t). However, the coverage of such KBs is still far from complete compared with real-world knowledge [Dong et al., 2014] . Traditional KB completion approaches, such as Markov logic networks<cite> [Richardson and Domingos, 2006]</cite> , suffer from feature sparsity and low efficiency. Recently, encoding the entire knowledge base into a lowdimensional vector space to learn latent representations of entity and relation has attracted widespread attention. These knowledge embedding models yield better performance in terms of low complexity and high scalability compared with previous works. Among these methods, TransE [Bordes et al., 2013 ] is a classical neural-based model, which assumes that each relation can be regarded as a translation from head to tail and uses a score function S(h,r,t)= h + r \u2212 t to measure the plausibility for triples. TransH [Wang et al., 2014] and TransR<cite> [Lin et al., 2015b]</cite> are representative variants of TransE. These variants consider entities from multiple aspects and various relations on different aspects. However, the majority of these approaches only exploit direct links that connect head and tail entities to predict potential relations between entities. These approaches do not explore the fact that relation paths, which are denoted as the sequences of relations, i.e., p=(r 1 , r 2 , . . ., r m ), play an important role in knowledge base completion.",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_3",
  "x": "Consequently, a promising new research direction is to use relation paths to learn knowledge embeddings<cite> [Neelakantan et al., 2015</cite>; Guu et al., 2015;<cite> Toutanova et al., 2016]</cite> . For a relation path, consistent semantics is a semantic interpretation via composition of the meaning of the component elements. Each relation path contains its respective consistent semantics. However, the consistent semantics expressed by some relation paths p is unreliable for reasoning new facts of that entity pair<cite> [Lin et al., 2015a]</cite> . For instance, there is a common relation path h but this path is meaningless for inferring additional relationships between h and t. Therefore, reliable relation paths are urgently needed. Moreover, their consistent semantics, which is essential for knowledge representation learning, is consistent with the semantics of relation r. Based on this intuition, we propose a compositional learning model of relation path embedding (RPE), which extends the projection and type constraints of the specific relation to the specific path. As the path ranking algorithm (PRA)<cite> [Lao et al., 2011]</cite> suggests, relation paths that end in many possible tail entities are more likely to be unreliable for the entity pair. Reliable relation paths can thus be filtered using PRA.",
  "y": "motivation"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_4",
  "x": "Consequently, a promising new research direction is to use relation paths to learn knowledge embeddings<cite> [Neelakantan et al., 2015</cite>; Guu et al., 2015;<cite> Toutanova et al., 2016]</cite> . For a relation path, consistent semantics is a semantic interpretation via composition of the meaning of the component elements. Each relation path contains its respective consistent semantics. However, the consistent semantics expressed by some relation paths p is unreliable for reasoning new facts of that entity pair<cite> [Lin et al., 2015a]</cite> . For instance, there is a common relation path h but this path is meaningless for inferring additional relationships between h and t. Therefore, reliable relation paths are urgently needed. Moreover, their consistent semantics, which is essential for knowledge representation learning, is consistent with the semantics of relation r. Based on this intuition, we propose a compositional learning model of relation path embedding (RPE), which extends the projection and type constraints of the specific relation to the specific path. As the path ranking algorithm (PRA)<cite> [Lao et al., 2011]</cite> suggests, relation paths that end in many possible tail entities are more likely to be unreliable for the entity pair. Reliable relation paths can thus be filtered using PRA.",
  "y": "motivation"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_5",
  "x": "For a relation path, consistent semantics is a semantic interpretation via composition of the meaning of the component elements. Each relation path contains its respective consistent semantics. However, the consistent semantics expressed by some relation paths p is unreliable for reasoning new facts of that entity pair<cite> [Lin et al., 2015a]</cite> . For instance, there is a common relation path h but this path is meaningless for inferring additional relationships between h and t. Therefore, reliable relation paths are urgently needed. Moreover, their consistent semantics, which is essential for knowledge representation learning, is consistent with the semantics of relation r. Based on this intuition, we propose a compositional learning model of relation path embedding (RPE), which extends the projection and type constraints of the specific relation to the specific path. As the path ranking algorithm (PRA)<cite> [Lao et al., 2011]</cite> suggests, relation paths that end in many possible tail entities are more likely to be unreliable for the entity pair. Reliable relation paths can thus be filtered using PRA. Figure 1 illustrates the basic idea for relation-specific and path-specific projections.",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_6",
  "x": "By exploiting these limited rules, the harmful influence of a merely data-driven pattern can be avoided. Typeconstrained TransE [Krompass et al., 2015] imposes these constraints on the global margin-loss function to better distinguish similar embeddings in latent space. A third current related work is PTransE<cite> [Lin et al., 2015a</cite> ] and the path ranking algorithm (PRA)<cite> [Lao et al., 2011]</cite> . PTransE considers relation paths as translations between head and tail entities and primarily addresses two problems: 1) exploit a variant of PRA to select reliable relation paths, and 2) explore three path representations by compositions of relation embeddings. PRA, as one of the most promising research innovations for knowledge base completion, has also attracted considerable attention<cite> [Lao et al., 2015</cite>; Gardner and Mitchell, 2015; Wang et al., 2016;<cite> Nickel et al., 2016]</cite> . PRA uses the path-constrained random walk probabilities as path features to train linear classifiers for different relations. In large-scale KBs, relation paths have great significance for enhancing the reasoning ability for more complicated situations. However, none of the aforementioned models take full advantage of the consistent semantics of relation paths. ----------------------------------",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_7",
  "x": "The projected entity vectors are h r =M r h and t r =M r t; thus, the new score function is defined as S(h,r,t)= h r + r \u2212 t r . Another research direction focuses on improving the prediction performance by using prior knowledge in the form of relation-specific type constraints [Krompass et al., 2015; Chang et al., 2014; Wang et al., 2015] . Note that each relation should possess Domain and Range fields to indicate the subject and object types, respectively. For example, the relation haschildren's Domain and Range types both belong to a person. By exploiting these limited rules, the harmful influence of a merely data-driven pattern can be avoided. Typeconstrained TransE [Krompass et al., 2015] imposes these constraints on the global margin-loss function to better distinguish similar embeddings in latent space. A third current related work is PTransE<cite> [Lin et al., 2015a</cite> ] and the path ranking algorithm (PRA)<cite> [Lao et al., 2011]</cite> . PTransE considers relation paths as translations between head and tail entities and primarily addresses two problems: 1) exploit a variant of PRA to select reliable relation paths, and 2) explore three path representations by compositions of relation embeddings. PRA, as one of the most promising research innovations for knowledge base completion, has also attracted considerable attention<cite> [Lao et al., 2015</cite>; Gardner and Mitchell, 2015; Wang et al., 2016;<cite> Nickel et al., 2016]</cite> .",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_8",
  "x": "**EVALUATION PROTOCOL** We follow the same evaluation procedures as used in [Bordes et al., 2013; Wang et al., 2014;<cite> Lin et al., 2015b]</cite> . First, for each test triple (h,r,t), we replace h or t with every entity in \u03b6. Second, each corrupted triple is calculated by the corresponding score function S(h,r,t). The final step is to rank the original correct entity with these scores in descending order. Two evaluation metrics are reported: the average rank of correct entities (Mean Rank) and the proportion of correct entities ranked in the top 10 (Hits@10). Note that if a corrupted triple already exists in the knowledge base, then it should not be considered to be incorrect. We prefer to remove these corrupted triples from our dataset, and call this setting \"Filter\". If these corrupted triples are reserved, then we call this setting \"Raw\".",
  "y": "uses"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_9",
  "x": "Second, each corrupted triple is calculated by the corresponding score function S(h,r,t). The final step is to rank the original correct entity with these scores in descending order. Two evaluation metrics are reported: the average rank of correct entities (Mean Rank) and the proportion of correct entities ranked in the top 10 (Hits@10). Note that if a corrupted triple already exists in the knowledge base, then it should not be considered to be incorrect. We prefer to remove these corrupted triples from our dataset, and call this setting \"Filter\". If these corrupted triples are reserved, then we call this setting \"Raw\". In both settings, if the latent representations of entity and relation are better, then a lower mean rank and a higher Hits@10 should be achieved. Because we use the same dataset, the baseline results reported in<cite> [Lin et al., 2015b</cite>;<cite> Lin et al., 2015a</cite>; Ji et al., 2016] are directly used for comparison. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_10",
  "x": "We believe that this result is primarily because RPE (PC) only focuses on local information provided by related embeddings, ignoring some global information compared with the approach of randomly selecting corrupted entities. In terms of mean rank, RPE (ACOM) achieves the best performance with 14.5% and 24.1% error reduction compared with PTransE's performance in the raw and filter settings, respectively. In terms of Hits@10, RPE (ACOM) brings few improvements. RPE with path-specific type constraints and projection (RPE (PC + ACOM) and RPE (PC + MCOM)) is a compromise between RPE (PC) and RPE (ACOM). Table 3 presents the separated evaluation results by mapping properties of relations on FB15K. Mapping properties of relations follows the same rules in [Bordes et al., 2013] , and the metrics are Hits@10 on head and tail entities. From Table 3 , we can conclude that 1) RPE (ACOM) outperforms all baselines in all mapping properties of relations. In particular, for the 1-to-N, N-to-1, and N-to-N types of relations [Bordes et al., 2013] 75.9 70.9 77.8 TransE (bern) [Bordes et al., 2013] 75.9 81.5 85.3 TransH (unif) [Wang et al., 2014] 77.7 76.5 78.4 TransH (bern) [Wang et al., 2014] 78.8 83.3 85.8 TransR (unif)<cite> [Lin et al., 2015b]</cite> 85.5 74.7 79.2 TransR (bern)<cite> [Lin et al., 2015b]</cite> 85.9 82.5 87.0 PTransE (ADD, 2-hop)<cite> [Lin et al., 2015a]</cite> 80.9 73.5 83.4 PTransE (MUL, 2-hop)<cite> [Lin et al., 2015a]</cite> 79.4 73.6 79.3 PTransE (ADD, 3-hop)<cite> [Lin et al., 2015a]</cite> 80 that plague knowledge embedding models, RPE (ACOM) improves 4.1%, 4.6%, and 4.9% on head entity's prediction and 6.9%, 7.0%, and 5.1% on tail entity's prediction compared with previous state-of-the-art performances achieved by PTransE (ADD, 2-hop). 2) RPE (MCOM) does not perform as well as RPE (ACOM), and we believe that this result is because RPE's path representation is not consistent with RPE (MCOM)'s composition of projections.",
  "y": "differences"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_11",
  "x": "We set different relation-specific thresholds {\u03b4 r } to perform this task. For a test triple (h,r,t), if its score S(h,r,t) is below \u03b4 r , then we predict it as a positive one; otherwise, it is negative. {\u03b4 r } is obtained by maximizing the classification accuracies on the valid set. ---------------------------------- **IMPLEMENTATION** We directly compare our model with prior work using the results about knowledge embedding models reported in<cite> [Lin et al., 2015b]</cite> n=50, m=50, \u03b3 1 =5, \u03b3 2 =6, \u03b1=0.0001, B=1440, \u03bb=0.8, and \u03b7=0.05, taking the L 1 norm on WN11; n=100, m=100, \u03b3 1 =3, \u03b3 2 =6, \u03b1=0.0001, B=960, \u03bb=0.8, and \u03b7=0.05, taking the L 1 norm on FB13; and n=100, m=100, \u03b3 1 =4, \u03b3 2 =5, \u03b1=0.0001, B=4800, \u03bb=1, and \u03b7 =0.05, taking the L 1 norm on FB15K. We exploit RPE (initial) for initiation, and we set the path length as 2 and the maximum epoch as 500. Table 4 lists the results for triple classification on different datasets, and the evaluation metric is classification accuracy. The results demonstrate that 1) RPE (PC + ACOM) achieves the best performance on all datasets, which takes good advantage of path-specific projection and type constraints; 2) RPE (PC) improves the performance of RPE (initial) by 4.5%, 6.0%, and 13.2%, particularly on FB15K; thus, we consider that lengthening the distances for similar entities in embedding space is essential to specific problems.",
  "y": "uses"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_0",
  "x": "While several techniques for merging both spaces have been proposed [<cite>1</cite>, 2, 3, 4, 5, 6] , little effort has been made in finding the most appropriate image embeddings to be used in that process. In fact, <cite>most approaches</cite> simply use a one-layer CNN embedding [7, 8] . In this paper we explore the impact of using a Full-Network embedding (FNE) [9] to generate the required image embedding, replacing the one-layer embedding. We do so by integrating the FNE into the multimodal embedding pipeline defined by <cite>Kiros et al. [1]</cite> , which is based in the use of a Gated Recurrent Units neural network (GRU) [10] for text encoding and CNN for image encoding. Unlike one-layer embeddings, the FNE represents features of varying specificity in the context of the visual dataset, while also discretizes the features to regularize the space and alleviate the curse of dimensionality. These particularities result in a richer visual embedding space, which may be more reliably mapped to a common visual-textual embedding space. The generic pipeline defined by <cite>Kiros et al. [1]</cite> has been recently outperformed in image annotation and image search tasks by methods specifically targeting one of those tasks [4, 18] . We choose to test our contribution on <cite>this pipeline</cite> for its overall competitive performance, expecting that any conclusion may generalize when applied to other solutions and tasks (e.g., caption generation). This assumption would be dimmer if a more problem-specific methodology was chosen instead.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_1",
  "x": "Image annotation (also known as caption retrieval) is the task of automatically associating an input image with a describing text. Image annotation methods are an emerging technology, enabling semantic image indexing and search applications. The complementary task of associating an input text with a fitting image (known as image retrieval or image search) is also of relevance for the same sort of applications. State-of-the-art image annotation methods are currently based on deep neural net representations, where an image embedding (e.g., obtained from a convolutional neural network or CNN) and a text embedding (e.g., obtained from a recurrent neural network or RNN) are combined into a unique multimodal embedding space. While several techniques for merging both spaces have been proposed [<cite>1</cite>, 2, 3, 4, 5, 6] , little effort has been made in finding the most appropriate image embeddings to be used in that process. In fact, <cite>most approaches</cite> simply use a one-layer CNN embedding [7, 8] . In this paper we explore the impact of using a Full-Network embedding (FNE) [9] to generate the required image embedding, replacing the one-layer embedding. We do so by integrating the FNE into the multimodal embedding pipeline defined by <cite>Kiros et al. [1]</cite> , which is based in the use of a Gated Recurrent Units neural network (GRU) [10] for text encoding and CNN for image encoding. Unlike one-layer embeddings, the FNE represents features of varying specificity in the context of the visual dataset, while also discretizes the features to regularize the space and alleviate the curse of dimensionality.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_2",
  "x": "In this paper we explore the impact of using a Full-Network embedding (FNE) [9] to generate the required image embedding, replacing the one-layer embedding. We do so by integrating the FNE into the multimodal embedding pipeline defined by <cite>Kiros et al. [1]</cite> , which is based in the use of a Gated Recurrent Units neural network (GRU) [10] for text encoding and CNN for image encoding. Unlike one-layer embeddings, the FNE represents features of varying specificity in the context of the visual dataset, while also discretizes the features to regularize the space and alleviate the curse of dimensionality. These particularities result in a richer visual embedding space, which may be more reliably mapped to a common visual-textual embedding space. The generic pipeline defined by <cite>Kiros et al. [1]</cite> has been recently outperformed in image annotation and image search tasks by methods specifically targeting one of those tasks [4, 18] . We choose to test our contribution on <cite>this pipeline</cite> for its overall competitive performance, expecting that any conclusion may generalize when applied to other solutions and tasks (e.g., caption generation). This assumption would be dimmer if a more problem-specific methodology was chosen instead. Our main goal is to establish the competitiveness of the FNE as an image representation to be used in caption related tasks. We test the suitability of this approach by evaluating its performance on both image annotation and image retrieval using three publicly available datasets: Flickr8k [11] , Flickr30k [12] and MSCOCO [13] .",
  "y": "extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_3",
  "x": "In this paper we explore the impact of using a Full-Network embedding (FNE) [9] to generate the required image embedding, replacing the one-layer embedding. We do so by integrating the FNE into the multimodal embedding pipeline defined by <cite>Kiros et al. [1]</cite> , which is based in the use of a Gated Recurrent Units neural network (GRU) [10] for text encoding and CNN for image encoding. Unlike one-layer embeddings, the FNE represents features of varying specificity in the context of the visual dataset, while also discretizes the features to regularize the space and alleviate the curse of dimensionality. These particularities result in a richer visual embedding space, which may be more reliably mapped to a common visual-textual embedding space. The generic pipeline defined by <cite>Kiros et al. [1]</cite> has been recently outperformed in image annotation and image search tasks by methods specifically targeting one of those tasks [4, 18] . We choose to test our contribution on <cite>this pipeline</cite> for its overall competitive performance, expecting that any conclusion may generalize when applied to other solutions and tasks (e.g., caption generation). This assumption would be dimmer if a more problem-specific methodology was chosen instead. Our main goal is to establish the competitiveness of the FNE as an image representation to be used in caption related tasks. We test the suitability of this approach by evaluating its performance on both image annotation and image retrieval using three publicly available datasets: Flickr8k [11] , Flickr30k [12] and MSCOCO [13] .",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_4",
  "x": "We choose to test our contribution on <cite>this pipeline</cite> for its overall competitive performance, expecting that any conclusion may generalize when applied to other solutions and tasks (e.g., caption generation). This assumption would be dimmer if a more problem-specific methodology was chosen instead. Our main goal is to establish the competitiveness of the FNE as an image representation to be used in caption related tasks. We test the suitability of this approach by evaluating its performance on both image annotation and image retrieval using three publicly available datasets: Flickr8k [11] , Flickr30k [12] and MSCOCO [13] . Results obtained by the pipeline including the FNE are compared with the original pipeline of <cite>Kiros et al. [1]</cite> using a one-layer embedding, and also with the methods currently obtaining state-of-the-art results on the three datasets. ---------------------------------- **RELATED WORK** In the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross-domain search [<cite>1</cite>, 2, 3, 4, 5] . This paper builds upon the methodology described by <cite>Kiros et al. [1]</cite> , <cite>which</cite> is in turn based on previous works in the area of Neural Machine Translation [14] .",
  "y": "uses motivation"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_5",
  "x": "The generic pipeline defined by <cite>Kiros et al. [1]</cite> has been recently outperformed in image annotation and image search tasks by methods specifically targeting one of those tasks [4, 18] . We choose to test our contribution on <cite>this pipeline</cite> for its overall competitive performance, expecting that any conclusion may generalize when applied to other solutions and tasks (e.g., caption generation). This assumption would be dimmer if a more problem-specific methodology was chosen instead. Our main goal is to establish the competitiveness of the FNE as an image representation to be used in caption related tasks. We test the suitability of this approach by evaluating its performance on both image annotation and image retrieval using three publicly available datasets: Flickr8k [11] , Flickr30k [12] and MSCOCO [13] . Results obtained by the pipeline including the FNE are compared with the original pipeline of <cite>Kiros et al. [1]</cite> using a one-layer embedding, and also with the methods currently obtaining state-of-the-art results on the three datasets. ---------------------------------- **RELATED WORK** In the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross-domain search [<cite>1</cite>, 2, 3, 4, 5] .",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_6",
  "x": "We test the suitability of this approach by evaluating its performance on both image annotation and image retrieval using three publicly available datasets: Flickr8k [11] , Flickr30k [12] and MSCOCO [13] . Results obtained by the pipeline including the FNE are compared with the original pipeline of <cite>Kiros et al. [1]</cite> using a one-layer embedding, and also with the methods currently obtaining state-of-the-art results on the three datasets. ---------------------------------- **RELATED WORK** In the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross-domain search [<cite>1</cite>, 2, 3, 4, 5] . This paper builds upon the methodology described by <cite>Kiros et al. [1]</cite> , <cite>which</cite> is in turn based on previous works in the area of Neural Machine Translation [14] . In their work, <cite>Kiros et al. [1]</cite> define a vectorized representation of an input text by using GRU RNNs. In this setting, each word in the text is codified into a vector using a word dictionary, vectors which are then fed one by one into the GRUs. Once the last word vector has been processed, the activations of the GRUs at the last time step conveys the representation of the whole input text in the multimodal embedding space.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_7",
  "x": "Our main goal is to establish the competitiveness of the FNE as an image representation to be used in caption related tasks. We test the suitability of this approach by evaluating its performance on both image annotation and image retrieval using three publicly available datasets: Flickr8k [11] , Flickr30k [12] and MSCOCO [13] . Results obtained by the pipeline including the FNE are compared with the original pipeline of <cite>Kiros et al. [1]</cite> using a one-layer embedding, and also with the methods currently obtaining state-of-the-art results on the three datasets. ---------------------------------- **RELATED WORK** In the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross-domain search [<cite>1</cite>, 2, 3, 4, 5] . This paper builds upon the methodology described by <cite>Kiros et al. [1]</cite> , <cite>which</cite> is in turn based on previous works in the area of Neural Machine Translation [14] . In their work, <cite>Kiros et al. [1]</cite> define a vectorized representation of an input text by using GRU RNNs. In this setting, each word in the text is codified into a vector using a word dictionary, vectors which are then fed one by one into the GRUs.",
  "y": "extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_8",
  "x": "This assumption would be dimmer if a more problem-specific methodology was chosen instead. Our main goal is to establish the competitiveness of the FNE as an image representation to be used in caption related tasks. We test the suitability of this approach by evaluating its performance on both image annotation and image retrieval using three publicly available datasets: Flickr8k [11] , Flickr30k [12] and MSCOCO [13] . Results obtained by the pipeline including the FNE are compared with the original pipeline of <cite>Kiros et al. [1]</cite> using a one-layer embedding, and also with the methods currently obtaining state-of-the-art results on the three datasets. ---------------------------------- **RELATED WORK** In the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross-domain search [<cite>1</cite>, 2, 3, 4, 5] . This paper builds upon the methodology described by <cite>Kiros et al. [1]</cite> , <cite>which</cite> is in turn based on previous works in the area of Neural Machine Translation [14] . In their work, <cite>Kiros et al. [1]</cite> define a vectorized representation of an input text by using GRU RNNs.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_9",
  "x": "Once the last word vector has been processed, the activations of the GRUs at the last time step conveys the representation of the whole input text in the multimodal embedding space. In parallel, images are processed through a Convolutional Neural Network (CNN) pre-trained on ImageNet [15] , extracting the activations of the last fully connected layer to be used as a representation of the images. To solve the dimensionality matching between both representations (the output of the GRUs and the last fully-connected of the CNN) an affine transformation is applied on the image representation. Similarly to the approach of <cite>Kiros et al. [1]</cite> , most image annotation and image retrieval approaches rely on the use of CNN features for image representation. The current best overall performing model (considering both image annotation and image retrieval tasks) is the Fisher Vector (FV) [4] , although its performance is most competitive on the image retrieval task. FV are computed with respect to the parameters of a Gaussian Mixture Model (GMM) and an Hybrid Gaussian-Laplacian Mixture Model (HGLMM). For both images and text, FV are build using deep neural network features; a VGG [16] CNN for images features, and a word2vec [17] for text features. For the specific problem of image annotation, the current state-of-art is obtained with the Word2VisualVec (W2VV) model [18] . This approach uses as a multimodal embedding space the same visual space where images are represented, involving a deeper text processing.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_10",
  "x": "The mapping of standardized values into these three categories is done through the definition of two constant thresholds. The optimal values of these thresholds can be found empirically for a labeled dataset [21] . Instead, we use threshold values shown to perform consistently across several domains [9] . ---------------------------------- **MULTIMODAL EMBEDDING** In our approach, we integrate the FNE with the multimodal embedding pipeline of <cite>Kiros et al. [1]</cite> . To do so we use the FNE to obtain an image representation instead of the output of the last layer of a CNN, as the <cite>original model</cite> does. The encoder architecture processing the text is used as it is, using a GRUs recurrent neural network to encode the sentences. To combine both embeddings, <cite>Kiros et al. [1]</cite> use an affine transformation on the image representation (in our case, the FNE) identical to a fully connected neural network layer.",
  "y": "extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_11",
  "x": "Specifically, the FNE discretization maps the feature values to the {\u22121, 0, 1} domain, where -1 indicates an unusually low value (i.e., the feature is significant by its absence for an image in the context of the dataset), 0 indicates that the feature has an average value (i.e., the feature is not significant) and 1 indicates an uncommonly high activation (i.e., the feature is significant by its presence for an image in the context of the dataset). The mapping of standardized values into these three categories is done through the definition of two constant thresholds. The optimal values of these thresholds can be found empirically for a labeled dataset [21] . Instead, we use threshold values shown to perform consistently across several domains [9] . ---------------------------------- **MULTIMODAL EMBEDDING** In our approach, we integrate the FNE with the multimodal embedding pipeline of <cite>Kiros et al. [1]</cite> . To do so we use the FNE to obtain an image representation instead of the output of the last layer of a CNN, as the <cite>original model</cite> does. The encoder architecture processing the text is used as it is, using a GRUs recurrent neural network to encode the sentences.",
  "y": "extends differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_12",
  "x": "---------------------------------- **MULTIMODAL EMBEDDING** In our approach, we integrate the FNE with the multimodal embedding pipeline of <cite>Kiros et al. [1]</cite> . To do so we use the FNE to obtain an image representation instead of the output of the last layer of a CNN, as the <cite>original model</cite> does. The encoder architecture processing the text is used as it is, using a GRUs recurrent neural network to encode the sentences. To combine both embeddings, <cite>Kiros et al. [1]</cite> use an affine transformation on the image representation (in our case, the FNE) identical to a fully connected neural network layer. This extra layer is trained simultaneously with the GRUs. The elements of the <cite>multimodal pipeline</cite> that are tuned during the training phase of the model are shown in orange in Figure  1 . In simple terms, the training procedure consist on the optimization of the pairwise ranking loss between the correct image-caption pair and a random pair.",
  "y": "differences background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_13",
  "x": "To combine both embeddings, <cite>Kiros et al. [1]</cite> use an affine transformation on the image representation (in our case, the FNE) identical to a fully connected neural network layer. This extra layer is trained simultaneously with the GRUs. The elements of the <cite>multimodal pipeline</cite> that are tuned during the training phase of the model are shown in orange in Figure  1 . In simple terms, the training procedure consist on the optimization of the pairwise ranking loss between the correct image-caption pair and a random pair. Assuming that a correct pair of elements should be closer in the multimodal space than a random pair. The loss L can be formally defined as follows: Where i is an image vector, c is its correct caption vector, and i k and c k are sets of random images and captions respectively. The operator s(\u2022, \u2022) defines the cosine similarity. This formulation includes a margin term \u03b1 to avoid pulling the image and caption closer once their distance is smaller than the margin.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_14",
  "x": "In this section we evaluate the impact of using the FNE in a multimodal pipeline (FN-MME) for both image annotation and image retrieval tasks. To properly measure the relevance of the FNE, we compare the results of the FN-MME with those of the <cite>original multimodal pipeline</cite> reported by <cite>Kiros et al. [1]</cite> (CNN-MME). Additionally, we define a second baseline by using the <cite>original multimodal pipeline</cite> with a training configuration closer to the one used for the FNE experiments (i.e., same source CNN, same MME dimensionality, etc.). We refer to this second baseline as CNN-MME*. ---------------------------------- **DATASETS** In our experiments we use three different publicly available datasets: The Flickr8K dataset [11] contains 8,000 hand-selected images from Flickr, depicting actions and events. Five correct captions are provided for each image.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_15",
  "x": "This makes the optimization focus on distant pairs instead of improving the ones that are already close. ---------------------------------- **EXPERIMENTS** In this section we evaluate the impact of using the FNE in a multimodal pipeline (FN-MME) for both image annotation and image retrieval tasks. To properly measure the relevance of the FNE, we compare the results of the FN-MME with those of the <cite>original multimodal pipeline</cite> reported by <cite>Kiros et al. [1]</cite> (CNN-MME). Additionally, we define a second baseline by using the <cite>original multimodal pipeline</cite> with a training configuration closer to the one used for the FNE experiments (i.e., same source CNN, same MME dimensionality, etc.). We refer to this second baseline as CNN-MME*. ---------------------------------- **DATASETS**",
  "y": "extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_16",
  "x": "Five correct captions are provided for each image. Following the provided splits, 6,000 images are used for train, 1,000 are used in validation and 1,000 more are kept for testing. The Flickr30K dataset [12] is an extension of Flickr8K. It contains 31,783 photographs of everyday activities, events and scenes. Five correct captions are provided for each image. In our experiments 29,000 images are used for training, 1,014 conform the validation set and 1,000 are kept for test. These splits are the same ones used by <cite>Kiros et al. [1]</cite> and by Karpathy and Fei-Fei [22] . The MSCOCO dataset [13] includes images of everyday scenes containing common objects in their natural context. For captioning, 82,783 images and 413,915 captions are available for training, while 40,504 images and 202,520 captions are available for validation.",
  "y": "uses"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_17",
  "x": "In our experiments we consider both settings. ---------------------------------- **IMPLEMENTATION AND EVALUATION DETAILS** The caption sentences are word-tokenized using the Natural Language Toolkit (NLTK) for Python [23] . The choice of the word embedding size and the number of GRUs has been analyzed to obtain a range of suitable parameters to test in the validation set. The total number of different words is 8,919 for Flickr8k, 22,962 for Flickr30k and 32,775 for MSCOCO. Using all the words present in the dataset is likely to produce overfitting problems when training on examples containing words that only occur a few times. This overfitting problem may not have a huge impact on performance, but it may add undesired noise in the multimodal representation. The original setup [<cite>1]</cite> limited the word embedding to the 300 most frequent words, while using 300 GRUs.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_18",
  "x": "**RESULTS** For both image annotation and image retrieval tasks on the Flickr8k dataset, Table 1 shows the results of the proposed FN-MME, the reported results of the <cite>original model</cite> <cite>CNN-MME</cite>, the results of the original model when using our configuration CNN-MME*, and the current state-of-the-art (SotA). Tables 2 and  3 are analogous for the Flickr30k and MSCOCO datasets. Additional results of the CNN-MME model were made publicly available later on by the original authors [26] . We include these for the MSCOCO dataset, which was not evaluated in the <cite>original paper</cite> <cite>[1]</cite> . First, let us consider the impact of using the FNE. On all cases, the multimodal pipeline proposed by <cite>Kiros et al. [1]</cite> obtains equal or better results when using the FNE. This is the case for the <cite>originally reported results</cite> (<cite>CNN-MME</cite>), for the results made available later on by the original authors (CNN-MME \u2020), and for the experiments we do using same configuration as the FN-MME (CNN-MME*). The comparison we consider to be the most relevant is the FN-MME against the CNN-MME*, as these contain the least differences besides the image embedding being used.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_19",
  "x": "\u2022 Recall@K (R@K) is the fraction of images for which a correct caption is ranked within the top-K retrieved results (and vice-versa for sentences). Results are provided for R@1, R@5 and R@10. \u2022 Median rank (Med r) of the highest ranked ground truth result. ---------------------------------- **RESULTS** For both image annotation and image retrieval tasks on the Flickr8k dataset, Table 1 shows the results of the proposed FN-MME, the reported results of the <cite>original model</cite> <cite>CNN-MME</cite>, the results of the original model when using our configuration CNN-MME*, and the current state-of-the-art (SotA). Tables 2 and  3 are analogous for the Flickr30k and MSCOCO datasets. Additional results of the CNN-MME model were made publicly available later on by the original authors [26] . We include these for the MSCOCO dataset, which was not evaluated in the <cite>original paper</cite> <cite>[1]</cite> .",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_20",
  "x": "We include these for the MSCOCO dataset, which was not evaluated in the <cite>original paper</cite> <cite>[1]</cite> . First, let us consider the impact of using the FNE. On all cases, the multimodal pipeline proposed by <cite>Kiros et al. [1]</cite> obtains equal or better results when using the FNE. This is the case for the <cite>originally reported results</cite> (<cite>CNN-MME</cite>), for the results made available later on by the original authors (CNN-MME \u2020), and for the experiments we do using same configuration as the FN-MME (CNN-MME*). The comparison we consider to be the most relevant is the FN-MME against the CNN-MME*, as these contain the least differences besides the image embedding being used. In this particular case, the FN-MME outperforms the CNN-MME* by 3 percentual points on average for the Flickr datasets, and roughly by 4 points for the MSCOCO dataset. To measure the relevance of the improvement provided by using the FNE, we compare the FN-MME model with the current state-of-the-art for image annotation and image retrieval. For the Flickr datasets, particularly for image annotation tasks, the performance of the FN-MME is significantly closer to the state of the art than the other variants of the same model (CNN-MME, CNN-MME \u2020, CNN-MME*). Remarkably, the FN-MME provides the best reported results on image annotation for the MSCOCO dataset.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_21",
  "x": "On all cases, the multimodal pipeline proposed by <cite>Kiros et al. [1]</cite> obtains equal or better results when using the FNE. This is the case for the <cite>originally reported results</cite> (<cite>CNN-MME</cite>), for the results made available later on by the original authors (CNN-MME \u2020), and for the experiments we do using same configuration as the FN-MME (CNN-MME*). The comparison we consider to be the most relevant is the FN-MME against the CNN-MME*, as these contain the least differences besides the image embedding being used. In this particular case, the FN-MME outperforms the CNN-MME* by 3 percentual points on average for the Flickr datasets, and roughly by 4 points for the MSCOCO dataset. To measure the relevance of the improvement provided by using the FNE, we compare the FN-MME model with the current state-of-the-art for image annotation and image retrieval. For the Flickr datasets, particularly for image annotation tasks, the performance of the FN-MME is significantly closer to the state of the art than the other variants of the same model (CNN-MME, CNN-MME \u2020, CNN-MME*). Remarkably, the FN-MME provides the best reported results on image annotation for the MSCOCO dataset. However, let us remark that the competitive W2VV method [18] has no reported results for MSCOCO. The results of the FN-MME for image retrieval tasks are significantly further from the stateof-the-art.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_22",
  "x": "Remarkably, the FN-MME provides the best reported results on image annotation for the MSCOCO dataset. However, let us remark that the competitive W2VV method [18] has no reported results for MSCOCO. The results of the FN-MME for image retrieval tasks are significantly further from the stateof-the-art. Overall, the competitiveness of FN-MME increases with dataset size. ---------------------------------- **CONCLUSIONS** For the multimodal pipeline of <cite>Kiros et al. [1]</cite> , using the Full-Network image embedding results in consistently higher performances than using a one-layer image embedding. These results suggest that the visual representation provided by the FNE is superior to the current standard for the construction of most multimodal embeddings. When compared to the current state-of-the-art, the results obtained by the FN-MME are significantly less competitive than problem-specific methods.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_23",
  "x": "The results of the FN-MME for image retrieval tasks are significantly further from the stateof-the-art. Overall, the competitiveness of FN-MME increases with dataset size. ---------------------------------- **CONCLUSIONS** For the multimodal pipeline of <cite>Kiros et al. [1]</cite> , using the Full-Network image embedding results in consistently higher performances than using a one-layer image embedding. These results suggest that the visual representation provided by the FNE is superior to the current standard for the construction of most multimodal embeddings. When compared to the current state-of-the-art, the results obtained by the FN-MME are significantly less competitive than problem-specific methods. Since this happens for all models using the same pipeline (CNN-MME, CNN-MME \u2020, CNN-MME*), these results indicate that the original architecture of <cite>Kiros et al. [1]</cite> is itself outperformed in general by more problem-specific techniques. Since the FNE is compatible with most multimodal pipelines based on CNN embeddings, as future work of this paper we intend to evaluate the performance of the FNE when integrated into the current state-of-the-art on image annotation (W2VV [18] ) and image retrieval (FV [4] ).",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_24",
  "x": "The results of the FN-MME for image retrieval tasks are significantly further from the stateof-the-art. Overall, the competitiveness of FN-MME increases with dataset size. ---------------------------------- **CONCLUSIONS** For the multimodal pipeline of <cite>Kiros et al. [1]</cite> , using the Full-Network image embedding results in consistently higher performances than using a one-layer image embedding. These results suggest that the visual representation provided by the FNE is superior to the current standard for the construction of most multimodal embeddings. When compared to the current state-of-the-art, the results obtained by the FN-MME are significantly less competitive than problem-specific methods. Since this happens for all models using the same pipeline (CNN-MME, CNN-MME \u2020, CNN-MME*), these results indicate that the original architecture of <cite>Kiros et al. [1]</cite> is itself outperformed in general by more problem-specific techniques. Since the FNE is compatible with most multimodal pipelines based on CNN embeddings, as future work of this paper we intend to evaluate the performance of the FNE when integrated into the current state-of-the-art on image annotation (W2VV [18] ) and image retrieval (FV [4] ).",
  "y": "future_work"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_0",
  "x": "Transition-based statistical parsing associates scores with each decision in the parsing process, selecting the parse which is built by the highest scoring sequence of decisions (Briscoe and Carroll, 1993; Nivre et al., 2006) . The parsing algorithm is typically some form of bottom-up shiftreduce algorithm, so that scores are associated with actions such as shift and reduce. One advantage of this approach is that the parsing can be highly efficient, for example by pursuing a greedy strategy in which a single action is chosen at each decision point. The alternative approach, exemplified by Collins (1997) and Charniak (2000) , is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar results (McDonald et al., 2005; Nivre et al., 2006) . For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005) . In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008) . For constituent-based parsing using the Chinese Treebank (CTB), <cite>Wang et al. (2006)</cite> have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.'s approach.",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_1",
  "x": "The alternative approach, exemplified by Collins (1997) and Charniak (2000) , is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar results (McDonald et al., 2005; Nivre et al., 2006) . For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005) . In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008) . For constituent-based parsing using the Chinese Treebank (CTB), <cite>Wang et al. (2006)</cite> have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.'s approach. We apply the same shift-reduce procedure as <cite>Wang et al. (2006)</cite> , but instead of using a local classifier for each transition-based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses. We apply beam search to decoding instead of greedy search. The parser still operates in linear time, but the use of beam-search allows the correction of local decision errors by global comparison.",
  "y": "motivation"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_2",
  "x": "One advantage of this approach is that the parsing can be highly efficient, for example by pursuing a greedy strategy in which a single action is chosen at each decision point. The alternative approach, exemplified by Collins (1997) and Charniak (2000) , is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar results (McDonald et al., 2005; Nivre et al., 2006) . For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005) . In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008) . For constituent-based parsing using the Chinese Treebank (CTB), <cite>Wang et al. (2006)</cite> have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.'s approach. We apply the same shift-reduce procedure as <cite>Wang et al. (2006)</cite> , but instead of using a local classifier for each transition-based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses. We apply beam search to decoding instead of greedy search.",
  "y": "uses differences"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_3",
  "x": "We also present accuracy scores for the much larger CTB5, using both a constituent-based and dependency-based evaluation. The scores for the dependency-based evaluation were higher than the state-of-the-art dependency parsers for the CTB5 data. ---------------------------------- **THE SHIFT-REDUCE PARSING PROCESS** The shift-reduce process used by our beam-search decoder is based on the greedy shift-reduce parsers of Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> . The process assumes binary-branching trees; section 2.1 explains how these are obtained from the arbitrary-branching trees in the Chinese Treebank. The input is assumed to be segmented and POS tagged, and the word-POS pairs waiting to be processed are stored in a queue. A stack holds the partial parse trees that are built during the parsing process. A parse state is defined as a stack,queue pair.",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_4",
  "x": "Parser actions, including SHIFT and various kinds of REDUCE, define functions from states to states by shifting word-POS pairs onto the stack and building partial parse trees. The actions used by the parser are: \u2022 SHIFT, which pushes the next word-POS pair in the queue onto the stack; \u2022 REDUCE-unary-X, which makes a new unary-branching node with label X; the stack is popped and the popped node becomes the child of the new node; the new node is pushed onto the stack; \u2022 REDUCE-binary-{L/R}-X, which makes a new binary-branching node with label X; the stack is popped twice, with the first popped node becoming the right child of the new node and the second popped node becoming the left child; the new node is pushed onto the stack; \u2022 TERMINATE, which pops the root node off the stack and ends parsing. This action is novel in our parser. Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> only used the first three transition actions, setting the final state as all incoming words having been processed, and the stack containing only one node. However, there are a small number of sentences (14 out of 3475 from the training data) that have unary-branching roots.",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_5",
  "x": "\u2022 SHIFT, which pushes the next word-POS pair in the queue onto the stack; \u2022 REDUCE-unary-X, which makes a new unary-branching node with label X; the stack is popped and the popped node becomes the child of the new node; the new node is pushed onto the stack; \u2022 REDUCE-binary-{L/R}-X, which makes a new binary-branching node with label X; the stack is popped twice, with the first popped node becoming the right child of the new node and the second popped node becoming the left child; the new node is pushed onto the stack; \u2022 TERMINATE, which pops the root node off the stack and ends parsing. This action is novel in our parser. Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> only used the first three transition actions, setting the final state as all incoming words having been processed, and the stack containing only one node. However, there are a small number of sentences (14 out of 3475 from the training data) that have unary-branching roots. For these sentences, Wang's parser will be unable to produce the unary-branching roots because the parsing process terminates as soon as the root is found. We define a separate action to terminate parsing, allowing unary reduces to be applied to the root item before parsing finishes.",
  "y": "extends"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_6",
  "x": "Note also that, since the parser is building binary trees, the X label in the REDUCE rules can be one of the temporary constituent labels, such as NP * , which are needed for the binarization process described in Section 2.1. Hence the number of left and right binary reduce rules is the number of constituent labels in the binarized grammar. <cite>Wang et al. (2006)</cite> give a detailed example showing how a segmented and POS-tagged sentence can be incrementally processed using the shift-reduce actions to produce a binary tree. We show this example in Figure 1 . ---------------------------------- **THE BINARIZATION PROCESS** The algorithm in Figure 2 is used to map CTB trees into binarized trees, which are required by the shift-reduce parsing process. For any tree node with more than two child nodes, the algorithm works by first finding the head node, and then processing its right-hand-side and left-hand-side, respectively. The head-finding rules are taken from Zhang and Clark (2008) .",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_7",
  "x": "Hence the number of left and right binary reduce rules is the number of constituent labels in the binarized grammar. <cite>Wang et al. (2006)</cite> give a detailed example showing how a segmented and POS-tagged sentence can be incrementally processed using the shift-reduce actions to produce a binary tree. We show this example in Figure 1 . ---------------------------------- **THE BINARIZATION PROCESS** The algorithm in Figure 2 is used to map CTB trees into binarized trees, which are required by the shift-reduce parsing process. For any tree node with more than two child nodes, the algorithm works by first finding the head node, and then processing its right-hand-side and left-hand-side, respectively. The head-finding rules are taken from Zhang and Clark (2008) . Y = X 1 ..X m represents a tree node Y with child nodes X 1 ...X m (m \u2265 1).",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_8",
  "x": "The label of the newly generated node Y * is based on the constituent label of the original node Y , but marked with an asterix. Hence binarization enlarges the set of constituent labels. We call the constituents marked with * temporary constituents. The binarization process is reversible, in that output from the shift-reduce parser can be unbinarized into CTB format, which is required for evaluation. ---------------------------------- **RESTRICTIONS ON THE SEQUENCE OF ACTIONS** Not all sequences of actions produce valid binarized trees. In the deterministic parser of <cite>Wang et al. (2006)</cite> , the highest scoring action predicted by the classifier may prevent a valid binary tree from being built. In this case, Wang et al. simply return a partial parse consisting of all the subtrees on the stack.",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_9",
  "x": "The context S 0 , S 1 , S 2 , S 3 and N 0 , N 1 , N 2 , N 3 for the feature templates is taken from <cite>Wang et al. (2006)</cite> . However, <cite>Wang et al. (2006)</cite> used a polynomial kernel function with an SVM and did not manually create feature combinations. Since we used the linear perceptron algorithm we manually combined Unigram features into Bigram and Trigram features. The \"Bracket\" row shows bracket-related features, which were inspired by <cite>Wang et al. (2006)</cite> . Here brackets refer to left brackets including \"\uff08\", \"\"\" and \"\u300a\" and right brackets including \"\uff09\", \"\"\" and \"\u300b\". In the table, b represents the matching status of the last left bracket (if any) on the stack. It takes three different values: 1 (no matching right bracket has been pushed onto stack), 2 (a matching right bracket has been pushed onto stack) and 3 (a matching right bracket has been pushed onto stack, but then popped off). The \"Separator\" row shows features that include one of the separator punctuations (i.e. \"\uff0c\", \"\u3002\", \"\u3001\" and \"\uff1b\") between the head words of S 0 and S 1 . These templates apply only when the stack contains at least two nodes; p represents a separator punctuation symbol.",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_10",
  "x": "S0cS1cp, S0cS1cq Table 1 : Feature templates these templates by first instantiating a template with particular labels, words and tags, and then pairing the instantiated template with a particular action. In the table, the symbols S 0 , S 1 , S 2 , and S 3 represent the top four nodes on the stack, and the symbols N 0 , N 1 , N 2 and N 3 represent the first four words in the incoming queue. S 0 L, S 0 R and S 0 U represent the left and right child for binary branching S 0 , and the single child for unary branching S 0 , respectively; w represents the lexical head token for a node; c represents the label for a node. When the corresponding node is a terminal, c represents its POS-tag, whereas when the corresponding node is non-terminal, c represents its constituent label; t represents the POS-tag for a word. The context S 0 , S 1 , S 2 , S 3 and N 0 , N 1 , N 2 , N 3 for the feature templates is taken from <cite>Wang et al. (2006)</cite> . However, <cite>Wang et al. (2006)</cite> used a polynomial kernel function with an SVM and did not manually create feature combinations. Since we used the linear perceptron algorithm we manually combined Unigram features into Bigram and Trigram features. The \"Bracket\" row shows bracket-related features, which were inspired by <cite>Wang et al. (2006)</cite> . Here brackets refer to left brackets including \"\uff08\", \"\"\" and \"\u300a\" and right brackets including \"\uff09\", \"\"\" and \"\u300b\".",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_11",
  "x": "In the table, the symbols S 0 , S 1 , S 2 , and S 3 represent the top four nodes on the stack, and the symbols N 0 , N 1 , N 2 and N 3 represent the first four words in the incoming queue. S 0 L, S 0 R and S 0 U represent the left and right child for binary branching S 0 , and the single child for unary branching S 0 , respectively; w represents the lexical head token for a node; c represents the label for a node. When the corresponding node is a terminal, c represents its POS-tag, whereas when the corresponding node is non-terminal, c represents its constituent label; t represents the POS-tag for a word. The context S 0 , S 1 , S 2 , S 3 and N 0 , N 1 , N 2 , N 3 for the feature templates is taken from <cite>Wang et al. (2006)</cite> . However, <cite>Wang et al. (2006)</cite> used a polynomial kernel function with an SVM and did not manually create feature combinations. Since we used the linear perceptron algorithm we manually combined Unigram features into Bigram and Trigram features. The \"Bracket\" row shows bracket-related features, which were inspired by <cite>Wang et al. (2006)</cite> . Here brackets refer to left brackets including \"\uff08\", \"\"\" and \"\u300a\" and right brackets including \"\uff09\", \"\"\" and \"\u300b\". In the table, b represents the matching status of the last left bracket (if any) on the stack.",
  "y": "differences"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_12",
  "x": "In the table, the symbols S 0 , S 1 , S 2 , and S 3 represent the top four nodes on the stack, and the symbols N 0 , N 1 , N 2 and N 3 represent the first four words in the incoming queue. S 0 L, S 0 R and S 0 U represent the left and right child for binary branching S 0 , and the single child for unary branching S 0 , respectively; w represents the lexical head token for a node; c represents the label for a node. When the corresponding node is a terminal, c represents its POS-tag, whereas when the corresponding node is non-terminal, c represents its constituent label; t represents the POS-tag for a word. The context S 0 , S 1 , S 2 , S 3 and N 0 , N 1 , N 2 , N 3 for the feature templates is taken from <cite>Wang et al. (2006)</cite> . However, <cite>Wang et al. (2006)</cite> used a polynomial kernel function with an SVM and did not manually create feature combinations. Since we used the linear perceptron algorithm we manually combined Unigram features into Bigram and Trigram features. The \"Bracket\" row shows bracket-related features, which were inspired by <cite>Wang et al. (2006)</cite> . Here brackets refer to left brackets including \"\uff08\", \"\"\" and \"\u300a\" and right brackets including \"\uff09\", \"\"\" and \"\u300b\". In the table, b represents the matching status of the last left bracket (if any) on the stack.",
  "y": "similarities"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_13",
  "x": "These templates apply only when the stack contains at least two nodes; p represents a separator punctuation symbol. Each unique separator punctuation between S 0 and S 1 is only counted once when generating the global feature vector. q represents the count of any separator punctuation between S 0 and S 1 . Whenever an action is being considered at each point in the beam-search process, templates from Table 1 are matched with the context defined by the parser state and combined with the action to generate features. Negative features, which are the features from incorrect parser outputs but not from any training example, are included in the model. There are around a million features in our experiments with the CTB2 dataset. <cite>Wang et al. (2006)</cite> used a range of other features, including rhythmic features of S 0 and S 1 (Sun and Jurafsky, 2003) , features from the most recently found node that is to the left or right of S 0 and S 1 , the number of words and the number of punctuations in S 0 and S 1 , the distance between S 0 and S 1 and so on. We did not include these features in our parser, because they did not lead to improved performance during development experiments. ----------------------------------",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_14",
  "x": "Negative features, which are the features from incorrect parser outputs but not from any training example, are included in the model. There are around a million features in our experiments with the CTB2 dataset. <cite>Wang et al. (2006)</cite> used a range of other features, including rhythmic features of S 0 and S 1 (Sun and Jurafsky, 2003) , features from the most recently found node that is to the left or right of S 0 and S 1 , the number of words and the number of punctuations in S 0 and S 1 , the distance between S 0 and S 1 and so on. We did not include these features in our parser, because they did not lead to improved performance during development experiments. ---------------------------------- **EXPERIMENTS** The experiments were performed using the Chinese Treebank 2 and Chinese Treebank 5 data. Standard data preparation was performed before the experiments: empty terminal nodes were removed; any non-terminal nodes with no children were removed; any unary X \u2192 X nodes resulting from the previous steps were collapsed into one X node. For all experiments, we used the EVALB tool 1 for evaluation, and used labeled recall (LR), labeled precision (LP ) and F 1 score (which is the harmonic mean of LR and LP ) to measure parsing accuracy.",
  "y": "differences"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_15",
  "x": "Hence we set the beam size to 16 for the rest of the experiments. ---------------------------------- **THE INFLUENCE OF BEAM-SIZE** ---------------------------------- **TEST RESULTS ON CTB2** The experiments in this section were performed using CTB2 to allow comparison with previous work, with the CTB2 data extracted from Chinese Treebank 5 (CTB5 Table 3 : Accuracies on CTB2 with gold-standard POS-tags own implementation of the perceptron-based tagger from Collins (2002) . The results of various models measured using sentences with less than 40 words and using goldstandard POS-tags are shown in Table 3 . The rows represent the model from Bikel and Chiang (2000) , Bikel (2004) , the SVM and ensemble models from <cite>Wang et al. (2006)</cite> , and our parser, respectively. The accuracy of our parser is competitive using this test set.",
  "y": "similarities"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_16",
  "x": "The experiments in this section were performed using CTB2 to allow comparison with previous work, with the CTB2 data extracted from Chinese Treebank 5 (CTB5 Table 3 : Accuracies on CTB2 with gold-standard POS-tags own implementation of the perceptron-based tagger from Collins (2002) . The results of various models measured using sentences with less than 40 words and using goldstandard POS-tags are shown in Table 3 . The rows represent the model from Bikel and Chiang (2000) , Bikel (2004) , the SVM and ensemble models from <cite>Wang et al. (2006)</cite> , and our parser, respectively. The accuracy of our parser is competitive using this test set. The results of various models using automatically assigned POS-tags are shown in Table 4 . The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003) , Xiong et al. (2005) , Bikel (2004), Chiang and Bikel (2002) , the SVM model from <cite>Wang et al. (2006)</cite> and the ensemble system from <cite>Wang et al. (2006)</cite> , and the parser of this paper, respectively. Our parser gave comparable accuracies to the SVM and ensemble models from <cite>Wang et al. (2006)</cite> . However, comparison with Table 3 shows that our parser is more sensitive to POS-tagging errors than some of the other models. One possible reason is that some of the other parsers, e.g. Bikel (2004) , use the parser model itself to resolve tagging ambiguities, whereas we rely on a POS tagger to accurately assign a single tag to each word.",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_17",
  "x": "---------------------------------- **TEST RESULTS ON CTB2** The experiments in this section were performed using CTB2 to allow comparison with previous work, with the CTB2 data extracted from Chinese Treebank 5 (CTB5 Table 3 : Accuracies on CTB2 with gold-standard POS-tags own implementation of the perceptron-based tagger from Collins (2002) . The results of various models measured using sentences with less than 40 words and using goldstandard POS-tags are shown in Table 3 . The rows represent the model from Bikel and Chiang (2000) , Bikel (2004) , the SVM and ensemble models from <cite>Wang et al. (2006)</cite> , and our parser, respectively. The accuracy of our parser is competitive using this test set. The results of various models using automatically assigned POS-tags are shown in Table 4 . The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003) , Xiong et al. (2005) , Bikel (2004), Chiang and Bikel (2002) , the SVM model from <cite>Wang et al. (2006)</cite> and the ensemble system from <cite>Wang et al. (2006)</cite> , and the parser of this paper, respectively. Our parser gave comparable accuracies to the SVM and ensemble models from <cite>Wang et al. (2006)</cite> .",
  "y": "similarities"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_18",
  "x": "The results of various models measured using sentences with less than 40 words and using goldstandard POS-tags are shown in Table 3 . The rows represent the model from Bikel and Chiang (2000) , Bikel (2004) , the SVM and ensemble models from <cite>Wang et al. (2006)</cite> , and our parser, respectively. The accuracy of our parser is competitive using this test set. The results of various models using automatically assigned POS-tags are shown in Table 4 . The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003) , Xiong et al. (2005) , Bikel (2004), Chiang and Bikel (2002) , the SVM model from <cite>Wang et al. (2006)</cite> and the ensemble system from <cite>Wang et al. (2006)</cite> , and the parser of this paper, respectively. Our parser gave comparable accuracies to the SVM and ensemble models from <cite>Wang et al. (2006)</cite> . However, comparison with Table 3 shows that our parser is more sensitive to POS-tagging errors than some of the other models. One possible reason is that some of the other parsers, e.g. Bikel (2004) , use the parser model itself to resolve tagging ambiguities, whereas we rely on a POS tagger to accurately assign a single tag to each word. In fact, for the Chinese data, POS tagging accuracy is not very high, with the perceptron-based tagger achieving an accuracy of only 93%.",
  "y": "differences"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_19",
  "x": "The same measures are taken and the accuracies with gold-standard POS-tags are shown in Table 8 . Our constituent parser gave higher accuracy than the dependency parser. It is interesting that, though the constituent parser uses many fewer feature templates than the dependency parser, the features do include constituent information, which is unavailable to dependency parsers. ---------------------------------- **RELATED WORK** Our parser is based on the shift-reduce parsing process from Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> , and therefore it can be classified as a transition-based parser (Nivre et al., 2006 ). An important difference between our parser and the <cite>Wang et al. (2006)</cite> parser is that our parser is based on a discriminative learning model with global features, whilst the parser from <cite>Wang et al. (2006)</cite> is based on a local classifier that optimizes each individual choice. Instead of greedy local decoding, we used beam search in the decoder. An early work that applies beam search to constituent parsing is Ratnaparkhi (1999) .",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_20",
  "x": "Our parser is based on the shift-reduce parsing process from Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> , and therefore it can be classified as a transition-based parser (Nivre et al., 2006 ). An important difference between our parser and the <cite>Wang et al. (2006)</cite> parser is that our parser is based on a discriminative learning model with global features, whilst the parser from <cite>Wang et al. (2006)</cite> is based on a local classifier that optimizes each individual choice. Instead of greedy local decoding, we used beam search in the decoder. An early work that applies beam search to constituent parsing is Ratnaparkhi (1999) . The main difference between our parser and Ratnaparkhi's is that we use a global discriminative model, whereas Ratnaparkhi's parser has separate probabilities of actions chained together in a conditional model. Both our parser and the parser from Collins and Roark (2004) use a global discriminative model and an incremental parsing process. The major difference is the use of different incremental parsing processes. To achieve better performance for Chinese parsing, our parser is based on the shiftreduce parsing process. In addition, we did not include a generative baseline model in the discriminative model, as did Collins and Roark (2004) .",
  "y": "differences"
 },
 {
  "id": "e9b2f32ed29589b4a6d49d3b30fc3a_0",
  "x": "The cross-lingual search engine supports a fast search capability (sub-second response for typical queries) and achieves state-of-the-art performance in the high precision region of the result list. The on demand statistical machine translation uses the Direct Translation model along with a novel statistical Arabic Morphological Analyzer to yield state-of-the-art translation quality. The on demand SMT uses an efficient dynamic programming decoder that achieves reasonable speed for translating web documents. ---------------------------------- **OVERVIEW** Morphologically rich languages like Arabic (Beesley, K. 1996 ) present significant challenges to many natural language processing applications as the one described above because a word often conveys complex meanings decomposable into several morphemes (i.e. prefix, stem, suffix) . By segmenting words into morphemes, we can improve the performance of natural language systems including machine translation <cite>(Brown et al. 1993</cite> ) and information retrieval (Franz, M. and McCarley, S. 2002) . In this paper, we present a cross-lingual English-Arabic search engine combined with an on demand ArabicEnglish statistical machine translation system that relies on source language analysis for both improved search and translation. We developed novel statistical learning algorithms for performing Arabic word segmentation (Lee, Y. et al 2003) into morphemes and morphological source language (Arabic) analysis (Lee, Y. et al 2003b) .",
  "y": "motivation extends"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_0",
  "x": "**INTRODUCTION** Neural network-based encoder-decoder models are cutting-edge methodologies for tackling natural language generation (NLG) tasks, i.e., machine translation (Cho et al., 2014) , image captioning (Vinyals et al., 2015) , video description (Venugopalan et al., 2015) , and headline generation (<cite>Rush et al., 2015</cite>) . This paper also shares a similar goal and motivation to <cite>previous work</cite>: improving the encoderdecoder models for natural language generation. There are several directions for enhancement. This paper respects the fact that NLP researchers have expended an enormous amount of effort to develop fundamental NLP techniques such as POS tagging, dependency parsing, named entity recognition, and semantic role labeling. Intuitively, this structural, syntactic, and semantic information underlying input text has the potential for improving the quality of NLG tasks. However, to the best of our knowledge, there is no clear evidence that syntactic and semantic information can enhance the recently developed encoder-decoder models in NLG tasks. To answer this research question, this paper proposes and evaluates a headline generation method based on an encoder-decoder architecture on Abstract Meaning Representation (AMR). The method is essentially an extension of <cite>attention-based summarization</cite> (<cite>ABS</cite>) (<cite>Rush et al., 2015</cite>) .",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_1",
  "x": "However, to the best of our knowledge, there is no clear evidence that syntactic and semantic information can enhance the recently developed encoder-decoder models in NLG tasks. To answer this research question, this paper proposes and evaluates a headline generation method based on an encoder-decoder architecture on Abstract Meaning Representation (AMR). The method is essentially an extension of <cite>attention-based summarization</cite> (<cite>ABS</cite>) (<cite>Rush et al., 2015</cite>) . Our proposed method encodes results obtained from an AMR parser by using a modified version of Tree-LSTM encoder (Tai et al., 2015) as additional information of the <cite>baseline ABS model</cite>. Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (<cite>ABS</cite> and AMR). ---------------------------------- **<cite>ATTENTION-BASED SUMMARIZATION (ABS)</cite>** <cite>ABS</cite> proposed in <cite>Rush et al. (2015)</cite> has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007) .",
  "y": "extends"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_2",
  "x": "The method is essentially an extension of <cite>attention-based summarization</cite> (<cite>ABS</cite>) (<cite>Rush et al., 2015</cite>) . Our proposed method encodes results obtained from an AMR parser by using a modified version of Tree-LSTM encoder (Tai et al., 2015) as additional information of the <cite>baseline ABS model</cite>. Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (<cite>ABS</cite> and AMR). ---------------------------------- **<cite>ATTENTION-BASED SUMMARIZATION (ABS)</cite>** <cite>ABS</cite> proposed in <cite>Rush et al. (2015)</cite> has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007) . Figure 1 illustrates the model structure of <cite>ABS</cite>. <cite>The model</cite> predicts a word sequence (summary) based on the combination of the neural network language model and an input sentence encoder.",
  "y": "extends"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_5",
  "x": "The method is essentially an extension of <cite>attention-based summarization</cite> (<cite>ABS</cite>) (<cite>Rush et al., 2015</cite>) . Our proposed method encodes results obtained from an AMR parser by using a modified version of Tree-LSTM encoder (Tai et al., 2015) as additional information of the <cite>baseline ABS model</cite>. Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (<cite>ABS</cite> and AMR). ---------------------------------- **<cite>ATTENTION-BASED SUMMARIZATION (ABS)</cite>** <cite>ABS</cite> proposed in <cite>Rush et al. (2015)</cite> has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007) . Figure 1 illustrates the model structure of <cite>ABS</cite>. <cite>The model</cite> predicts a word sequence (summary) based on the combination of the neural network language model and an input sentence encoder.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_6",
  "x": "Our proposed method encodes results obtained from an AMR parser by using a modified version of Tree-LSTM encoder (Tai et al., 2015) as additional information of the <cite>baseline ABS model</cite>. Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (<cite>ABS</cite> and AMR). ---------------------------------- **<cite>ATTENTION-BASED SUMMARIZATION (ABS)</cite>** <cite>ABS</cite> proposed in <cite>Rush et al. (2015)</cite> has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007) . Figure 1 illustrates the model structure of <cite>ABS</cite>. <cite>The model</cite> predicts a word sequence (summary) based on the combination of the neural network language model and an input sentence encoder. Let V be a vocabulary.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_7",
  "x": "---------------------------------- **<cite>ATTENTION-BASED SUMMARIZATION (ABS)</cite>** <cite>ABS</cite> proposed in <cite>Rush et al. (2015)</cite> has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007) . Figure 1 illustrates the model structure of <cite>ABS</cite>. <cite>The model</cite> predicts a word sequence (summary) based on the combination of the neural network language model and an input sentence encoder. Let V be a vocabulary. x i is the i-th indicator vector corresponding to the i-th word in the input sentence. Suppose we have M words of an input sentence. X represents an input sentence, which <s> canadian prime \u2026 year <s> canada \u2026 nato",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_8",
  "x": "X represents an input sentence, which <s> canadian prime \u2026 year <s> canada \u2026 nato input sentence headline is represented as a sequence of indicator vectors, whose length is M . That is, x i \u2208 {0, 1} |V | , and whose length is L. Here, we assume L < M . Y C,i is a short notation of the list of vectors, which consists of the sub-sequence in Y from y i\u2212C+1 to y i . We assume a one-hot vector for a special start symbol, such as \"\u27e8S\u27e9\", when i < 1. Then, <cite>ABS</cite> outputs a summary\u0176 given an input sentence X as follows: where nnlm(Y C,i ) is a feed-forward neural network language model proposed in (Bengio et al., 2003) , and enc(X, Y C,i ) is an input sentence encoder with attention mechanism. This paper uses D and H as denoting sizes (dimensions) of vectors for word embedding and hidden layer, respectively.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_9",
  "x": "Let S \u2208 R D\u00d7(CD) be a weight matrix for mapping the context embedding of C output words onto embeddings obtained from nodes. Then, we define the attention-based AMR encoder 'encAMR(A, Y C,i )' as follows: Finally, we combine our attention-based AMR encoder shown in Equation 14 as an additional term of Equation 3 to build our headline generation system. ---------------------------------- **EXPERIMENTS** To demonstrate the effectiveness of our proposed method, we conducted experiments on benchmark data of the abstractive headline generation task described in <cite>Rush et al. (2015)</cite> . ---------------------------------- **DUC-2004** <cite>Gigaword</cite> test data used <cite>Gigaword</cite> in (<cite>Rush et al., 2015</cite>) Our sampled test data (<cite>Rush et al., 2015</cite>)",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_11",
  "x": "For a fair comparison, we followed their evaluation setting. The training data was obtained from the first sentence and the headline of a document in the annotated <cite>Gigaword</cite> corpus (Napoles et al., 2012) 4 . The development data is DUC-2003 data, and test data are both DUC-2004 (Over et al., 2007) and sentence-headline pairs obtained from the annotated <cite>Gigaword</cite> corpus as well as training data 5 . All of the generated headlines were evaluated by ROUGE (Lin, 2004) 6 . For evaluation on DUC-2004, we removed strings after 75-characters for each generated headline as described in the DUC-2004 evaluation. For evaluation on <cite>Gigaword</cite>, we forced the system outputs to be at most 8 words as in <cite>Rush et al. (2015)</cite> since the average length of headline in <cite>Gigaword</cite> is 8.3 words. For the preprocessing for all data, all letters were converted to lower case, all digits were replaced with '#', and words appearing less than five times with 'UNK'. Note that, for further evaluation, we prepared 2,000 sentence-headline pairs randomly sampled from the test data section of the <cite>Gigaword</cite> corpus as our additional test data. In our experiments, we refer to the baseline neural attention-based abstractive summarization method described in <cite>Rush et al. (2015)</cite> as \"<cite>ABS</cite>\", and our proposed method of incorporating AMR structural information by a neural encoder to the baseline method described in Section 3 as \"<cite>ABS</cite>+AMR\".",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_12",
  "x": "---------------------------------- **DUC-2004** <cite>Gigaword</cite> test data used <cite>Gigaword</cite> in (<cite>Rush et al., 2015</cite>) Our sampled test data (<cite>Rush et al., 2015</cite>) For a fair comparison, we followed their evaluation setting. The training data was obtained from the first sentence and the headline of a document in the annotated <cite>Gigaword</cite> corpus (Napoles et al., 2012) 4 . The development data is DUC-2003 data, and test data are both DUC-2004 (Over et al., 2007) and sentence-headline pairs obtained from the annotated <cite>Gigaword</cite> corpus as well as training data 5 . All of the generated headlines were evaluated by ROUGE (Lin, 2004) 6 . For evaluation on DUC-2004, we removed strings after 75-characters for each generated headline as described in the DUC-2004 evaluation. For evaluation on <cite>Gigaword</cite>, we forced the system outputs to be at most 8 words as in <cite>Rush et al. (2015)</cite> since the average length of headline in <cite>Gigaword</cite> is 8.3 words.",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_13",
  "x": "For a fair comparison, we followed their evaluation setting. The training data was obtained from the first sentence and the headline of a document in the annotated <cite>Gigaword</cite> corpus (Napoles et al., 2012) 4 . The development data is DUC-2003 data, and test data are both DUC-2004 (Over et al., 2007) and sentence-headline pairs obtained from the annotated <cite>Gigaword</cite> corpus as well as training data 5 . All of the generated headlines were evaluated by ROUGE (Lin, 2004) 6 . For evaluation on DUC-2004, we removed strings after 75-characters for each generated headline as described in the DUC-2004 evaluation. For evaluation on <cite>Gigaword</cite>, we forced the system outputs to be at most 8 words as in <cite>Rush et al. (2015)</cite> since the average length of headline in <cite>Gigaword</cite> is 8.3 words. For the preprocessing for all data, all letters were converted to lower case, all digits were replaced with '#', and words appearing less than five times with 'UNK'. Note that, for further evaluation, we prepared 2,000 sentence-headline pairs randomly sampled from the test data section of the <cite>Gigaword</cite> corpus as our additional test data. In our experiments, we refer to the baseline neural attention-based abstractive summarization method described in <cite>Rush et al. (2015)</cite> as \"<cite>ABS</cite>\", and our proposed method of incorporating AMR structural information by a neural encoder to the baseline method described in Section 3 as \"<cite>ABS</cite>+AMR\".",
  "y": "uses similarities"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_14",
  "x": "For a fair comparison, we followed their evaluation setting. The training data was obtained from the first sentence and the headline of a document in the annotated <cite>Gigaword</cite> corpus (Napoles et al., 2012) 4 . The development data is DUC-2003 data, and test data are both DUC-2004 (Over et al., 2007) and sentence-headline pairs obtained from the annotated <cite>Gigaword</cite> corpus as well as training data 5 . All of the generated headlines were evaluated by ROUGE (Lin, 2004) 6 . For evaluation on DUC-2004, we removed strings after 75-characters for each generated headline as described in the DUC-2004 evaluation. For evaluation on <cite>Gigaword</cite>, we forced the system outputs to be at most 8 words as in <cite>Rush et al. (2015)</cite> since the average length of headline in <cite>Gigaword</cite> is 8.3 words. For the preprocessing for all data, all letters were converted to lower case, all digits were replaced with '#', and words appearing less than five times with 'UNK'. Note that, for further evaluation, we prepared 2,000 sentence-headline pairs randomly sampled from the test data section of the <cite>Gigaword</cite> corpus as our additional test data. In our experiments, we refer to the baseline neural attention-based abstractive summarization method described in <cite>Rush et al. (2015)</cite> as \"<cite>ABS</cite>\", and our proposed method of incorporating AMR structural information by a neural encoder to the baseline method described in Section 3 as \"<cite>ABS</cite>+AMR\".",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_15",
  "x": "For evaluation on <cite>Gigaword</cite>, we forced the system outputs to be at most 8 words as in <cite>Rush et al. (2015)</cite> since the average length of headline in <cite>Gigaword</cite> is 8.3 words. For the preprocessing for all data, all letters were converted to lower case, all digits were replaced with '#', and words appearing less than five times with 'UNK'. Note that, for further evaluation, we prepared 2,000 sentence-headline pairs randomly sampled from the test data section of the <cite>Gigaword</cite> corpus as our additional test data. In our experiments, we refer to the baseline neural attention-based abstractive summarization method described in <cite>Rush et al. (2015)</cite> as \"<cite>ABS</cite>\", and our proposed method of incorporating AMR structural information by a neural encoder to the baseline method described in Section 3 as \"<cite>ABS</cite>+AMR\". Additionally, we also evaluated the performance of the AMR encoder without the attention mechanism, which we refer to as \"<cite>ABS</cite>+AMR(w/o attn)\", to investigate the contribution of the attention mechanism on the AMR encoder. For the parameter estimation (training), we used stochastic gradient descent to learn parameters. We tried several values for the initial learning rate, and selected the value that achieved the best performance for each method. We decayed the learning rate by half if the log-likelihood on the validation set did not improve for an epoch. Hyper-parameters we selected were D = 200, H = 400, N = 200, E = 50, C = 5, and Q = 2.",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_17",
  "x": "In our experiments, we refer to the baseline neural attention-based abstractive summarization method described in <cite>Rush et al. (2015)</cite> as \"<cite>ABS</cite>\", and our proposed method of incorporating AMR structural information by a neural encoder to the baseline method described in Section 3 as \"<cite>ABS</cite>+AMR\". Additionally, we also evaluated the performance of the AMR encoder without the attention mechanism, which we refer to as \"<cite>ABS</cite>+AMR(w/o attn)\", to investigate the contribution of the attention mechanism on the AMR encoder. For the parameter estimation (training), we used stochastic gradient descent to learn parameters. We tried several values for the initial learning rate, and selected the value that achieved the best performance for each method. We decayed the learning rate by half if the log-likelihood on the validation set did not improve for an epoch. Hyper-parameters we selected were D = 200, H = 400, N = 200, E = 50, C = 5, and Q = 2. We re-normalized the embedding after each epoch (Hinton et al., 2012) . For <cite>ABS</cite>+AMR, we used the two-step training scheme to accelerate the training speed. The first phase learns the parameters of the <cite>ABS</cite>.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_18",
  "x": "For <cite>ABS</cite>+AMR, we used the two-step training scheme to accelerate the training speed. The first phase learns the parameters of the <cite>ABS</cite>. The second phase trains the parameters of the AMR encoder by using 1 million training pairs while the parameters of the baseline <cite>ABS</cite> were fixed and unchanged to prevent overfitting. Table 1 shows the recall of ROUGE (Lin, 2004 ) on each dataset. <cite>ABS</cite> (re-run) represents the performance of <cite>ABS</cite> re-trained by the distributed scripts 7 . We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets. In particular, <cite>ABS</cite>+AMR achieved statistically significant gain from <cite>ABS</cite> (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004. However in contrast, we observed that the improvements on <cite>Gigaword</cite> (the same test data as <cite>Rush et al. (2015)</cite> ) seem to be limited compared with the DUC-2004 dataset. We assume that this limited gain is caused largely by the quality of AMR parsing results.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_19",
  "x": "Hyper-parameters we selected were D = 200, H = 400, N = 200, E = 50, C = 5, and Q = 2. We re-normalized the embedding after each epoch (Hinton et al., 2012) . For <cite>ABS</cite>+AMR, we used the two-step training scheme to accelerate the training speed. The first phase learns the parameters of the <cite>ABS</cite>. The second phase trains the parameters of the AMR encoder by using 1 million training pairs while the parameters of the baseline <cite>ABS</cite> were fixed and unchanged to prevent overfitting. Table 1 shows the recall of ROUGE (Lin, 2004 ) on each dataset. <cite>ABS</cite> (re-run) represents the performance of <cite>ABS</cite> re-trained by the distributed scripts 7 . We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets. In particular, <cite>ABS</cite>+AMR achieved statistically significant gain from <cite>ABS</cite> (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_20",
  "x": "We decayed the learning rate by half if the log-likelihood on the validation set did not improve for an epoch. Hyper-parameters we selected were D = 200, H = 400, N = 200, E = 50, C = 5, and Q = 2. We re-normalized the embedding after each epoch (Hinton et al., 2012) . For <cite>ABS</cite>+AMR, we used the two-step training scheme to accelerate the training speed. The first phase learns the parameters of the <cite>ABS</cite>. The second phase trains the parameters of the AMR encoder by using 1 million training pairs while the parameters of the baseline <cite>ABS</cite> were fixed and unchanged to prevent overfitting. Table 1 shows the recall of ROUGE (Lin, 2004 ) on each dataset. <cite>ABS</cite> (re-run) represents the performance of <cite>ABS</cite> re-trained by the distributed scripts 7 . We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_21",
  "x": "We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets. In particular, <cite>ABS</cite>+AMR achieved statistically significant gain from <cite>ABS</cite> (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004. However in contrast, we observed that the improvements on <cite>Gigaword</cite> (the same test data as <cite>Rush et al. (2015)</cite> ) seem to be limited compared with the DUC-2004 dataset. We assume that this limited gain is caused largely by the quality of AMR parsing results. This means that the 7 https://github.com/facebook/NAMAS I(1): crown prince abdallah ibn abdel aziz left saturday at the head of saudi arabia 's delegation to the islamic summit in islamabad , the official news agency spa reported . G: saudi crown prince leaves for islamic summit A: crown prince leaves for islamic summit in saudi arabia P: saudi crown prince leaves for islamic summit in riyadh I(2): a massive gothic revival building once christened the lunatic asylum west of the <unk> was auctioned off for $ #. # million -lrbeuro# . # million -rrb-. G: massive ##th century us mental hospital fetches $ #.",
  "y": "differences"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_22",
  "x": "Hyper-parameters we selected were D = 200, H = 400, N = 200, E = 50, C = 5, and Q = 2. We re-normalized the embedding after each epoch (Hinton et al., 2012) . For <cite>ABS</cite>+AMR, we used the two-step training scheme to accelerate the training speed. The first phase learns the parameters of the <cite>ABS</cite>. The second phase trains the parameters of the AMR encoder by using 1 million training pairs while the parameters of the baseline <cite>ABS</cite> were fixed and unchanged to prevent overfitting. Table 1 shows the recall of ROUGE (Lin, 2004 ) on each dataset. <cite>ABS</cite> (re-run) represents the performance of <cite>ABS</cite> re-trained by the distributed scripts 7 . We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets. In particular, <cite>ABS</cite>+AMR achieved statistically significant gain from <cite>ABS</cite> (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004.",
  "y": "differences"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_23",
  "x": "We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets. In particular, <cite>ABS</cite>+AMR achieved statistically significant gain from <cite>ABS</cite> (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004. However in contrast, we observed that the improvements on <cite>Gigaword</cite> (the same test data as <cite>Rush et al. (2015)</cite> ) seem to be limited compared with the DUC-2004 dataset. We assume that this limited gain is caused largely by the quality of AMR parsing results. This means that the 7 https://github.com/facebook/NAMAS I(1): crown prince abdallah ibn abdel aziz left saturday at the head of saudi arabia 's delegation to the islamic summit in islamabad , the official news agency spa reported . G: saudi crown prince leaves for islamic summit A: crown prince leaves for islamic summit in saudi arabia P: saudi crown prince leaves for islamic summit in riyadh I(2): a massive gothic revival building once christened the lunatic asylum west of the <unk> was auctioned off for $ #. # million -lrbeuro# . # million -rrb-. G: massive ##th century us mental hospital fetches $ #.",
  "y": "similarities uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_26",
  "x": "# million -rrb-. G: massive ##th century us mental hospital fetches $ #. # million at auction A: west african art sells for $ #. # million in P: west african art auctioned off for $ #. # million I(3): brooklyn , the new bastion of cool for many new yorkers , is poised to go mainstream chic . G: high-end retailers are scouting sites in brooklyn A: new yorkers are poised to go mainstream with chic P: new york city is poised to go mainstream chic Figure 3 : Examples of generated headlines on <cite>Gigaword</cite>. I: input, G: true headline, A: <cite>ABS</cite> (re-run), and P: <cite>ABS</cite>+AMR. <cite>Gigaword</cite> test data provided by <cite>Rush et al. (2015)</cite> is already pre-processed. Therefore, the quality of the AMR parsing results seems relatively worse on this pre-processed data since, for example, many low-occurrence words in the data were already replaced with \"UNK\".",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_27",
  "x": "<cite>Gigaword</cite> test data provided by <cite>Rush et al. (2015)</cite> is already pre-processed. Therefore, the quality of the AMR parsing results seems relatively worse on this pre-processed data since, for example, many low-occurrence words in the data were already replaced with \"UNK\". To provide evidence of this assumption, we also evaluated the performance on our randomly selected 2,000 sentence-headline test data also taken from the test data section of the annotated <cite>Gigaword</cite> corpus. \"<cite>Gigaword</cite> (randomly sampled)\" in Table 1 shows the results of this setting. We found the statistical difference between <cite>ABS</cite>(re-run) and <cite>ABS</cite>+AMR on ROUGE-1 and ROUGE-2. We can also observe that <cite>ABS</cite>+AMR achieved the best ROUGE-1 scores on all of the test data. According to this fact, <cite>ABS</cite>+AMR tends to successfully yield semantically important words. In other words, embeddings encoded through the AMR encoder are useful for capturing important concepts in input sentences. Figure 3 supports this observation.",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_29",
  "x": "Therefore, the quality of the AMR parsing results seems relatively worse on this pre-processed data since, for example, many low-occurrence words in the data were already replaced with \"UNK\". To provide evidence of this assumption, we also evaluated the performance on our randomly selected 2,000 sentence-headline test data also taken from the test data section of the annotated <cite>Gigaword</cite> corpus. \"<cite>Gigaword</cite> (randomly sampled)\" in Table 1 shows the results of this setting. We found the statistical difference between <cite>ABS</cite>(re-run) and <cite>ABS</cite>+AMR on ROUGE-1 and ROUGE-2. We can also observe that <cite>ABS</cite>+AMR achieved the best ROUGE-1 scores on all of the test data. According to this fact, <cite>ABS</cite>+AMR tends to successfully yield semantically important words. In other words, embeddings encoded through the AMR encoder are useful for capturing important concepts in input sentences. Figure 3 supports this observation. For example, <cite>ABS</cite>+AMR successfully added the correct modifier 'saudi' to \"crown prince\" in the first example.",
  "y": "differences"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_35",
  "x": "The comparison between <cite>ABS</cite>+AMR(w/o attn) and <cite>ABS</cite>+AMR (with attention) suggests that the attention mechanism is necessary for AMR encoding. In other words, the encoder without the attention mechanism tends to be overfitting. ---------------------------------- **RELATED WORK** Recently, the Recurrent Neural Network (RNN) and its variant have been applied successfully to various NLP tasks. For headline generation tasks, Chopra et al. (2016) exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of <cite>Rush et al. (2015)</cite> : the combination of the feed-forward neural network language model and attention-based sentence encoder. also adapted the RNN encoder-decoder with attention for headline generation tasks. Moreover, they made some efforts such as hierarchical attention to improve the performance. In addition to using a variant of RNN, proposed a method to handle infrequent words in natural language generation. Note that these recent developments do not conflict with our method using the AMR encoder.",
  "y": "background"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_0",
  "x": "****UTILIZING MONOLINGUAL DATA IN NMT FOR SIMILAR LANGUAGES: SUBMISSION TO SIMILAR LANGUAGE TRANSLATION TASK.**** **ABSTRACT** This paper describes our submission to Shared Task on Similar Language Translation in Fourth Conference on Machine Translation (WMT 2019). We submitted three systems for Hindi \u2192 Nepali direction in which we have examined the performance of a Recursive Neural Network (RNN) based Neural Machine Translation (NMT) system, a semi-supervised NMT system where monolingual data of both languages is utilized using the architecture by <cite>(Artetxe et al., 2017)</cite> and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data. ---------------------------------- **INTRODUCTION** In this paper, we present the submission for Similar Language Translation Task in WMT 2019. The task focuses on improving machine translation results for three language pairs Czech-Polish (Slavic languages), Hindi-Nepali (Indo-Aryan languages) and Spanish-Portuguese (Romance languages). The main focus of the task is to utilize monolingual data in addition to parallel data because the provided parallel data is very small in amount.",
  "y": "uses"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_1",
  "x": "Transformer based encoder-decoder architecture for NMT is proposed in (Vaswani et al., 2017) , which is completely based on self-attention and positional encoding. This does not follow recurrent architecture. Positional encoding provides the system with information of order of words. NMT needs lots of parallel data to train a system. This task basically focuses on how to improve performance for languages which are similar but resource scarce. There are many language pairs for which parallel data does not exist or exist in a very small amount. In past, to improve the performance of NMT systems various techniques like Back-Translation (Sennrich et al., 2016a) , utilizing other similar language pairs through pivoting (Cheng et al., 2017) or transfer learning (Zoph et al., 2016) , complete unsupervised architectures <cite>(Artetxe et al., 2017)</cite> (Lample et al., 2018 ) and many others have been proposed. ---------------------------------- **UTILIZING MONOLINGUAL DATA IN NMT**",
  "y": "background"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_2",
  "x": "In (Currey et al., 2017) , target side monolingual data is copied to generate source synthetic translations and the system is trained by combining this synthetic data with parallel data. In (Zhang and Zong, 2016) , source side monolingual data is utilized to iteratively generate synthetic sentences from the same model. In (Domhan and Hieber, 2017) , there is a separate layer for target side language model in training, decoder utilize both source dependent and source independent representations to generate a particular target word. In (Burlot and Yvon, 2018) , it is claimed that quality of back-translated sentences is important. Recently many systems have been proposed for Unsupervised NMT, where only monolingual data is utilized. The Unsupervised NMT approach proposed in <cite>(Artetxe et al., 2017)</cite> follows an architecture where encoder is shared and decoder is separate for each language. Encoder tries to map sentences from both languages in the same space, which is supported by cross-lingual word embeddings. They fix cross-lingual word embeddings in the encoder while training, which helps in generating cross-lingual sentence representations in the same space. The system with one shared encoder and two separate decoders with no parallel data is trained by iterating between Denoising and Back-Translation.",
  "y": "background"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_3",
  "x": "\u2022 A pure RNN based NMT system \u2022 Semi-supervised RNN based NMT system \u2022 Utilization of copied data in RNN based NMT First system is pure RNN based NMT system. To train this we have utilized only parallel corpora. Second system is trained using a semi-supervised NMT system where monolingual data from both languages is utilized. We have utilized architecture proposed in <cite>(Artetxe et al., 2017)</cite> where encoder is shared and decoders are separate for each language and model is trained by alternating between denoising and back-translation. This architecture can also be utilized for completely unsupervised setting. Third system is also a pure RNN based NMT system where additional parallel data (synthetic data) is created by copying source side sentences to target side and target side sentences to source side, but we do this only for the available parallel sentences, no additional monolingual data is utilized. In this way the amount of available data becomes three times of the original data.",
  "y": "uses"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_4",
  "x": "\u2022 Utilization of copied data in RNN based NMT First system is pure RNN based NMT system. To train this we have utilized only parallel corpora. Second system is trained using a semi-supervised NMT system where monolingual data from both languages is utilized. We have utilized architecture proposed in <cite>(Artetxe et al., 2017)</cite> where encoder is shared and decoders are separate for each language and model is trained by alternating between denoising and back-translation. This architecture can also be utilized for completely unsupervised setting. Third system is also a pure RNN based NMT system where additional parallel data (synthetic data) is created by copying source side sentences to target side and target side sentences to source side, but we do this only for the available parallel sentences, no additional monolingual data is utilized. In this way the amount of available data becomes three times of the original data. All the data is combined together, shuffled and then provided to the NMT system, there is no identification provided to distinguish between parallel data and copy data. To train all three systems we have utilized the implementation of <cite>(Artetxe et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_0",
  "x": "**INTRODUCTION** Several papers recently demonstrated the potential of very weakly supervised or entirely unsupervised approaches to bilingual dictionary induction (BDI) (Barone, 2016; Artetxe et al., 2017; Zhang et al., 2017; <cite>Conneau et al., 2018</cite>; , the task of identifying translational equivalents across two languages. These approaches cast BDI as a problem of aligning monolingual word embeddings. Pairs of monolingual word vector spaces can be aligned without any explicit crosslingual supervision, solely based on their distributional properties (for an adversarial approach, see<cite> Conneau et al. (2018)</cite> ). Alternatively, weak supervision can be provided in the form of numerals (Artetxe et al., 2017) or identically spelled words . Successful unsupervised or weakly supervised alignment of word vector spaces would remove much of the data bottleneck for machine translation and push horizons for cross-lingual learning . In addition to an unsupervised approach to aligning monolingual word embedding spaces with adversarial training,<cite> Conneau et al. (2018)</cite> present a supervised alignment algorithm that assumes a gold-standard seed dictionary and performs Procrustes Analysis (Sch\u00f6nemann, 1966) . show that this approach, weakly supervised with a dictionary seed of crosslingual homographs, i.e. words with identical spelling across source and target language, is superior to the completely unsupervised approach. We therefore focus on weakly-supervised Procrustes Analysis (PA) for BDI here.",
  "y": "background"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_2",
  "x": "Alternatively, weak supervision can be provided in the form of numerals (Artetxe et al., 2017) or identically spelled words . Successful unsupervised or weakly supervised alignment of word vector spaces would remove much of the data bottleneck for machine translation and push horizons for cross-lingual learning . In addition to an unsupervised approach to aligning monolingual word embedding spaces with adversarial training,<cite> Conneau et al. (2018)</cite> present a supervised alignment algorithm that assumes a gold-standard seed dictionary and performs Procrustes Analysis (Sch\u00f6nemann, 1966) . show that this approach, weakly supervised with a dictionary seed of crosslingual homographs, i.e. words with identical spelling across source and target language, is superior to the completely unsupervised approach. We therefore focus on weakly-supervised Procrustes Analysis (PA) for BDI here. The implementation of PA in<cite> Conneau et al. (2018)</cite> yields notable improvements over earlier work on BDI, even though it learns a simple linear transform of the source language space into the target language space. Seminal work in supervised alignment of word vector spaces indeed reported superior performance with linear models as compared to non-linear neural approaches (Mikolov et al., 2013) . The relative success of the simple linear approach can be explained in terms of isomorphism across monolingual semantic spaces, 1 an idea that receives support from cognitive science (Youn et al., 1999) . Word vector spaces are not perfectly isomorphic, however, as shown by , who use a Laplacian graph similarity metric to measure this property.",
  "y": "background"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_3",
  "x": "These approaches cast BDI as a problem of aligning monolingual word embeddings. Pairs of monolingual word vector spaces can be aligned without any explicit crosslingual supervision, solely based on their distributional properties (for an adversarial approach, see<cite> Conneau et al. (2018)</cite> ). Alternatively, weak supervision can be provided in the form of numerals (Artetxe et al., 2017) or identically spelled words . Successful unsupervised or weakly supervised alignment of word vector spaces would remove much of the data bottleneck for machine translation and push horizons for cross-lingual learning . In addition to an unsupervised approach to aligning monolingual word embedding spaces with adversarial training,<cite> Conneau et al. (2018)</cite> present a supervised alignment algorithm that assumes a gold-standard seed dictionary and performs Procrustes Analysis (Sch\u00f6nemann, 1966) . show that this approach, weakly supervised with a dictionary seed of crosslingual homographs, i.e. words with identical spelling across source and target language, is superior to the completely unsupervised approach. We therefore focus on weakly-supervised Procrustes Analysis (PA) for BDI here. The implementation of PA in<cite> Conneau et al. (2018)</cite> yields notable improvements over earlier work on BDI, even though it learns a simple linear transform of the source language space into the target language space. Seminal work in supervised alignment of word vector spaces indeed reported superior performance with linear models as compared to non-linear neural approaches (Mikolov et al., 2013) .",
  "y": "background"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_4",
  "x": "show that this approach, weakly supervised with a dictionary seed of crosslingual homographs, i.e. words with identical spelling across source and target language, is superior to the completely unsupervised approach. We therefore focus on weakly-supervised Procrustes Analysis (PA) for BDI here. The implementation of PA in<cite> Conneau et al. (2018)</cite> yields notable improvements over earlier work on BDI, even though it learns a simple linear transform of the source language space into the target language space. Seminal work in supervised alignment of word vector spaces indeed reported superior performance with linear models as compared to non-linear neural approaches (Mikolov et al., 2013) . The relative success of the simple linear approach can be explained in terms of isomorphism across monolingual semantic spaces, 1 an idea that receives support from cognitive science (Youn et al., 1999) . Word vector spaces are not perfectly isomorphic, however, as shown by , who use a Laplacian graph similarity metric to measure this property. In this work, we show that projecting both source and target vector spaces into a third space (Faruqui and Dyer, 2014) , using a variant of PA known as Generalized Procrustes Analysis (Gower, 1975) , makes it easier to learn the alignment between two word vector spaces, as compared to the single linear transform used in<cite> Conneau et al. (2018)</cite> . Contributions We show that Generalized Procrustes Analysis (GPA) (Gower, 1975) , a method that maps two vector spaces into a third, latent space, is superior to PA for BDI, e.g., improving the state-of-the-art on the widely used EnglishItalian dataset (Dinu et al., 2015) from a P@1 score of 66.2% to 67.6%. We compare GPA to PA 1 Two vector spaces are isomorphic if there is an invertible linear transformation from one to the other.",
  "y": "differences"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_5",
  "x": "Due to the fact that G is partly based on E 1 , these two spaces are bound to be more similar to each other than E 1 and E 2 are. 4 Finding a good mapping between E 1 and G, i.e. a good setting of T 1 , should therefore be easier than finding a good mapping from E 1 to E 2 directly. In this sense, by mapping E 1 onto G, rather than onto E 2 (as PA would do), we are solving an easier problem and reducing the chance of a poor solution. ---------------------------------- **EXPERIMENTS** In our experiments, we generally use the same hyper-parameters as used in<cite> Conneau et al. (2018)</cite> , unless otherwise stated. When extracting dictionaries for the bootstrapping procedure, we use cross-domain local scaling (CSLS, see<cite> Conneau et al. (2018)</cite> for details) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15,000. We do not put any restrictions on the initial seed dictionaries, based on cross-lingual homographs: those vary considerably in size, from 17,012 for Hebrew to 85,912 for Spanish. Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in<cite> Conneau et al. (2018)</cite> , i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS.",
  "y": "uses similarities"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_6",
  "x": "**EXPERIMENTS** In our experiments, we generally use the same hyper-parameters as used in<cite> Conneau et al. (2018)</cite> , unless otherwise stated. When extracting dictionaries for the bootstrapping procedure, we use cross-domain local scaling (CSLS, see<cite> Conneau et al. (2018)</cite> for details) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15,000. We do not put any restrictions on the initial seed dictionaries, based on cross-lingual homographs: those vary considerably in size, from 17,012 for Hebrew to 85,912 for Spanish. Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in<cite> Conneau et al. (2018)</cite> , i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS. Our metric is Precision at k\u00d7100 (P@k), i.e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set<cite> (Conneau et al., 2018)</cite> . Unless stated otherwise, experiments were carried out using the publicly available pre-trained fastText embeddings, trained on Wikipedia data, 5 and bilingual dictionaries-consisting of 5000 and 1500 unique word pairs for training and testing, respectively-provided by<cite> Conneau et al. (2018)</cite> 6 . ---------------------------------- **COMPARISON OF PA AND GPA**",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_7",
  "x": "Due to the fact that G is partly based on E 1 , these two spaces are bound to be more similar to each other than E 1 and E 2 are. 4 Finding a good mapping between E 1 and G, i.e. a good setting of T 1 , should therefore be easier than finding a good mapping from E 1 to E 2 directly. In this sense, by mapping E 1 onto G, rather than onto E 2 (as PA would do), we are solving an easier problem and reducing the chance of a poor solution. ---------------------------------- **EXPERIMENTS** In our experiments, we generally use the same hyper-parameters as used in<cite> Conneau et al. (2018)</cite> , unless otherwise stated. When extracting dictionaries for the bootstrapping procedure, we use cross-domain local scaling (CSLS, see<cite> Conneau et al. (2018)</cite> for details) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15,000. We do not put any restrictions on the initial seed dictionaries, based on cross-lingual homographs: those vary considerably in size, from 17,012 for Hebrew to 85,912 for Spanish. Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in<cite> Conneau et al. (2018)</cite> , i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS.",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_8",
  "x": "We do not put any restrictions on the initial seed dictionaries, based on cross-lingual homographs: those vary considerably in size, from 17,012 for Hebrew to 85,912 for Spanish. Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in<cite> Conneau et al. (2018)</cite> , i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS. Our metric is Precision at k\u00d7100 (P@k), i.e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set<cite> (Conneau et al., 2018)</cite> . Unless stated otherwise, experiments were carried out using the publicly available pre-trained fastText embeddings, trained on Wikipedia data, 5 and bilingual dictionaries-consisting of 5000 and 1500 unique word pairs for training and testing, respectively-provided by<cite> Conneau et al. (2018)</cite> 6 . ---------------------------------- **COMPARISON OF PA AND GPA** High resource setting We first present a direct comparison of PA and GPA on BDI from English to five fairly high-resource languages: Arabic, Finnish, German, Russian, and Spanish. The Wikipedia corpus sizes for these languages are reported in Table 1 . Results are listed in Table 2 .",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_9",
  "x": "In this sense, by mapping E 1 onto G, rather than onto E 2 (as PA would do), we are solving an easier problem and reducing the chance of a poor solution. ---------------------------------- **EXPERIMENTS** In our experiments, we generally use the same hyper-parameters as used in<cite> Conneau et al. (2018)</cite> , unless otherwise stated. When extracting dictionaries for the bootstrapping procedure, we use cross-domain local scaling (CSLS, see<cite> Conneau et al. (2018)</cite> for details) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15,000. We do not put any restrictions on the initial seed dictionaries, based on cross-lingual homographs: those vary considerably in size, from 17,012 for Hebrew to 85,912 for Spanish. Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in<cite> Conneau et al. (2018)</cite> , i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS. Our metric is Precision at k\u00d7100 (P@k), i.e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set<cite> (Conneau et al., 2018)</cite> . Unless stated otherwise, experiments were carried out using the publicly available pre-trained fastText embeddings, trained on Wikipedia data, 5 and bilingual dictionaries-consisting of 5000 and 1500 unique word pairs for training and testing, respectively-provided by<cite> Conneau et al. (2018)</cite> 6 .",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_10",
  "x": "**COMPARISON OF PA AND GPA** High resource setting We first present a direct comparison of PA and GPA on BDI from English to five fairly high-resource languages: Arabic, Finnish, German, Russian, and Spanish. The Wikipedia corpus sizes for these languages are reported in Table 1 . Results are listed in Table 2 . GPA improves over PA consistently for all five languages. Most notably, for Finnish it scores 2.5% higher than PA. Common benchmarks For a more extensive comparison with previous work, we include results on English-{Finnish, German, Italian} dictionaries used in<cite> Conneau et al. (2018)</cite> and Artetxe et al. (2018) -the second best approach to BDI known to us, which also uses Procrustes Analysis. We conduct experiments using three forms of supervision: gold-standard seed dictionaries of 5000 word pairs, cross-lingual homographs, and numerals. We use train and test bilingual dictionaries from Dinu et al. (2015) for English-Italian and from Artetxe et al. (2017) for English-{Finnish, German}. Following<cite> Conneau et al. (2018)</cite> , we report results with a set of CBOW embeddings trained on the WaCky corpus (Barone, 2016) , and with Wikipedia embeddings.",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_11",
  "x": "The Wikipedia corpus sizes for these languages are reported in Table 1 . Results are listed in Table 2 . GPA improves over PA consistently for all five languages. Most notably, for Finnish it scores 2.5% higher than PA. Common benchmarks For a more extensive comparison with previous work, we include results on English-{Finnish, German, Italian} dictionaries used in<cite> Conneau et al. (2018)</cite> and Artetxe et al. (2018) -the second best approach to BDI known to us, which also uses Procrustes Analysis. We conduct experiments using three forms of supervision: gold-standard seed dictionaries of 5000 word pairs, cross-lingual homographs, and numerals. We use train and test bilingual dictionaries from Dinu et al. (2015) for English-Italian and from Artetxe et al. (2017) for English-{Finnish, German}. Following<cite> Conneau et al. (2018)</cite> , we report results with a set of CBOW embeddings trained on the WaCky corpus (Barone, 2016) , and with Wikipedia embeddings. Results are reported in Table 3 . We observe that GPA outperforms PA consistently on Italian and German with the WaCky embeddings, and on all languages with the Wikipedia embeddings.",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_13",
  "x": "This serves to show that the learned alignments are generally good, but they are not sufficiently precise. This issue can have two sources: a suboptimal method for learning the alignment and/or a ceiling effect on how good of an alignment can be obtained, within the space of orthogonal linear transformations. ---------------------------------- **PROCRUSTES FIT** To explore the latter issue and to further compare the capabilities of PA and GPA, we perform a Procrustes fit test, where we learn alignments in a fully supervised fashion, using the test dictionaries of<cite> Conneau et al. (2018)</cite> 9 for both training and evaluation 10 . In the ideal case, i.e. if the subspaces defined by the words in the seed dictionaries are perfectly alignable, this setup should result in precision of 100%. We found the difference between the fit with PA and GPA to be negligible, 0.20 on average across all 10 languages (5 low-resource and 5 high-source languages). It is not surprising that PA and GPA results in almost equivalent fits-the two algorithms both rely on linear transformations, i.e. they are equal in expressivity. As pointed out earlier, the superiority of GPA over PA stems from its Tables 2 and 4.",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_14",
  "x": "**MULTILINGUALITY** Finally, we note that there may be specific advantages to including support languages for which large monolingual corpora exist, as those should, theoretically, be easier to align with English (also a high-resource language): variance in vector directionality, as studied in Mimno and Thompson (2017) , increases with corpus size, so we would expect embedding spaces learned from corpora comparable in size, to also be more similar in shape. ---------------------------------- **RELATED WORK** Bilingual embeddings Many diverse crosslingual word embedding models have been proposed . The most popular kind learns a linear transformation from source to target language space (Mikolov et al., 2013) . In most recent work, this mapping is constrained to be orthogonal and solved using Procrustes Analysis (Xing et al., 2015; Artetxe et al., 2017 Artetxe et al., , 2018 <cite>Conneau et al., 2018</cite>; Lu et al., 2015) . The approach most similar to ours, Faruqui and Dyer (2014) , uses canonical correlation analysis (CCA) to project both source and target language spaces into a third, joint space. In this setup, similarly to GPA, the third space is iteratively updated, such that at timestep t, it is a product of the two language spaces as transformed by the mapping learned at timestep t \u2212 1.",
  "y": "background"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_0",
  "x": "Experimental results show that our proposed model, with fewer features and a smaller size, achieves competitive accuracy to state-of-the-art models. ---------------------------------- **INTRODUCTION** To date, standard approaches to named entity classification rely on supervised models, that typically require a large-scale annotated corpus and a widecoverage dictionary. However, since new named entities arise regularly, it becomes increasingly difficult to maintain an up-to-date dictionary and/or adapt a named entity classifier to a new domain; for example, sequence labeling techniques that use feature templates (Finkel et al., 2005; Sarawagi and Cohen, 2004) are not robust for unknown named entities because their feature space is very sparse<cite> (Primadhanty et al., 2015)</cite> . This problem worsens when we attempt to use a combination of features for sparse named entity classification. Therefore, in this paper, we propose the use of matrix factorization for named entity classification to consider the relationships between sparse features. Through our experiments, we achieved competitive accuracy to models developed in previous works in terms of using fewer features and compactness using factorization machines (Rendle, 2010) . The main contributions of this paper are as follows:",
  "y": "background"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_1",
  "x": "Thus, in this paper, we address the problem of named entity classification using matrix factorization to overcome the problem of feature sparsity. Experimental results show that our proposed model, with fewer features and a smaller size, achieves competitive accuracy to state-of-the-art models. ---------------------------------- **INTRODUCTION** To date, standard approaches to named entity classification rely on supervised models, that typically require a large-scale annotated corpus and a widecoverage dictionary. However, since new named entities arise regularly, it becomes increasingly difficult to maintain an up-to-date dictionary and/or adapt a named entity classifier to a new domain; for example, sequence labeling techniques that use feature templates (Finkel et al., 2005; Sarawagi and Cohen, 2004) are not robust for unknown named entities because their feature space is very sparse<cite> (Primadhanty et al., 2015)</cite> . This problem worsens when we attempt to use a combination of features for sparse named entity classification. Therefore, in this paper, we propose the use of matrix factorization for named entity classification to consider the relationships between sparse features. Through our experiments, we achieved competitive accuracy to models developed in previous works in terms of using fewer features and compactness using factorization machines (Rendle, 2010) .",
  "y": "motivation"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_2",
  "x": "---------------------------------- **RELATED WORK** A standard approach to named entity classification is to formulate a task as a sequence labeling problem and use a supervised method, such as conditional random fields (Lafferty et al., 2001; Finkel et al., 2005; Sarawagi and Cohen, 2004) . These studies heavily rely on feature templates for learning combinations of features; however, since combinations of features in conventional supervised learning are treated independently, this approach is not robust for named entities that do not appear in the training data. To address the task of unknown named entity classification, <cite>Primadhanty et al. (2015)</cite> explored the use of sparse combinatorial features. They proposed a log-bilinear model that defines a score function considering interactions between features; the score function is regularized via a nuclear norm on a feature weight matrix. Further, heir method employs singular value decomposition (SVD)-based regularization to handle the combination of features. They reported that their regularization achieved higher accuracy than L1 and L2 regularization, frequently used in natural language processing (Okanohara and Tsujii, 2009 ). However, nuclear norm regularization (i.e., SVDbased regularization) is not necessarily the best way to incorporate interactions between features, because it does not directly optimize classification accuracy.",
  "y": "background"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_3",
  "x": "Here, parameter learning can be accomplished via Markov chain Monte Carlo or stochastic gradient descent. ---------------------------------- **EXPERIMENTS** As described above, we aim to classify named entities that rarely appear in a given training corpus. We compared factorization machines with a log-linear model, a polynomial-kernel SVM, and a state-ofthe-art log-bilinear model using nuclear norm for regularization<cite> (Primadhanty et al., 2015)</cite> . ---------------------------------- **SETTINGS** Data. We used the dataset provided by <cite>Primadhanty et al. (2015)</cite> ; this dataset was created for evaluating unknown named entity classification and is context features: Right and left contexts of the candidate in a sentence (do not take the order into account).",
  "y": "uses"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_4",
  "x": "**EXPERIMENTS** As described above, we aim to classify named entities that rarely appear in a given training corpus. We compared factorization machines with a log-linear model, a polynomial-kernel SVM, and a state-ofthe-art log-bilinear model using nuclear norm for regularization<cite> (Primadhanty et al., 2015)</cite> . ---------------------------------- **SETTINGS** Data. We used the dataset provided by <cite>Primadhanty et al. (2015)</cite> ; this dataset was created for evaluating unknown named entity classification and is context features: Right and left contexts of the candidate in a sentence (do not take the order into account). cap=1, cap=0: Whether the first letter of the candidate is uppercase, or not. all-low=1, all-low=0: Whether all letters of the candidate are lowercase, or not. all-cap1=1, all-cap1=0: Whether all letters of the candidate are uppercase, or not.",
  "y": "uses"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_5",
  "x": "Table 1 shows the number of tokens and types in the given dataset. This dataset contains five tags: person (PER), location (LOC), organization (ORG), miscellaneous (MISC), and non-entities (O). Features. We used a subset of features from experiments performed by <cite>Primadhanty et al. (2015)</cite> . Table 3 summarizes the features used in our experiment, including context and entity features. Tools. In terms of tools, we used scikit-learn 0.17 to implement a log-linear model and polynomial kernel in an SVM. Further, we employed libFM 1.4.2 1 (Rendle, 2012) to build a named entity classifier using factorization machines. In the interaction of both the SVM and the factorization machine, we fixed the degree of the polynomial kernel to d = 2.",
  "y": "uses"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_6",
  "x": "Note that <cite>Primadhanty et al. (2015)</cite> used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use. Table 4 and Figure 2 show the performance and precision-recall curves of named entity classification for each tag, respectively. We observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by <cite>Primadhanty et al. (2015)</cite> with fewer features. Overall, the microaveraged F1 score improved by 1.4 points. From these results, we conclude that unknown named entity classification can be successfully achieved by taking combinatorial features into account using factorization machines. ---------------------------------- **DISCUSSION** Experimental results show that performance on ORG was improved. For example, the term \"VicePresident\" appears in both contexts of ORG and O, and our method correctly handled this sparse combination of context and entity features.",
  "y": "differences"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_7",
  "x": "The scores were calculated on all tags except for non-entities (O). ---------------------------------- **RESULTS** Table 2 presents results of our experiments. Note that <cite>Primadhanty et al. (2015)</cite> used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use. Table 4 and Figure 2 show the performance and precision-recall curves of named entity classification for each tag, respectively. We observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by <cite>Primadhanty et al. (2015)</cite> with fewer features. Overall, the microaveraged F1 score improved by 1.4 points. From these results, we conclude that unknown named entity classification can be successfully achieved by taking combinatorial features into account using factorization machines.",
  "y": "similarities"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_8",
  "x": "The accuracy of LOC, however, was lower than that of the log-bilinear model<cite> (Primadhanty et al., 2015)</cite> . Upon investigating the confusion matrix, we found that the LOC tag was often misclassified as PER. We therefore conclude here that clustering and POS features are necessary to distinguish these tags. Figure 1 plots the F1-score of our proposed method as dimension k changes for matrix factor- ization using the same development data as that of <cite>Primadhanty et al. (2015)</cite> . Our method yielded the best F1-score (i.e., 57.1) at k = 5, whereas the log-bilinear model achieved the best F1-score (i.e., 61.73) at k = 40. These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus. It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points. This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization. Both our approach and the methods of <cite>Primadhanty et al. (2015)</cite> address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized.",
  "y": "differences"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_9",
  "x": "Figure 1 plots the F1-score of our proposed method as dimension k changes for matrix factor- ization using the same development data as that of <cite>Primadhanty et al. (2015)</cite> . Our method yielded the best F1-score (i.e., 57.1) at k = 5, whereas the log-bilinear model achieved the best F1-score (i.e., 61.73) at k = 40. These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus. It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points. This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization. Both our approach and the methods of <cite>Primadhanty et al. (2015)</cite> address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized. <cite>Primadhanty et al. (2015)</cite> use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines. ---------------------------------- **CONCLUSION**",
  "y": "uses"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_10",
  "x": "These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus. It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points. This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization. Both our approach and the methods of <cite>Primadhanty et al. (2015)</cite> address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized. <cite>Primadhanty et al. (2015)</cite> use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines. ---------------------------------- **CONCLUSION** In this paper, we proposed the use of factorization machines to handle the combinations of sparse features in unknown named entity classification. Our experimental results showed that we were able to achieve competitive accuracy to state-of-the-art methods using fewer features and a compact model.",
  "y": "similarities differences"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_11",
  "x": "These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus. It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points. This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization. Both our approach and the methods of <cite>Primadhanty et al. (2015)</cite> address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized. <cite>Primadhanty et al. (2015)</cite> use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines. ---------------------------------- **CONCLUSION** In this paper, we proposed the use of factorization machines to handle the combinations of sparse features in unknown named entity classification. Our experimental results showed that we were able to achieve competitive accuracy to state-of-the-art methods using fewer features and a compact model.",
  "y": "background"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_12",
  "x": "Figure 1 plots the F1-score of our proposed method as dimension k changes for matrix factor- ization using the same development data as that of <cite>Primadhanty et al. (2015)</cite> . Our method yielded the best F1-score (i.e., 57.1) at k = 5, whereas the log-bilinear model achieved the best F1-score (i.e., 61.73) at k = 40. These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus. It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points. This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization. Both our approach and the methods of <cite>Primadhanty et al. (2015)</cite> address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized. <cite>Primadhanty et al. (2015)</cite> use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines. ---------------------------------- **CONCLUSION**",
  "y": "differences"
 },
 {
  "id": "f1a800c7cd47ac2edf2172cedb5889_0",
  "x": "Character-level models (Ling et al., 2015; Kim et al., 2016) learn relationship between similar word forms and have shown to be effective for parsing MRLs (Ballesteros et al., 2015; Dozat et al., 2017; Shi et al., 2017; Bj\u00f6rkelund et al., 2017) . Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by <cite>Tsarfaty et al. (2010</cite> Tsarfaty et al. ( , 2013 : \u2022 Can we represent words abstractly so as to reflect shared morphological aspects between them? \u2022 Which types of morphological information should we include in the parsing model? It is tempting to hypothesize that character-level models effectively solve the first problem. For the second, <cite>Tsarfaty et al. (2010)</cite> and Seeker and Kuhn (2013) reported that morphological case is beneficial across morphologically rich languages with extensive case systems, where case syncretism is pervasive and often hurts parsing performance. But <cite>these studies</cite> focus on vintage parsers; do neural parsers with character-level representations also solve this second problem? We attempt to answer this question by asking whether an explicit model of morphological case helps dependency parsing, and our results show that it does. Furthermore, a pipeline model in which we feed predicted case to the parser outperforms multi-task learning in which case prediction is an auxiliary task. These results suggest that neural dependency parsers do not adequately infer this crucial linguistic feature directly from the input text. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "f1a800c7cd47ac2edf2172cedb5889_1",
  "x": "Parsing morphologically rich languages (MRLs) is difficult due to the complex relationship of syntax to morphology. But the success of neural networks offer an appealing solution to this problem by computing word representation from characters. Character-level models (Ling et al., 2015; Kim et al., 2016) learn relationship between similar word forms and have shown to be effective for parsing MRLs (Ballesteros et al., 2015; Dozat et al., 2017; Shi et al., 2017; Bj\u00f6rkelund et al., 2017) . Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by <cite>Tsarfaty et al. (2010</cite> Tsarfaty et al. ( , 2013 : \u2022 Can we represent words abstractly so as to reflect shared morphological aspects between them? \u2022 Which types of morphological information should we include in the parsing model? It is tempting to hypothesize that character-level models effectively solve the first problem. For the second, <cite>Tsarfaty et al. (2010)</cite> and Seeker and Kuhn (2013) reported that morphological case is beneficial across morphologically rich languages with extensive case systems, where case syncretism is pervasive and often hurts parsing performance. But <cite>these studies</cite> focus on vintage parsers; do neural parsers with character-level representations also solve this second problem? We attempt to answer this question by asking whether an explicit model of morphological case helps dependency parsing, and our results show that it does. Furthermore, a pipeline model in which we feed predicted case to the parser outperforms multi-task learning in which case prediction is an auxiliary task.",
  "y": "motivation background"
 },
 {
  "id": "f1a800c7cd47ac2edf2172cedb5889_2",
  "x": "These results are interesting, since in vintage parsers, predicted case usually harmed accuracy (<cite>Tsarfaty et al., 2010</cite>) . However, we note that our taggers use gold POS, which might help. Pipeline model vs. Multi-task learning In general, MTL models achieve similar or slightly better performance than the character-only models, suggesting that supplying case in this way is beneficial. However, we found that using predicted case in a pipeline model gives more improvements than MTL. We also observe an interesting pattern in which MTL achieves better tagging accuracy than the pipeline model but lower performance in parsing (Table 2 ). This is surprising since it suggests that the MTL model must learn to effectively encode case in the model's representation, but must not effectively use it for parsing. ---------------------------------- **CONCLUSION** Vintage dependency parsers rely on hand-crafted feature engineering to encode morphology.",
  "y": "background"
 },
 {
  "id": "f1a800c7cd47ac2edf2172cedb5889_3",
  "x": "We train a morphological tagger to predict case information. The tagger has the same structure as the parser's encoder, with an additional feedforward neural network with one hidden layer followed by a softmax layer. We found that predicted case improves accuracy, although the effect is different across languages. These results are interesting, since in vintage parsers, predicted case usually harmed accuracy (<cite>Tsarfaty et al., 2010</cite>) . However, we note that our taggers use gold POS, which might help. Pipeline model vs. Multi-task learning In general, MTL models achieve similar or slightly better performance than the character-only models, suggesting that supplying case in this way is beneficial. However, we found that using predicted case in a pipeline model gives more improvements than MTL. We also observe an interesting pattern in which MTL achieves better tagging accuracy than the pipeline model but lower performance in parsing (Table 2 ). This is surprising since it suggests that the MTL model must learn to effectively encode case in the model's representation, but must not effectively use it for parsing.",
  "y": "differences"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_0",
  "x": "****CONSTITUENCY AND DEPENDENCY RELATIONSHIP FROM A TREE ADJOINING GRAMMAR AND ABSTRACT CATEGORIAL GRAMMARS PERSPECTIVE**** **ABSTRACT** This paper gives an Abstract Categorial Grammar (ACG) account of<cite> (Kallmeyer and Kuhlmann, 2012)</cite>'s process of transformation of the derivation trees of Tree Adjoining Grammar (TAG) into dependency trees. We make explicit how the requirement of keeping a direct interpretation of dependency trees into strings results into lexical ambiguity. Since the ACG framework has already been used to provide a logical semantics from TAG derivation trees, we have a unified picture where derivation trees and dependency trees are related but independent equivalent ways to account for the same surface-meaning relation. ---------------------------------- **INTRODUCTION** Tree Adjoining Grammars (TAG) (Joshi et al., 1975; Joshi and Schabes, 1997 ) is a tree grammar formalism relying on two operations between trees: substitution and adjunction. In addition to the tree generated by a sequence of such operations, there is a derivation tree which records this sequence.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_1",
  "x": "Solving these problems often leads to modifications of derivation tree structures (Schabes and Shieber, 1994; Kallmeyer, 2002; Joshi et al., 2003; Rambow et al., 2001; Chen-Main and Joshi, To appear) . While alternative proposals have succeeded in linking derivation trees to semantic representations using unification (Kallmeyer and Romero, 2004; Kallmeyer and Romero, 2007) or using an encoding (Pogodalla, 2004; Pogodalla, 2009) of TAG into the ACG framework (de Groote, 2001) , only recently<cite> (Kallmeyer and Kuhlmann, 2012)</cite> has proposed a transformation from standard derivation trees to dependency trees. This paper provides an ACG perspective on this transformation. The goal is twofold. First, it exhibits the underlying lexical blow up of the yield functions associated with the elementary trees in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> . Second, using the same framework as (Pogodalla, 2004; Pogodalla, 2009 ) allows us to have a shared perspective on a phrase-structure architecture and a dependency one and an equivalence on the surface-meaning relation they define. ---------------------------------- **ABSTRACT CATEGORIAL GRAMMARS** ACGs provide a framework in which several grammatical formalisms may be encoded (de Groote and Pogodalla, 2004) .",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_2",
  "x": "The goal is twofold. First, it exhibits the underlying lexical blow up of the yield functions associated with the elementary trees in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> . Second, using the same framework as (Pogodalla, 2004; Pogodalla, 2009 ) allows us to have a shared perspective on a phrase-structure architecture and a dependency one and an equivalence on the surface-meaning relation they define. ---------------------------------- **ABSTRACT CATEGORIAL GRAMMARS** ACGs provide a framework in which several grammatical formalisms may be encoded (de Groote and Pogodalla, 2004) . They generate languages of linear \u03bb-terms, which generalize both string and tree languages. A key feature is to provide the user direct control over the parse structures of the grammar, the abstract language, which allows several grammatical formalisms to be defined in terms of ACG, in particular TAG (de Groote, 2002) . We refer the reader to (de Groote, 2001; Pogodalla, 2009) for the details and introduce here only few relevant definitions and notations.",
  "y": "background motivation"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_3",
  "x": ") with the proviso that for any constant c : 2 We refer the reader to (Pogodalla, 2009 ) for the details. 3 The TAG literature typically uses this example, and<cite> (Kallmeyer and Kuhlmann, 2012)</cite> as well, to show the mismatch between the derivation trees and the expected se- This sentence is usually analyzed in TAG with a derivation tree where the to love component scopes over all the other arguments, and where claims and seems are unrelated, as Fig. 2(a) shows. The three higher-order signatures are: \u03a3 der\u03b8 : Its atomic types include s, vp, np, s A , vp A . . . where the X types stand for the categories X of the nodes where a substitution can occur while the X A types stand for the categories X of the nodes where an adjunction can occur. For each elementary tree \u03b3 lex. entry it contains a constant C lex. entry whose type is based on the adjunction and substitution sites as Table 1 shows. It additionally contains constants I X : X A that are meant to provide a fake auxiliary tree on adjunction sites where no adjunction actually takes place in a TAG derivation. \u03a3 trees : Its unique atomic type is \u03c4 the type of trees.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_4",
  "x": "4 L yield is defined as follows: \u2022 L yield (\u03c4 ) = \u03c3; \u2022 for n = 0, X 0 : \u03c4 represents a terminal symmantics and the relative scopes of the predicates. 4 With L d-ed trees (XA) = \u03c4 \u03c4 and for any other type bol and L yield (X 0 ) = X. Then, the derivation tree, the derived tree, and the yield of Fig. 2 are represented by: Trees<cite> (Kallmeyer and Kuhlmann, 2012)</cite> 's process to translate derivation trees into dependency trees is a two-step process. The first one does the actual transformation, using macro-tree transduction, while the second one modifies the way to get the yield from the dependency trees rather than from the derivation ones. ---------------------------------- **FROM DERIVATION TO DEPENDENCY TREES** This transformation aims at modeling the differences in scope of the argument between the derivation tree for (1) shown in Fig. 2 (a) and the corresponding dependency tree shown in Fig. 2 (b).",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_5",
  "x": "---------------------------------- **FROM DERIVATION TO DEPENDENCY TREES** This transformation aims at modeling the differences in scope of the argument between the derivation tree for (1) shown in Fig. 2 (a) and the corresponding dependency tree shown in Fig. 2 (b). For instance, in the derivation trees, claims and seems are under the scope of to love while in the dependency tree this order is reversed. According to<cite> (Kallmeyer and Kuhlmann, 2012)</cite> , such edge reversal is due to the fact that an edge between a complement taking adjunction (CTA) and an initial tree has to be reversed, while the other edges remain unchanged. Moreover, in case an initial tree accepts several adjunction of CTAs,<cite> (Kallmeyer and Kuhlmann, 2012)</cite> hypothesizes that the farther from the head a CTA is, the higher it is in the dependency tree. In the case of to love, the s node is farther from the head than the vp node. Therefore any adjunction on the s node (e.g. claims) should be higher than the one on the vp node (e.g. seems) in the dependency tree. We represent the dependency tree for (1) as",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_6",
  "x": "For instance, in the derivation trees, claims and seems are under the scope of to love while in the dependency tree this order is reversed. According to<cite> (Kallmeyer and Kuhlmann, 2012)</cite> , such edge reversal is due to the fact that an edge between a complement taking adjunction (CTA) and an initial tree has to be reversed, while the other edges remain unchanged. Moreover, in case an initial tree accepts several adjunction of CTAs,<cite> (Kallmeyer and Kuhlmann, 2012)</cite> hypothesizes that the farther from the head a CTA is, the higher it is in the dependency tree. In the case of to love, the s node is farther from the head than the vp node. Therefore any adjunction on the s node (e.g. claims) should be higher than the one on the vp node (e.g. seems) in the dependency tree. We represent the dependency tree for (1) as In order to do such reversing operations,<cite> (Kallmeyer and Kuhlmann, 2012)</cite> uses Macro Tree Transducers (MTTs) (Engelfriet and Vogler, 1985) . Note that the MTTs they use are linear, i.e. non-copying. It means that any node of an input tree cannot be translated more than once.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_7",
  "x": "In order to do such reversing operations,<cite> (Kallmeyer and Kuhlmann, 2012)</cite> uses Macro Tree Transducers (MTTs) (Engelfriet and Vogler, 1985) . Note that the MTTs they use are linear, i.e. non-copying. It means that any node of an input tree cannot be translated more than once. (Yoshinaka, 2006) has shown how to encode such MTTs as the composition G \u2022 G \u22121 of two ACGs, and we will use a very similar construct. (Kallmeyer and Kuhlmann, 2012) adds to the transformation from derivation trees to dependency trees the additional constraint that the string associated with a dependency structure is computed directly from the latter, without any reference to the derivation tree. To achieve this, they use two distinct yield functions: yield TAG from derivation trees to strings, and yield dep from dependency trees to strings. ---------------------------------- **THE YIELD FUNCTIONS** Let us imagine an initial tree \u03b3 i and an auxiliary tree \u03b3 a with no substitution nodes.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_8",
  "x": "Let us imagine an initial tree \u03b3 i and an auxiliary tree \u03b3 a with no substitution nodes. The yield of the derived tree resulting from the operations of the derivation tree \u03b3 of Fig. 3 defined in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> , w 2 where x, y denotes a tuple of strings. Because of the adjunction, the corresponding dependency structure has a reverse order (\u03b3 = \u03b3 a (\u03b3 i )), the requirement on yield dep imposes that In the interpretation of derivation trees as strings, initial trees (with no substitution nodes) Abstract Indeed, an initial tree can have several adjunction sites. In this case, to be ready for another adjunction after a first one, the first result itself should be a tuple of strings. So an initial tree (with no substitution nodes) with n adjunction sites is interpreted as a (2n + 1)-tuple of strings. Accordingly, depending on the location where it can adjoin, an auxiliary tree is interpreted as a function from (2k + 1)-tuple of strings to (2k \u2212 1)-tuple of strings. Taking into account that to model trees having the substitution nodes is then just a matter of adding k string parameters where k is the number of substitution nodes in a tree.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_9",
  "x": "5 Accordingly, we need multiple interpretations for the auxiliary trees, for instance for the two occurrences of seems in (3) where the yield of the last one yield dep (d seems ) maps a 5-tuple to a 3-tuple, and the yield of the first one maps a 3-tuple to a 3-tuple. And yield dep (d claims ) maps a 3-tuple to a 1-tuple of strings. We will mimic this behavior by introducing as many different non-terminal symbols for the dependency structures in our ACG setting. (2) a. John Bill claims Mary seems to love b. John Mary seems to love (3) John Bill seems to claim Mary seems to love Remark. Were we not interested in the yields but only in the dependency structures, we wouldn't have to manage this ambiguity. This is true both for<cite> (Kallmeyer and Kuhlmann, 2012)</cite> 's approach and ours. But as we have here a unified framework for the two-step process they propose, this lexical blow up will result in a multiplicity of types as Section 5 shows. ---------------------------------- **DISAMBIGUATED DERIVATION TREES** In order to encode the MTT acting on derivation trees, we introduce a new abstract vocabulary \u03a3 der\u03b8 for disambiguated derivation trees as in (Yoshinaka, 2006 to love is used to model sentences where both adjunctions are performed into \u03b3 to love .",
  "y": "motivation background similarities"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_10",
  "x": "---------------------------------- **DISAMBIGUATED DERIVATION TREES** In order to encode the MTT acting on derivation trees, we introduce a new abstract vocabulary \u03a3 der\u03b8 for disambiguated derivation trees as in (Yoshinaka, 2006 to love is used to model sentences where both adjunctions are performed into \u03b3 to love . C 10 to love and C 01 to love are used for sentences where only one adjunction at the s or at the vp node occurs respectively. C 00 to love : np np s is used when no adjunction occurs. 6 This really mimics (Yoshinaka, 2006) 's encoding of<cite> (Kallmeyer and Kuhlmann, 2012)</cite> MTT rules: . . . are designed in order to indicate that a given adjunction has n adjunctions above it (i.e. which scope over it). The superscripts (2(n + 1))(2(n \u2212 1)) express that an adjunction that has n adjunctions above it is translated as a function that takes a 2(n + 1)-tuple as argument and returns a 2(n \u2212 1)-tuple. To model auxiliary trees which are CTAs we need a different strategy. For each such adjunction tree T we have two sets in \u03a3 der\u03b8 : S 1 T the set of constants which can be adjoined into initial trees and S 2 T the set of constants which can be adjoined into auxiliary trees.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_11",
  "x": "yield : It translates any atomic type \u03c4 n or \u03c4 n X with X \u2208 {n A , n d A . . .} as a n-tuple of string of non-complement-taking verbal or sentential adjunctions \u03c4 2 vp and \u03c4 2 s are translated as t t. Let us show for the sentence (1) how the ACGs defined above work with the data provided in Table 2 . Its representation in \u03a3 der\u03b8 is: f (John + (Bill + (claims + ((Mary + ((seems + to love) + )) + )))) and L dep. log (t 0 ) = claim bill (seem (love john mary) ---------------------------------- **CONCLUSION** In this paper, we have given an ACG perspective on the transformation of the derivation trees of TAG to the dependency trees proposed in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> . Figure 4 illustrates the architecture we propose. This transformation is a two-step process using first a macrotree transduction then an interpretation of dependency trees as (tuples of) strings.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_12",
  "x": "In this paper, we have given an ACG perspective on the transformation of the derivation trees of TAG to the dependency trees proposed in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> . Figure 4 illustrates the architecture we propose. This transformation is a two-step process using first a macrotree transduction then an interpretation of dependency trees as (tuples of) strings. It was known from (Yoshinaka, 2006) how to encode a macrotree transducer into a G dep \u2022G \u22121 der ACG composition. Dealing with typed trees to represent derivation trees allows us to provide a meaningful (wrt. the TAG formalism) abstract vocabulary \u03a3 der\u03b8 encoding this macro-tree transducer. The encoding of the second step then made explicit the lexical blow up for the interpretation of the functional symbols of the dependency trees in<cite> (Kallmeyer and Kuhlmann, 2012</cite> )'s construct. It also provides a push out (in the categorical sense) of the two morphisms from the disambiguated derivation trees to the derived trees and to the dependency trees. The diagram is completed with the yield function from the derived trees and from the dependency trees to the string vocabulary.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_13",
  "x": "In this paper, we have given an ACG perspective on the transformation of the derivation trees of TAG to the dependency trees proposed in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> . Figure 4 illustrates the architecture we propose. This transformation is a two-step process using first a macrotree transduction then an interpretation of dependency trees as (tuples of) strings. It was known from (Yoshinaka, 2006) how to encode a macrotree transducer into a G dep \u2022G \u22121 der ACG composition. Dealing with typed trees to represent derivation trees allows us to provide a meaningful (wrt. the TAG formalism) abstract vocabulary \u03a3 der\u03b8 encoding this macro-tree transducer. The encoding of the second step then made explicit the lexical blow up for the interpretation of the functional symbols of the dependency trees in<cite> (Kallmeyer and Kuhlmann, 2012</cite> )'s construct. It also provides a push out (in the categorical sense) of the two morphisms from the disambiguated derivation trees to the derived trees and to the dependency trees. The diagram is completed with the yield function from the derived trees and from the dependency trees to the string vocabulary.",
  "y": "background"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_0",
  "x": "By contrast, information about the sender of an email message has always been explicitly represented in the message headers, starting with early standardization attempts (Bhushan et al., 1973) and including the two decade old current standard (Crocker, 1982) . Applications that aim to present voicemail messages through an email-like interface -take as an example the idea of a \"uniform inbox\" presentation of email, voicemail, and other kinds of messages 2 -must deal with the problem of how to obtain information analogous to what would be contained in email headers. Here we will discuss one way of addressing this problem, treating it exclusively as the task of extracting relevant information from voicemail transcripts. In practice, e.g. in the context of a sophisticated voicemail front-end ) that is tightly integrated with an organization-wide voicemail system and private branch exchange (PBX), additional sources of information may be available: the voicemail system or the PBX might provide information about the originating station of a call, and speaker identification can be used to match a caller's voice against models of known callers ). Restricting our attention to voicemail transcripts means that our focus and goals are similar to those of <cite>Huang et al. (2001)</cite> , but the features and techniques we use are very different. While the present task may seem broadly similar to named entity extraction from broadcast news (Gotoh and Renals, 2000) , it is quite distinct from the latter: first, we are only interested in a small subset of the named entities; and second, the structure of the voicemail transcripts in our corpus is very different from broadcast news and certain aspects of this structure can be exploited for extracting caller names. <cite>Huang et al. (2001)</cite> discuss three approaches: hand-crafted rules; grammatical inference of subsequential transducers; and log-linear classifiers with bigram and trigram features used as taggers (Ratnaparkhi, 1996) . While the latter are reported to yield the best overall performance, the hand-crafted rules resulted in higher recall. Our phone number extractor is based on a two-phase procedure that employs a small hand-crafted component to propose candidate phrases, followed by a classifier that retains the desirable candidates.",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_1",
  "x": "<cite>Huang et al. (2001)</cite> discuss three approaches: hand-crafted rules; grammatical inference of subsequential transducers; and log-linear classifiers with bigram and trigram features used as taggers (Ratnaparkhi, 1996) . While the latter are reported to yield the best overall performance, the hand-crafted rules resulted in higher recall. Our phone number extractor is based on a two-phase procedure that employs a small hand-crafted component to propose candidate phrases, followed by a classifier that retains the desirable candidates. This allows for more or less inde-pendent optimization of recall and precision, somewhat similar to the PNrule classifier learner . We shall see that hand-crafted rules achieve very good recall, just as <cite>Huang et al. (2001)</cite> had observed, and the pruning phase successfully eliminates most undesirable candidates without affecting recall too much. Overall performance of our method is better than if we employ a log-linear model with trigram features. The success of the method proposed here is also due to the use of a rich set of features for candidate classification. For example, the majority of phone numbers in voicemail messages has either four, seven, or ten digits, whereas nine digits would indicate a social security number. In our two-phase approach it is straightforward for the second-phase classifier to take the length of a candidate phone number into account.",
  "y": "background"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_2",
  "x": "Our phone number extractor is based on a two-phase procedure that employs a small hand-crafted component to propose candidate phrases, followed by a classifier that retains the desirable candidates. This allows for more or less inde-pendent optimization of recall and precision, somewhat similar to the PNrule classifier learner . We shall see that hand-crafted rules achieve very good recall, just as <cite>Huang et al. (2001)</cite> had observed, and the pruning phase successfully eliminates most undesirable candidates without affecting recall too much. Overall performance of our method is better than if we employ a log-linear model with trigram features. The success of the method proposed here is also due to the use of a rich set of features for candidate classification. For example, the majority of phone numbers in voicemail messages has either four, seven, or ten digits, whereas nine digits would indicate a social security number. In our two-phase approach it is straightforward for the second-phase classifier to take the length of a candidate phone number into account. On the other hand, standard named entity taggers that use trigram features do not exploit this information, and doing so would entail significant changes to the underlying models and parameter estimation procedures. The rest of this paper is organized as follows.",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_3",
  "x": "While this is less of a problem when evaluating on manual transcriptions, the experience reported in<cite> (Huang et al., 2001)</cite> suggests that the relatively high error rate of speech recognizers may negatively affect performance of caller name extraction on automatically generated transcripts. We therefore avoid using anything but a small number of greetings and commonly occurring words like 'hi', 'this', 'is' etc. and a small number of common first names for extracting caller phrases and use positional information in addition to word-based features. We locate caller phrases by first identifying their start position in the message and then predicting the length of the phrase. The empirical distribution of caller phrase lengths in the development data is shown in Figure 2 . Most caller phrases are between two and four words long ('it's Pat', 'this is Pat Caller') and there are moderately good lexical indicators that signal the end of a caller phrase ('I', 'could', 'please', etc.). Again, we avoid the use of names as features and rely on a small set of features based on common words, in addition to phrase length, for predicting the length of the caller phrase. We have thus identified two classes of features that allow us to predict the start of the caller phrase relative to the beginning of the message, as well as the end of the caller phrase relative to its start. Since we are dealing with discrete word indices in both cases, we treat this as a classification task, rather than a regression task. A large number of classifier learners can be used to automatically infer classifiers for the two subtasks at hand.",
  "y": "background"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_4",
  "x": "We chose a decision tree learner for convenience and note that this choice does not affect the overall results nearly as much as modifying our feature inventory. Since a direct comparison to the log-linear named entity tagger described in<cite> (Huang et al., 2001</cite> ) (we refer to this approach as HZP log-linear below) is not possible due to the use of different corpora and annotation standards, we applied a similar named entity tagger based on a log-linear model with trigram features to our data (we refer to this approach as Col log-linear as the tagger was provided by Michael Collins). Table 1 summarizes precision (P), recall (R), and F-measure (F) for three approaches evaluated on manual transcriptions: row HZP loglinear repeats the results of the best model from<cite> (Huang et al., 2001</cite> ); row Col log-linear contains the results we obtained using a similar named entity tagger on our own data; and row JA classifiers shows the performance of the classifier method proposed in this section. Like <cite>Huang et al. (2001)</cite> , we count a proposed caller phrase as correct if and only if it matches the annotation of the evaluation data perfectly. The numbers could be made to look better by using containment as the evaluation criterion, i.e., we would count a proposed phrase as correct if it contained an actual phrase plus perhaps some additional material. While this may be more useful in practice (see below), it is not the objective that was maximized during training, and so we prefer the stricter criterion for evaluation on previously annotated transcripts. While the results for the approach proposed here appear clearly worse than those reported by <cite>Huang et al. (2001)</cite> , we hasten to point out that this is most likely not due to any difference in the corpora that were used. This is corroborated by the fact that we were able to obtain performance much closer to that of the best, finely tuned log-linear model from<cite> (Huang et al., 2001</cite> ) by using a generic named entity tagger that was not adapted in any way to the particular task at hand. The log-linear taggers employ n-gram features based on family names and other particular aspects of the development data that do not necessarily generalize to other settings, where the family names of the callers may be different or may not be transcribed properly.",
  "y": "uses"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_6",
  "x": "Since we are dealing with discrete word indices in both cases, we treat this as a classification task, rather than a regression task. A large number of classifier learners can be used to automatically infer classifiers for the two subtasks at hand. We chose a decision tree learner for convenience and note that this choice does not affect the overall results nearly as much as modifying our feature inventory. Since a direct comparison to the log-linear named entity tagger described in<cite> (Huang et al., 2001</cite> ) (we refer to this approach as HZP log-linear below) is not possible due to the use of different corpora and annotation standards, we applied a similar named entity tagger based on a log-linear model with trigram features to our data (we refer to this approach as Col log-linear as the tagger was provided by Michael Collins). Table 1 summarizes precision (P), recall (R), and F-measure (F) for three approaches evaluated on manual transcriptions: row HZP loglinear repeats the results of the best model from<cite> (Huang et al., 2001</cite> ); row Col log-linear contains the results we obtained using a similar named entity tagger on our own data; and row JA classifiers shows the performance of the classifier method proposed in this section. Like <cite>Huang et al. (2001)</cite> , we count a proposed caller phrase as correct if and only if it matches the annotation of the evaluation data perfectly. The numbers could be made to look better by using containment as the evaluation criterion, i.e., we would count a proposed phrase as correct if it contained an actual phrase plus perhaps some additional material. While this may be more useful in practice (see below), it is not the objective that was maximized during training, and so we prefer the stricter criterion for evaluation on previously annotated transcripts. While the results for the approach proposed here appear clearly worse than those reported by <cite>Huang et al. (2001)</cite> , we hasten to point out that this is most likely not due to any difference in the corpora that were used.",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_7",
  "x": "Since a direct comparison to the log-linear named entity tagger described in<cite> (Huang et al., 2001</cite> ) (we refer to this approach as HZP log-linear below) is not possible due to the use of different corpora and annotation standards, we applied a similar named entity tagger based on a log-linear model with trigram features to our data (we refer to this approach as Col log-linear as the tagger was provided by Michael Collins). Table 1 summarizes precision (P), recall (R), and F-measure (F) for three approaches evaluated on manual transcriptions: row HZP loglinear repeats the results of the best model from<cite> (Huang et al., 2001</cite> ); row Col log-linear contains the results we obtained using a similar named entity tagger on our own data; and row JA classifiers shows the performance of the classifier method proposed in this section. Like <cite>Huang et al. (2001)</cite> , we count a proposed caller phrase as correct if and only if it matches the annotation of the evaluation data perfectly. The numbers could be made to look better by using containment as the evaluation criterion, i.e., we would count a proposed phrase as correct if it contained an actual phrase plus perhaps some additional material. While this may be more useful in practice (see below), it is not the objective that was maximized during training, and so we prefer the stricter criterion for evaluation on previously annotated transcripts. While the results for the approach proposed here appear clearly worse than those reported by <cite>Huang et al. (2001)</cite> , we hasten to point out that this is most likely not due to any difference in the corpora that were used. This is corroborated by the fact that we were able to obtain performance much closer to that of the best, finely tuned log-linear model from<cite> (Huang et al., 2001</cite> ) by using a generic named entity tagger that was not adapted in any way to the particular task at hand. The log-linear taggers employ n-gram features based on family names and other particular aspects of the development data that do not necessarily generalize to other settings, where the family names of the callers may be different or may not be transcribed properly. In fact, it seems rather likely that the log-linear models and the features they employ over-fit the training data.",
  "y": "differences"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_8",
  "x": "We chose a decision tree learner for convenience and note that this choice does not affect the overall results nearly as much as modifying our feature inventory. Since a direct comparison to the log-linear named entity tagger described in<cite> (Huang et al., 2001</cite> ) (we refer to this approach as HZP log-linear below) is not possible due to the use of different corpora and annotation standards, we applied a similar named entity tagger based on a log-linear model with trigram features to our data (we refer to this approach as Col log-linear as the tagger was provided by Michael Collins). Table 1 summarizes precision (P), recall (R), and F-measure (F) for three approaches evaluated on manual transcriptions: row HZP loglinear repeats the results of the best model from<cite> (Huang et al., 2001</cite> ); row Col log-linear contains the results we obtained using a similar named entity tagger on our own data; and row JA classifiers shows the performance of the classifier method proposed in this section. Like <cite>Huang et al. (2001)</cite> , we count a proposed caller phrase as correct if and only if it matches the annotation of the evaluation data perfectly. The numbers could be made to look better by using containment as the evaluation criterion, i.e., we would count a proposed phrase as correct if it contained an actual phrase plus perhaps some additional material. While this may be more useful in practice (see below), it is not the objective that was maximized during training, and so we prefer the stricter criterion for evaluation on previously annotated transcripts. While the results for the approach proposed here appear clearly worse than those reported by <cite>Huang et al. (2001)</cite> , we hasten to point out that this is most likely not due to any difference in the corpora that were used. This is corroborated by the fact that we were able to obtain performance much closer to that of the best, finely tuned log-linear model from<cite> (Huang et al., 2001</cite> ) by using a generic named entity tagger that was not adapted in any way to the particular task at hand. The log-linear taggers employ n-gram features based on family names and other particular aspects of the development data that do not necessarily generalize to other settings, where the family names of the callers may be different or may not be transcribed properly.",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_10",
  "x": "The log-linear taggers employ n-gram features based on family names and other particular aspects of the development data that do not necessarily generalize to other settings, where the family names of the callers may be different or may not be transcribed properly. In fact, it seems rather likely that the log-linear models and the features they employ over-fit the training data. This becomes clearer when one evaluates on unseen transcripts produced by an automatic speech recognizer (ASR), 3 as summarized in Table 2 . Rows HZP strict and HZP containment repeat the figures for the best model from<cite> (Huang et al., 2001</cite> ) when evaluated on automatic transcriptions. The difference is that HZP strict uses the strict evaluation criterion described above, whereas HZP containment uses the weaker criterion of containment, i.e., an extracted phrase counts as correct if it contains exactly one whole actual phrase. Row JA containment summarizes the performance of our approach when evaluated on 101 unseen automatically transcribed messages. Since we did not have any labeled automatic transcriptions available to compare with the predicted caller phrase labels using the strict criterion, we only report results based on the weaker criterion of containment. In fact, we count caller phrases as correct as long as they contain the full name of the caller, since this is the common denominator in the otherwise somewhat heterogeneous labeling of our training corpus; more on this issue in the next section. The difference between the approach in<cite> (Huang et al., 2001</cite> ) and ours may be partly due to the performance of the ASR components: <cite>Huang et al. (2001)</cite> report a word error rate of 'about 35%', whereas we used a recognizer (Bacchiani, 2001 ) with a word error rate of only 23%.",
  "y": "differences"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_11",
  "x": "Rows HZP rules and HZP log-linear refer to the rule-based baseline and the best log-linear model of<cite> (Huang et al., 2001</cite> ) and the figures are simply taken from that paper; row Col log-linear refers to the same named entity tagger we used in the previous section and is included for comparison with the HZP models; row JA digits refers to the simple baseline where we extract strings of spoken digits of plausible lengths. Our main results appear in the remaining rows. The performance of our hand-crafted extraction grammar (in row JA extract) was about what we had seen on the development data before, with recall being as high as one could reasonably expect. As mentioned above, using a simple pruning step in the second phase (see JA extract + prune) results in a doubling of precision and leaves recall essentially unaffected (a single fragmentary phone number was wrongly excluded). Finally, if we use a decision tree classifier in the second phase, we can achieve extremely high precision with a minimal impact on recall. Our two-phase procedure outperforms all other methods we considered. We evaluated the performance of our best models on the same 101 unseen ASR transcripts used above in the evaluation of the caller phrase extraction. The results are summarized in Table 5 , which also repeats the best results from<cite> (Huang et al., 2001)</cite> , using the same terminology as earlier: rows HZP strict and HZP containment refer to the best model from<cite> (Huang et al., 2001</cite> ) -corresponding to row HZP log-linear in Table 4 -when evaluated using the strict criterion and containment, respectively; and row JA containment refers to our own best model -corresponding to row JA extract + classify in Ta It is not very plausible that the differences between the approaches in Table 5 would be due to a difference in the performance of the ASR components that generated the message transcripts. From inspecting our own data it is clear that ASR mistakes inside phone numbers are virtually absent, and we would expect the same to hold even of an automatic recognizer with an overall much higher word error rate.",
  "y": "background"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_12",
  "x": "Our two-phase procedure outperforms all other methods we considered. We evaluated the performance of our best models on the same 101 unseen ASR transcripts used above in the evaluation of the caller phrase extraction. The results are summarized in Table 5 , which also repeats the best results from<cite> (Huang et al., 2001)</cite> , using the same terminology as earlier: rows HZP strict and HZP containment refer to the best model from<cite> (Huang et al., 2001</cite> ) -corresponding to row HZP log-linear in Table 4 -when evaluated using the strict criterion and containment, respectively; and row JA containment refers to our own best model -corresponding to row JA extract + classify in Ta It is not very plausible that the differences between the approaches in Table 5 would be due to a difference in the performance of the ASR components that generated the message transcripts. From inspecting our own data it is clear that ASR mistakes inside phone numbers are virtually absent, and we would expect the same to hold even of an automatic recognizer with an overall much higher word error rate. Also, for most phone numbers the labeling is uncontroversial, so we expect the corpora used by <cite>Huang et al. (2001)</cite> and ourselves to be extremely similar in terms of mark-up of phone numbers. So the observed performance difference is most likely due to the difference in extraction methods. ---------------------------------- **CONCLUSION AND OUTLOOK** The novel contributions of this paper can be summarized as follows:",
  "y": "uses"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_13",
  "x": "Our main results appear in the remaining rows. The performance of our hand-crafted extraction grammar (in row JA extract) was about what we had seen on the development data before, with recall being as high as one could reasonably expect. As mentioned above, using a simple pruning step in the second phase (see JA extract + prune) results in a doubling of precision and leaves recall essentially unaffected (a single fragmentary phone number was wrongly excluded). Finally, if we use a decision tree classifier in the second phase, we can achieve extremely high precision with a minimal impact on recall. Our two-phase procedure outperforms all other methods we considered. We evaluated the performance of our best models on the same 101 unseen ASR transcripts used above in the evaluation of the caller phrase extraction. The results are summarized in Table 5 , which also repeats the best results from<cite> (Huang et al., 2001)</cite> , using the same terminology as earlier: rows HZP strict and HZP containment refer to the best model from<cite> (Huang et al., 2001</cite> ) -corresponding to row HZP log-linear in Table 4 -when evaluated using the strict criterion and containment, respectively; and row JA containment refers to our own best model -corresponding to row JA extract + classify in Ta It is not very plausible that the differences between the approaches in Table 5 would be due to a difference in the performance of the ASR components that generated the message transcripts. From inspecting our own data it is clear that ASR mistakes inside phone numbers are virtually absent, and we would expect the same to hold even of an automatic recognizer with an overall much higher word error rate. Also, for most phone numbers the labeling is uncontroversial, so we expect the corpora used by <cite>Huang et al. (2001)</cite> and ourselves to be extremely similar in terms of mark-up of phone numbers.",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_14",
  "x": "\u2022 We showed that good performance on the task of extracting caller information can be achieved using a very small inventory of lexical and positional features. \u2022 We argued that for extracting telephone numbers it is extremely useful to take the length of their numeric representation into account. Our grammar-based extractor translates spoken numbers into such a numeric representation. \u2022 Our two-phase approach allows us to efficiently develop a simple extraction grammar for which the only requirement is high recall. This places less of a burden on the grammar developers than having to write an accurate set of rules like the baseline of<cite> (Huang et al., 2001</cite> ). \u2022 The combined performance of our simple extraction grammar and the second-phase classifier exceeded the performance of all other methods, including the current state of the art<cite> (Huang et al., 2001</cite> ). Our results point towards approaches that use a small inventory of features that have been tailored to specific tasks. Generic methods like the named entity tagger used by <cite>Huang et al. (2001)</cite> may not be the best tools for particular tasks; in fact, we do not expect the bigram and trigram features used by such taggers to be sufficient for accurately extracting phone numbers. We also believe that using all available lexical information for extracting caller information can easily lead to over-fitting, which can partly be avoid by not relying on names being transcribed correctly by an ASR component.",
  "y": "uses"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_15",
  "x": "\u2022 Our two-phase approach allows us to efficiently develop a simple extraction grammar for which the only requirement is high recall. This places less of a burden on the grammar developers than having to write an accurate set of rules like the baseline of<cite> (Huang et al., 2001</cite> ). \u2022 The combined performance of our simple extraction grammar and the second-phase classifier exceeded the performance of all other methods, including the current state of the art<cite> (Huang et al., 2001</cite> ). Our results point towards approaches that use a small inventory of features that have been tailored to specific tasks. Generic methods like the named entity tagger used by <cite>Huang et al. (2001)</cite> may not be the best tools for particular tasks; in fact, we do not expect the bigram and trigram features used by such taggers to be sufficient for accurately extracting phone numbers. We also believe that using all available lexical information for extracting caller information can easily lead to over-fitting, which can partly be avoid by not relying on names being transcribed correctly by an ASR component. In practice, determining the identity of a caller might have to take many diverse sources of information into account. The self-identification of a caller and the phone numbers mentioned in the same message are not uncorrelated, since there is usually only a small number of ways to reach any particular caller. In an application we might therefore try to use a combination of speaker identification , caller name extraction, and recognized phone numbers to establish the identity of the caller.",
  "y": "differences"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_16",
  "x": "\u2022 Our two-phase approach allows us to efficiently develop a simple extraction grammar for which the only requirement is high recall. This places less of a burden on the grammar developers than having to write an accurate set of rules like the baseline of<cite> (Huang et al., 2001</cite> ). \u2022 The combined performance of our simple extraction grammar and the second-phase classifier exceeded the performance of all other methods, including the current state of the art<cite> (Huang et al., 2001</cite> ). Our results point towards approaches that use a small inventory of features that have been tailored to specific tasks. Generic methods like the named entity tagger used by <cite>Huang et al. (2001)</cite> may not be the best tools for particular tasks; in fact, we do not expect the bigram and trigram features used by such taggers to be sufficient for accurately extracting phone numbers. We also believe that using all available lexical information for extracting caller information can easily lead to over-fitting, which can partly be avoid by not relying on names being transcribed correctly by an ASR component. In practice, determining the identity of a caller might have to take many diverse sources of information into account. The self-identification of a caller and the phone numbers mentioned in the same message are not uncorrelated, since there is usually only a small number of ways to reach any particular caller. In an application we might therefore try to use a combination of speaker identification , caller name extraction, and recognized phone numbers to establish the identity of the caller.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_0",
  "x": "****THE UTILITY OF PARSE-DERIVED FEATURES FOR AUTOMATIC DISCOURSE SEGMENTATION**** **ABSTRACT** We investigate different feature sets for performing automatic sentence-level discourse segmentation within a general machine learning approach, including features derived from either finite-state or contextfree annotations. We achieve the best reported performance on this task, and demonstrate that our SPADE-inspired context-free features are critical to achieving this level of accuracy. This counters recent results suggesting that purely finite-state approaches can perform competitively. ---------------------------------- **INTRODUCTION** Discourse structure annotations have been demonstrated to be of high utility for a number of NLP applications, including automatic text summarization (Marcu, 1998; Marcu, 1999; Cristea et al., 2005) , sentence compression<cite> (Sporleder and Lapata, 2005)</cite> , natural language generation (Prasad et al., 2005) and question answering (Verberne et al., 2006) . These annotations include sentence segmentation into discourse units along with the linking of discourse units, both within and across sentence boundaries, into a labeled hierarchical structure.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_1",
  "x": "Their models and algorithm -subsequently packaged together into the publicly available SPADE discourse parser 1 -make use of the output of the Charniak (2000) parser to derive syntactic indicator features for segmentation and discourse parsing. <cite>Sporleder and Lapata (2005)</cite> also used the RST Treebank as training data for data-driven discourse parsing algorithms, though their focus, in contrast to Soricut and Marcu (2003) , was to avoid contextfree parsing and rely exclusively on features in their model that could be derived via finite-state chunkers and taggers. The annotations that they derive are dis-course \"chunks\", i.e., sentence-level segmentation and non-hierarchical nucleus/span labeling of segments. They demonstrate that their models achieve comparable results to SPADE without the use of any context-free features. Once again, segmentation is the part of the process where the automatic algorithms most seriously underperform. In this paper we take up the question posed by the results of <cite>Sporleder and Lapata (2005)</cite> : how much, if any, accuracy reduction should we expect if we choose to use only finite-state derived features, rather than those derived from full contextfree parses? If little accuracy is lost, as their results suggest, then it would make sense to avoid relatively expensive context-free parsing, particularly if the amount of text to be processed is large or if there are real-time processing constraints on the system. If, however, the accuracy loss is substantial, one might choose to avoid context-free parsing only in the most time-constrained scenarios. While <cite>Sporleder and Lapata (2005)</cite> demonstrated that their finite-state system could perform as well as the SPADE system, which uses context-free parse trees, this does not directly answer the question of the utility of context-free derived features for this task. SPADE makes use of a particular kind of feature from the parse trees, and does not train a general classifier making use of other features beyond the parse-derived indicator features.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_2",
  "x": "Their models and algorithm -subsequently packaged together into the publicly available SPADE discourse parser 1 -make use of the output of the Charniak (2000) parser to derive syntactic indicator features for segmentation and discourse parsing. <cite>Sporleder and Lapata (2005)</cite> also used the RST Treebank as training data for data-driven discourse parsing algorithms, though their focus, in contrast to Soricut and Marcu (2003) , was to avoid contextfree parsing and rely exclusively on features in their model that could be derived via finite-state chunkers and taggers. The annotations that they derive are dis-course \"chunks\", i.e., sentence-level segmentation and non-hierarchical nucleus/span labeling of segments. They demonstrate that their models achieve comparable results to SPADE without the use of any context-free features. Once again, segmentation is the part of the process where the automatic algorithms most seriously underperform. In this paper we take up the question posed by the results of <cite>Sporleder and Lapata (2005)</cite> : how much, if any, accuracy reduction should we expect if we choose to use only finite-state derived features, rather than those derived from full contextfree parses? If little accuracy is lost, as their results suggest, then it would make sense to avoid relatively expensive context-free parsing, particularly if the amount of text to be processed is large or if there are real-time processing constraints on the system. If, however, the accuracy loss is substantial, one might choose to avoid context-free parsing only in the most time-constrained scenarios. While <cite>Sporleder and Lapata (2005)</cite> demonstrated that their finite-state system could perform as well as the SPADE system, which uses context-free parse trees, this does not directly answer the question of the utility of context-free derived features for this task. SPADE makes use of a particular kind of feature from the parse trees, and does not train a general classifier making use of other features beyond the parse-derived indicator features.",
  "y": "motivation"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_3",
  "x": "If, however, the accuracy loss is substantial, one might choose to avoid context-free parsing only in the most time-constrained scenarios. While <cite>Sporleder and Lapata (2005)</cite> demonstrated that their finite-state system could perform as well as the SPADE system, which uses context-free parse trees, this does not directly answer the question of the utility of context-free derived features for this task. SPADE makes use of a particular kind of feature from the parse trees, and does not train a general classifier making use of other features beyond the parse-derived indicator features. As we shall show, its performance is not the highest that can be achieved via context-free parser derived features. In this paper, we train a classifier using a general machine learning approach and a range of finitestate and context-free derived features. We investigate the impact on discourse segmentation performance when one feature set is used versus another, in such a way establishing the utility of features derived from context-free parses. In the course of so doing, we achieve the best reported performance on this task, an absolute F-score improvement of 5.0% over SPADE, which represents a more than 34% relative error rate reduction. By focusing on segmentation, we provide an approach that is generally applicable to all of the various annotation approaches, given the similarities between the various sentence-level segmentation guidelines. Given that segmentation has been shown to be a primary impediment to high accuracy sentence-level discourse structure annotation, this represents a large step forward in our ability to automatically parse the discourse structure of text, whatever annotation approach we choose.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_4",
  "x": "<cite>Sporleder and Lapata (2005)</cite> went further and established a smaller subset of 608 sentences, which omitted sentences with only one segment, i.e., sentences which themselves are atomic edus. Since the primary focus of this paper is on segmentation, there is no strong reason to omit any sentences from the test set, hence our results will evaluate on all 991 test sentences, with two exceptions. First, in Section 2.3, we compare SPADE results under our configuration with results from <cite>Sporleder and Lapata (2005)</cite> in order to establish comparability, and this is done on their 608 sentence subset. Second, in Section 3.2, we investigate feeding our segmentation into the SPADE system, in order to evaluate the impact of segmentation improvements on their sentence-level discourse parsing performance. For those trials, the 951 sentence subset from Soricut and Marcu (2003) is used. All other trials use the full 991 sentence test set. Segmentation evaluation is done with precision, recall and F1-score of segmentation boundaries. Given a word string w 1 . . . w k , we can index word boundaries from 0 to k, so that each word w i falls between boundaries i\u22121 and i. For sentence-based segmentation, indices 0 and k, representing the beginning and end of the string, are known to be segment boundaries.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_5",
  "x": "Previous research into RST-DT segmentation and parsing has focused on subsets of the 991 sentence test set during evaluation. Soricut and Marcu (2003) omitted sentences that were not exactly spanned by a subtree of the treebank, so that they could focus on sentence-level discourse parsing. By our count, this eliminates 40 of the 991 sentences in the test set from consideration. <cite>Sporleder and Lapata (2005)</cite> went further and established a smaller subset of 608 sentences, which omitted sentences with only one segment, i.e., sentences which themselves are atomic edus. Since the primary focus of this paper is on segmentation, there is no strong reason to omit any sentences from the test set, hence our results will evaluate on all 991 test sentences, with two exceptions. First, in Section 2.3, we compare SPADE results under our configuration with results from <cite>Sporleder and Lapata (2005)</cite> in order to establish comparability, and this is done on their 608 sentence subset. Second, in Section 3.2, we investigate feeding our segmentation into the SPADE system, in order to evaluate the impact of segmentation improvements on their sentence-level discourse parsing performance. For those trials, the 951 sentence subset from Soricut and Marcu (2003) is used. All other trials use the full 991 sentence test set.",
  "y": "differences"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_6",
  "x": "First, in Section 2.3, we compare SPADE results under our configuration with results from <cite>Sporleder and Lapata (2005)</cite> in order to establish comparability, and this is done on their 608 sentence subset. Second, in Section 3.2, we investigate feeding our segmentation into the SPADE system, in order to evaluate the impact of segmentation improvements on their sentence-level discourse parsing performance. For those trials, the 951 sentence subset from Soricut and Marcu (2003) is used. All other trials use the full 991 sentence test set. Segmentation evaluation is done with precision, recall and F1-score of segmentation boundaries. Given a word string w 1 . . . w k , we can index word boundaries from 0 to k, so that each word w i falls between boundaries i\u22121 and i. For sentence-based segmentation, indices 0 and k, representing the beginning and end of the string, are known to be segment boundaries. Hence Soricut and Marcu (2003) evaluate with respect to sentence internal segmentation boundaries, i.e., with indices j such that 0<j<k for a sentence of length k. Let g be the number of sentence-internal segmentation boundaries in the gold standard, t the number of sentence-internal segmentation boundaries in the system output, and m the number of correct sentence-internal segmentation boundaries in the system output. Then",
  "y": "uses"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_7",
  "x": "In <cite>Sporleder and Lapata (2005)</cite> , they were primarily interested in labeled segmentation, where the segment initial boundary was labeled with the segment type. In such a scenario, the boundary at index 0 is no longer known, hence their evaluation included those boundaries, even when reporting unlabeled results. Thus, in section 2.3, for comparison with reported results in <cite>Sporleder and Lapata (2005)</cite> , our F1-score is defined accordingly, i.e., seg- mentation boundaries j such that 0 \u2264 j < k. In addition, we will report unlabeled bracketing precision, recall and F1-score, as defined in the PARSEVAL metrics (Black et al., 1991) and evaluated via the widely used evalb package. We also use evalb when reporting labeled and unlabeled discourse parsing results in Section 3.2. ---------------------------------- **BASELINE SPADE SETUP** The publicly available SPADE package, which encodes the approach in Soricut and Marcu (2003) , is taken as the baseline for this paper. We made several modifications to the script from the default, which account for better baseline performance than is achieved with the default configuration. First, we modified the script to take given parse trees as input, rather than running the Charniak parser itself.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_8",
  "x": "In <cite>Sporleder and Lapata (2005)</cite> , they were primarily interested in labeled segmentation, where the segment initial boundary was labeled with the segment type. In such a scenario, the boundary at index 0 is no longer known, hence their evaluation included those boundaries, even when reporting unlabeled results. Thus, in section 2.3, for comparison with reported results in <cite>Sporleder and Lapata (2005)</cite> , our F1-score is defined accordingly, i.e., seg- mentation boundaries j such that 0 \u2264 j < k. In addition, we will report unlabeled bracketing precision, recall and F1-score, as defined in the PARSEVAL metrics (Black et al., 1991) and evaluated via the widely used evalb package. We also use evalb when reporting labeled and unlabeled discourse parsing results in Section 3.2. ---------------------------------- **BASELINE SPADE SETUP** The publicly available SPADE package, which encodes the approach in Soricut and Marcu (2003) , is taken as the baseline for this paper. We made several modifications to the script from the default, which account for better baseline performance than is achieved with the default configuration. First, we modified the script to take given parse trees as input, rather than running the Charniak parser itself.",
  "y": "uses"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_9",
  "x": "These two modifications to the Charniak parsing output used by the SPADE system lead to improvements in its performance compared to previously reported results. Table 1 compares segmentation results of three systems on the <cite>Sporleder and Lapata (2005)</cite> 608 sentence subset of the evaluation data: (1) their best reported system; (2) the SPADE system results reported in that paper; and (3) the SPADE system results with our current configuration. The evaluation uses the unlabeled F1 measure as defined in that paper, which counts sentence initial boundaries in the scoring, as discussed in the previous section. As can be seen from these results, our improved configuration of SPADE gives us large improvements over the previously reported SPADE performance on this subset. As a result, we feel that we can use SPADE 490 as a very strong baseline for evaluation on the entire test set. Additionally, we modified the SPADE script to allow us to provide our segmentations to the full discourse parsing that it performs, in order to evaluate the improvements to discourse parsing yielded by any improvements to segmentation. ---------------------------------- **SEGMENTATION CLASSIFIER** For this paper, we trained a binary classifier, which was applied independently at each word w i in the string w 1 . . .",
  "y": "uses"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_10",
  "x": "For our three sequences (lexical, POS-tag and shallow tag), we define n-gram features surrounding the potential discourse boundary. If the current word is w i , the hypothesized boundary will occur between w i and w i+1 . For this boundary position, the 6-gram including the three words before and the three words after the boundary is included as a feature; additionally, all n-grams for n < 6 such that either w i or w i+1 (or both) is in the n-gram are included as features. In other words, all n-grams in a six word window of boundary position i are included as features, except those that include neither w i nor w i+1 in the n-gram. The identical feature templates are used with POS-tag and shallow tag sequences as well, to define tag n-gram features. This feature set is very close to that used in <cite>Sporleder and Lapata (2005)</cite> , but not identical. Their n-gram feature definitions were different (though similar), and they made use of cue phrases from Knott (1996) . In addition, they used a rulebased clauser that we did not. Despite such differences, this feature set is quite close to what is described in that paper.",
  "y": "similarities"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_11",
  "x": "**DISCOURSE PARSING** It has been shown that accurate discourse segmentation within a sentence greatly improves the overall parsing accuracy to near human levels (Soricut and Marcu, 2003) . Given our improved segmentation results presented in the previous section, improvements would be expected in full sentencelevel discourse parsing. To achieve this, we modified the SPADE script to accept our segmentations when building the fully hierarchical discourse tree. The results for three systems are presented in Table 3 : SPADE, our \"Full finite-state\" system, and our system with all features. Results for unlabeled bracketing are presented, along with results for labeled bracketing, where the label is either Nucleus or Satellite, depending upon whether or not the node is more central (Nucleus) to the coherence of the text than its sibling(s) (Satellite). This label set has been shown to be of particular utility for indicating which segments are more important to include in an automatically created summary or compressed sentence<cite> (Sporleder and Lapata, 2005</cite>; Marcu, 1998; Marcu, 1999; Cristea et al., 2005) . Once again, the finite-state system does not perform statistically significantly different from SPADE on either labeled or unlabeled discourse parsing. Using all features, however, results in greater than 5% absolute accuracy improvement over both of these systems, which is significant, in all cases, at p < 0.001.",
  "y": "background"
 },
 {
  "id": "f54235664f013f0fec918222be9198_0",
  "x": "Where van Zaanen (2000) and Clark (2001) induced unlabeled phrase structure for small domains like the ATIS, obtaining around 40% unlabeled f-score, Klein and Manning (2002) report 71.1% f-score on Penn WSJ part-of-speech strings \u2264 10 words (WSJ10) using a constituentcontext model called CCM. Klein and Manning (2004) further show that a hybrid approach which combines constituency and dependency models, yields 77.6% f-score on WSJ10. While Klein and Manning's approach may be described as an \"all-substrings\" approach to unsupervised parsing, an even richer model consists of an \"all-subtrees\" approach to unsupervised parsing, called U-DOP <cite>(Bod 2006)</cite> . U-DOP initially assigns all unlabeled binary trees to a training set, efficiently stored in a packed forest, and next trains subtrees thereof on a heldout corpus, either by taking their relative frequencies, or by iteratively training the subtree parameters using the EM algorithm (referred to as \"UML-DOP\"). The main advantage of an allsubtrees approach seems to be the direct inclusion of discontiguous context that is not captured by (linear) substrings. Discontiguous context is important not only for learning structural dependencies but also for learning a variety of noncontiguous constructions such as nearest \u2026 to\u2026 or take \u2026 by surprise. Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Manning (2002, 2004) . Unfortunately, his experiments heavily depend on a priori sampling of subtrees, and the model becomes highly inefficient if larger corpora are used or longer sentences are included. In this paper we will also test an alternative model for unsupervised all-subtrees 400 parsing, termed U-DOP*, which is based on the DOP* estimator by Zollmann and Sima'an (2005) , and which computes the shortest derivations for sentences from a held-out corpus using all subtrees from all trees from an extraction corpus.",
  "y": "background"
 },
 {
  "id": "f54235664f013f0fec918222be9198_1",
  "x": "U-DOP initially assigns all unlabeled binary trees to a training set, efficiently stored in a packed forest, and next trains subtrees thereof on a heldout corpus, either by taking their relative frequencies, or by iteratively training the subtree parameters using the EM algorithm (referred to as \"UML-DOP\"). The main advantage of an allsubtrees approach seems to be the direct inclusion of discontiguous context that is not captured by (linear) substrings. Discontiguous context is important not only for learning structural dependencies but also for learning a variety of noncontiguous constructions such as nearest \u2026 to\u2026 or take \u2026 by surprise. Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Manning (2002, 2004) . Unfortunately, his experiments heavily depend on a priori sampling of subtrees, and the model becomes highly inefficient if larger corpora are used or longer sentences are included. In this paper we will also test an alternative model for unsupervised all-subtrees 400 parsing, termed U-DOP*, which is based on the DOP* estimator by Zollmann and Sima'an (2005) , and which computes the shortest derivations for sentences from a held-out corpus using all subtrees from all trees from an extraction corpus. While we do not achieve as high an f-score as the UML-DOP model in<cite> Bod (2006)</cite> , we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in<cite> Bod (2006)</cite> . We will extend our experiments to 4 million sentences from the NANC corpus (Graff 1995) , showing that an f-score of 70.7% can be obtained on the standard Penn WSJ test set by means of unsupervised parsing. Moreover, U-DOP* can be directly put to use in bootstrapping structures for concrete applications such as syntax-based machine translation and speech recognition.",
  "y": "uses differences"
 },
 {
  "id": "f54235664f013f0fec918222be9198_2",
  "x": "On the basis of this shortest-derivation assumption, Zollmann and Sima'an come up with a model that uses held-out estimation: the training corpus is randomly split into two parts proportional to a fixed ratio: an extraction corpus EC and a held-out corpus HC. Applied to DOP, held-out estimation would mean to extract fragments from the trees in EC and to assign their weights such that the likelihood of HC is maximized. If we combine their estimation method with Goodman's reduction of DOP, Zollman and Sima'an's procedure operates as follows: (1) Divide a treebank into an EC and HC (2) Convert the subtrees from EC into a PCFG reduction (3) Compute the shortest derivations for the sentences in HC (by simply assigning each subtree equal weight and applying Viterbi 1-best) (4) From those shortest derivations, extract the subtrees and their relative frequencies in HC to form an STSG Zollmann and Sima'an show that the resulting estimator is consistent. But equally important is the fact that this new DOP* model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003) . Moreover, DOP*'s estimation procedure is very efficient, while the EM training procedure for UML-DOP proposed in<cite> Bod (2006)</cite> is particularly time consuming and can only operate by randomly sampling trees. Given the advantages of DOP*, we will generalize this model in the current paper to unsupervised parsing. We will use the same allsubtrees methodology as in<cite> Bod (2006)</cite> , but now by applying the efficient and consistent DOP*-based estimator. The resulting model, which we will call U-DOP*, roughly operates as follows:",
  "y": "extends"
 },
 {
  "id": "f54235664f013f0fec918222be9198_3",
  "x": "But equally important is the fact that this new DOP* model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003) . Moreover, DOP*'s estimation procedure is very efficient, while the EM training procedure for UML-DOP proposed in<cite> Bod (2006)</cite> is particularly time consuming and can only operate by randomly sampling trees. Given the advantages of DOP*, we will generalize this model in the current paper to unsupervised parsing. We will use the same allsubtrees methodology as in<cite> Bod (2006)</cite> , but now by applying the efficient and consistent DOP*-based estimator. The resulting model, which we will call U-DOP*, roughly operates as follows: (1) Divide a corpus into an EC and HC (2) Assign all unlabeled binary trees to the sentences in EC, and store them in a shared parse forest (3) Convert the subtrees from the parse forests into a compact PCFG reduction (see next section) (4) Compute the shortest derivations for the sentences in HC (as in DOP*) (5) From those shortest derivations, extract the subtrees and their relative frequencies in HC to form an STSG (6) Use the STSG to compute the most probable parse trees for new test data by means of Viterbi n-best (see next section) We will use this U-DOP* model to investigate our main research question: how far can we get with unsupervised parsing if we make our training corpus several orders of magnitude larger than has hitherto be attempted? ---------------------------------- **CONVERTING SHARED PARSE FORESTS INTO PCFG REDUCTIONS**",
  "y": "background"
 },
 {
  "id": "f54235664f013f0fec918222be9198_4",
  "x": "Then, instead of assigning a unique address to each node in each tree, as done by the PCFG reduction for supervised DOP, we now assign a unique address to each node in each parse forest for each sentence. However, the same node may be part of more than one tree. A shared parse forest is an AND-OR graph where AND-nodes correspond to the usual parse tree nodes, while OR-nodes correspond to distinct subtrees occurring in the same context. The total number of nodes is cubic in sentence length n. This means that there are O(n 3 ) many nodes that receive a unique address as described above, to which next our PCFG reduction is applied. This is a huge reduction compared to<cite> Bod (2006)</cite> where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work. Since U-DOP* computes the shortest derivations (in the training phase) by combining subtrees from unlabeled binary trees, the PCFG reduction in figure 1 can be represented as in figure 2 , where X refers to the generalized category while B and C either refer to part-of-speech categories or are equivalent to X. The equal weights follow from the fact that the shortest derivation is equivalent to the most probable derivation if all subtrees are assigned equal probability (see Bod 2000; Goodman 2003) . Figure 2. PCFG reduction for U-DOP* Once we have parsed HC with the shortest derivations by the PCFG reduction in figure 2, we extract the subtrees from HC to form an STSG. The number of subtrees in the shortest derivations is linear in the number of nodes (see Zollmann and Sima'an 2005, theorem 5.2) .",
  "y": "differences"
 },
 {
  "id": "f54235664f013f0fec918222be9198_5",
  "x": "This is a huge reduction compared to<cite> Bod (2006)</cite> where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work. Since U-DOP* computes the shortest derivations (in the training phase) by combining subtrees from unlabeled binary trees, the PCFG reduction in figure 1 can be represented as in figure 2 , where X refers to the generalized category while B and C either refer to part-of-speech categories or are equivalent to X. The equal weights follow from the fact that the shortest derivation is equivalent to the most probable derivation if all subtrees are assigned equal probability (see Bod 2000; Goodman 2003) . Figure 2. PCFG reduction for U-DOP* Once we have parsed HC with the shortest derivations by the PCFG reduction in figure 2, we extract the subtrees from HC to form an STSG. The number of subtrees in the shortest derivations is linear in the number of nodes (see Zollmann and Sima'an 2005, theorem 5.2) . This means that U-DOP* results in an STSG which is much more succinct than previous DOP-based STSGs. Moreover, as in Bod (1998 Bod ( , 2000 , we use an extension of Good-Turing to smooth the subtrees and to deal with 'unknown' subtrees. Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP <cite>(Bod 2006)</cite> . This can be accomplished by training the PCFG reduction on the held-out corpus HC by means of the expectation-maximization algorithm, where the weights in figure 1 are taken as initial parameters.",
  "y": "uses"
 },
 {
  "id": "f54235664f013f0fec918222be9198_6",
  "x": "Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP <cite>(Bod 2006)</cite> . This can be accomplished by training the PCFG reduction on the held-out corpus HC by means of the expectation-maximization algorithm, where the weights in figure 1 are taken as initial parameters. Both U-DOP*'s and UML-DOP's estimators are known to be statistically consistent. But while U-DOP*'s training phase merely consists of the computation of the shortest derivations and the extraction of subtrees, UML-DOP involves iterative training of the parameters. Once we have extracted the STSG, we compute the most probable parse for new sentences by Viterbi n-best, summing up the probabilities of derivations resulting in the same tree (the exact computation of the most probable parse is NP hard -see Sima 'an 1996) . We have incorporated the technique by Huang and Chiang (2005) into our implementation which allows for efficient Viterbi n-best parsing. ---------------------------------- **EVALUATION ON HAND-ANNOTATED CORPORA** To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Manning (2002, 2004) and<cite> Bod (2006)</cite> : Penn's WSJ10 which contains 7422 sentences \u2264 10 words after removing empty elements and punctuation, the German NEGRA10 corpus and the Chinese Treebank CTB10 both containing 2200+ sentences \u2264 10 words after removing punctuation. As with most other unsupervised parsing models, we train and test on p-o-s strings rather than on word strings.",
  "y": "uses"
 },
 {
  "id": "f54235664f013f0fec918222be9198_7",
  "x": "The extension to word strings is straightforward as there exist highly accurate unsupervised part-of-speech taggers (e.g. Sch\u00fctze 1995) which can be directly combined with unsupervised parsers, but for the moment we will stick to p-o-s strings (we will come back to word strings in section 5). Each corpus was divided into 10 training/test set splits of 90%/10% (n-fold testing), and each training set was randomly divided into two equal parts, that serve as EC and HC and vice versa. We used the same evaluation metrics for unlabeled precision (UP) and unlabeled recall (UR) as in Manning (2002, 2004) . The two metrics of UP and UR are combined by the unlabeled f-score F1 = 2*UP*UR/(UP+UR). All trees in the test set were binarized beforehand, in the same way as in<cite> Bod (2006)</cite> . For UML-DOP the decrease in crossentropy became negligible after maximally 18 iterations. The training for U-DOP* consisted in the computation of the shortest derivations for the HC from which the subtrees and their relative frequencies were extracted. We used the technique in Bod (1998 Bod ( , 2000 to include 'unknown' subtrees. Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in<cite> Bod (2006)</cite> , the CCM model in Klein and Manning (2002) , the DMV dependency model in Klein and Manning (2004) It should be kept in mind that an exact comparison can only be made between U-DOP* and UML-DOP in table 1, since these two models were tested on 90%/10% splits, while the other models were applied to the full WSJ10, NEGRA10 and CTB10 corpora.",
  "y": "uses"
 },
 {
  "id": "f54235664f013f0fec918222be9198_8",
  "x": "The two metrics of UP and UR are combined by the unlabeled f-score F1 = 2*UP*UR/(UP+UR). All trees in the test set were binarized beforehand, in the same way as in<cite> Bod (2006)</cite> . For UML-DOP the decrease in crossentropy became negligible after maximally 18 iterations. The training for U-DOP* consisted in the computation of the shortest derivations for the HC from which the subtrees and their relative frequencies were extracted. We used the technique in Bod (1998 Bod ( , 2000 to include 'unknown' subtrees. Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in<cite> Bod (2006)</cite> , the CCM model in Klein and Manning (2002) , the DMV dependency model in Klein and Manning (2004) It should be kept in mind that an exact comparison can only be made between U-DOP* and UML-DOP in table 1, since these two models were tested on 90%/10% splits, while the other models were applied to the full WSJ10, NEGRA10 and CTB10 corpora. Table 1 shows that U-DOP* performs worse than UML-DOP in all cases, although the differences are small and was statistically significant only for WSJ10 using paired t-testing. As explained above, the main advantage of U-DOP* over UML-DOP is that it works with a more succinct grammar extracted from the shortest derivations of HC. Table 2 shows the size of the grammar (number of rules or subtrees) of the two models for resp.",
  "y": "uses"
 },
 {
  "id": "f54235664f013f0fec918222be9198_9",
  "x": "In putting our best f-score in table 4 into perspective, it should be kept in mind that the gold standard trees from Penn-WSJ section 23 were binarized. It is well known that such a binarization has a negative effect on the f-score. Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences \u2264 40 words, while the binarized version achieves only 64.6% f-score. To compare U-DOP*'s results against some supervised parsers, we additionally evaluated a PCFG treebank grammar and the supervised DOP* parser using the same test set. For these supervised parsers, we employed the standard training set, i.e. Penn's WSJ sections 2-21, but only by taking the p-o-s strings as we did for our unsupervised U-DOP* model. Table 5 . Comparison between the (best version of) U-DOP*, the supervised treebank PCFG and the supervised DOP* for section 23 of Penn's WSJ As seen in table 5, U-DOP* outperforms the binarized treebank PCFG on the WSJ test set. While a similar result was obtained in<cite> Bod (2006)</cite> , the absolute difference between unsupervised parsing and the treebank grammar was extremely small in<cite> Bod (2006)</cite>: 1.8%, while the difference in table 5 is 7.2%, corresponding to 19.7% error reduction. Our f-score remains behind the supervised version of DOP* but the gap gets narrower as more training data is being added to U-DOP*.",
  "y": "differences"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_0",
  "x": "Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets. ---------------------------------- **INTRODUCTION** BERT (Devlin et al., 2018) represents the latest refinement in a series of neural models that take advantage of pretraining on a language modeling task (Peters et al., 2018; Radford et al., 2018) . Researchers have demonstrated impressive gains in a broad range of NLP tasks, from sentence classification to sequence labeling. Recently, <cite>Yang et al. (2019)</cite> showed that combining a BERT-based reader with passage retrieval using the Anserini IR toolkit yields a large improvement in question answering directly from a Wikipedia corpus, measured in terms of exact match on a standard benchmark (Chen et al., 2017) . Interestingly, the approach of <cite>Yang et al. (2019)</cite> represents a simple method to combining BERT with off-the-shelf IR. In this paper, we build on these initial successes to explore how much further we can push this simple architecture by data augmentation, taking advantage of distant supervision techniques to gather more and higher-quality * equal contribution training data to fine tune BERT. Experiments show that, using the same reader model as <cite>Yang et al. (2019)</cite> , our simple data-augmentation techniques yield additional large improvements.",
  "y": "background"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_1",
  "x": "Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets. ---------------------------------- **INTRODUCTION** BERT (Devlin et al., 2018) represents the latest refinement in a series of neural models that take advantage of pretraining on a language modeling task (Peters et al., 2018; Radford et al., 2018) . Researchers have demonstrated impressive gains in a broad range of NLP tasks, from sentence classification to sequence labeling. Recently, <cite>Yang et al. (2019)</cite> showed that combining a BERT-based reader with passage retrieval using the Anserini IR toolkit yields a large improvement in question answering directly from a Wikipedia corpus, measured in terms of exact match on a standard benchmark (Chen et al., 2017) . Interestingly, the approach of <cite>Yang et al. (2019)</cite> represents a simple method to combining BERT with off-the-shelf IR. In this paper, we build on these initial successes to explore how much further we can push this simple architecture by data augmentation, taking advantage of distant supervision techniques to gather more and higher-quality * equal contribution training data to fine tune BERT. Experiments show that, using the same reader model as <cite>Yang et al. (2019)</cite> , our simple data-augmentation techniques yield additional large improvements.",
  "y": "background"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_2",
  "x": "**INTRODUCTION** BERT (Devlin et al., 2018) represents the latest refinement in a series of neural models that take advantage of pretraining on a language modeling task (Peters et al., 2018; Radford et al., 2018) . Researchers have demonstrated impressive gains in a broad range of NLP tasks, from sentence classification to sequence labeling. Recently, <cite>Yang et al. (2019)</cite> showed that combining a BERT-based reader with passage retrieval using the Anserini IR toolkit yields a large improvement in question answering directly from a Wikipedia corpus, measured in terms of exact match on a standard benchmark (Chen et al., 2017) . Interestingly, the approach of <cite>Yang et al. (2019)</cite> represents a simple method to combining BERT with off-the-shelf IR. In this paper, we build on these initial successes to explore how much further we can push this simple architecture by data augmentation, taking advantage of distant supervision techniques to gather more and higher-quality * equal contribution training data to fine tune BERT. Experiments show that, using the same reader model as <cite>Yang et al. (2019)</cite> , our simple data-augmentation techniques yield additional large improvements. To illustrate the robustness of our methods, we also demonstrate consistent gains on another English QA dataset and present baselines for two additional Chinese QA datasets (which have not to date been evaluated in an \"end-to-end\" manner). In addition to achieving state-of-the-art results, we contribute important lessons on how to leverage BERT effectively for question answering.",
  "y": "extends"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_3",
  "x": "Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets. ---------------------------------- **INTRODUCTION** BERT (Devlin et al., 2018) represents the latest refinement in a series of neural models that take advantage of pretraining on a language modeling task (Peters et al., 2018; Radford et al., 2018) . Researchers have demonstrated impressive gains in a broad range of NLP tasks, from sentence classification to sequence labeling. Recently, <cite>Yang et al. (2019)</cite> showed that combining a BERT-based reader with passage retrieval using the Anserini IR toolkit yields a large improvement in question answering directly from a Wikipedia corpus, measured in terms of exact match on a standard benchmark (Chen et al., 2017) . Interestingly, the approach of <cite>Yang et al. (2019)</cite> represents a simple method to combining BERT with off-the-shelf IR. In this paper, we build on these initial successes to explore how much further we can push this simple architecture by data augmentation, taking advantage of distant supervision techniques to gather more and higher-quality * equal contribution training data to fine tune BERT. Experiments show that, using the same reader model as <cite>Yang et al. (2019)</cite> , our simple data-augmentation techniques yield additional large improvements.",
  "y": "uses differences"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_4",
  "x": "---------------------------------- **APPROACH** In this work, we fix the underlying model and focus on data augmentation techniques to explore how to best fine-tune BERT. We use the same exact setup as the \"paragraph\" variant of BERTserini<cite> (Yang et al., 2019)</cite> , where the input corpus is pre-segmented into paragraphs at index time, each of which is treated as a \"document\" for retrieval purposes. The question is used as a \"bag of words\" query to retrieve the top k candidate paragraphs using BM25 ranking. Each paragraph is then fed into the BERT reader along with the original natural language question for inference. Our reader is built using Google's reference implementation, but with a small tweak: to allow comparison and aggregation of results from different segments, we remove the final softmax layer over different answer spans; cf. . For each candidate paragraph, we apply inference over the entire paragraph, and the reader selects the best text span and provides a score. We then combine the reader score with the retriever score via linear interpolation: S = (1 \u2212 \u00b5) \u00b7 S Anserini + \u00b5 \u00b7 S BERT , where \u00b5 \u2208 [0, 1] is a hyperparameter (tuned on a training sample).",
  "y": "uses"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_5",
  "x": "We use the same exact setup as the \"paragraph\" variant of BERTserini<cite> (Yang et al., 2019)</cite> , where the input corpus is pre-segmented into paragraphs at index time, each of which is treated as a \"document\" for retrieval purposes. The question is used as a \"bag of words\" query to retrieve the top k candidate paragraphs using BM25 ranking. Each paragraph is then fed into the BERT reader along with the original natural language question for inference. Our reader is built using Google's reference implementation, but with a small tweak: to allow comparison and aggregation of results from different segments, we remove the final softmax layer over different answer spans; cf. . For each candidate paragraph, we apply inference over the entire paragraph, and the reader selects the best text span and provides a score. We then combine the reader score with the retriever score via linear interpolation: S = (1 \u2212 \u00b5) \u00b7 S Anserini + \u00b5 \u00b7 S BERT , where \u00b5 \u2208 [0, 1] is a hyperparameter (tuned on a training sample). One major shortcoming with BERTserini is that <cite>Yang et al. (2019)</cite> only fine tune on SQuAD, which means that the BERT reader is exposed to an impoverished set of examples; all SQuAD data come from a total of only 442 documents. This contrasts with the diversity of paragraphs that the model will likely encounter at inference time, since they are selected from potentially millions of articles. The solution to this problem, of course, is to fine tune BERT with the types of paragraphs it is likely to see at inference time.",
  "y": "differences"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_6",
  "x": "We also examine two Chinese datasets: CMRC (Cui et al., 2018) and DRCD (Shao et al., 2018) . For these, we use the 2018-12-01 dump of Chinese Wikipedia, tokenized with Lucene's CJKAnalyzer into overlapping bigrams. We apply hanziconv 1 to transform the corpus into simplified characters for CMRC and traditional characters for DRCD. Following <cite>Yang et al. (2019)</cite> , to evaluate answers in an end-to-end setup, we disregard the paragraph context from the original datasets and use only the answer spans. As in previous work, exact match (EM) score and F 1 score (at the token level) serve as the two primary evaluation metrics. In addition, we compute recall (R), the fraction of questions for which the correct answer appears in any retrieved paragraph; to make our results comparable to <cite>Yang et al. (2019)</cite> , Anserini returns the top k = 100 paragraphs to feed into the BERT reader. Note that this recall is not the same as the token-level recall component in the F 1 score. Statistics for the datasets are shown in Table 4. 2 For data augmentation, based on preliminary experiments, we find that examining n = 10 candidates from passage retrieval works well, and we further discover that effectiveness is insensitive to the amount of negative samples. Thus, we eliminate the need to tune d by simply using all passages that do not contain the answer as negative examples.",
  "y": "uses"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_7",
  "x": "As in previous work, exact match (EM) score and F 1 score (at the token level) serve as the two primary evaluation metrics. In addition, we compute recall (R), the fraction of questions for which the correct answer appears in any retrieved paragraph; to make our results comparable to <cite>Yang et al. (2019)</cite> , Anserini returns the top k = 100 paragraphs to feed into the BERT reader. Note that this recall is not the same as the token-level recall component in the F 1 score. Statistics for the datasets are shown in Table 4. 2 For data augmentation, based on preliminary experiments, we find that examining n = 10 candidates from passage retrieval works well, and we further discover that effectiveness is insensitive to the amount of negative samples. Thus, we eliminate the need to tune d by simply using all passages that do not contain the answer as negative examples. The second block of Table 4 shows the sizes of the augmented datasets constructed using our distant supervision techniques: DS(+) contains positive examples only, while DS(\u00b1) includes both positive and negative examples. There are two additional characteristics to note about our data augmentation techniques: The most salient characteristic is that SQuAD, CMRC, and DRCD all have source answers drawn from Wikipedia (English or Chinese), while TriviaQA includes web pages as well as Wikipedia. Therefore, for the first three collections, the source and augmented datasets share the same document genre-the primary difference is that data augmentation increases the amount and diversity of answer passages seen by the model during training. For TriviaQA, however, we consider the source and augmented datasets as coming from different genres (noisy web text vs. higher quality Wikipedia articles).",
  "y": "similarities"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_8",
  "x": "All inputs to the model are padded to 384 tokens; the learning rate is set to 3 \u00d7 10 \u22125 and all other defaults settings are used. ---------------------------------- **RESULTS** Our main results on SQuAD are shown in Table 2 . The row marked \"SRC\" indicates fine tuning with SQuAD data only and matches the BERTserini condition of <cite>Yang et al. (2019)</cite> ; we report higher scores due to engineering improvements (primarily a Lucene version upgrade). As expected, fine tuning with augmented data improves effectiveness, and experiments show that while training with positive examples using DS(+) definitely (Wang et al., 2017) 29.1 37.5 - Kratzwald and Feuerriegel (2018) 29.8 --Par. R. 28.5 -83.1 Par. R. + Answer Agg. 28.9 --Par.",
  "y": "differences"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_0",
  "x": "In addition, the authors cite the use of networks in applications aiming at holistic typology and stylistic variations. In this context, I will discuss some possible directions that could be followed in future research directed towards the understanding of language via topological characterization of complex linguistic networks. In addition, I will comment the use of network models for language processing applications. Additional prospects for future practical research lines will also be discussed in this comment. The topological analysis of complex textual networks has been widely studied in the recent years. As for cooccurrence networks of characters, it was possible to verify that they follow the scale-free and small-world features [4] . Co-occurrence networks of words (or adjacency networks) have accounted for most of the models tackling textual applications. In special, they have been more prevalent than syntactical networks because they represent a simplified representation of the complex syntactical analysis [7, 8] , as most of the syntactical links occur between neighboring words. Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition <cite>[9]</cite> , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] .",
  "y": "background"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_1",
  "x": "The topological analysis of complex textual networks has been widely studied in the recent years. As for cooccurrence networks of characters, it was possible to verify that they follow the scale-free and small-world features [4] . Co-occurrence networks of words (or adjacency networks) have accounted for most of the models tackling textual applications. In special, they have been more prevalent than syntactical networks because they represent a simplified representation of the complex syntactical analysis [7, 8] , as most of the syntactical links occur between neighboring words. Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition <cite>[9]</cite> , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies [8, <cite>9</cite> ] that novel topological measurements should be introduced to capture a wider range of linguistic features.",
  "y": "motivation future_work background"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_2",
  "x": "Co-occurrence networks of words (or adjacency networks) have accounted for most of the models tackling textual applications. In special, they have been more prevalent than syntactical networks because they represent a simplified representation of the complex syntactical analysis [7, 8] , as most of the syntactical links occur between neighboring words. Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition <cite>[9]</cite> , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies [8, <cite>9</cite> ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ). However, when deep analyzes are performed, network-based strategies usually do not perform better than other techniques making extensive use of semantic resources and tools.",
  "y": "motivation"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_3",
  "x": "Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition <cite>[9]</cite> , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies [8, <cite>9</cite> ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ). However, when deep analyzes are performed, network-based strategies usually do not perform better than other techniques making extensive use of semantic resources and tools. In order to improve the performance of network-based applications, I suggest a twofold research line: (i) the introduction of measurements consistent with the nature of the problem; and (ii) the combination of topological strategies with other traditional natural language processing methods. More specifically, in (i), I propose 1 E-mail:diego.raphael@gmail.com, diego@icmc.usp.br",
  "y": "future_work motivation"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_4",
  "x": "In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in <cite>[9]</cite> , [19] and [20]. In sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels. Despite the limitations imposed by the restrict understanding of the mechanisms behind the classification, it is worth noting that the such representation remains entirely generic, being therefore useful to many tasks as well as for analyzing the evolution of languages, cultures and emotional trends. For this reason, I believe that the use of complex networks in both practical and theoretical investigations shall yield novels insights into the mechanisms behind the language. ---------------------------------- **** the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors [8] . Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [17] ), I believe that the creation of novel semantic-based measurements would improve the state of the art.",
  "y": "background"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_5",
  "x": "Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in <cite>[9]</cite> , [19] and [20]. In sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels. Despite the limitations imposed by the restrict understanding of the mechanisms behind the classification, it is worth noting that the such representation remains entirely generic, being therefore useful to many tasks as well as for analyzing the evolution of languages, cultures and emotional trends. For this reason, I believe that the use of complex networks in both practical and theoretical investigations shall yield novels insights into the mechanisms behind the language. ---------------------------------- **** the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors [8] .",
  "y": "uses"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_6",
  "x": "---------------------------------- **** the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors [8] . Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [17] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in <cite>[9]</cite> , [19] and [20] . In sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels. Despite the limitations imposed by the restrict understanding of the mechanisms behind the classification, it is worth noting that the such representation remains entirely generic, being therefore useful to many tasks as well as for analyzing the evolution of languages, cultures and emotional trends.",
  "y": "background"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_7",
  "x": "For this reason, I believe that the use of complex networks in both practical and theoretical investigations shall yield novels insights into the mechanisms behind the language. ---------------------------------- **** the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors [8] . Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [17] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in <cite>[9]</cite> , [19] and [20] . In sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels.",
  "y": "uses"
 },
 {
  "id": "f856c4fb5e6e00729d33b15b24aff6_0",
  "x": "They must count on their interlocutors to recognize the background knowledge they presuppose by general inference from the logic of their behavior as a cooperative contribution to the task (Thomason et al., 2006) . Such reasoning becomes particularly important in problematic cases, such as when systems must finetune the form and meaning of a clarification request so that the response is more likely to resolve a pending task ambiguity (DeVault and Stone, 2007) . I expect many further exciting developments in our understanding of meaning and interpretation as we enrich the social intelligence of NLG. Modeling efforts will remain crucial to the exploration of these new capabilities. When we build and assemble models of actions and interpretations, we get systems that can plan their own behavior simply by exploiting what they know about communication. These systems give new evidence about the information and problem-solving that's involved. The challenge is that these models must describe semantics and pragmatics, as well as syntax and behavior. My own slow progress <cite>(Cassell et al., 2000</cite>; Koller and Stone, 2007) shows that there's still lots of hard work needed to develop suitable techniques. I keep going because of the methodological payoffs I see on the horizon.",
  "y": "motivation future_work"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_0",
  "x": "**INTRODUCTION** The field of Neural Machine Translation (NMT), which seeks to use end-to-end neural networks to translate natural language text, has existed for only three years. In that time, researchers have explored architectures ranging from convolutional neural networks (Kalchbrenner and Blunsom, 2013) to recurrent neural networks (Chung et al., 2014) to attentional models (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> and achieved better performance than traditional statistical or syntax-based MT techniques on many language pairs. NMT models first achieved state-of-the-art performance on the WMT English\u2192German news-domain task in 2015<cite> (Luong et al., 2015)</cite> and subsequent improvements have been reported since then (Sennrich et al., 2015a; Li and Jurafsky, 2016) . The problem of machine translation is fundamentally a sequence-to-sequence transduction task, and most approaches have been based on an encoder-decoder architecture (Sutskever et al., 2014; . This entails coupled neural networks that encode the input sentence into a vector or set of vectors and decode that vector representation into an output sentence in a different language respectively. Recently, a third component has been added to many of these models: an attention mechanism, whereby the decoder can attend directly to localized information from the input sentence during the output generation process (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> . The encoder and decoder in these models typically consist of one-layer or multi-layer recurrent neural networks (RNNs); we use four-and five-layer long short-term memory (LSTM) RNNs. The attention mechanism in our four-layer model is what <cite>Luong (2015)</cite> describes as \"Global attention (dot)\"; the mechanism in our five-layer Y-LSTM model is described in Section 2.1.",
  "y": "background"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_1",
  "x": "The field of Neural Machine Translation (NMT), which seeks to use end-to-end neural networks to translate natural language text, has existed for only three years. In that time, researchers have explored architectures ranging from convolutional neural networks (Kalchbrenner and Blunsom, 2013) to recurrent neural networks (Chung et al., 2014) to attentional models (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> and achieved better performance than traditional statistical or syntax-based MT techniques on many language pairs. NMT models first achieved state-of-the-art performance on the WMT English\u2192German news-domain task in 2015<cite> (Luong et al., 2015)</cite> and subsequent improvements have been reported since then (Sennrich et al., 2015a; Li and Jurafsky, 2016) . The problem of machine translation is fundamentally a sequence-to-sequence transduction task, and most approaches have been based on an encoder-decoder architecture (Sutskever et al., 2014; . This entails coupled neural networks that encode the input sentence into a vector or set of vectors and decode that vector representation into an output sentence in a different language respectively. Recently, a third component has been added to many of these models: an attention mechanism, whereby the decoder can attend directly to localized information from the input sentence during the output generation process (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> . The encoder and decoder in these models typically consist of one-layer or multi-layer recurrent neural networks (RNNs); we use four-and five-layer long short-term memory (LSTM) RNNs. The attention mechanism in our four-layer model is what <cite>Luong (2015)</cite> describes as \"Global attention (dot)\"; the mechanism in our five-layer Y-LSTM model is described in Section 2.1. Every NMT system must contend with the problem of unbounded output vocabulary: systems that restrict possible output words to the most common 50,000 or 100,000 that can fit comfortably in a softmax classifier will perform poorly due to large numbers of \"out-of-vocabulary\" or \"unknown\" outputs.",
  "y": "background"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_2",
  "x": "The field of Neural Machine Translation (NMT), which seeks to use end-to-end neural networks to translate natural language text, has existed for only three years. In that time, researchers have explored architectures ranging from convolutional neural networks (Kalchbrenner and Blunsom, 2013) to recurrent neural networks (Chung et al., 2014) to attentional models (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> and achieved better performance than traditional statistical or syntax-based MT techniques on many language pairs. NMT models first achieved state-of-the-art performance on the WMT English\u2192German news-domain task in 2015<cite> (Luong et al., 2015)</cite> and subsequent improvements have been reported since then (Sennrich et al., 2015a; Li and Jurafsky, 2016) . The problem of machine translation is fundamentally a sequence-to-sequence transduction task, and most approaches have been based on an encoder-decoder architecture (Sutskever et al., 2014; . This entails coupled neural networks that encode the input sentence into a vector or set of vectors and decode that vector representation into an output sentence in a different language respectively. Recently, a third component has been added to many of these models: an attention mechanism, whereby the decoder can attend directly to localized information from the input sentence during the output generation process (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> . The encoder and decoder in these models typically consist of one-layer or multi-layer recurrent neural networks (RNNs); we use four-and five-layer long short-term memory (LSTM) RNNs. The attention mechanism in our four-layer model is what <cite>Luong (2015)</cite> describes as \"Global attention (dot)\"; the mechanism in our five-layer Y-LSTM model is described in Section 2.1. Every NMT system must contend with the problem of unbounded output vocabulary: systems that restrict possible output words to the most common 50,000 or 100,000 that can fit comfortably in a softmax classifier will perform poorly due to large numbers of \"out-of-vocabulary\" or \"unknown\" outputs.",
  "y": "background"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_3",
  "x": "The field of Neural Machine Translation (NMT), which seeks to use end-to-end neural networks to translate natural language text, has existed for only three years. In that time, researchers have explored architectures ranging from convolutional neural networks (Kalchbrenner and Blunsom, 2013) to recurrent neural networks (Chung et al., 2014) to attentional models (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> and achieved better performance than traditional statistical or syntax-based MT techniques on many language pairs. NMT models first achieved state-of-the-art performance on the WMT English\u2192German news-domain task in 2015<cite> (Luong et al., 2015)</cite> and subsequent improvements have been reported since then (Sennrich et al., 2015a; Li and Jurafsky, 2016) . The problem of machine translation is fundamentally a sequence-to-sequence transduction task, and most approaches have been based on an encoder-decoder architecture (Sutskever et al., 2014; . This entails coupled neural networks that encode the input sentence into a vector or set of vectors and decode that vector representation into an output sentence in a different language respectively. Recently, a third component has been added to many of these models: an attention mechanism, whereby the decoder can attend directly to localized information from the input sentence during the output generation process (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> . The encoder and decoder in these models typically consist of one-layer or multi-layer recurrent neural networks (RNNs); we use four-and five-layer long short-term memory (LSTM) RNNs. The attention mechanism in our four-layer model is what <cite>Luong (2015)</cite> describes as \"Global attention (dot)\"; the mechanism in our five-layer Y-LSTM model is described in Section 2.1. Every NMT system must contend with the problem of unbounded output vocabulary: systems that restrict possible output words to the most common 50,000 or 100,000 that can fit comfortably in a softmax classifier will perform poorly due to large numbers of \"out-of-vocabulary\" or \"unknown\" outputs.",
  "y": "similarities uses"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_4",
  "x": "Even models that can produce every word found in the training corpus for the target language (Jean et al., 2015) may be unable to output words found only in the test corpus. There are three main techniques for achieving fully open-ended decoder output. Models may use computed alignments between source and target sentences to directly copy or transform a word from the input sentence whose corresponding translation is not present in the vocabulary<cite> (Luong et al., 2015)</cite> or they may conduct sentence tokenization at the level of individual characters (Ling et al., 2015) or subword units such as morphemes (Sennrich et al., 2015b) . The latter techniques allow the decoder to construct words it has not previously encountered out of known characters or morphemes; we apply the subword splitting strategy using Morfessor 2.0, an unsupervised morpheme segmentation model (Virpioja et al., 2013) . Another focus of recent research has been ways of using monolingual corpus data, available in much larger quantities, to augment the limited parallel corpora used to train translation models. One way to accomplish this is to train a separate monolingual language model on a large corpus of the target language, then use this language model as an additional input to the decoder or for re-ranking output translations (G\u00fcl\u00e7ehre et al., 2015) . More recently, Sennrich (2015b) introduced the concept of augmentation through back-translation, where an entirely separate translation model is trained on a parallel corpus from the target language to the source language. This backwards translation model is then used to machine-translate a monolingual corpus from the target language into the source language, producing a pseudo-parallel corpus to augment the original parallel training corpus. We extend this back-translation method by translating a very large monolingual German corpus into English, then concatenating a unique subset of this augmentation corpus to the original parallel corpus for each training epoch.",
  "y": "background"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_5",
  "x": "The model identified as metamind-single is based on the attention-based encoder-decoder framework described in <cite>Luong (2015)</cite> , using the attention mechanism referred to as \"Global attention (dot). \" The encoder is a four-layer stacked LSTM recurrent neural network whose inputs (at the bottom layer) are vectors w in t corresponding to the subword units in the input sentence and which saves the topmost output state at each timestep e t as the variable-length encoding matrix E. The decoder also contains a four-layer stacked LSTM whose states (c 0 and h 0 for each layer) are initialized to the last states for each layer of the encoder. At the first timestep, the decoder LSTM receives as input an initialization word vector w out 0 ; its topmost output state h t is concatenated with an encoder context vector \u03ba t computed as: score(h t , e s ) = h t e s \u03b1 st = softmax all s (score(h t , e s )) This concatenated output is then fed through an additional neural network layer to produce a final attentional output vectorh, which serves as input to the output softmax:h output probabilities = softmax(W outh ) For subsequent timesteps, the decoder LSTM receives as input the previous word vector w out t\u22121 concatenated with the previous output vectorh. Decoding is performed using beam search, with beam width 16. The beam search decoder differs slightly from <cite>Luong (2015)</cite> in that we normalize output sentence probabilities by length, following , rather than performing ad-hoc adjustments to correct for short output sentences. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_6",
  "x": "The model identified as metamind-single is based on the attention-based encoder-decoder framework described in <cite>Luong (2015)</cite> , using the attention mechanism referred to as \"Global attention (dot). \" The encoder is a four-layer stacked LSTM recurrent neural network whose inputs (at the bottom layer) are vectors w in t corresponding to the subword units in the input sentence and which saves the topmost output state at each timestep e t as the variable-length encoding matrix E. The decoder also contains a four-layer stacked LSTM whose states (c 0 and h 0 for each layer) are initialized to the last states for each layer of the encoder. At the first timestep, the decoder LSTM receives as input an initialization word vector w out 0 ; its topmost output state h t is concatenated with an encoder context vector \u03ba t computed as: score(h t , e s ) = h t e s \u03b1 st = softmax all s (score(h t , e s )) This concatenated output is then fed through an additional neural network layer to produce a final attentional output vectorh, which serves as input to the output softmax:h output probabilities = softmax(W outh ) For subsequent timesteps, the decoder LSTM receives as input the previous word vector w out t\u22121 concatenated with the previous output vectorh. Decoding is performed using beam search, with beam width 16. The beam search decoder differs slightly from <cite>Luong (2015)</cite> in that we normalize output sentence probabilities by length, following , rather than performing ad-hoc adjustments to correct for short output sentences. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_7",
  "x": "The run submitted as metamind-ylstm uses a single snapshot of this model after 9 total training epochs. The run submitted as metamind-ensemble uses an equally-weighted ensemble of three snapshots of the metamind-single model (after 10, 11, and 12 epochs) and a single snapshot of the metamind-ylstm model after 9 total training epochs. ---------------------------------- **RESULTS** Results for all three runs described above are presented in Table 1 . Only the ensemble was submitted to the human evaluation process, with a final ranking of second place (behind U. Edinburgh's ensemble of four independently initialized models). Our best single model matches the performance of the best model from U. Edinburgh, which applies a similar attentional framework, subword splitting, and back-translated augmentation. The Y-LSTM model underperformed relative to the model based on <cite>Luong (2015)</cite> , but provided a small additional boost to the ensemble. The primary contribution of this model is to demonstrate that purely attentional NMT is possible: the only inputs to the decoder are through the attention mechanism.",
  "y": "differences"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_0",
  "x": "In this paper, we provide a review of two representative efforts on this topic and propose the novel concept of fine-grained disentangled speech representation learning. ---------------------------------- **INTRODUCTION** Representation learning is a fundamental challenge in machine learning and artificial intelligence. While there are multiple criteria for an ideal representation, disentangled representation (illustrated in Figure 1 ), which explicitly separates the underlying causal factors of the observed data, has been of particular interest, because it can be useful for a large variety of tasks and domains [1, 2, 3, 4, 5] . For example, in [5] , the authors show that learning disentangling latent factors corresponding to pose and identity in photos of human faces can improve the performance of both pose estimation and face verification. Learning disentangled representation from high-dimensional data is not a trivial task and multiple techniques, such as \u03b2-VAE [1] , Info-GAN [2] , and DC-IGN [3] , have been developed to address this problem. While disentangling natural image representation has been studied extensively, much less work has focused on natural speech, leaving a rather large void in the understanding of this problem. In this paper, we first present a short review and comparison of two representative efforts on this topic [<cite>6</cite>, 7] , where both efforts involve using an auto-encoder and can be applied to the same task (i.e., voice conversion), but the key disentangling algorithms and underlying ideas are very different.",
  "y": "uses background"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_1",
  "x": "While disentangling natural image representation has been studied extensively, much less work has focused on natural speech, leaving a rather large void in the understanding of this problem. In this paper, we first present a short review and comparison of two representative efforts on this topic [<cite>6</cite>, 7] , where both efforts involve using an auto-encoder and can be applied to the same task (i.e., voice conversion), but the key disentangling algorithms and underlying ideas are very different. In [<cite>6</cite>] , the authors proposed an unsupervised <cite>factorized hierarchical variational autoencoder (FHVAE)</cite>. The key idea is that assuming that the speech data is generated from two separate latent variable sets z1 and z2, where z1 contains segment-level (short-term) variables and z2 contains sequencelevel (long-term) variables (z2 that are further conditioned on an s-vector \u00b52). Leveraging the multi-scale nature that different factors affect speech at different time scales (e.g., speaker identity affects the fundamental frequency and volume of speech signal at the sequence level while the phonetic content affects the speech signal at the segment level), by training an autoencoder in a sequence-to-sequence manner, z1 can be forced to encode segment-level information (e.g., speech content), while z2 and \u00b52 can be forced to encode sequence-level information (e.g., speaker identity). In the experiments, by keeping z2 fixed and changing z1, speech of the same content, but by different speakers, can be synthesized naturally, demonstrating the clean separation between content and speaker information. Further, the learned s-vector is shown to be a stronger feature than the conventional i-vector in the speaker verification task, demonstrating that it encodes speaker-level information well. In [<cite>6</cite>] and subsequent efforts [8, 9, 10] , the authors further showed that the disentangled representation is also helpful in the speech recognition task. These efforts convey two primary insights: 1) by adding the appropriate prior assumptions on the latent variables, speech content information and speaker-level information can be separated out in an unsupervised learning manner; 2) the learned disentangled representations are useful to improve both speech synthesis and broader inference tasks.",
  "y": "background"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_2",
  "x": "Further, the learned s-vector is shown to be a stronger feature than the conventional i-vector in the speaker verification task, demonstrating that it encodes speaker-level information well. In [<cite>6</cite>] and subsequent efforts [8, 9, 10] , the authors further showed that the disentangled representation is also helpful in the speech recognition task. These efforts convey two primary insights: 1) by adding the appropriate prior assumptions on the latent variables, speech content information and speaker-level information can be separated out in an unsupervised learning manner; 2) the learned disentangled representations are useful to improve both speech synthesis and broader inference tasks. Different from [<cite>6</cite>] , in [7] , the authors propose a supervised approach based on adversarial training [11, 12, 13, 14] (illustrated in Figure 2 (left)). In addition to a regular autoencoder, the authors add a regularization term in its objective function to force the latent variables (i.e., the encoding) to not contain speaker information. This is done by introducing an auxiliary speaker verification classifier C. C is trained to correctly identify the speaker y from the latent variables z (i.e., minimizing the misclassification loss Lc = \u2212logP (y|z)), while the encoder is trained to maximize Lc, i.e., to avoid encoding speaker information in z. Both z and speaker label y are fed to the decoder for reconstruction, and the complete objective function of the auto-encoder is hence minimizing Lrec \u2212 \u03bbLc (where Lrec is the point-wise L1-norm loss). By alternatively training the auto-encoder and C, the z is learned to be an encoding of speech content information. Further, the residual information of the speech is reconstructed through another GAN and auxiliary classifier. The experiment shows that such a scheme can be successfully applied to a voice conversion task.",
  "y": "background"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_3",
  "x": "In the experiments, by keeping z2 fixed and changing z1, speech of the same content, but by different speakers, can be synthesized naturally, demonstrating the clean separation between content and speaker information. Further, the learned s-vector is shown to be a stronger feature than the conventional i-vector in the speaker verification task, demonstrating that it encodes speaker-level information well. In [<cite>6</cite>] and subsequent efforts [8, 9, 10] , the authors further showed that the disentangled representation is also helpful in the speech recognition task. These efforts convey two primary insights: 1) by adding the appropriate prior assumptions on the latent variables, speech content information and speaker-level information can be separated out in an unsupervised learning manner; 2) the learned disentangled representations are useful to improve both speech synthesis and broader inference tasks. Different from [<cite>6</cite>] , in [7] , the authors propose a supervised approach based on adversarial training [11, 12, 13, 14] (illustrated in Figure 2 (left)). In addition to a regular autoencoder, the authors add a regularization term in its objective function to force the latent variables (i.e., the encoding) to not contain speaker information. This is done by introducing an auxiliary speaker verification classifier C. C is trained to correctly identify the speaker y from the latent variables z (i.e., minimizing the misclassification loss Lc = \u2212logP (y|z)), while the encoder is trained to maximize Lc, i.e., to avoid encoding speaker information in z. Both z and speaker label y are fed to the decoder for reconstruction, and the complete objective function of the auto-encoder is hence minimizing Lrec \u2212 \u03bbLc (where Lrec is the point-wise L1-norm loss). By alternatively training the auto-encoder and C, the z is learned to be an encoding of speech content information. Further, the residual information of the speech is reconstructed through another GAN and auxiliary classifier.",
  "y": "differences"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_4",
  "x": "Such a constraint can be a prior assumption (in the unsupervised case) or a regularization term (in the supervised case). While both efforts show good empirical performance in real tasks and lay the groundwork for future efforts, the learned disentangled representation is relatively coarse-grained. That is, in [<cite>6</cite>] , z1 and z2 are in fact corresponding to general fast-changing and slow-changing information, i.e., z1 may contain other fast-changing information such as emotion, while z2 may contain slow-changing factors such as background and channel noise. In [7] , the authors actually separate out speaker information and general non-speaker information, which may contain a lot of detailed information. Coarse-grained disentangled representations are enough for some tasks, such as voice speaker conversion, but might be limited for other tasks. Next, we discuss the need for (and benefits of) fine-grained disentangled representations. ---------------------------------- **FINE-GRAINED DISENTANGLED SPEECH REPRESENTATION** Natural speech signal inherently contains both linguistic and paralinguistic information; common paralinguistic information include gender, age, health status, personality, friendliness, mood, and emotion (sorted from long-term to short-term) [15] .",
  "y": "background"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_5",
  "x": "Natural speech signals can be viewed as produced by multiple finegrained causal factors and disentangling these factors leads to the following benefits: Synthesis: Learning fine-grained disentangled representation can help more flexible speech synthesis. Assume that disentangled latent variables corresponding to age, personality, friendliness, emotion, and content are learned; we may then be able to synthesize speech signals corresponding to arbitrary combinations of these factors, according to the requirement of the application scenario. Further, this may support novel AI applications, such as speech style transfer and predicting the future voice of a given subject (similar technology has been adopted in computer vision, e.g., image style transfer [16] and face aging [17] ). In contrast, a coarse-grained disentangled representation [<cite>6</cite>, 7] may only support a simple voice speaker conversion task. Inference: Learning fine-grained disentangled representation can also help with more accurate inference and reasoning. When we attempt to predict one target variable, we usually want to eliminate the interference of other factors. For example, a speech recognition system is expected to be emotionindependent, while a speech emotion recognition system is expected to be text-independent. Historically, some manually designed algorithms are used to eliminate the effects of unrelated factors, e.g., speaker normalization [18, 19] and speaker adaptation [20, 21] are commonly used to eliminate the impacts of speaker variability. However, it is difficult to manually design algorithms for all underlying factors.",
  "y": "differences"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_0",
  "x": "In addition, conclusions from different reports can be contradictory. For example, most work observes that stochastic gradient descent (SGD) gives best performance on NER task (<cite>Chiu and Nichols, 2016;</cite> Lample et al., 2016; Ma and Hovy, 2016) , while Reimers and Gurevych (2017b) report that SGD is the worst optimizer on the same datasets. The comparison between different deep neural models is challenging due to sensitivity on experimental settings. We list six inconsistent configurations in literature, which lead to difficulties for fair comparison. \u2022 Dataset. Most work reports sequence labeling results on both CoNLL 2003 English NER (Tjong Kim Sang and De Meulder, 2003) and PTB POS (Marcus et al., 1993) datasets (Collobert et al., 2011; Ma and Hovy, 2016) . Ling et al. (2015) give results only on POS dataset, while some papers (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Strubell et al., 2017) report results on the NER dataset only. dos Santos et al. (2015) conducts experiments on NER for Portuguese and Spanish. Most work uses the development set to select hyperparameters (Lample et al., 2016; Ma and Hovy, 2016) , while others add development set into training set (<cite>Chiu and Nichols, 2016</cite>; Peters et al., 2017) .",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_1",
  "x": "The comparison between different deep neural models is challenging due to sensitivity on experimental settings. We list six inconsistent configurations in literature, which lead to difficulties for fair comparison. \u2022 Dataset. Most work reports sequence labeling results on both CoNLL 2003 English NER (Tjong Kim Sang and De Meulder, 2003) and PTB POS (Marcus et al., 1993) datasets (Collobert et al., 2011; Ma and Hovy, 2016) . Ling et al. (2015) give results only on POS dataset, while some papers (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Strubell et al., 2017) report results on the NER dataset only. dos Santos et al. (2015) conducts experiments on NER for Portuguese and Spanish. Most work uses the development set to select hyperparameters (Lample et al., 2016; Ma and Hovy, 2016) , while others add development set into training set (<cite>Chiu and Nichols, 2016</cite>; Peters et al., 2017) . Reimers and Gurevych (2017b) use a smaller dataset (13862 vs 14987 sentences). Different from Ma and Hovy (2016) and Liu et al. (2018) , choose a different data split on the POS dataset.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_2",
  "x": "\u2022 Dataset. Most work reports sequence labeling results on both CoNLL 2003 English NER (Tjong Kim Sang and De Meulder, 2003) and PTB POS (Marcus et al., 1993) datasets (Collobert et al., 2011; Ma and Hovy, 2016) . Ling et al. (2015) give results only on POS dataset, while some papers (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Strubell et al., 2017) report results on the NER dataset only. dos Santos et al. (2015) conducts experiments on NER for Portuguese and Spanish. Most work uses the development set to select hyperparameters (Lample et al., 2016; Ma and Hovy, 2016) , while others add development set into training set (<cite>Chiu and Nichols, 2016</cite>; Peters et al., 2017) . Reimers and Gurevych (2017b) use a smaller dataset (13862 vs 14987 sentences). Different from Ma and Hovy (2016) and Liu et al. (2018) , choose a different data split on the POS dataset. Liu et al. (2018) and Hashimoto et al. (2017) use different development sets for chunking. \u2022 Preprocessing.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_3",
  "x": "Reimers and Gurevych (2017b) use a smaller dataset (13862 vs 14987 sentences). Different from Ma and Hovy (2016) and Liu et al. (2018) , choose a different data split on the POS dataset. Liu et al. (2018) and Hashimoto et al. (2017) use different development sets for chunking. \u2022 Preprocessing. A typical data preprocessing step is to normize digit characters (<cite>Chiu and Nichols, 2016;</cite> Lample et al., 2016; Yang et al., 2016; Strubell et al., 2017) . Reimers and Gurevych (2017b) use fine-grained representations for less frequent words. Ma and Hovy (2016) do not use preprocessing. \u2022 Features. Strubell et al. (2017) and <cite>Chiu and Nichols (2016)</cite> apply word spelling features and further integrate context features.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_4",
  "x": "Ma and Hovy (2016) do not use preprocessing. \u2022 Features. Strubell et al. (2017) and <cite>Chiu and Nichols (2016)</cite> apply word spelling features and further integrate context features. Collobert et al. (2011) and use neural features to represent external gazetteer information. Besides, Lample et al. (2016) and Ma and Hovy (2016) use end-to-end structure without handcrafted features. \u2022 Hyperparameters including learning rate, dropout rate (Srivastava et al., 2014) , number of layers, hidden size etc. can strongly affect the model performance. <cite>Chiu and Nichols (2016)</cite> search for the hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters. However, existing models use different parameter settings, which affects the fair comparison. \u2022 Evaluation.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_5",
  "x": "Ma and Hovy (2016) do not use preprocessing. \u2022 Features. Strubell et al. (2017) and <cite>Chiu and Nichols (2016)</cite> apply word spelling features and further integrate context features. Collobert et al. (2011) and use neural features to represent external gazetteer information. Besides, Lample et al. (2016) and Ma and Hovy (2016) use end-to-end structure without handcrafted features. \u2022 Hyperparameters including learning rate, dropout rate (Srivastava et al., 2014) , number of layers, hidden size etc. can strongly affect the model performance. <cite>Chiu and Nichols (2016)</cite> search for the hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters. However, existing models use different parameter settings, which affects the fair comparison. \u2022 Evaluation.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_6",
  "x": "Besides, Lample et al. (2016) and Ma and Hovy (2016) use end-to-end structure without handcrafted features. \u2022 Hyperparameters including learning rate, dropout rate (Srivastava et al., 2014) , number of layers, hidden size etc. can strongly affect the model performance. <cite>Chiu and Nichols (2016)</cite> search for the hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters. However, existing models use different parameter settings, which affects the fair comparison. \u2022 Evaluation. Some literature reports results using mean and standard deviation under different random seeds (<cite>Chiu and Nichols, 2016</cite>; Peters et al., 2017; Liu et al., 2018) . Others report the best result among different trials (Ma and Hovy, 2016) , which cannot be compared directly. \u2022 Hardware environment can also affect system accuracy. Liu et al. (2018) observes that the system gives better accuracy on NER task when trained using GPU as compared to using CPU.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_7",
  "x": "Strubell et al. (2017) built a deeper dilated CNN architecture to capture larger local features. Hammerton (2003) was the first to exploit LSTM for sequence labeling. built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (Lample et al., 2016; Liu et al., 2018) , GRU (Yang et al., 2016) , and CNN (<cite>Chiu and Nichols, 2016</cite>; Ma and Hovy, 2016) features. Yang et al. (2017a) proposed a neural reranking model to improve NER models. <cite>These models</cite> achieve state-of-the-art results in the literature. Reimers and Gurevych (2017b) compared several word-based LSTM models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value. They investigated the influence of various hyperparameters and configurations. Our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects: 1) Their experiments are based on a BiLSTM with handcrafted word features, while our experiments are based on end-to-end neural models without human knowledge. 2) Their system gives relatively low performances on standard benchmarks 2 , while ours can give comparable or better results with state-of-the-art models, rendering our observations more informative for practitioners.",
  "y": "extends"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_8",
  "x": "Collobert et al. (2011) proposed a seminal neural architecture for sequence labeling. It captures word sequence information with a one-layer CNN based on pretrained word embeddings and handcrafted neural features, followed with a CRF output layer. dos Santos et al. (2015) extended this model by integrating character-level CNN features. Strubell et al. (2017) built a deeper dilated CNN architecture to capture larger local features. Hammerton (2003) was the first to exploit LSTM for sequence labeling. built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (Lample et al., 2016; Liu et al., 2018) , GRU (Yang et al., 2016) , and CNN (<cite>Chiu and Nichols, 2016</cite>; Ma and Hovy, 2016) features. Yang et al. (2017a) proposed a neural reranking model to improve NER models. <cite>These models</cite> achieve state-of-the-art results in the literature. Reimers and Gurevych (2017b) compared several word-based LSTM models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_9",
  "x": "They investigated the influence of various hyperparameters and configurations. Our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects: 1) Their experiments are based on a BiLSTM with handcrafted word features, while our experiments are based on end-to-end neural models without human knowledge. 2) Their system gives relatively low performances on standard benchmarks 2 , while ours can give comparable or better results with state-of-the-art models, rendering our observations more informative for practitioners. 3) Our findings are more consistent with <cite>most previous work</cite> on configurations such as usefulness of character information (Lample et al., 2016; Ma and Hovy, 2016) , optimizer (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Ma and Hovy, 2016) and tag scheme (Ratinov and Roth, 2009; Dai et al., 2015) . In contrast, many results of Reimers and Gurevych (2017b) contradict existing reports. 4) We conduct a wider range of comparison for word sequence representations, including all combinations of character CNN/LSTM and word CNN/LSTM structures, while Reimers and Gurevych (2017b) studied the word LSTM models only. ---------------------------------- **NEURAL SEQUENCE LABELING MODELS** Our neural sequence labeling framework contains three layers, i.e., a character sequence representation layer, a word sequence representation layer and an inference layer, as shown in Figure 1 .",
  "y": "similarities"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_10",
  "x": "4) We conduct a wider range of comparison for word sequence representations, including all combinations of character CNN/LSTM and word CNN/LSTM structures, while Reimers and Gurevych (2017b) studied the word LSTM models only. ---------------------------------- **NEURAL SEQUENCE LABELING MODELS** Our neural sequence labeling framework contains three layers, i.e., a character sequence representation layer, a word sequence representation layer and an inference layer, as shown in Figure 1 . Character information has been proven to be critical for sequence labeling tasks (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Ma and Hovy, 2016) , with LSTM and CNN being used to model character sequence information (\"Char Rep.\"). Similarly, on the word level, LSTM or CNN structures can be leveraged to capture long-term information or local features (\"Word Rep.\"), respectively. Subsequently, the inference layer assigns labels to each word using the hidden states of word sequence representations. ---------------------------------- **CHARACTER SEQUENCE REPRESENTATIONS**",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_11",
  "x": "In this work, we focus on neural character sequence representations without hand-engineered features. Character CNN. Using a CNN structure to encode character sequences was firstly proposed by Santos and Zadrozny (2014), and followed by many subsequent investigations (dos Santos et al., 2015; <cite>Chiu and Nichols, 2016</cite>; Ma and Hovy, 2016) . In our experiments, we take the same structure as Ma and Hovy (2016) , using one layer CNN structure with max-pooling to capture character-level representations. Figure 2 (a) shows the CNN structure on representing word \"Mexico\". Character LSTM. Shown as Figure 2 (b), in order to model the global character sequence information of a word \"Mexico\", we utilize a bidirectional LSTM on the character sequence of each word and concatenate the left-to-right final state F LST M and the right-to-left final state B LST M as character sequence representations. Liu et al. (2018) applied one bidirectional LSTM for the character sequence over a sentence rather than each word individually. We examined both structures and found that they give comparable accuracies on sequence labeling tasks.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_12",
  "x": "Liu et al. (2018) applied one bidirectional LSTM for the character sequence over a sentence rather than each word individually. We examined both structures and found that they give comparable accuracies on sequence labeling tasks. We choose Lample et al. (2016) 's structure as its character LSTMs can be calculated in parallel, making the system more efficient. ---------------------------------- **WORD SEQUENCE REPRESENTATIONS** Similar to character sequences in words, we can model word sequence information through LSTM or CNN structures. LSTM has been widely used in sequence labeling (Lample et al., 2016; Ma and Hovy, 2016; <cite>Chiu and Nichols, 2016;</cite> Liu et al., 2018) . CNN can be much faster than LSTM due to the fact that convolution calculation can be parallel on the input sequence (Collobert et al., 2011; dos Santos et al., 2015; Strubell et al., 2017) . Word CNN.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_13",
  "x": "To reduce the volatility of the system, we conduct each experiment 5 times under different random seeds, and report the max, mean, and standard deviation for each model. ---------------------------------- **RESULTS** Tables 4, 5 and 6 show the results of the twelve models on NER, chunking and POS datasets, respectively. Existing work has also been listed in the tables for comparison. To simplify the description, we use \"CLSTM\" and \"CCNN\" to represent character LSTM and character CNN encoder, respectively. Similarly, \"WLSTM\" and \"WCNN\" represent word LSTM and word CNN structure, respectively. As shown in Table 4 , most NER work focuses on WLSTM+CRF structures with different character sequence representations. We re-implement the structure of several reports (<cite>Chiu and Nichols, 2016</cite>; Ma and Hovy, 2016; Peters et al., 2017) , which take the CCNN+WLSTM+CRF architecture.",
  "y": "uses"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_14",
  "x": "We compare different optimizers including SGD, Adagrad (Duchi et al., 2011 ), Adadelta (Zeiler, 2012 , RMSProp (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2014) . The results are shown in Figure 5 5 . In contrast to Reimers and Gurevych (2017b) , who reported that SGD is the worst optimizer, our results show that SGD outperforms all other optimizers significantly (p < 0.01), with a slower convergence process during training. Our observation is consistent with most literature (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Ma and Hovy, 2016) . ---------------------------------- **ANALYSIS** Decoding speed. We test the decoding speeds of the twelve models on the NER dataset using a Nvidia GTX 1080 GPU. Figure 6 shows the decoding times on 10000 NER sentences.",
  "y": "similarities"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_0",
  "x": "**INTRODUCTION** Unknown words constitute a major source of difficulty for Chinese part-of-speech (POS) tagging, yet relatively little work has been done on POS guessing of Chinese unknown words. The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (Chen et al., 1997; <cite>Wu and Jiang, 2000</cite>; Goh, 2003) . This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models. The rule-based approach was rejected with the claim that rules are bound to overgenerate<cite> (Wu and Jiang, 2000)</cite> . In this paper, we present a hybrid model that combines the strengths of a rule-based model with those of two statistical models for this task. The three models make use of different sources of information. The rule-based model is sensitive to the type, length, and internal structure of unknown words, with overgeneration controlled by additional constraints. The two statistical models make use of contextual information and the likelihood for a character to appear in a particular position of words of a particular length and POS category respectively.",
  "y": "background"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_1",
  "x": "By combining models that use different sources of information, the hybrid model achieves a precision of 89%, a significant improvement over the best result reported in previous studies, which was 69%. ---------------------------------- **INTRODUCTION** Unknown words constitute a major source of difficulty for Chinese part-of-speech (POS) tagging, yet relatively little work has been done on POS guessing of Chinese unknown words. The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (Chen et al., 1997; <cite>Wu and Jiang, 2000</cite>; Goh, 2003) . This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models. The rule-based approach was rejected with the claim that rules are bound to overgenerate<cite> (Wu and Jiang, 2000)</cite> . In this paper, we present a hybrid model that combines the strengths of a rule-based model with those of two statistical models for this task. The three models make use of different sources of information.",
  "y": "background"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_2",
  "x": "This paper describes a hybrid model that combines a rule-based model with two statistical models for the task of POS guessing of Chinese unknown words. The rule-based model is sensitive to the type, length, and internal structure of unknown words, and the two statistical models utilize contextual information and the likelihood for a character to appear in a particular position of words of a particular length and POS category. By combining models that use different sources of information, the hybrid model achieves a precision of 89%, a significant improvement over the best result reported in previous studies, which was 69%. ---------------------------------- **INTRODUCTION** Unknown words constitute a major source of difficulty for Chinese part-of-speech (POS) tagging, yet relatively little work has been done on POS guessing of Chinese unknown words. The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (Chen et al., 1997; <cite>Wu and Jiang, 2000</cite>; Goh, 2003) . This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models. The rule-based approach was rejected with the claim that rules are bound to overgenerate<cite> (Wu and Jiang, 2000)</cite> .",
  "y": "motivation background"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_3",
  "x": "This claim does not fully hold, as the POS information about the component words or morphemes of many unknown words is available in the training lexicon. Second,<cite> Wu and Jiang (2000)</cite> argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will \"result in massive overgeneration\" (p. 48). We will show that overgeneration can be controlled by additional constraints. ---------------------------------- **PROPOSED APPROACH** We propose a hybrid model that combines the strengths of different models to arrive at better results for this task. The models we will consider are a rule-based model, the trigram model, and the statistical model developed by<cite> Wu and Jiang (2000)</cite> . Combination of the three models will be based on the evaluation of their individual performances on the training data. ----------------------------------",
  "y": "background"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_4",
  "x": "First, Chen et al. (1997) claimed that it does not work because the syntactic and semantic information for each character or morpheme is unavailable. This claim does not fully hold, as the POS information about the component words or morphemes of many unknown words is available in the training lexicon. Second,<cite> Wu and Jiang (2000)</cite> argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will \"result in massive overgeneration\" (p. 48). We will show that overgeneration can be controlled by additional constraints. ---------------------------------- **PROPOSED APPROACH** We propose a hybrid model that combines the strengths of different models to arrive at better results for this task. The models we will consider are a rule-based model, the trigram model, and the statistical model developed by<cite> Wu and Jiang (2000)</cite> . Combination of the three models will be based on the evaluation of their individual performances on the training data.",
  "y": "differences background"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_5",
  "x": "Several reasons were suggested for rejecting the rule-based approach. First, Chen et al. (1997) claimed that it does not work because the syntactic and semantic information for each character or morpheme is unavailable. This claim does not fully hold, as the POS information about the component words or morphemes of many unknown words is available in the training lexicon. Second,<cite> Wu and Jiang (2000)</cite> argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will \"result in massive overgeneration\" (p. 48). We will show that overgeneration can be controlled by additional constraints. ---------------------------------- **PROPOSED APPROACH** We propose a hybrid model that combines the strengths of different models to arrive at better results for this task. The models we will consider are a rule-based model, the trigram model, and the statistical model developed by<cite> Wu and Jiang (2000)</cite> .",
  "y": "uses"
 },
 {
  "id": "fddb1d19895976661babdc17d232ee_0",
  "x": "Figure 1 . Sample social media video frames and its headline with visualized importance of their parts on predicted video popularity. The top row shows 5 consecutive frames of a social media video with their attention weights above. We use those weights to scale the magnitudes of Grad-CAM [9] heatmaps when visualizing the importance of video elements on the popularity score. The bottom row shows the frame with the highest attention weight (left) with its popularity importance visualization (right). For the headline text darker color corresponds to higher importance. In this paper, we outline a fundamentally different approach to online video popularity analysis that allows social media creators both to predict video popularity as well as to understand the impact of its headline or video frames on the future popularity. To that end, we propose to use an attention-based model and gradient-weighted class activation maps [9] , inspired by the recent successes of the attention mechanism in other domains [15, <cite>16]</cite> . Although some works focused on understanding the influence of image parts on its popularity [6, 1] , our method addresses videos, not images, and exploits the temporal characteristics of video clips through the attention mechanism.",
  "y": "uses"
 },
 {
  "id": "fddb1d19895976661babdc17d232ee_1",
  "x": "Video frames. We extract N = 18 evenly distributed frames from the first 6 seconds of a video 1 . We use 2048-dimensional output of the penultimate layer of ResNet50 [5] pre-trained on ImageNet [3] to get a high-level frame representation as in [13] . For each frame feature vector we apply a learnable linear transformation followed by ReLU, obtaining a sequence of frame embeddings (q j ) N j=1 . The final video embedding is a weighted average of these embed- Weights \u03b1 i are computed with attention mechanism implemented as a two-layer neural network<cite> [16]</cite> : the first layer produces a hidden representation u i = tanh(W u q i + b u ) and the second layer outputs unnormalized importance a i = W a u i + b a . W a can be interpreted as a trainable high level representation of the most informative vector in u i space. Final weights are normalized with softmax: Headline.",
  "y": "uses"
 },
 {
  "id": "fddb1d19895976661babdc17d232ee_2",
  "x": "We visualize the importance of visual features using Grad-CAM [9] . More precisely, we generate heatmaps pointing to regions contributing to popularity in each frame. To this end, we compute gradients of the popular class score\u015d with respect to the output of the last convolutional layer of ResNet50 A \u2208 R K\u00d7K\u00d7F . Gradients are then used to compute weights \u2202\u015d \u2202A f i,j that applied to the convolutional output create class activation map H = max(0, We then normalize the heatmap values to [0, 1] and use attention weights to 1 We use the first seconds of a video as this is how Facebook counts views, but we can extend our method to longer videos through sampling. scale the heatmap by \u03b1 i / max(\u03b1). This way we obtain a sequence-wide normalized heatmap of frame regions influencing the final popularity score. For visualizations in the text domain, we use attention weights \u03b2 t used to compute text representation d. These weights capture relative importance of words in their context to headline popularity, as shown in<cite> [16]</cite> in the context of sentiment analysis.",
  "y": "uses background"
 },
 {
  "id": "fe1d6ca4a88c03cfb2ae94ef45030d_0",
  "x": "Keyphrases could either be extractive (part of the document) or abstractive. Keyphrase generation is the process of predicting both extractive and abstractive keyphrases from a given document. This process is similar to abstractive summarization but instead of a summary the models generate keyphrases. Researchers have achieved considerable success in the field of abstractive summarization using conditional-GANs (Wang and Lee 2018). There has also been growing interest in deep learning models for keyphrase generation <cite>(Meng et al. 2017</cite>; Chan et al. 2019) . Inspired by these advances, we propose a new GAN architecture for keyphrase generation where the generator produces a sequence of keyphrases from a given document and the discriminator distinguishes between human-curated and machine-generated keyphrases. ---------------------------------- **PROPOSED ADVERSARIAL MODEL** As with most GAN architectures, our model also consists of a generator (G) and discriminator (D), which are trained in an alternating fashion<cite> (Goodfellow et al. 2014)</cite> .",
  "y": "motivation"
 },
 {
  "id": "fe1d6ca4a88c03cfb2ae94ef45030d_1",
  "x": "Keyphrase generation is the process of predicting both extractive and abstractive keyphrases from a given document. This process is similar to abstractive summarization but instead of a summary the models generate keyphrases. Researchers have achieved considerable success in the field of abstractive summarization using conditional-GANs (Wang and Lee 2018). There has also been growing interest in deep learning models for keyphrase generation <cite>(Meng et al. 2017</cite>; Chan et al. 2019) . Inspired by these advances, we propose a new GAN architecture for keyphrase generation where the generator produces a sequence of keyphrases from a given document and the discriminator distinguishes between human-curated and machine-generated keyphrases. ---------------------------------- **PROPOSED ADVERSARIAL MODEL** As with most GAN architectures, our model also consists of a generator (G) and discriminator (D), which are trained in an alternating fashion<cite> (Goodfellow et al. 2014)</cite> . Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org).",
  "y": "uses"
 },
 {
  "id": "fe1d6ca4a88c03cfb2ae94ef45030d_2",
  "x": "**PROPOSED ADVERSARIAL MODEL** As with most GAN architectures, our model also consists of a generator (G) and discriminator (D), which are trained in an alternating fashion<cite> (Goodfellow et al. 2014)</cite> . Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 1 Code is available at https://github.com/avinsit123/keyphrasegan Generator -Given a document d = {x 1 , x 2 , ..., x n }, where x i is the i th token, the generator produces a sequence of keyphrases: y = {y 1 , y 2 , ..., y m }, where each keyphrase y i is composed of tokens y 1 i , y 2 i , ..., y li i . We employ catSeq model<cite> (Yuan et al. 2018)</cite> for the generation process, which uses an encoder-decoder framework: the encoder being a bidirectional Gated Recurrent Unit (bi-GRU) and the decoder a forward GRU. To incorporate the out-of-vocabulary words, we use a copying mechanism (Gu et al. 2016). We also make use of attention mechanism to help the generator identify the relevant components of the source text. Discriminator -We propose a new hierarchical-attention model as the discriminator, which is trained to distinguish between human-curated and machine-generated keyphrases.",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_0",
  "x": "Moreover, we investigate in how far the knowledge gained from these contexts can compensate the lack of large amounts of actually labeled training data in supervised classification by considering various amounts of labeled training sets. ---------------------------------- **RELATED WORK** There has been much research on supervised learning for OH extraction. Choi et al. (2005) explore OH extraction using CRFs with several manually defined linguistic features and automatically learnt surface patterns. The linguistic features focus on named-entity information and syntactic relations to opinion words. Kim and Hovy (2006) and Bethard et al. (2004) examine the usefulness of semantic roles provided by FrameNet 1 for both OH and opinion target extraction. More recently, <cite>Wiegand and Klakow (2010)</cite> explored convolution kernels for OH extraction and found that tree kernels outperform all other kernel types. In (Johansson and Moschitti, 2010) , a re-ranking approach modeling complex relations between multiple opinions in a sentence is presented.",
  "y": "background"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_1",
  "x": "---------------------------------- **DATA** As a large unlabeled (training) corpus, we chose the North American News Text Corpus. As a labeled (test) corpus, we use the MPQA corpus. 2 We use the definition of OHs as described in<cite> (Wiegand and Klakow, 2010)</cite> . The instance space are all noun phrases (NP) in that corpus. ---------------------------------- **METHOD** In this paper, we propose to leverage contextual information from prototypical opinion holders (protoOHs) by which we mean common nouns denoting particular groups of people whose profession or occupation it is to form and express opinions towards specific items.",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_2",
  "x": "---------------------------------- **SUPERVISED LEARNING** The simplest way of using the contexts of agentive protoOHs is by using supervised learning. This means that on our unlabeled training corpus we consider each NP with the head being an agentive protoOH as a positive data instance and all the remaining NPs occurring in those sentences as negative instances. With this definition we train a supervised classifier based on convolution kernels (Collins and Duffy, 2001 ) as this method has been shown to be quite effective for OH extraction<cite> (Wiegand and Klakow, 2010)</cite> . Convolution kernels derive features automatically from complex discrete structures, such as syntactic parse trees or part-of-speech sequences, that are directly provided to the learner. Thus a classifier can be built without the taking the burden of implementing an explicit feature extraction. We chose the best performing set of tree kernels (Collins and Duffy, 2001; Moschitti, 2006) from that work. It comprises two tree kernels based on constituency parse trees and a tree kernel based on semantic role trees.",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_3",
  "x": "Moreover, the features in the vector kernel heavily rely on taskspecific resources, e.g. a sentiment lexicon, which are deliberately avoided in our low-resource classifier as our method should be applicable to any language (and for many languages sentiment resources are either sparse or do not exist at all). In addition to <cite>Wiegand and Klakow (2010)</cite> , we have to discard the content of candidate NPs (e.g. the candidate opinion holder NP [N P Cand [N N S advocates] ] is reduced to [N P Cand ]), the reason for this being that in our automatically generated training set, OHs will always be protoOHs. Retaining them in the training data would cause the learner to develop a detrimental bias towards these nouns (our resulting classifier should detect any OH and not only protoOHs). ---------------------------------- **RULE-BASED CLASSIFIER** Instead of training a supervised classifier, we can also construct a rule-based classifier on the basis of the agentive protoOHs. The classifier is built on the insight that the most predictive cues for OH extraction are predicates (Wiegand and 2010). We, therefore, mine the contexts of agentive protoOHs (left half of Table 2 ) for discriminant predicates (i.e. verbs, nouns, and adjectives). That is, we rank every predicate according to its correlation, i.e. we use Pointwise Mutual Information, of having agentive protoOHs as an argument.",
  "y": "extends"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_4",
  "x": "3 One simply adds the entire set of synonyms of each of the predicates. ---------------------------------- **INCORPORATION INTO SUPERVISED CLASSIFIERS WITH ACTUALLY LABELED DATA** We also want to investigate the effectiveness of the knowledge from our rule-based classifier that has been learned on the unlabeled corpus ( \u00a74.2) in supervised learning using actually labeled training data from our target corpus, i.e. the MPQA corpus. In particular, we will examine in how far this knowledge (when used as a feature in supervised learning) can compensate the lack of a sufficiently large labeled training set. For that experiment the labeled corpus, i.e. MPQA corpus, will be split into a training set and a test set. Again, we use the supervised learner based on tree kernels ( \u00a74.1). We also augment the tree kernels themselves with additional information by 3 wordnet.princeton.edu following <cite>Wiegand and Klakow (2010)</cite> who add for each word that belongs to a predictive semantic class another node that directly dominates the pertaining leaf node and assign it a label denoting that class. While <cite>Wiegand and Klakow (2010)</cite> made use of manually built lexicons, we use our predictive predicates extracted from contexts of protoOHs.",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_5",
  "x": "We also want to investigate the effectiveness of the knowledge from our rule-based classifier that has been learned on the unlabeled corpus ( \u00a74.2) in supervised learning using actually labeled training data from our target corpus, i.e. the MPQA corpus. In particular, we will examine in how far this knowledge (when used as a feature in supervised learning) can compensate the lack of a sufficiently large labeled training set. For that experiment the labeled corpus, i.e. MPQA corpus, will be split into a training set and a test set. Again, we use the supervised learner based on tree kernels ( \u00a74.1). We also augment the tree kernels themselves with additional information by 3 wordnet.princeton.edu following <cite>Wiegand and Klakow (2010)</cite> who add for each word that belongs to a predictive semantic class another node that directly dominates the pertaining leaf node and assign it a label denoting that class. While <cite>Wiegand and Klakow (2010)</cite> made use of manually built lexicons, we use our predictive predicates extracted from contexts of protoOHs. For instance, if doubt is such a predicate, we would replace the subtree . Moreover, we devise a simple vector kernel incorporating the prediction of the rule-based classifier.",
  "y": "differences"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_6",
  "x": "For verbs, F-Score reaches its maximum at approximately 250 which is the value we chose in our subsequent experiments. In a similar fashion, we determined 100 for both nouns and adjectives. Table 4 lists the most highly ranked verbs that are extracted. 6 As an indication of the intrinsic quality of the extracted words, we mark the words which can also be found in task-specific resources, i.e. communication verbs from the Appraisal Lexicon (AL) (Bloom et al., 2007) and opinion words from the Subjectivity Lexicon (SL) (Wilson et al., 2005) . Both resources have been found predictive for OH extraction (Bloom et al., 2007;<cite> Wiegand and Klakow, 2010)</cite> . Table 3 (lower part) shows the performance of the rule-based classifiers based on protoOHs using different parts of speech. As hard baselines, the table also shows other rule-based classifiers using the same dependency relations as our rulebased classifier (see Table 2 ) but employing different predicates. As lexical resources for these predicates, we again use AL and SL. The table also compares two different versions of the rule-based classifier being the classifier as presented in \u00a74.2 (left half of Table 3 ) and a classifier additionally incorporating the two heuristics (right half):",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_7",
  "x": "Table 3 (lower part) shows the performance of the rule-based classifiers based on protoOHs using different parts of speech. As hard baselines, the table also shows other rule-based classifiers using the same dependency relations as our rulebased classifier (see Table 2 ) but employing different predicates. As lexical resources for these predicates, we again use AL and SL. The table also compares two different versions of the rule-based classifier being the classifier as presented in \u00a74.2 (left half of Table 3 ) and a classifier additionally incorporating the two heuristics (right half): \u2022 If the candidate NP follows according to, then it is labeled as an OH. \u2022 The candidate NP can only be an OH if it represents a person or a group of persons. These are commonly accepted heuristics which have already been used in previous work as features (Choi et al., 2005;<cite> Wiegand and Klakow, 2010)</cite> . The latter rule requires the output of a named-entity recognizer 7 for checking proper nouns and WordNet for common nouns. As far as the classifier built with the help of protoOHs is concerned, adding highly ranked adjectives and nouns consistently improves the performance (mostly recall) when added to the set of 7 We use the Stanford tagger: nlp.stanford.edu/software/CRF-NER.shtml Table 4 : List of verbs most highly correlating with protoOHs; \u2020 : included in AL; * : included in SL.",
  "y": "background"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_8",
  "x": "Table 3 (lower part) shows the performance of the rule-based classifiers based on protoOHs using different parts of speech. As hard baselines, the table also shows other rule-based classifiers using the same dependency relations as our rulebased classifier (see Table 2 ) but employing different predicates. As lexical resources for these predicates, we again use AL and SL. The table also compares two different versions of the rule-based classifier being the classifier as presented in \u00a74.2 (left half of Table 3 ) and a classifier additionally incorporating the two heuristics (right half): \u2022 If the candidate NP follows according to, then it is labeled as an OH. \u2022 The candidate NP can only be an OH if it represents a person or a group of persons. These are commonly accepted heuristics which have already been used in previous work as features (Choi et al., 2005;<cite> Wiegand and Klakow, 2010)</cite> . The latter rule requires the output of a named-entity recognizer 7 for checking proper nouns and WordNet for common nouns. As far as the classifier built with the help of protoOHs is concerned, adding highly ranked adjectives and nouns consistently improves the performance (mostly recall) when added to the set of 7 We use the Stanford tagger: nlp.stanford.edu/software/CRF-NER.shtml Table 4 : List of verbs most highly correlating with protoOHs; \u2020 : included in AL; * : included in SL.",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_9",
  "x": "**INCORPORATING KNOWLEDGE FROM PROTOOHS INTO SUPERVISED LEARNING** As a maximum amount of labeled training data we chose 60000 instances (i.e. NPs) which is even a bit more than used in<cite> (Wiegand and Klakow, 2010)</cite> . In addition, we also test 1%, 5%, 10%, 25% and 50% of the training set. From the remaining data instances, we use 25000 instances as test data. In order to deliver generalizing results, we randomly sample the training and test partitions five times and report the averaged results. We compare four different classifiers, a plain classifier using only the convolution kernel configuration from previous experiments (TKPlain), the augmented convolution kernels (TKAug) where additional nodes are added indicating the presence of an OH predicate ( \u00a74.3), the augmented convolution kernels with the vector kernel encoding the prediction of the best rule-based classifier (induced by protoOHs) without heuristics (TKAug+VK) and the classifier incorporating those heuristics (TKAug+VK[heur] ). Instead of just using one feature encoding the overall prediction we use several binary features representing the occurrence of the individual groups of predicates (i.e. verbs, nouns, or adjectives) and prediction types (direct predicate or predicate from cluster extension). We also include the prediction of the self-trained classifier. The performance of these different classifiers is listed in Table 6 .",
  "y": "differences"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_10",
  "x": "Instead of just using one feature encoding the overall prediction we use several binary features representing the occurrence of the individual groups of predicates (i.e. verbs, nouns, or adjectives) and prediction types (direct predicate or predicate from cluster extension). We also include the prediction of the self-trained classifier. The performance of these different classifiers is listed in Table 6 . Recall from \u00a74.1 that we want to examine cases in which no task-specific resources and no or few labeled training data are available. This is why the different classifiers presented should primarily be compared to our own baseline (TKPlain) and not the numbers presented in previous work as they always use the maximal size of labeled training data and additionally task-specific resources (e.g. sentiment lexicons). The results show that using the information extracted from the unlabeled data can be usefully combined with the labeled training data. Tree augmentation causes both precision and recall to rise. This observation is consistent with<cite> (Wiegand and Klakow, 2010)</cite> where, however, AL and SL are considered for augmentation. When the vector kernel with the prediction of the rule-based classifier is also included, precision drops slightly but recall is notably boosted resulting in an even more increased F-Score.",
  "y": "similarities"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_0",
  "x": "These are especially useful when dealing with ambiguous kNCs. The need for annotated data is a drawback of supervised approaches. Manual annotations are costly and time-consuming. To circumvent this need for annotated data, previous work has used cross-lingual supervision based on parallel corpora. Bergsma et al. (2011) made use of small amounts of annotated data on the target side and complement this with bilingual features from unlabeled bitext in a co-trained classifier for coordination disambiguation in complex NPs. Previous work on using cross-lingual data for the analysis of multi-word expressions (MWEs) of different types include Busa and Johnston (1996) ; Girju (2007) ; Sinha (2009); Tsvetkov and Wintner (2010) ; Ziering et al. (2013) . <cite>Ziering and Van der Plas (2014)</cite> propose an approach that refrains from using any human annotation. They use the fact, that languages differ in their preference for open or closed compounding (i.e., multiword vs. one-word compounds), for inducing the English bracketing of 3NCs. English open 3NCs like human rights abuses can be translated to partially closed phrases as in German Verletzungen der Menschenrechte, (abuses of human rights), from which we can induce the LEFT-branching structure.",
  "y": "background"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_1",
  "x": "To circumvent this need for annotated data, previous work has used cross-lingual supervision based on parallel corpora. Bergsma et al. (2011) made use of small amounts of annotated data on the target side and complement this with bilingual features from unlabeled bitext in a co-trained classifier for coordination disambiguation in complex NPs. Previous work on using cross-lingual data for the analysis of multi-word expressions (MWEs) of different types include Busa and Johnston (1996) ; Girju (2007) ; Sinha (2009); Tsvetkov and Wintner (2010) ; Ziering et al. (2013) . <cite>Ziering and Van der Plas (2014)</cite> propose an approach that refrains from using any human annotation. They use the fact, that languages differ in their preference for open or closed compounding (i.e., multiword vs. one-word compounds), for inducing the English bracketing of 3NCs. English open 3NCs like human rights abuses can be translated to partially closed phrases as in German Verletzungen der Menschenrechte, (abuses of human rights), from which we can induce the LEFT-branching structure. Although this approach achieves a solid accuracy, a crucial limitation is coverage, because restricting to six paraphrasing patterns ignores many other predictive cases. Moreover, the system needs part of speech (PoS) tags and splitting information for determining 2NCs and is therefore rather language-dependent. In this paper, we present a precise, high-coverage and knowledge-lean method for bracketing kNCs (for k \u2265 3) occurring in parallel data.",
  "y": "motivation"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_2",
  "x": "che le violazioni gravi e sistematiche dei diritti umani . . . . The fact, that the alignment of the third noun, violations (violazioni), is separated from the rest, points us in the direction of LEFT-branching. Using less restricted forms of cross-lingual supervision, we achieve a much higher coverage than <cite>Ziering and Van der Plas (2014)</cite> . Furthermore, our results are more accurate. In contrast to previous unsupervised methods, our system is applicable in both token-and type-based modes. Token-based bracketing is context-dependent and allows for a better treatment of structural ambiguity (as in luxury cattle truck). We generate large amounts of high-quality bracketed kNCs in a multilingual context that can be used to train supervised learners. ---------------------------------- **ALIGNED WORD DISTANCE BRACKETING**",
  "y": "differences"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_3",
  "x": "While AWDB is designed for bracketing NPs of any length, we first experiment with bracketing 3NCs, the largest class of 3 + NCs (93.8% on the basic dataset of <cite>Ziering and Van der Plas (2014)</cite>), for which bracketing is a binary classification (i.e., LEFT or RIGHT). For bracketing longer NCs we often have to make do with partial information from a language, instead of a full structure. In future work, we plan to investigate methods to combine these partial results. Moreover, in contrast to previous work (e.g., Vadas and Curran (2007b) ), we take only common nouns as components into account rather than named entities. We consider the task of bracketing 3NCs composed of common nouns more ambitious, because named entities often form a single concept that is easy to spot, e.g., Apple II owners. Although AWDB can also process compounds including adjectives (e.g., active inclusion policy aligned to the Dutch beleid voor actieve insluiting (policy for active inclusion)), for a direct comparison with the system of <cite>Ziering and Van der Plas (2014)</cite> , that analyses 3NCs, we restrict ourselves to noun sequences. We use the Europarl 2 compound database 3 developed by <cite>Ziering and Van der Plas (2014)</cite> . This database has been compiled from the OPUS 4 corpus (Tiedemann, 2012) and comprises ten languages: Danish, Dutch, English, French, German, Greek, Italian, Portuguese, Spanish and Swedish. We use the initial version (basic dataset), that contains English word sequences that conform PoS chunks and their alignments.",
  "y": "background"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_4",
  "x": "For bracketing longer NCs we often have to make do with partial information from a language, instead of a full structure. In future work, we plan to investigate methods to combine these partial results. Moreover, in contrast to previous work (e.g., Vadas and Curran (2007b) ), we take only common nouns as components into account rather than named entities. We consider the task of bracketing 3NCs composed of common nouns more ambitious, because named entities often form a single concept that is easy to spot, e.g., Apple II owners. Although AWDB can also process compounds including adjectives (e.g., active inclusion policy aligned to the Dutch beleid voor actieve insluiting (policy for active inclusion)), for a direct comparison with the system of <cite>Ziering and Van der Plas (2014)</cite> , that analyses 3NCs, we restrict ourselves to noun sequences. We use the Europarl 2 compound database 3 developed by <cite>Ziering and Van der Plas (2014)</cite> . This database has been compiled from the OPUS 4 corpus (Tiedemann, 2012) and comprises ten languages: Danish, Dutch, English, French, German, Greek, Italian, Portuguese, Spanish and Swedish. We use the initial version (basic dataset), that contains English word sequences that conform PoS chunks and their alignments. We select English word sequences whose PoS pattern conforms three nouns.",
  "y": "uses"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_5",
  "x": "Tools and data. While AWDB is designed for bracketing NPs of any length, we first experiment with bracketing 3NCs, the largest class of 3 + NCs (93.8% on the basic dataset of <cite>Ziering and Van der Plas (2014)</cite>), for which bracketing is a binary classification (i.e., LEFT or RIGHT). For bracketing longer NCs we often have to make do with partial information from a language, instead of a full structure. In future work, we plan to investigate methods to combine these partial results. Moreover, in contrast to previous work (e.g., Vadas and Curran (2007b) ), we take only common nouns as components into account rather than named entities. We consider the task of bracketing 3NCs composed of common nouns more ambitious, because named entities often form a single concept that is easy to spot, e.g., Apple II owners. Although AWDB can also process compounds including adjectives (e.g., active inclusion policy aligned to the Dutch beleid voor actieve insluiting (policy for active inclusion)), for a direct comparison with the system of <cite>Ziering and Van der Plas (2014)</cite> , that analyses 3NCs, we restrict ourselves to noun sequences. We use the Europarl 2 compound database 3 developed by <cite>Ziering and Van der Plas (2014)</cite> . This database has been compiled from the OPUS 4 corpus (Tiedemann, 2012) and comprises ten languages: Danish, Dutch, English, French, German, Greek, Italian, Portuguese, Spanish and Swedish.",
  "y": "uses"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_6",
  "x": "Systems in comparison. We compare AWDB with the bracketing approach of <cite>Ziering and Van der Plas (2014)</cite>. For both systems, we use the majority vote across all nine aligned languages, in a token-and type-based version. We implemented an unsupervised method based on statistics on bi-grams extracted from the English part of the Europarl corpus. 6 As scorer, we use the Chi squared (\u03c7 2 ) measure, which worked best in previous work (Nakov and Hearst, 2005) . We consider both the adjacency (i.e., (N 1 , N 2 ) vs. (N 2 , N 3 ) , (Marcus, 1980) ) and the dependency (i.e., (N 1 , N 2 ) vs. (N 1 , N 3 ) , (Lauer, 1995) ) model. We created a back-off model for the bracketing system of <cite>Ziering and Van der Plas (2014)</cite> and for AWDB that falls back to using \u03c7 2 if no bracketing structure can be derived (system \u2192 \u03c7 2 ). Finally, we compare with a baseline, that always predicts the majority class: LEFT. Human annotation.",
  "y": "uses"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_7",
  "x": "Systems in comparison. We compare AWDB with the bracketing approach of <cite>Ziering and Van der Plas (2014)</cite>. For both systems, we use the majority vote across all nine aligned languages, in a token-and type-based version. We implemented an unsupervised method based on statistics on bi-grams extracted from the English part of the Europarl corpus. 6 As scorer, we use the Chi squared (\u03c7 2 ) measure, which worked best in previous work (Nakov and Hearst, 2005) . We consider both the adjacency (i.e., (N 1 , N 2 ) vs. (N 2 , N 3 ) , (Marcus, 1980) ) and the dependency (i.e., (N 1 , N 2 ) vs. (N 1 , N 3 ) , (Lauer, 1995) ) model. We created a back-off model for the bracketing system of <cite>Ziering and Van der Plas (2014)</cite> and for AWDB that falls back to using \u03c7 2 if no bracketing structure can be derived (system \u2192 \u03c7 2 ). Finally, we compare with a baseline, that always predicts the majority class: LEFT. Human annotation.",
  "y": "uses"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_8",
  "x": "The test set used by <cite>Ziering and Van der Plas (2014)</cite> is very small and the labeling is less fine-grained. Thus, we decided to create our own test set. 2 statmt.org/europarl 3 ims.uni-stuttgart.de/data/NCDatabase.html 4 opus.lingfil.uu.se 5 en.wikipedia.org 6 For a fair comparison, we leave systems that have access to external knowledge, such as web search engines, aside. A trained independent annotator classified a set of 1100 tokens in context with one of the following labels: LEFT, RIGHT, EXTRACTION (for extraction errors that survived the high-confidence noun filter P noun (word)), UNDECIDED (for 3NCs that cannot be disambiguated within the one-sentence context) and SEMANTIC INDETERMINATE (for 3NCs for which LEFT and RIGHT have the same meaning such as book price fixing (i.e., price fixing for books is equivalent to fixing of the book price)). We consider the full dataset to compare the coverage of the systems in comparison. For the accuracy figures, in order to keep annotation efforts small, we asked evaluators to annotate just those tokens that our system provides an answer to, because tokens that our system has no answer to will not be evaluated in the comparative evaluation on accuracy anyhow. Two additional trained independent annotators each classified one half of the dataset for checking inter-annotator agreement. For the classes LEFT/RIGHT (308 tokens), we achieve an agreement rate of 90.3% and \u03ba = 0.717 (Cohen, 1960) , which means good agreement (Landis and Koch, 1977) . We use the LEFT/RIGHT consensus of the 3NC tokens as final test set (278 tokens).",
  "y": "motivation"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_9",
  "x": "Coverage is measured as LEFT/RIGHT labels divided by all 3NC tokens in the full dataset. We refer to the harmonic mean of Acc \u2126 and Coverage as harmonic(\u2126). As it turned out that the adjacency model outperforms the dependency model, we only report results for the first. Table 1 presents the coverage of each system, based on the full dataset. Our first result is that type-based cross-lingual bracketing outperforms token-based and achieves up to 91.2% in coverage. As expected, the system of <cite>Ziering and Van der Plas (2014)</cite> does not cover more than 48.1%. The \u03c7 2 method and the back-off models cover all 3NCs in our dataset. The fact that AWDB type misses 8.8% of the dataset is mainly due to equal distances between aligned words (e.g., crisis resolution mechanism is only aligned to closed compounds, such as the Swedish krisl\u00f6sningsmekanism or to nouns separated by one preposition, such as the Spanish mecanismo de resoluci\u00f3n de crisis). In future work, we will add more languages in the hope to find more variation and thus get an even higher coverage.",
  "y": "differences"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_10",
  "x": "AWDB outperforms <cite>Ziering and Van der Plas (2014)</cite> significantly 7 . This can be explained with the flexible structure of AWDB, which can exploit more data and is thus more robust to word alignment errors. AWDB significantly outperforms \u03c7 2 in accuracy but is inferior in harmonic(com). The last four lines of Table 2 show all systems with full coverage. AWDB's back-off model achieves the best harmonic(com) with 96.6% and an accuracy comparable to human performance. For AWDB, types and tokens show the same accuracy. The harmonic mean numbers for the system of <cite>Ziering and Van der Plas (2014)</cite> illustrate that coverage gain of types outweighs a higher accuracy of tokens. Our intuition that token-based approaches are superior in accuracy is hardly reflected in the present results. We believe that this is due to the domain-specificity of Europarl.",
  "y": "differences"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_11",
  "x": "The main reason why cross-lingual systems make bracketing errors is the quality of automatic word alignment. AWDB outperforms <cite>Ziering and Van der Plas (2014)</cite> significantly 7 . This can be explained with the flexible structure of AWDB, which can exploit more data and is thus more robust to word alignment errors. AWDB significantly outperforms \u03c7 2 in accuracy but is inferior in harmonic(com). The last four lines of Table 2 show all systems with full coverage. AWDB's back-off model achieves the best harmonic(com) with 96.6% and an accuracy comparable to human performance. For AWDB, types and tokens show the same accuracy. The harmonic mean numbers for the system of <cite>Ziering and Van der Plas (2014)</cite> illustrate that coverage gain of types outweighs a higher accuracy of tokens. Our intuition that token-based approaches are superior in accuracy is hardly reflected in the present results.",
  "y": "background"
 },
 {
  "id": "ff73758fbef3ddc779a772e634b74e_0",
  "x": "In previous work, explanations have been categorised as either explaining 1) machine learning as in [11] who showed that they can increase trust; 2) explaining plans [2, 13] ; 3) verbalising robot [12] or agent rationalisation <cite>[3]</cite> . However, humans do not present a constant verbalisation of their actions but they do need to be able to provide information on-demand about what they are doing and why during a live mission. We present here, MIRIAM, (Multimodal Intelligent inteRactIon for Autonomous systeMs), as seen in Figure 1 . MIRIAM allows for these 'on-demand' queries for status and explanations of behaviour. MIRIAM interfaces with the Neptune autonomy software provided by SeeByte Ltd and runs alongside their SeeTrack interface. In this paper, we focus on explanations of behaviours and describe a method that is agnostic to the type of autonomy method. With respect to providing communication for monitoring, please refer to [5] for further details and an overview of the system. ---------------------------------- **EXPLANATIONS FOR REMOTE AUTONOMY**",
  "y": "background"
 },
 {
  "id": "ff73758fbef3ddc779a772e634b74e_1",
  "x": "**EXPLANATIONS FOR REMOTE AUTONOMY** Types of explanations include why to provide a trace or reasoning and why not to elaborate on the system's control method or strategy [4] . Lim et al. (2009) [10] show that both why and why not explanations increase understanding but only why increases trust. We adopt here the 'speak-aloud' method whereby an expert provides rationalisation of the AxV behaviours while watching videos of missions on the SeeTrack software. This has the advantage of being agnostic to the method of autonomy and could be used to describe rule-based autonomous behaviours but also complex deep models. Similar human-provided rationalisation has been used to generate explanations of deep neural models for game play <cite>[3]</cite> . An interpretable model of autonomy was then derived from the expert, as partially shown in Figure 2 . If a why request is made, the decision tree is checked against the current mission status and history and the possible reasons are determined, along with a probability. As we can see from example outputs in Figure 3A , there may be multiple reasons with varying levels of certainty depending on the information available at a given point in the mission.",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_0",
  "x": "****PARSING AS LANGUAGE MODELING**** **ABSTRACT** We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing -93.8 F 1 on section 23, using 2-21 as training, 24 as development, plus tri-training. When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%. ---------------------------------- **INTRODUCTION** Recent work on deep learning syntactic parsing models has achieved notably good results, e.g.,<cite> Dyer et al. (2016)</cite> with 92.4 F 1 on Penn Treebank constituency parsing and Vinyals et al. (2015) with 92.8 F 1 . In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F 1 , with a comparatively simple architecture. In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem.",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_1",
  "x": "**PREVIOUS WORK** We look here at three neural net (NN) models closest to our research along various dimensions. The first (Zaremba et al., 2014) gives the basic language modeling architecture that we have adopted, while the other two (Vinyals et al., 2015; <cite>Dyer et al., 2016)</cite> are parsing models that have the current best results in NN parsing. ---------------------------------- **LSTM-LM** The LSTM-LM of Zaremba et al. (2014) turns (x 1 , \u00b7 \u00b7 \u00b7 , x t\u22121 ) into h t , a hidden state of an LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2003; Graves, 2013) , and uses h t to guess x t : where W is a parameter matrix and [i] indexes ith element of a vector. The simplicity of the model makes it easily extendable and scalable, which has inspired a character-based LSTM-LM that works well for many languages (Kim et al., 2016) and an ensemble of large LSTM-LMs for English with astonishing perplexity of 23.7 (Jozefowicz et al., 2016) . In this paper, we build a parsing model based on the LSTM-LM of Zaremba et al. (2014) .",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_2",
  "x": "Finally the decoder predicts y t given h t . Inspired by MTP, our model processes sequential trees. ---------------------------------- **RNNG** Recurrent Neural Network Grammars (RNNG), a generative parsing model, defines a joint distribution over a tree in terms of actions the model takes to generate the tree<cite> (Dyer et al., 2016)</cite> : where a is a sequence of actions whose output precisely matches the sequence of symbols in z, which implies Equation (3) is the same as Equation (2). RNNG and our model differ in how they compute the conditioning event (z 1 , \u00b7 \u00b7 \u00b7 , z t\u22121 ): RNNG combines hidden states of three LSTMs that keep track of actions the model has taken, an incomplete tree the model has generated and words the model has generated whereas our model uses one LSTM's hidden state as shown in the next section. ---------------------------------- **MODEL**",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_3",
  "x": "To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (Parker et al., 2011 ) with a product of eight Berkeley parsers (Petrov, 2010) 2 and ZPar (Zhu et al., 2013) and select 24 million trees on which both parsers agree (Li et al., 2014) . We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et 1 The code and trained models used for experiments are available at github.com/cdg720/emnlp2016. 2 We use the reimplementation by Huang et al. (2010) . (Charniak, 2000) performed better when trained on all of 24 million trees than when trained on resampled two million trees. Given x, we produce Y (x), 50-best trees, with Charniak parser and find y with LSTM-LM as<cite> Dyer et al. (2016)</cite> do with their discriminative and generative models. 3 ---------------------------------- **TRAINING AND DEVELOPMENT** ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_5",
  "x": "We train LSTM-LM (GS) on the WSJ and a different set of 400,000 NYT trees for each epoch except for the last one during which we use the WSJ only. Training takes 26 epochs and 68 hours on a Titan X. LSTM-LM (GS) achieves 92.5 F 1 on the development. ---------------------------------- **RESULTS** ---------------------------------- **SUPERVISION** As shown in Table 2 , with 92.6 F 1 LSTM-LM (G) outperforms an ensemble of five MTPs (Vinyals et al., 2015) and RNNG<cite> (Dyer et al., 2016)</cite> , both of which are trained on the WSJ only. ---------------------------------- **SEMI-SUPERVISION**",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_6",
  "x": "The generative parsing model we presented in this paper is very powerful. In fact, we see that a generative parsing model, LSTM-LM, is more effective than discriminative parsing models<cite> (Dyer et al., 2016)</cite> . We suspect building large models with character embeddings would lead to further improvement as in language modeling (Kim et al., 2016; Jozefowicz et al., 2016) . We also wish to develop a complete parsing model using the LSTM-LM framework. Table 3 : Evaluation of models trained on the WSJ and additional resources. Note that the numbers of Vinyals et al. (2015) and Luong et al. (2016) are not directly comparable as their models are evaluated on OntoNotesstyle trees instead of PTB-style trees. E(LSTM-LMs (GS)) is an ensemble of eight LSTM-LMs (GS). X/Y in Silver column indicates the number of silver trees used to train Charniak parser and LSTM-LM. For the ensemble model, we report the maximum number of trees used to train one of LSTM-LMs (GS).",
  "y": "differences"
 }
]